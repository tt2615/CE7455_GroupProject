Abstract:This work proposes the creation of a model called Cassiopeia, whose aim is to allow for knowledge discovery in textual bases in distinct and/or antagonic domains by using a process of summarization and clusterization to obtain these pieces of knowledge. By proposing the Cassiopeia model, we hope to obtain a better cohesion of the clusters and to make feasible the issue of high dimensionality in knowledge discovery in textual bases.


Abstract:This paper introduces the implementation and integration of a sentiment analysis pipeline into the ongoing open source cross-media analysis framework. The pipeline includes the following components; chat room cleaner, NLP and sentiment analyzer. Before the integration, we also compare two broad categories of sentiment analysis methods, namely lexicon-based and machine learning approaches. We mainly focus on finding out which method is appropriate to detect sentiments from forum discussion posts. In order to conduct our experiments, we use the apache-hadoop framework with its lexicon-based sentiment prediction algorithm and Stanford coreNLP library with the Recursive Neural Tensor Network (RNTN) model. The lexicon-based uses sentiment dictionary containing words annotated with sentiment labels and other basic lexical features, and the later one is trained on Sentiment Treebank with 215,154 phrases, labeled using Amazon Turk. Our overall performance evaluation shows that RNTN outperforms the lexicon-based by 9.88% accuracy on variable length positive, negative, and neutral comments. However, the lexicon-based shows better performance on classifying positive comments. We also found out that the F1-score values of the Lexicon-based is greater by 0.16 from the RNTN.


Towards an Understanding of Arab Women Researchers Contribution in Arabic NLP
Arabic language is spoken by around 400 million people living in the Middle East, North Africa, and the Horn of Africa. Like other languages, literary Arabic continues to evolve; this suggests a large potential audience for Arabic Natural language processing (NLP) and the need for a fair contribution from both men and women Arabic NLP researchers. Our paper will explore Arab women researchers’ contributions in the field of Arabic Natural Language processing research (ANLP) by answering 9 research questions. We answer these questions through examining research metadata from Digital Bibliography & Library Project (DBLP) and Google Scholar. We found that the proportion of Arab women researchers in ANLP compared to men is similar to the international level. We also observed their publication, collaboration and citation patterns. Our results point to the need for gender equity initiatives to increase the number of Arab female researchers in ANLP.

Automatic assessment of spoken modern standard Arabic
Proficiency testing is an important ingredient in successful language teaching. However, repeated testing for course placement, over the course of instruction or for certification can be time-consuming and costly. We present the design and validation of the Versant Arabic Test, a fully automated test of spoken Modern Standard Arabic, that evaluates test-takers' facility in listening and speaking. Experimental data shows the test to be highly reliable (test-retest r=0.97) and to strongly predict performance on the ILR OPI (r=0.87), a standard interview test that assesses oral proficiency.

A 50-year Retrospective on Academic Integrity and Computer Ethics in CS Education
In this project, we explore how academic integrity and teaching computer ethics have evolved within the SIGCSE community over the past 50 years. We apply Natural Language Processing (NLP) techniques to identify the topics in the records in 50 years of SIGCSE Symposia. We extract records that include these two themes and apply a temporal model identifying periods with an increase in the number of records that include these themes and a significant increase in 2017-2018.


A New Approach to Improve CD Based on the Context


Bi-objective optimization of multi-product EPQ model with backorders, rework process and random defective rate

Shared-task evaluations in HLT: lessons for NLG
While natural language generation (NLG) has a strong evaluation tradition, in particular in userbased and task-oriented evaluation, it has never evaluated different approaches and techniques by comparing their performance on the same tasks (shared-task evaluation, STE). NLG is characterised by a lack of consolidation of results, and by isolation from the rest of NLP where STE is now standard. It is, moreover, a shrinking field (state-of-the-art MT and summarisation no longer perform generation as a subtask) which lacks the kind of funding and participation that natural language understanding (NLU) has attracted.

An Improved LSTM Structure for Natural Language Processing
Abstract:Natural language processing technology is widely used in artificial intelligence fields such as machine translation, human-computer interaction and speech recognition. Natural language processing is a daunting task due to the variability, ambiguity and context-dependent interpretation of human language. The current deep learning technology has made great progress in NLP technology. However, many NLP systems still have practical problems, such as high training complexity, computational difficulties in large-scale content scenarios, high retrieval complexity and lack of probabilistic significance. This paper proposes an improved NLP method based on long short-term memory (LSTM) structure, whose parameters are randomly discarded when they are passed backwards in the recursive projection layer. Compared with baseline and other LSTM, the improved method has better F1 score results on the Wall Street Journal dataset, including the word2vec word vector and the one-hot word vector, which indicates that our method is more suitable for NLP in limited computing resources and high amount of data.

Analysis about event annotation and information structure in sudden events discourse
Abstract:This paper attempts to analyze the setup about the sudden event discourses. All the analysis is based on a large scale of events annotation. Paper gives an overview of events annotations. The event words, event arguments and event attributes are all necessary to annotate. It also does some analysis on the information structure of the texts and defines two information chains from the perspective of researchers. The article also performs a statistical analysis between the content which can be easily identified by the computer and which can not Anyway, all the research are just to lay the foundation for Information Extraction.

Session details: NLP related machine learning
No abstract available.


Non-factoid Question Answering in the Legal Domain

Abstract:This paper presents a model for predicting the behavior of customers in an intelligent manner in the form of customer credit risk for use in decision making for executives in business organizations by applying management's decision to credit corporate customers by machine learning techniques. Machine learning techniques, which are important technique that are the heart of artificial intelligence (Ai). A decision Tree model that is analyzed for credit business customers in business organizations which consists of six attributes, the number customers of 1,100 records. The risk is classified into three classes, namely, low, medium and high. Our model analysis can be done with accuracy of 85.82% of the customer genius in the form of risk to credit customers of the trading organization that has been developed to support the decision making of executives more quickly and more effectively.



Improving accuracy in word class tagging through the combination of machine learning systems

Fast Confidence Prediction of Uncertainty based on Knowledge Graph Embedding
The uncertainty is an inherent feature of Knowledge Graph (KG), which is often modelled as confidence scores of relation facts. Although Knowledge Graph Embedding (KGE) has been a great success recently, it is still a big challenge to predict confidence of unseen facts in KG in the continuous vector space. There are several reasons for this situation. First, the current KGE is often concerned with the deterministic knowledge, in which unseen facts’ confidence are treated as zero, otherwise as one. Second, in the embedding space, uncertainty features are not well preserved. Third, approximate reasoning in embedding spaces is often unexplainable and not intuitive. Furthermore, the time and space cost of obtaining embedding spaces with uncertainty preserved are always very high. To address these issues, considering Uncertain Knowledge Graph (UKG), we propose a fast and effective embedding method, UKGsE, in which approximate reasoning and calculation can be quickly performed after generating an Uncertain Knowledge Graph Embedding (UKGE) space in a high speed and reasonable accuracy. The idea is that treating relation facts as short sentences and pre-handling are benefit to the learning and training confidence scores of them. The experiment shows that the method is suitable for the downstream task, confidence prediction of relation facts, whether they are seen in UKG or not. It achieves the best tradeoff between efficiency and accuracy of predicting uncertain confidence of knowledge. Further, we found that the model outperforms state-of-the-art uncertain link prediction baselines on CN15k dataset.

Using Different RNN Variants to Generate Realist Reviews
This project aims to implement LSTM and GRU to generate realistic text. First, some basic background information will be introduced. After that, the design of models will be presented, then the performance of different RNN variants will be compared. In addition, the time consumption also as a factor of performance to compare. At last, the reason for the difference in performance will be discussed.

Discovering IMRaD Structure with Different Classifiers
Abstract:Information within published papers around the world in scientific journals are structured in the format of Introduction, Methodology, Results, and Conclusion (IMRaD). Human ability to read and analyze is not capable of processing these large amounts of information. If we could identify the structure and consequently extract it to a user who needs a part of the structure, particularly an article in a foreign language, time will be saved as result. Computational approaches like Machine Learning (ML) and Natural Language Processing (NLP) have been widely used for similar purposes. However, it is very important to identify which one, or which group of classifiers work better for a specific kind of problem. The objective of this work is to identify applicable classifiers by analyzing and comparing results produced by different ML classifiers used in locating and classifying sentences from abstract of a paper into the IMRaD structure. This work demonstrates the possibility of integrating ML and NLP for the articles' sentence classification based on the IMRaD structure. It also verifies that it is possible to achieve good results with simple implementations without the need of too many computational resources.


Sentiment analysis on covid-19 vaccine using Twitter data: A NLP approach

NU:BRIEF – A Privacy-aware Newsletter Personalization Engine for Publishers
Newsletters have (re-) emerged as a powerful tool for publishers to engage with their readers directly and more effectively. Despite the diversity in their audiences, publishers’ newsletters remain largely a one-size-fits-all offering, which is suboptimal. In this paper, we present NU:BRIEF, a web application for publishers that enables them to personalize their newsletters without harvesting personal data. Personalized newsletters build a habit and become a great conversion tool for publishers, providing an alternative readers-generated revenue model to a declining ad/clickbait-centered business model.  Demo: https://demo.nubrief.com/md03PaAJSwXMegL5BbKpQlArK3elb3hDUglcHodx4gE=/ Explainer video: https://www.youtube.com/watch?v=AUZGuyPJYH4

A standoff annotation interface between DELPH-IN components
We present a standoff annotation framework for the integration of NLP components, currently implemented in the context of the DELPH-IN tools. This provides a flexible standoff pointer scheme suitable for various types of data, a lattice encodes structural ambiguity, intra-annotation relationships are encoded, and annotations are decorated with structured content. We provide an XML serialization for intercomponent communication.

Automated knowledge extraction from the federal acquisition regulations system (FARS)
Abstract:With increasing regulation of Big Data, it is becoming essential for organizations to ensure compliance with various data protection standards. The Federal Acquisition Regulations System (FARS) within the Code of Federal Regulations (CFR) includes facts and rules for individuals and organizations seeking to do business with the US Federal government. Parsing and gathering knowledge from such lengthy regulation documents is currently done manually and is time and human intensive. Hence, developing a cognitive assistant for automated analysis of such legal documents has become a necessity. We have developed semantically rich approach to automate the analysis of legal documents and have implemented a system to capture various facts and rules contributing towards building an efficient legal knowledge base that contains details of the relationships between various legal elements, semantically similar terminologies, deontic expressions and cross-referenced legal facts and rules. In this paper, we describe our framework along with the results of automating knowledge extraction from the FARS document (Title 48, CFR). Our approach can be used by Big Data Users to automate knowledge extraction from Large Legal documents.

Abstract:This paper presents a new approach to part-of-speech (POS) tagging in which the basic unit being tagged is a contiguous sequence of words rather than a single word. We run experiments on two different tagsets: the UPENN treebank and a treebank annotated with more ambiguous tags that have a semantic component. We show that the phrase-based system alone is a respectable tagger that exceeds the performance of the ME tagger on the ambiguous tagset. Moreover, when a log-linear model is built using features from both phrase-and word-based techniques, the tagging accuracy improved on both of our data sets yielding the highest reported performance to date on the more ambiguous tagset.


Abstract:Multi-vehicle motion planning (MVMP) is a critical decision-making module in intelligent transportation systems. Compared to the decentralized MVMP methods, centralized MVMP methods are beneficial in being generic and complete, because information of all the vehicles is simultaneously considered. This study formulates the MVMP problems as centralized optimal control problems. These problems are parameterized into nonlinear programming (NLP) problems for the convenience of numerical solution. In solving those NLPs, the main challenges lie in the large scale of collision-avoidance constraints, and the high nonlinearity of vehicle kinematics. The typical NLP solvers are inefficient in directly handling such difficulties. It is widely known that the initialization has a significant influence on the NLP solving behavior. Therefore, homotopy initialization strategies are developed in this work to generate the initial guess. The main idea of homotopy is that simplified subproblems are solved in a sequence such that each subproblem is closer to the original problem; the solution to each subproblem serves as the initial guess to facilitate the solving process of the next subproblem. This process continues until the original problem is solved. The efficiency of the proposed initialization strategies is verified via numerical experimentation and theoretical analysis.



Application-Oriented Comparison and Evaluation of Six Semantic Similarity Measures Based on Wordnet

Abstract:This paper introduces a method of constructing a semantic dictionary automatically from the keywords and classify relations of the web encyclopedia Chinese WikiPedia. Semantic units, which are affixes (core/modifier) shared between many phrased-keywords, are selected using statistic method and string affix matching, also with other units to explain the semantic meanings. Then the result are used to mark the semantic explanations for most WikiPedia keywords by analyzing surface text or upper classes. The feature form ' structure or advantages comparing to other semantic resource are also concerned.


On Building an Automatic Identification of Country-Specific Feature Requests in Mobile App Reviews: Possibilities and Challenges
Mobile app stores are available in over 150 countries, allowing users from all over the world to leave public reviews of downloaded apps. Previous studies have shown that such reviews can serve as sources of requirements and suggested that users from different countries have different needs and expectations regarding the same app. However, the tremendous quantity of reviews from multiple countries, as well as several other factors, complicates identifying country-specific app feature requests. In this work, we present a simple approach to address this through NLP-based analysis and discuss some of the challenges involved in using the NLP-based analysis for this task.


Toward Explainable Users: Using NLP to Enable AI to Understand Users’ Perceptions of Cyber Attacks


HyperEmbed: Tradeoffs Between Resources and Performance in NLP Tasks with Hyperdimensional Computing Enabled Embedding of n-gram Statistics


Automatic Text Summarization using Word Embeddings

Elastic Search in Cache Based Service Management For Healthcare Automation
Abstract:Healthcare Automation is an area, which involves various services, such as the housekeeping & patient management system. There is a incredible quantity of consideration and focusing to improve health. Big healthcare survey organizations illustrate that sickness and death rate, to a great extent rely on accessing of suitable healthcare systems, which is not accessible to an enormous preponderance of the worldwide inhabitants. This research paper introduce automating healthcare functions such as monitoring, accessing of information of patients by doctors using effective grid search technology, interactive user interface assistance for the doctors and junior doctors for any queries & cache management technology based web application for efficient performance. We elucidate our thoughts on how technology can aid in this vital and crucial life sustaining goings-on. We provide insight hooked on our effort on automating medical sector by means of text mining techniques, Natural Language Processing, Elastic Search, PHP-MVC, Web Services and Cache and include some initial results.

Abstract:To learn a hierarchal representation of data, deep learning techniques can be used that use multiple processing layers, and produce state of art results. Many models and methods are designed in deep learning for classification in natural language processing (NLP). Various classification algorithms have been used for Arabic documents classification, but they have two problems High dimensional feature representation and the low accuracy of the classification. In this work, an important experiment is made by using deep related models and methods for classifying Arabic text also compare our model with various models. Also to forward a full understand, present and future of deep learning in Arabic text classification and have obtained encouraging results.


Abstract:The ability of dialogue systems to express pre-specified style during conversations has a direct, positive impact on their usability and user satisfaction. While it has attracted much research interest, existing methods often generate stylistic responses at the cost of content quality. In this work, we introduce a prototype-to-style (PS) framework to tackle the challenge of stylistic dialogue generation. The proposed framework first exploits an Information Retrieval (IR) system and extracts a response prototype from the retrieved response. A stylistic response generator then takes the response prototype and the desired style as input to produce a high-quality and stylistic response. To effectively train the proposed model and imitate the real testing environment, we introduce a new style-aware learning objective and a denoising learning strategy. Results on three benchmark datasets (gender, emotion, and sentiment) from two languages demonstrate that the proposed approach significantly outperforms existing baselines both in terms of in-domain and cross-domain evaluations.



Recent Trends in Deep Learning Based Natural Language Processing [Review Article]

Abstract:This paper proposes a platform for portal and local repositories. Our methodology aims not only at construction of portal site but also at supporting capture of digital contents transformed from interview videos with intellectualsView less


The study on text emotional orientation based on a three-dimensional emotion space model
Abstract:In order to recognize human emotion, a three-dimensional (3D) emotion space model was established and used to identify the text emotional orientation visually and intuitively. An emotion dictionary contains informations of eight basic emotions, was built based on the emotion corpus. By using this emotion dictionary, emotion keywords which behave distributions of 3D structure can be projected into the emotion space. Emotion distributions were transformed into an emotion matrix. By analyzing the emotion matrix, not only binary classification of texts but also multi-emotion attributes can be investigated. The best precision 91% of a binary classification is obtained in this work.

Fuzzy Explainable Attention-based Deep Active Learning on Mental-Health Data
Abstract:In this paper, we propose a fuzzy classification deep attention-based model that expands emotional lexicons by using linguistic properties of actual patient authored texts. The active learning methods can expand the trained dataset and fuzzy rules over some time. As a result, the model itself can reduce its labeling efforts for mental health application. Thus, the designed model can solve issues related to vocabulary sizes per class, data sources, methods of creation, and create a baseline for human performance levels. This paper also gives fuzzy explainability by visualizing weighted words. Our proposed method uses a similarity-based method that includes a subset of unstructured data as the training set. Next, using an active learning mechanism cycle, our method updates the training model using new training points. This cycle is repeatedly performed until an optimal solution is reached. The designed model also converts all unlabeled texts into the training set. Our in-depth experimental results show that the emotion-based expansion enhances the testing accuracy and helps to build quality rules.

Annotating ESL errors: challenges and rewards
In this paper, we present a corrected and error-tagged corpus of essays written by non-native speakers of English. The corpus contains 63000 words and includes data by learners of English of nine first language backgrounds. The annotation was performed at the sentence level and involved correcting all errors in the sentence. Error classification includes mistakes in preposition and article usage, errors in grammar, word order, and word choice. We show an analysis of errors in the annotated corpus by error categories and first language backgrounds, as well as inter-annotator agreement on the task. We also describe a computer program that was developed to facilitate and standardize the annotation procedure for the task. The program allows for the annotation of various types of mistakes and was used in the annotation of the corpus.

Abstract:The experimental results from comparative study of acoustical properties in speech as emotional indicator based on spectral characteristics of speech signal have formerly been studied and reported for its quantitative information in association with the emotional states in persons suffering depression. This symptom affects speech production system of speaker, which modulates in spoken sound. MFCC has been reported for its characteristic change corresponding to severity of depression. The sixteenth MFCCs from remitted, depressed and suicidal patient groups were extracted, statistically tested and classified in pairwise fashion by using ML, LS and LMS classifiers. The best score of classification can be obtained at 0.2487 in error based on ML classifier with 80% of MFCC samples in testing phase. Results suggest the dominant property of MFCC in separation between suicidal and recovering speakers from depression.


Abstract:Entity name recognition and entity relationship extraction are the most critical foundation for building knowledge graph, and it is also the basic task of NPL. The main purpose of entity relationship extraction is to extract the semantic relationship between the pairs of marked entities in the sentence, that is, to determine the relationship categories between entity pairs in unstructured text based on entity identification, and to form structured data for storage and retrieval. This paper proposes a BERT-BIGRU-CRF entity relationship extraction method, which effectively changes the relationship between the pre-training generated word vector and the downstream specific NLP task, and gradually moves the downstream specific NLP task to the pre-training generated word vector. Our method achieves better performance of relationship extraction and entity name recognition, which helps to construct the knowledge graph more accurately.


Archaisms and neologisms identification in texts
Abstract:In this paper we present an application for identifying archaisms and neologisms in texts. The application also provides the ability to view graphically the evolution trends of these words for a better interpretation of the results. The presented solution consists of two phases: the learning phase in which we identify the general evolution trends of three categories of words (archaisms, neologisms and common words) and the classification phase in which we label new words with their corresponding category. For both phases, the application requires Internet access because it is using the Google Books N-gram Viewer to generate the images that back up the decisions.


SAFLOOR: Smart Fall Detection System for the Elderly

Abstract:This paper is mainly about the method of an Intelligent Representing System of Search Results with the ability to cluster the search results from famous search engines into several classes. Firstly, we combine the N-gram statistical model with syntactic method to conduct word segmentation after preprocessing; and then some feature words are selected by their part of speech and frequency; finally, a unique clustering method is proposed in this paper, which is based on a target function by calculating term covers and their overlaps. In addition to the clustering ability, our intelligent system is capable of labelling the main idea of the cluster results with a Chinese word, which is superior to many other text categorization systems.


Event-event relation identification: A CRF based approach
Abstract:Temporal information extraction is a popular and interesting research field in the area of Natural Language Processing (NLP). The main tasks involve the identification of event-time, event-document creation time and event-event relations in a text. In this paper, we take up Task C that involves identification of relations between the events in adjacent sentences under the TimeML framework. We use a supervised machine learning technique, namely Conditional Random Field (CRF). Initially, a baseline system is developed by considering the most frequent temporal relation in the task's training data. For CRF, we consider only those features that are already available in the TempEval-2007 training set. Evaluation results on the Task C test set yield precision, recall and F-score values of 55.1%, 55.1% and 55.1%, respectively under the strict evaluation scheme and 56.9%, 56.9 and 56.9%, respectively under the relaxed evaluation scheme. Results also show that the proposed system performs better than the baseline system.

Confidence-weighted linear classification
We introduce confidence-weighted linear classifiers, which add parameter confidence information to linear classifiers. Online learners in this setting update both classifier parameters and the estimate of their confidence. The particular online algorithms we study here maintain a Gaussian distribution over parameter vectors and update the mean and covariance of the distribution with each instance. Empirical evaluation on a range of NLP tasks show that our algorithm improves over other state of the art online and batch methods, learns faster in the online setting, and lends itself to better classifier combination after parallel training.

Finite state lazy operations in NLP
Finite state networks can represent dictionaries and lexical relations. Traditional finite-state operations like composition can produce huge networks with prohibitive computation space and time. For a subset of finite state operations, these drawbacks can be avoided by using virtual networks, which rely on structures that are partially built on demand. This paper addresses the implementation of virtual network operations in xfst (XEROX Finite State Technology software). The example of "priority union" which is particularly useful in NLP, is developed.

Abstract:In this paper, we investigate the problem of power optimization in CMOS circuits using gate sizing and voltage selection for a given clock period specification. Several solutions have been proposed for power optimization during gate sizing and voltage selection. Since the problem formulation is nonlinear in nature, nonlinear programming (NLP) based solutions yield better accuracy, however, convergence is difficult for large circuits. On the other hand, heuristic solutions result in faster but less accurate solutions. In this work, we propose a new algorithm for gate sizing and voltage selection based on NLP for power optimization. The algorithm uses gate level heuristics for delay assignment which disassociates the delays of all the paths to the individual gate level, and each gate is then separately optimized for power with its delay constraint. Since the optimization is done at the individual gate level, NLP converges quickly while maintaining accuracy. Experimental results are presented for ISCAS benchmarks which clearly illustrate the efficacy of the proposed solution.



Formation of SQL from Natural Language Query using NLP

Abstract:Sentiment analysis is a methodology used to analyse the emotion or view of an individual to a situation or topic. In present scenario, Social media is the source for the collection of individual's feedbacks, user's emotions, reviews and personal experiences which lead to a need for efficient mining of the text to derive knowledge. An optimal classification of text based on emotion is an unsolved problem in text mining. To extract knowledge from text many machine learning tools and techniques were proposed. An onto-based process is proposed to analyse the customer's emotion in this paper. The input emotional text that needs to be classified is given as input to the NLP and processed and an emotional ontology is created for better understanding of the semantics and relationships. When adding new instances, Ontology can be automatically classify them based on emotional relationship. The Emowords from ontology can be further classified using any of the standard machine learning techniques which definitively gives a better performance. This paper is a review of all the machine learning techniques that can be applied on the semantic analysis of sentiments.


A lattice-valued model of computing with words
Abstract:Computing with words, as a methodology, means computing and reasoning by the use of words in place of numbers or symbols. In this paper, we deal with computing with words via lattice-valued finite state automata and lattice-valued regular grammars. Specifically, 1) we show that computing with words via lattice-valued finite state automata can be implemented with computing with values via lattice-valued finite state automata; 2) we show that computing with words via lattice-valued regular grammars can be implemented with computing with values via lattice-valued regular grammars; 3) the equivalence between lattice-valued finite state automata and lattice-valued regular grammars are demonstrated.


Ecovillages, values, and interactive technology: balancing sustainability with daily life in 21st century america

Abstract:The increased demand for machine learning applications made companies offer Machine-Learning-as-a-Service (MLaaS). In MLaaS (a market estimated 8000M USD by 2025), users pay for well-performing ML models without dealing with the complicated training procedure. Among MLaaS, text-based applications are the most popular ones (e.g., language translators). Given this popularity, MLaaS must provide resiliency to adversarial manipulations. For example, a wrong translation might lead to a misunderstanding between two parties. In the text domain, state-of-the-art attacks mainly focus on strategies that leverage ML models' weaknesses. Unfortunately, not much attention has been given to the other pipeline' stages, such as the indexing stage (i.e., when a sentence is converted from a textual to a numerical representation) that, if manipulated, can significantly affect the final performance of the application. In this paper, we propose a novel text evasion technique called “Zero-Width attack” (ZeW) that leverages the injection of human non-readable characters, affecting indexing stage mechanisms. We demonstrate that our simple yet effective attack deceives MLaaS of “giants” such as Amazon, Google, IBM, and Microsoft. Our case study, based on the manipulation of hateful tweets, shows that out of 12 analyzed services, only one is resistant to our injection strategy. We finally introduce and test a simple input validation defense that can prevent our proposed attack.


A Comparative Study of Sentiment Analysis Using NLP and Different Machine Learning Techniques on US Airline Twitter Data
Abstract:Today's business ecosystem has become very competitive. Customer satisfaction has become a major focus for business growth. Business organizations are spending a lot of money and human resources on various strategies to understand and fulfill their customer's needs. But, because of defective manual analysis on multifarious needs of customers, many organizations are failing to achieve customer satisfaction. As a result, they are losing customer's loyalty and spending extra money on marketing. We can solve the problems by implementing Sentiment Analysis. It is a combined technique of Natural Language Processing (NLP) and Machine Learning (ML). Sentiment Analysis is broadly used to extract insights from wider public opinion behind certain topics, products, and services. We can do it from any online available data. In this paper, we have introduced two NLP techniques (Bag-of-Words and TF-IDF) and various ML classification algorithms (Support Vector Machine, Logistic Regression, Multinomial Naive Bayes, Random Forest) to find an effective approach for Sentiment Analysis on a large, imbalanced, and multi-classed dataset. Our best approaches provide 77% accuracy using Support Vector Machine and Logistic Regression with Bag-of-Words technique.


Effect of Nonlinear Phase Noise on the Performance of MM-Ary PSK Signals in Optical Fiber Links

Abstract:We have developed a system that can semi automatically extract numerical and named entity sets from a large number of Japanese documents and can create various kinds of tables and graphs. In our experiments, our system has semiautomatically created approximately 300 kinds of graphs and tables at precisions of 0.2–0.8 with only two hours of manual preparation from a two-year stack of newspapers articles. Note that these newspaper articles contained a large quantity of data, and all of them could not be read or checked manually in such a short amount of time. From this perspective, we concluded that our system is useful and convenient for extracting information from a large number of documents.


The role of named entities in web people search
The ambiguity of person names in the Web has become a new area of interest for NLP researchers. This challenging problem has been formulated as the task of clustering Web search results (returned in response to a person name query) according to the individual they mention. In this paper we compare the coverage, reliability and independence of a number of features that are potential information sources for this clustering task, paying special attention to the role of named entities in the texts to be clustered. Although named entities are used in most approaches, our results show that, independently of the Machine Learning or Clustering algorithm used, named entity recognition and classification per se only make a small contribution to solve the problem.

Abstract:Hardware Trojans (HTs) are emerging threats for integrated circuits. In this paper, we propose a novel scheme, named LMDet, to detect HTs through distinguishing the “unnaturalness” of HTs from the “naturalness” of normal circuits using the natural language processing technology. The key insight of LMDet is that we find clean circuits tend to be “natural” (i.e., to be highly repetitive in structure) and HTs appear to be “unnatural” (i.e., to be rare in structure) in some sense. LMDet models circuit gates sequentially, using the n-gram language model. Gate sequences from the circuit under detection (CUD) are assessed according to their probability in the model, and lowprobability sequences are marked as suspected Trojan-related gates. Evaluation with benchmarks and industrial circuits shows that LMDet is capable of detecting Trojan logic without the HT-free reference of CUD. LMDet has short execution time on large commercial circuits with acceptable space overhead. It is a promising method in real industry since plenty of HT-free designs are available as training corpus to ensure good statistical effects.


Abstract:The supervised learning has been applied into the query expansion techniques, which trains a model to predict the “goodness” or “utility” of the expanded term to the retrieval system. There are many features to measure the relatedness between the expanded word and the query, which can be incorporated in the supervised learning to select the expanded terms. The training data set is generated automatically by a tricky method. However, this method can be affected by many aspects. A severe problem is that the distribution of the features is query-dependent, which has not been discussed in previous work. With a different distribution on the features, it is questionable to merge these training instances together and use the whole data set to train one single model. In this paper, we first investigate the statistical distribution of the auto-generated training data and show the problems in the training data set. Based on our analysis, we proposed to use the bagging method to ensemble several regression models in order to get a better supervised model to make prediction on the expanded terms. We conducted the experiments on the TREC benchmark test collections. Our analysis on the training data reveals some interesting phenomena about the query expansion techniques. The experiment results also show that the bagging approach can achieve the state-of-art retrieval performance on the standard TREC data set.


Abstract:In the digital age, the Internet has enabled the circulation of ideas and information and, in turn, has increased awareness among people. However, this does not come with its drawbacks. With the proliferation of online platforms, hoaxers can easily lure people towards their propagandist views or false news. The need to root out such false information and hate speech during this COVID-19 pandemic has never been more essential. The following study presents a survey of various papers that attempt to tackle similar problem statements with fake news, sentiment classification, and topic extraction. The paper focuses on how existing quality research can help improve the current state of research on COVID-19 related datasets by guiding researchers towards valuable procedures to help governmental authorities combat the rise in the spread of false news and malicious and hate comments.


Abstract:With the development and popularization of power Internet of Things (IoT), IoT produces a large number of distributed data, the electronic text data and the concurrency of multiple services. To support unified management of all data on an existing basis and make the system has the ability to accurate and efficient data analysis, many researchers have devoted efforts to studying an effective algorithms for solving the problem of the electronic text retrieval, information extraction, and classification in the power grid system. However, the accuracy of these algorithms is not satisfactory. In our paper, we will use an effective deep learning model to classify the grid text. i.e., the hierarchical attention networks model. In particular, Attention mechanism is a commonly model with long-term memory mechanism in the field of NLP, which can intuitively evaluate the contribution of each word to the result in a text. Experimental results show that the Hierarchical Attention Networks model obtains the highest accuracy than the existing methods.


King Alfred: a translation environment for learners of Anglo-Saxon English
King Alfred is the name of both an innovative textbook and a computational environment deployed in parallel in an undergraduate course on Anglo-Saxon literature. This paper details the ways in which it brings dynamically-generated resources to the aid of the language student. We store the feature-rich grammar of Anglo-Saxon in a bi-level glossary, provide an annotation context for use during the translation task, and are currently working toward the implementation of automatic evaluation of student-generated translations.

Abstract:In order to investigate the picture superiority effect, we compared the ERP between picture combined word (picture-word) and pure word (word) at study and test phase. During encoding, the FN400 was more negative and lasted longer for picture-words than for words. The late positive component (LPC) was more positive and distributed broadly for words compared to picture-words. During retrieval, the old picture-word elicited remarkably FN400 familiarity effect and parietal old/new effect compared to the old word. We suggested that simultaneous image and verbal encoding of picture-word elicited better and faster recollection compared to word during the memory test. Our findings also demonstrated that the picture superiority effect was related to the ability of pictures enhancing encoding and facilitating recollection.


Abstract:Complete and accurate clinical documentation in the medical record has a direct impact on the assignment of codes, more accurate levels of reimbursement, and is critical to the higher quality of patient care. This paper describes the development of a system which can automatically flag the cases if there is an opportunity of improvement in patient clinical documents. Automated Clinical Documentation Improvement (CDI) leverages the natural language processing (NLP) and contextual understanding of health record structure with additional business rules logic, helping CDI specialists identify critical documentation information that may be missing from the medical record. This results in more specific coding opportunity and better understanding of the clinical complexity for accurate reimbursement. This system helped increase CDI specialists' productivity by efficiently filtering cases which need more attention from them.


Feature oriented sentiment analysis in social networking sites to track malicious campaigners
Abstract:Social networking websites are considered as major sources of opinions and views of the public on the prevalent social issues at a given point in time. Websites like the Twitter

SALT: an XML application for web-based multimodal dialog management
This paper describes the Speech Application Language Tags, or SALT, an XML based spoken dialog standard for multimodal or speech-only applications. A key premise in SALT design is that speech-enabled user interface shares a lot of the design principles and computational requirements with the graphical user interface (GUI). As a result, it is logical to introduce into speech the object-oriented, event-driven model that is known to be flexible and powerful enough in meeting the requirements for realizing sophisticated GUIs. By reusing this rich infrastructure, dialog designers are relieved from having to develop the underlying computing infrastructure and can focus more on the core user interface design issues than on the computer and software engineering details. The paper focuses the discussion on the Web-based distributed computing environment and elaborates how SALT can be used to implement multimodal dialog systems. How advanced dialog effects (e.g., cross-modality reference resolution, implicit confirmation, multimedia synchronization) can be realized in SALT is also discussed.


BanglaLM: Data Mining based Bangla Corpus for Language Model Research

). Consequently, those causal relations are also textually embedded in requirements. We want to extract this causal knowledge and utilize it to derive test cases automatically and to reason about dependencies between requirements. Existing NLP approaches fail to extract causality from natural language (NL) with reasonable performance. In this paper, we describe first steps towards building a new approach for causality extraction and contribute: (1) an NLP architecture based on Tree Recursive Neural Networks (TRNN) that we will train to identify causal relations in NL requirements and (2) an annotation scheme and a dataset that is suitable for training TRNNs. Our dataset contains 212,186 sentences from 463 publicly available requirement documents and is a first step towards a gold standard corpus for causality extraction. We encourage fellow researchers to contribute to our dataset and help us in finalizing the causality annotation process. Additionally, the dataset can also be annotated further to serve as a benchmark for other RE-relevant NLP tasks such as requirements classification.



Thai Language Tweet Emotion Prediction based on Use of Emojis

Abstract:In the era of social media and mobile internet, the design of automatic tools for online detection of hate speech and/or abusive language becomes crucial for society and community empowerment. Nowadays of current technology in this respect is still limited and many service providers are still relying on the manual check. This paper aims to advance in this topic by leveraging novel natural language processing, machine learning, and feature engineering techniques. The proposed approach advocates a classification-like technique that makes use of a special data design procedure. The latter enforces a balanced training scheme by exploring the negativity of the original dataset. This generates new transfer learning paradigms, Two classification schemes using convolution neural network and LSTN architecture that use FastText embeddings as input features are contrasted with baseline models constituted of Logistic regression and Naives' Bayes classifiers. Wikipedia Comment dataset constituted of Personal Attack, Aggression and Toxicity data are employed to test the validity and usefulness of the proposal.


Illuminating trouble tickets with sublanguage theory
A study was conducted to explore the potential of Natural Language Processing (NLP)-based knowledge discovery approaches for the task of representing and exploiting the vital information contained in field service (trouble) tickets for a large utility provider. Analysis of a subset of tickets, guided by sublanguage theory, identified linguistic patterns, which were translated into rule-based algorithms for automatic identification of tickets' discourse structure. The subsequent data mining experiments showed promising results, suggesting that sublanguage is an effective framework for the task of discovering the historical and predictive value of trouble ticket data.

An architecture to support intelligent user interfaces for Wikis by means of Natural Language Processing
We present an architecture for integrating a set of Natural Language Processing (NLP) techniques with a wiki platform. This entails support for adding, organizing, and finding content in the wiki. We perform a comprehensive analysis of how NLP techniques can support the user interaction with the wiki, using an intelligent interface to provide suggestions. The architecture is designed to be deployed with any existing wiki platform, especially those used in corporate environments. We implemented a prototype integrating the NLP techniques keyphrase extraction and text segmentation, as well as an improved search engine. The prototype is integrated with two widely used wiki platforms: Media-Wiki and TWiki.

Satire Detection from Web Documents Using Machine Learning Methods
Abstract:Satire exposes humanity's vices and foibles through the use of irony, wit, and sometimes sarcasm too. It is also frequently used in online communities. Recognition of satire can help in many NLP applications like dialogue system and review summarization. In this paper we filter online news articles as satirical or true news documents using SVM (Support Vector Machine) classification method combined with machine learning techniques. With ample training documents SVM tends to give good classification results. For obtaining promising results with SVM an understanding of its working and ways to influence its accuracy is required. We also use various feature extraction strategies and conclude that TF-IDF-BNS feature extraction gives maximum accuracy for detection of satire in web content.

Abstract:In this paper, we propose a novel interpolated language model that combines the interpolation and the backing-off along hierarchical classes based on class hierarchy. And the corresponding approach to the estimation of interpolation coefficients is also presented. We use the Minimum Discriminative Information (MDI) method to cluster the vocabulary into a word-clustering tree hierarchically. The tree is used to balance the generalization ability of classes' and word specificity when estimating the likelihood of a n-gram event. Experiments are performed on Reuter's corpus using a vocabulary of 27,000 words. Results show a reduction on the test perplexity over the standard Modified KN n-gram approach by 12%.


Automation of treebank annotation
This paper describes applications of stochastic and symbolic NLP methods to treebank annotation. In particular we focus on (1) the automation of treebank annotation, (2) the comparison of conflicting annotations for the same sentence and (3) the automatic detection of inconsistencies. These techniques are currently employed for building a German treebank.


A supervised algorithm for verb disambiguation into VerbNet classes

Abstract:Chatbot is a computer application that interacts with users using natural language in a similar way to imitate a human travel agent. A successful implementation of a chatbot system can analyze user preferences and predict collective intelligence. In most cases, it can provide better user-centric recommendations. Hence, the chatbot is becoming an integral part of the future consumer services. This paper is an implementation of an intelligent chatbot system in travel domain on Echo platform which would gather user preferences and model collective user knowledge base and recommend using the Restricted Boltzmann Machine (RBM) with Collaborative Filtering. With this chatbot based on DNN, we can improve human to machine interaction in the travel domain.


q
) as test data. In these experiments, the subjects were asked to identify which document from a pair was more important, and a high accuracy of 94% was obtained with more than 80% of them responding with the same answer. Furthermore, on using newspaper documents (D


Existential negative sentence translation in Japanese-Chinese machine translation


Picking them up and figuring them out: verb-particle constructions, noise and idiomaticity


The manually annotated sub-corpus: a community resource for and by the people

Extracting statistical data frames from text
We present a framework that bridges the gap between natural language processing (NLP) and text mining. Central to this is a new approach to text parameterization that captures many interesting attributes of text usually ignored by standard indices, like the term-document matrix. By storing NLP tags, the new index supports a higher degree of knowledge discovery and pattern finding from text. The index is relatively compact, enabling dynamic search of arbitrary relationships and events in large document collections. We can export search results in formats and data structures that are transparent to statistical analysis tools like S-PLUSID®. In a number of experiments, we demonstrate how this framework can turn mountains of unstructured information into informative statistical graphs.

Exploiting salient word dependency for Chinese NP identification: A study on classifier noun phrase
Abstract:NP identification is a challenging subtask of NLP. The reported literatures mainly focus on base noun phrase and maximal-length noun phrase, and deal with them as a sequence labeling problem. In this paper, unlike existing perspective, we concentrate on a special subcategory of Chinese NP, classifier noun phrase (CNP), and present a new approach which uses salient word dependency, such as classifier-noun collocation, for CNP identification. The experiment result is encouraging. Our study shows that salient relations between words should be fully utilized in NP identification as well as other NLP applications.

Abstract:This paper makes a brief research and analysis of current semantic annotation technology from different levels; and proposed a Chinese semantic annotation method which based on home appliance domain ontology according to the shortcomings of current semantic annotation prototypes. The method combined article level and lexical level annotation using SVM classification and NLP technology, which can improve annotating integrity and provides theoretical basis for intelligent retrieval systems.



A Serverless Academic Adviser Chatbot

Abstract:Effort estimation in Software development is becoming increasingly hard to do correctly, this is in part due to the growing complexity of software projects, but also due to the higher amount of projects in total. Specialists in multiple fields are required to work together to obtain a realistic estimate, but even then, there is a good amount of risk involved when planning based on the estimate. This leads to cost increases both for the actual estimation process and the losses taken due to wrong estimations. There are already a few project estimation systems out there which take into account presumed system size, development cycles (design, develop, test) or other project related variables. We want to introduce a more granular estimation system, which uses the text descriptions of various tasks but also takes into account available metadata like seniority of the employee which will be working on said tasks, client and project/project type when estimating. The system is built on a deep neural network. The results we are getting are promising so far and we are working on establishing the human baseline accuracy while the tool is available for company employees to use.


Abstract:It is very essential for lawyers and ordinary citizens to do an exhaustive research related to their case before they answer questions in court. For quite some time they have had to read extremely long judgements and try to pick out the useful information from them or hire legal editors to create summaries. We propose an automated text summarization system that generates short and useful summaries from lengthy judgements. We make use of a natural language processing technique called latent semantic analysis (LSA) to capture concepts within a single document. We use two approaches-a single document untrained approach and a multi-document trained approach depending on the type of input case (criminal or civil). Our data was collected from official government sites that included Supreme Court, high court and district court cases and our model achieved an average ROGUE-1 score of 0.58. Finally, our system was approved by professional lawyers. In the future we aim to provide better continuity within our generated summaries and evaluate our system more accurately.


An analytical approach to optimization of throughput for IEEE 802.15.4 slotted CSMA/CA networks
Abstract:As one of the most widely implemented standards for wireless networks, IEEE 802.15.4 defines physical and MAC specifications for low data rate wireless personal area networks (WPANs). To accurately analyze the performance of slotted CSMA/CA (SCSMA) algorithms for saturated uplink traffic with acknowledgement (ACK) and non-ACK modes, we adopt an embedded Markov chain based model in the MAC layer of the standard. From the analytic model, we optimize network throughput considering the number of 802.15.4 devices and the data payload by nonlinear programming (NLP). Moreover, according to analytical results of optimization, we propose an adaptive backoff mechanism for maximizing throughput by an approximate and simple Markov model. Furthermore, we present performance evaluation and comparison of IEEE 802.15.4 SCSMA protocol with our approach quantitatively with respect to saturation throughput and probability of successful transmission. Comparing with the SCSMA protocol of IEEE standard through the simulation, our scheme can improve network throughput with non-ACK and ACK modes up to 4.8% and 8.1% on average, respectively. The performance analysis also shows that the network scalability is improved, that is the system can accommodate more contending devices in our proposal.


Utilizing Natural Language Processing (NLP) to Evaluate Engagement in Project-Based Learning

XML-based phrase alignment in parallel treebanks
This paper describes the usage of XML for representing cross-language phrase alignments in parallel treebanks. We have developed a TreeAligner as a tool for interactively inserting and correcting such alignments as an independent level of treebank annotation.

Modeling coherence in ESOL learner texts
To date, few attempts have been made to develop new methods and validate existing ones for automatic evaluation of discourse coherence in the noisy domain of learner texts. We present the first systematic analysis of several methods for assessing coherence under the framework of automated assessment (AA) of learner free-text responses. We examine the predictive power of different coherence models by measuring the effect on performance when combined with an AA system that achieves competitive results, but does not use discourse coherence features, which are also strong indicators of a learner's level of attainment. Additionally, we identify new techniques that outperform previously developed ones and improve on the best published result for AA on a publically-available dataset of English learner free-text examination scripts.


The Impact of Sentence Embeddings in Turkish Paraphrase Detection

Portability of syntactic structure for language modeling
Abstract:Presents a study on the portability of statistical syntactic knowledge in the framework of the structured language model (SLM). We investigate the impact of porting SLM statistics from the Wall Street Journal (WSJ) to the Air Travel Information System (ATIS) domain. We compare this approach to applying the Microsoft rule-based parser (NLP-win) for the ATIS data and to using a small amount of data manually parsed at UPenn for gathering the initial SLM statistics. Surprisingly, despite the fact that it performs modestly in perplexity (PPL), the model initialized on WSJ parses outperforms the other initialization methods based on in-domain annotated data, achieving a significant 0.4% absolute and 7% relative reduction in word error rate (WER) over a baseline system whose word error rate is 5.8%; the improvement measured relative to the minimum WER achievable on the N-best lists we worked with is 12%.

Abstract:It takes a long time to build vocabularies and their definitions because they must be approved only by the experts in the meeting of building vocabularies and the definitions are also unstructured. To save time, we applied three techniques of classification to the experiments that are one-class SVMs, isolation forests, and local outlier factors, and also observed how well the method can suggest word definition status via the accuracy. As a result, the local outlier factors obtained the highest accuracy when they used vectors that were produced by USE. They can recognize the boundary of the approved class better and there are several approved clusters and outliers are scattered among them. Also, it is found that the detected status of definitions is both identical and opposite to the reference one. For the patterns of definition writing, the approved definitions are always written in the logical order, and start with wide or general information, then is followed by specific details, examples, and references of English terms or examples. In case of the rejected definitions, they are not always written in the logical order, and their definition patterns are also various - only Thai translation, Thai translation with related entries, parts of speech (POS), Thai translation, related entries, and English term references followed by definitions, etc.


 which also affords a multitude of auxiliary features enabling a guided, normalized, and efficient proofreading of any part of the factorized corpus. The paper first reviews the highly inflective and derivative nature of Arabic language, our Arabic language factorization models, and the associated statistical disambiguation methodology. Afterwards, we present Fassieh
reg


A Web-based Chinese question answering with answering validation


A contrastive study of Chinese progressive aspect structure, "ZAI+verb" and its English correspondents: a bilingual parallel corpus-based perspective

Influence on emotional impression of voice by changing prosodic features
Abstract:In this paper, investigate the relation between emotions represented by speech and features included in speech. There are two kinds of approaches in comparing experiment. Firstly, speech data are non-verbal such as white-noise, sine wave and vowel /a:/, so that investigate the relation between emotions and prosodic features only. Secondly, speech data include verbal information such as `ohayou'(hello), `usotsuki'(Ananias), so that investigate case speech data are words. Result of comparing shows the effect that of verbal information on emotions is strong, in case of words used this experiment. On the other hand, emotions felt by subjects are changed by variation of prosodic features in some words.

Abusive Language Detection in Online User Content
Detection of abusive language in user generated online content has become an issue of increasing importance in recent years. Most current commercial methods make use of blacklists and regular expressions, however these measures fall short when contending with more subtle, less ham-fisted examples of hate speech. In this work, we develop a machine learning based method to detect hate speech on online user comments from two domains which outperforms a state-of-the-art deep learning approach. We also develop a corpus of user comments annotated for abusive language, the first of its kind. Finally, we use our detection tool to analyze abusive language over time and in different settings to further enhance our knowledge of this behavior.

A Comparative Study of Dictionary-based and Machine Learning-based Named Entity Recognition in Pashto
Information Extraction (IE) is the process of extracting structured information from unstructured text using natural language processing (NLP). One important sub-task of IE is the extraction of names of persons, places, and organizations, called Named Entity Recognition (NER). NER plays an important role in many NLP applications such as Question Answering, Machine Translation, and Text Summarization. It has been widely studied for high-resource languages like English. However, no research has taken place in this regard for Pashto. We hypothesized that based on the research done for English and other languages in the area of NER a system can be developed for Pashto. We have developed two NER systems for detecting names of persons, places, and organizations in Pashto text. First, a dictionary-based NER that uses three dictionaries containing names of persons, locations, and organizations, respectively. Second, a learning-based approach that uses Hidden Markov Model (HMM) for the task. We have evaluated both systems on a dataset collected from sports news. Our evaluation showed F-Measure of 82% for HMM and 60% for dictionary-based NER. Our findings highlight that HMM outperforms dictionary based NER.

Implementation of Big Imaging Data Pipeline Adhering to FAIR Principles for Federated Machine Learning in Oncology
Abstract:Cancer is a fatal disease and one of the leading causes of death worldwide. The cure rate in cancer treatment remains low; hence, cancer treatment is gradually shifting toward personalized treatment. Artificial intelligence (AI) and radiomics have been recognized as one of the potential areas of research in personalized medicine in oncology. Several researchers have identified the capabilities of AI and radiomics to characterize phenotype and there by predict the outcome of treatment in oncology. Although AI and radiomics have shown promising initial results in diagnosis and treatment in oncology, these technologies are also facing challenges of standardization and scalability. In the last few years, researchers have been trying to develop a research infrastructure for federated machine learning that increases the usability of Big Data for clinical research. These research infrastructures are based on the findable, accessible, interoperable, and reusable (i.e., FAIR) data principles. The India-Dutch “big imaging data approach for oncology in a Netherlands India collaboration” (BIONIC) is a jointly funded initiative by the Dutch Research Council (NWO) and the Indian Ministry of Electronics and Information Technology (MeitY), aiming to introduce radiomic-based research into clinical environments using federated machine learning on geographically dispersed collections of FAIR data. This article described a prototype end-to-end research infrastructure implemented through the BIONIC partnership into a leading cancer care public hospital in India.


Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks

) are compared. The results showed that the extended myPOS version 2.0 improved the accuracies of automatic POS tagging methods compared with the original myPOS.


New Initiative: The Naturalness of Software
Abstract:This paper describes a new research consortium, studying the Naturalness of Software. This initiative is supported by a pair of grants by the US National Science Foundation, totaling $2,600,000: the first, exploratory ("EAGER") grant of $600,000 helped kickstart an inter-disciplinary effort, and demonstrate feasibility; a follow-on full grant of $2,000,000 was recently awarded. The initiative is led by the author, who is at UC Davis, and includes investigators from Iowa State University and Carnegie-Mellon University (Language Technologies Institute).

Abstract:Part Of Speech tagging (POS) is the basic process for almost all natural language processing (NLP) applications. The typical methods for combining different taggers, the program doing POS tagging, are voting or stacking techniques. We propose here a Master-Slaves Technique, which can combine Hidden Markov Model (HMM) tagger as master and any number of other taggers of any type as slaves. We describe the construction, whose main idea is that, prior to tagging an input sentence by the master tagger, its probabilities are modified to privilege the tags that slave taggers used in tagging the same sentence. Then we report about tests we did with this technique, using maximum match (MM) and Brill taggers as slaves. Additionally we report a similar, custom method of integrating HMM and MM, with a higher degree of integration of the component taggers. Despite its simplicity, this method offers an increase in accuracy in several tests on Arabic and English languages. We used a well-known data set (Brown corpus) for English language and private Arabic corpus consists of 45k word.


Abstract:In this paper we suggest a novel systematization of Information Retrieval and Natural Language Processing problems. Using this rather general description of problems we are able to discuss and proof the equivalence of some problems. We provide reformulations of well-known problems like Named Entity Recognition using our novel description and discuss further research and the expected outcome. We will discuss the relation of two problems, cluster labeling and search query finding. With these results we are able to provide a novel optimization approach to both problems. This novel systematization approach provides a yet unknown view generating new classes of problems in NLP. It brings application and algorithmic approaches together and offers a better description with concepts of theoretical computer science.



An automated method for constructing ontology

Learning syntactic tree patterns from a balanced Hungarian natural language database, the Szeged Treebank
Abstract:The current paper has a twofold objective. On the one hand, it describes the creation and the features of the Szeged Treebank, which is currently the largest manually processed Hungarian textual database serving as a reference material for research in natural language processing. On the other hand, detailed information is given about different experiments that aimed at the automatic recognition of syntactic structures with the use of machine learning algorithms. In order to provide comparable results, we applied methods of different categories, namely a rule-based, a logic and a numeric learner to pre-defined parsing problems. The aforementioned Szeged Treebank was used for the training and the testing of the algorithms.

Abstract:Today, the Internet is flooded with a wide variety of textual information that has enormous mining value and research prospects. Emotional analysis has penetrated into all walks of life, and the research on sentiment analysis of texts has important research significance and application prospects. In this paper, several deep learning models suitable for natural language processing are studied and applied to the sentiment analysis of online reviews. A two-layer CNN and two-way LSTM sentiment analysis model, called CC-BiLSTM, is proposed. The combination of deep learning and sentiment analysis will bring new opportunities for natural language processing.


Abstract:As the number of electronic resources is rapidly increasing, the need for automatic classification of such resources became very crucial. In the literature, extensive text classification schemes were proposed to classify text documents into predefined categories. Even though, most studies in this problem were conducted for English texts while very little studies were conducted for the Arabic language. This paper provides a review for text classification process, particularly for Arabic documents. As well, it presents the most important techniques used in the process of documents classification. Furthermore, the current unresolved issues regarding Arabic text classification will be discussed and thus highlight the future recommendations in this context.



Toward Deep Learning Software Repositories

NLP-based metadata extraction for legal text consolidation
The paper describes a system for the automatic consolidation of Italian legislative texts to be used as a support of an editorial consolidating activity and dealing with the following typology of textual amendments: repeal, substitution and integration. The focus of the paper is on the semantic analysis of the textual amendment provisions and the formalized representation of the amendments in terms of meta-data. The proposed approach to consolidation is metadata--oriented and based on Natural Language Processing (NLP) techniques: we use XML--based standards for metadata annotation of legislative acts and a flexible NLP architecture for extracting metadata from parsed texts. An evaluation of achieved results is also provided.

1
, then event


A Two-Stage Rapid Trajectory Optimization Algorithm for Hypersonic Entry

Abstract:Promotional infection is an attack in which the adversary exploits a website's weakness to inject illicit advertising content. Detection of such an infection is challenging due to its similarity to legitimate advertising activities. An interesting observation we make in our research is that such an attack almost always incurs a great semantic gap between the infected domain (e.g., a university site) and the content it promotes (e.g., selling cheap viagra). Exploiting this gap, we developed a semantic-based technique, called Semantic Inconsistency Search (SEISE), for efficient and accurate detection of the promotional injections on sponsored top-level domains (sTLD) with explicit semantic meanings. Our approach utilizes Natural Language Processing (NLP) to identify the bad terms (those related to illicit activities like fake drug selling, etc.) most irrelevant to an sTLD's semantics. These terms, which we call irrelevant bad terms (IBTs), are used to query search engines under the sTLD for suspicious domains. Through a semantic analysis on the results page returned by the search engines, SEISE is able to detect those truly infected sites and automatically collect new IBTs from the titles/URLs/snippets of their search result items for finding new infections. Running on 403 sTLDs with an initial 30 seed IBTs, SEISE analyzed 100K fully qualified domain names (FQDN), and along the way automatically gathered nearly 600 IBTs. In the end, our approach detected 11K infected FQDN with a false detection rate of 1.5% and over 90% coverage. Our study shows that by effective detection of infected sTLDs, the bar to promotion infections can be substantially raised, since other non-sTLD vulnerable domains typically have much lower Alexa ranks and are therefore much less attractive for underground advertising. Our findings further bring to light the stunning impacts of such promotional attacks, which compromise FQDNs under 3% of .edu, .gov domains and over one thousand gov.cn domains, including those of leading universities such as stanford.edu, mit.edu, princeton.edu, havard.edu and government institutes such as nsf.gov and nih.gov. We further demonstrate the potential to extend our current technique to protect generic domains such as .com and .org.


A Smart Chatbot Architecture based NLP and Machine Learning for Health Care Assistance
A chatbot or conversational agent is a software that can communicate with a human by using natural language. One of the essential tasks in artificial intelligence and natural language processing is the modeling of conversation. Since the beginning of artificial intelligence, its been the hardest challenge to create a good chatbot. Although chatbots can perform many tasks, the primary function they have to play is to understand the utterances of humans and to respond to them appropriately. In the past, simple statistic methods or handwritten templates and rules were used for the constructions of chatbot architectures. With the increasing learning capabilities, end-to-end neural networks have taken the place of these models in around 2015. Especially now, the encoder-decoder recurrent model is dominant in the modeling of conversations. This architecture is taken from the neural machine translation domain, and it performed very well there. Until now, plenty of features and variations are introduced that have remarkably enhanced the conversational capabilities of chatbots. In this paper, we performed a detailed survey on recent literature. We examined many publications from the last five years, which are related to chatbots. Then we presented different related works to our subject, and the AI concepts needed to build an intelligent conversational agent based on deep learning models Finally, we presented a functional architecture that we propose to build an intelligent chatbot for health care assistance.

NLP Tools for Knowledge Extraction from Italian Archaeological Free Text
Abstract:This paper deals with the development of advanced tools and technologies for creating relevant information and suitable metadata out of textual documentation produced by Italian archaeological research. A set of Natural Language Processing tools were developed to recognize and annotate various archaeological entities in Italian language textual reports. The CIDOC CRM is the ontology chosen for encoding resulting output, allowing for a maximum degree of standardisation of the produced metadata to guarantee interoperability with archaeological information already existing in other semantically enabled digital archives. The work took place as part of the development for the TEXTCROWD platform for the European Open Science Cloud for Research Pilot Project.

Abstract:Time series classification (TSC) is the problem of predicting class labels of time series generated by different signal sources. TSC has been a challenging problem in machine learning and statistics for many decades. TSC has many important applications in bioinformatics, biomedical engineering, and clinical predictions. A large number of classification algorithms have been developed to address TSC problem. However, there is still a lot of room for improving the accuracy of classification. Traditional approaches extract discriminative features from the time series data by applying different types of transformations. These features are then fed into standard classifiers for classification. After a tremendous success of deep neural networks in certain areas such as NLP, image processing, and speech recognition, some researchers have applied deep convolutional neural networks and recurrent neural network based approaches for TSC. Deep neural network based algorithms have established a new baseline for TSC. In this paper, we propose Ensemble Deep TimeNet (EDTNet), an ensemble of multiple deep neural networks for TSC. We have compared the accuracy of EDTNet with those of the state-of-the-art algorithms on 44 different datasets from UCR time series database. Through extensive experiments we show that EDTNet outperforms all the competing algorithms in most of the UCR datasets in terms of classification accuracy. Specifically, EDTNet outperforms the other algorithms on 26 out of the 44 datasets, whereas, the next best algorithm (FCN) has a better accuracy than the others in only 10 out of the 44. A GPU based python implementation of our algorithm will be posted at https://github.com/sudiptap/Timeseries-Classification when the paper is accepted.


Toward Collaborative Reinforcement Learning Agents that Communicate Through Text-Based Natural Language
Abstract:Communication between agents in collaborative multi-agent settings is in general implicit or a direct data stream. This paper considers text-based natural language as a novel form of communication between multiple agents trained with reinforcement learning. This could be considered first steps toward a truly autonomous communication without the need to define a limited set of instructions, and natural collaboration between humans and robots. Inspired by the game of Blind Leads, we propose an environment where one agent uses natural language instructions to guide another through a maze. We test the ability of reinforcement learning agents to effectively communicate through discrete word-level symbols and show that the agents are able to sufficiently communicate through natural language with a limited vocabulary. Although the communication is not always perfect English, the agents are still able to navigate the maze. We achieve a BLEU score of 0.85, which is an improvement of 0.61 over randomly generated sequences while maintaining a 100% maze completion rate. This is a 3.5 times the performance of the random baseline using our reference set.

Integrating Linguistic Patterns and Term-Entity Associations in Chinese Person Description Extraction
Abstract:Person description extraction is an important task in biography generation, question answering and summarization. Most previous extraction approaches select descriptive passages depending on sentence structure and/or word co-occurrence information. In this paper, we focus on Chinese person description extraction verification by measuring the associations between the recognized person entities and the surrounding terms, called Term-Entity associations. The associations are derived from both the semantic knowledge provided in a Chinese well-known thesaurus HowNet and the term distributional information gathered from the news corpus. Relying on Term-Entity associations, the ineligible extracted descriptions could be filtered out so that the higher precision could be achieved in turn. As far as we know, no work on Chinese person description extraction has been reported in the literature.

Abstract:Part of Speech (PoS) tagging is the task to assign the appropriate morphosyntactic category to each word according to the context. Several probabilistic methods have been adapted for PoS tagging such as Conditional Random Fields, Support Vector Machines, and Decision Trees. Based on these methods, language independent PoS taggers have been developed such as CRF++, Yamcha and TreeTagger. These POS taggers implement the process of assigning the correct PoS (noun, verb, adjective, adverb …) to each word of the sentence. PoS taggers are developed by modeling the morphosyntactic structure of natural language text. In this paper, we tried to improve the accuracy of existing Amazigh POS taggers using a voting algorithm. The three used Amazigh POS taggers are: (1) Conditional Random Fields (CRF) tagger (2) Support Vector Machines (SVM) tagger (3) TreeTagger (TT). These taggers are developed with an accuracy of 86.79 %, 84.64 % and 86.57 % respectively. An annotated corpus of 60,000 words is used to form all these taggers. An error analysis is done to find out the mistakes made by these taggers. Then, a voting algorithm is proposed to construct an Amazigh PoS tagger to achieve better results and we can reach an accuracy of 89.06 %. This accurate POS tagger could be used for a variety of NLP applications to offer the students and the researchers an opportunity to work with language data with variety of tools and techniques in terms of computational procedures and programs.



Adding meaning to facebook microposts via a mash-up API and tracking its data provenance

Abstract:Mastering several languages almost becomes a necessity in today's globalization environment. Computer Assisted language learning (CALL) systems are as a popular second language learning tool because of its inherent advantages. Through analyzing traditional language education and existing CALL tools, we have developed an experimental CALL system which can give relevance feedbacks through anatomy animation. It is based on deliberate data trained recognition to give error trend relevant feedbacks to language learners. The feedbacks are provided through different visualized speech features as well as articu-latory anatomy animation. The visual features are easier to interpret and compare to the correct pronunciations so that the learning process can be made easier. Comparative evaluations to a simple playback system without feedbacks show that the proposed system is more effective.


Intranet learning tools for NLP
This paper describes experience with the developed of tools for CL education using Java. Some are standalone Java applets and others are clients which connect to a parsing server using a LISP-based backend. The principal benefits are platform independence and reusability rather than world-wide web access, although intranet technology reduces the need for special purpose labs.

Abstract:Hardware Trojan has always been a major security threat to the integrated circuit industry. In this article, we propose a novel circuit gate embedding method called SeGa, which extracts the “semantic information” of gates in the netlist. The feature vectors that representing each type of gate extracted by SeGa are used as the inputs to the neural network classification model to detect Trojans. The experimental results on TRIT-TC benchmark show that SeGa can improve the performance of the neural network classification model to detect the Trojan gate sequence.



A phrase structure grammar of the Arabic language

Abstract:Intelligent Machines are playing an important role in performing various activities in the industry thereby reducing the human efforts, error rate and increase the efficiency and accuracy. Artificial Intelligence is the backbone for development of intelligent machines which use natural language processing and deep learning as fundamental tools. In this paper, we present the new perspective of intelligent machines in everyday usage, along with the importance of understanding the natural language and generating machine level natural language for intelligent machines. Further, we summarize an overview of deep learning concurrent neural network and recurrent neural network models. Also, we review the significance of sentimental analysis using natural language processing and the ability of machines to take decisions and solve problems.


Design of a Morph Analyzer for Non-Declinable Adjectives of Nepali Language
Morph Analyser is a fundamental tool Natural Language Processing (NLP) system which is used to split a given token into its constituents. As per the literature available, very less numbers of works are found in this specific area for Nepali language. In this paper, an attempt has been made to develop a successful morph analyser for non-declinable adjectives of Nepali language. The technique is developed using finite state grammar approach. It can work with minimum number of linguistic resources. The morph analyser is tested with sufficient number of input text and found satisfactory results.

Learning semantic-level information extraction rules by type-oriented ILP
This paper describes an approach to using semantic representations for learning information extraction (IE) rules by a type-oriented inductive logic programming (ILP) system. NLP components of a machine translation system are used to automatically generate semantic representations of text corpus that can be given directly to an ILP system. The latest experimental results show high precision and recall of the learned rules.

Abstract:For visually disabled people, navigating the streets is a challenging activity. It’s almost always difficult for them to comprehend basic street signboards or the surrounding environment. The proposed model suggests a system that can help visually impaired people understand better their surroundings. This research proposes a novel implementation of smart spectacles based on Image Captioning and Optical Character Recognition (OCR) to ease the navigation process. The system consists of a camera that is embedded in spectacles, an Image Captioning module, an OCR module, and Text-To-Speech (TTS) module. The Image Captioning module aids in the detection of notice or signboards in the immediate vicinity, while the OCR module aids in the reading of the text on the notice boards. Text-To- Speech is used primarily to translate this data into a format that is open to the visually impaired, such as voice notifications. The proposed research makes navigating on the street less challenging for visually impaired people. It also helps them to comprehend the signboards and to make optimum use of the available facilities. The presented model can successfully generate captions based on the surroundings and can read the text whenever required. The model can read texts in English, Hindi, and Marathi, making it adaptable to the needs of the vernacular population.


Application-driven statistical paraphrase generation
Paraphrase generation (PG) is important in plenty of NLP applications. However, the research of PG is far from enough. In this paper, we propose a novel method for statistical paraphrase generation (SPG), which can (1) achieve various applications based on a uniform statistical model, and (2) naturally combine multiple resources to enhance the PG performance. In our experiments, we use the proposed method to generate paraphrases for three different applications. The results show that the method can be easily transformed from one application to another and generate valuable and interesting paraphrases.

Compression of Deep Learning Models for NLP
In recent years, the fields of NLP and information retrieval have made tremendous progress thanks to deep learning models like RNNs and LSTMs, and Transformer[35] based models like BERT[9]. But these models are humongous in size. Real world applications however demand small model size, low response times and low computational power wattage. We will discuss six different types of methods (pruning, quantization, knowledge distillation, parameter sharing, matrix decomposition, and other Transformer based methods) for compression of such models to enable their deployment in real industry NLP projects. Given the critical need of building applications with efficient and small models, and the large amount of recently published work in this area, we believe that this tutorial is very timely. We will organize related work done by the 'deep learning for NLP' community in the past few years and present it as a coherent story.

Chinese text orientation analysis based on phrase
Abstract:Semantic orientation analysis of sentiment word is to determine its polarity and degree, including original orientation, dynamic orientation and modified orientation. In this paper, we correct the orientation in different contexts through dependency relationship and some rules. The result shows that accuracy and recall rate is improved a lot.

Experience of using GATE for NLP R&D
GATE, a General Architecture for Text Engineering, aims to provide a software infrastructure for researchers and developers working in NLP. GATE has now been widely available for four years. In this paper we review the objectives which motivated the creation of GATE and the functionality and design of the current system. We discuss the strengths and weaknesses of the current system, identify areas for improvement.

Natural Language Processing for Theoretical Framework Selection in Engineering Education Research
Abstract:This research paper presents recent work exploring the power of natural language processing (NLP) methods applied to qualitative engineering education data. As NLP and other machine learning methods are developed for qualitative data, it is important to prioritize the role that theory plays in rigorous qualitative research, where the selection of a theoretical framework serves as the lens by which the research project is framed, results are analyzed, and findings are brought to light. Indeed, the view from a different theoretical lens can highlight novel or new findings. In this work, we seek to explore the viability of NLP methods for helping researchers select appropriate frameworks. In this work, we present our method to train a Python-based NLP algorithm to analyze an existing data set of interview data using one theoretical lens: Community of Practice theory, an oft-used theory in graduate education literature, which is the topic of the interview corpus to investigate. We present and test two methods for developing dictionaries by which to train the algorithm: An expert-curated dictionary and a machine-generated dictionary compiled by mining the theoretical framework sections of published literature employing Community of Practice theory. We apply these two dictionaries to analyze a corpus of 54 interview transcripts investigating graduate engineering attrition. The high dimensional data from NLP can be compared using Principal Component Analysis (PCA) visualization and pairwise distance plots to determine which method results in the most well-defined structure indicating agreement between the dictionary and the corpus of interview transcripts. In the discussion, we highlight opportunities for using these automated methods to help researchers with qualitative data analysis and warn against potential dangers and ethical ramifications for using machine learning and NLP for social science data. This work will have impact on the disciplinary communities working to embed computational language-based methods into engineering education research, and for the qualitative methods communities across social science and education disciplines.

Deception detection for news: three types of fakes
A fake news detection system aims to assist users in detecting and filtering out varieties of potentially deceptive news. The prediction of the chances that a particular news item is intentionally deceptive is based on the analysis of previously seen truthful and deceptive news. A scarcity of deceptive news, available as corpora for predictive modeling, is a major stumbling block in this field of natural language processing (NLP) and deception detection. This paper discusses three types of fake news, each in contrast to genuine serious reporting, and weighs their pros and cons as a corpus for text analytics and predictive modeling. Filtering, vetting, and verifying online information continues to be essential in library and information science (LIS), as the lines between traditional news and online information are blurring.


What do language representations really represent?

Comparing corpora and lexical ambiguity
In this paper we compare two types of corpus, focusing on the lexical ambiguity of each of them. The first corpus consists mainly of newspaper articles and literature excerpts, while the second belongs to the medical domain. To conduct the study, we have used two different disambiguation tools. However, first of all, we must verify the performance of each system in its respective application domain. We then use these systems in order to assess and compare both the general ambiguity rate and the particularities of each domain. Quantitative results show that medical documents are lexically less ambiguous than unrestricted documents. Our conclusions show the importance of the application area in the design of NLP tools.


Convolutional Neural Networks for opinion mining on Drug reviews


A NLP Based Framework to Support Document Verification-as-a-Service

Beyond the pipeline: discrete optimization in NLP
We present a discrete optimization model based on a linear programming formulation as an alternative to the cascade of classifiers implemented in many language processing systems. Since NLP tasks are correlated with one another, sequential processing does not guarantee optimal solutions. We apply our model in an NLG application and show that it performs better than a pipeline-based system.

Scaling up a NLU system from text to dialogue understanding
In this paper we will present work carried out to scale up the system for text understanding called GETARUNS, and port it to be used in dialogue understanding. We will present the adjustments we made in order to cope with transcribed spoken dialogues like those produced in the ICSI Berkely project. In a final section we present preliminary evaluation of the system on non-referential pronominals individuation.

>View less


Chinese-English quasi-equivalent noun phrase definition and automatic identification
Abstract:After an examination of a Chinese-English bilingual corpus with 2239 sentence pairs, a new definition of Chinese noun phrase (NP), quasi-equivalent noun phrase (equNP), is proposed with a goal of translation from Chinese NPs to English NPs. Firstly, all the equNPs in the corpus are tagged manually according to the definition in this paper. A set of part of speech (POS) templates for equNP is automatically acquired. Secondly, all the possible equNPs in a sentence are identified using the templates. These equNPs are the candidates for equNP identification. Finally, a classification process and a chunking process are carried out. In classification process, the correct equNPs are chosen from the candidates set using a maximum entropy classifier which combined POS, syntactic and semantic information. In chunking process, the equNPs in the sentence are finally chosen. On open test set, the precision is 83.75% and recall is 86.50%.

Abstract:Dense word embeddings, which encode meanings of words to low-dimensional vector spaces, have become very popular in natural language processing (NLP) research due to their state-of-the-art performances in many NLP tasks. Word embeddings are substantially successful in capturing semantic relations among words, so a meaningful semantic structure must be present in the respective vector spaces. However, in many cases, this semantic structure is broadly and heterogeneously distributed across the embedding dimensions making interpretation of dimensions a big challenge. In this study, we propose a statistical method to uncover the underlying latent semantic structure in the dense word embeddings. To perform our analysis, we introduce a new dataset (SEMCAT) that contains more than 6500 words semantically grouped under 110 categories. We further propose a method to quantify the interpretability of the word embeddings. The proposed method is a practical alternative to the classical word intrusion test that requires human intervention.


Classifying Emotions in Roman Urdu Posts using Machine Learning
Abstract:Emotion, a state of human mind has a significant impact on human behavior, social interactions and decision making. Few emotions from Carroll E. Izard model such as long-term fear, anger and sadness cause mental disorders which can lead to deterioration of mental and physical health. For the well-being of mental health, different psychological parameters such as voice, facial expressions and body movements were used to extract these emotions in earlier times. This paper focuses on verbal language for emotion classification. With the huge popularity of Web 3.0, people feel more comfortable in sharing their emotions over social media due to the anonymity of the web. Recently, researchers around the globe are interested to classify the emotions from social media posts (i.e., tweets, comments or micro-blogs etc.) using resource-rich languages e.g., English. However, less attention is paid towards the resource-poor language such as Urdu in Roman script. According to the Gallup survey in Pakistan, Urdu in Roman script is widely used to transmit text. To detect emotions in Roman Urdu text is a big challenge due to its scripting and morphology. In this paper, the following contributions are made; 1) A corpus of Roman Urdu is developed for emotion classification. 2) We aim to identify user’s mental states using textual posts on social media plat-forms; Instagram, Facebook, Twitter and YouTube. Particularly, Natural Language Processing methods incorporated with feature extraction methods (TF-IDF) and four supervised classifiers such as Support Vector Machine (SVM),Naïve Bayes, Random Forest and K-nearest Neighbor are used. By utilizing 80% training and 20% testing data, Naive Bayes secures 70.57% accuracy. 3) Moreover, suitability analysis is performed to identify the most suitable classifier in terms of accuracy, training and testing time. Results show that Naive Bayes is the most suitable classifier for emotion classification.

Abstract:This work explores the use of standard Natural Language Processing (NLP) tools for the development of teaching support tools.We use these methods for developing an application that allows to automatically create different types of English exercises aimed at students at the beginners level. The web application contains two interfaces, where teachers can generate four types of exercises starting from English texts, and students can solve them. The four types of exercises are: verb form exercises, noun definition exercises, use of English exercises, and lexical relations exercises. Some of the NLP techniques and tools explored during development include: POS-tagging, language models, word embeddings, Levenshtein distance and WordNet. Developing this tool allowed us to test the potential of Natural Language Processing techniques to generate teaching activities. As a way of validating the tool, we tested it with local teachers and found that there is an important demand for these type of tools in our country.


Design of the moses decoder for statistical machine translation
We present a description of the implementation of the open source decoder for statistical machine translation which has become popular with many researchers in SMT research. The goal of the project is to create an open, high quality phrase-based decoder which can reduce the time and barrier to entry for researchers wishing to do SMT research. We discuss the major design objective for the Moses decoder, its performance relative to other SMT decoders, and the steps we are taking to ensure that its success will continue.


Health Care Spoken Dialogue System for Diagnostic Reasoning and Medical Product Recommendation

Optimisation of the Largest Annotated Tibetan Corpus Combining Rule-based, Memory-based, and Deep-learning Methods
This article presents a pipeline that converts collections of Tibetan documents in plain text or XML into a fully segmented and POS-tagged corpus. We apply the pipeline to the large extent collection of the Buddhist Digital Resource Center. The semi-supervised methods presented here not only result in a new and improved version of the largest annotated Tibetan corpus to date, the integration of rule-based, memory-based, and neural-network methods also serves as a good example of how to overcome challenges of under-researched languages. The end-to-end accuracy of our entire automatic pipeline of 91.99% is high enough to make the resulting corpus a useful resource for both linguists and scholars of Tibetan studies.

Abstract:Suicide is a major issue in the world. The number one reason for suicide is untreated depression. That is why it was decided to focus on depression symptoms more and identify them in order to prevent suicidal attempts. To cure depression, the best way is to talk about their feelings with someone they trusted and release their pain inside of them. Because of that this system has a Chat-bot for the user to interact with. Chat-bot will gather information about the users feelings through text and voice analysis. Also by analyzing their Facebook statuses and recent web history, the application gather more information about their mental state so that the system take more accurate conclusions. After analyzing all the information from each component the back brain will decide on how the chat-bot should act on the user. At the end, the product was able to give more than 75% accurate results for each component.


Abstract:To assist decision makers, there is need of providing an insight on current of scenario of market, considering really sensitive news involving economic events like acquisitions, stock splits, or dividend announcements. Similar to the work discussed above, to machine process natural language constraints, there is need of a mechanism that can automate events related information extraction and knowledge acquisition processes to facilitate software developers in fulfilling their cumbersome tasks, for quicker and accurate of software modeling. However, extraction of events and their related information from natural language constraints is a complex task considering the ambiguous nature of natural languages such as English. However, similar problems have been solved previously by other researchers with the help of using event semantic ontologies. However, there is need to plan a mechanism for developing an event semantic ontology for natural language constraints. It is discussed in the previous sections that the Ontology can provide assistance in identification of relations existing in various annotations. It is discussed in this thesis that it is significant to use ontology for identifying events from text. With respect to the scope of this thesis, a framework is developed to generate an event semantic ontology.


A default first order family weight determination procedure for WPDV models
Weighted Probability Distribution Voting (WPDV) is a newly designed machine learning algorithm, for which research is currently aimed at the determination of good weighting schemes. This paper describes a simple yet effective weight determination procedure, which leads to models that can produce competitive results for a number of NLP classification tasks.

Analysis of Users' Comments on Political Portal for Extraction of Suggestions and Opinion Mining
Online political portals contain a huge volume of users' comments. These comments express whether writer's attitude is positive, negative, or neutral towards the subject. In most of the comments, we find that there are suggestions or feedback also hidden along with the opinion. These suggestions or feedbacks from the mind of different people on various topics can potentially coin new fruitful idea or provide solution to a given problem. So, it is beneficial to determine the opinion and filter out the suggestions from comments. This helps greatly in social issues' analysis and development of people centric governance. Till date, most of the researches on sentiment analysis and suggestion mining have been done for comments related to product or services. Very less work has been done for analysis of comments related to social issues. Even recent algorithms give accuracy of 60 to 65 percent for opinion mining of social issues and a maximum of 73% accuracy for suggestion extraction. This paper focuses on evaluating the polarity of sentiment and extracting actionable key suggestions in users' comments on political portal. Current work gives 88% accuracy for suggestion extraction and approx 86% accuracy for Opinion Mining.

Abstract:This paper briefly tells about the different models used for image captioning. It also discusses how the progress in the areas of machine translation and object recognition which has eventually enhanced the performance of image captioning model greatly in current time. Implementation of models are also being discussed in this paper. In the end, standard evaluation matrices have been used to evaluate the performance of the models.


VisBERT: Hidden-State Visualizations for Transformers
Explainability and interpretability are two important concepts, the absence of which can and should impede the application of well-performing neural networks to real-world problems. At the same time, they are difficult to incorporate into the large, black-box models that achieve state-of-the-art results in a multitude of NLP tasks. Bidirectional Encoder Representations from Transformers (BERT) is one such black-box model. It has become a staple architecture to solve many different NLP tasks and has inspired a number of related Transformer models. Understanding how these models draw conclusions is crucial for both their improvement and application. We contribute to this challenge by presenting VisBERT, a tool for visualizing the contextual token representations within BERT for the task of (multi-hop) Question Answering. Instead of analyzing attention weights, we focus on the hidden states resulting from each encoder block within the BERT model. This way we can observe how the semantic representations are transformed throughout the layers of the model. VisBERT enables users to get insights about the model’s internal state and to explore its inference steps or potential shortcomings. The tool allows us to identify distinct phases in BERT’s transformations that are similar to a traditional NLP pipeline and offer insights during failed predictions.

Research on Government Integrity Evaluation Based on Big Data
Abstract:By studying the international advanced practices of government integrity evaluation represented by Standard & Poor's, Global Integrity, Transparency International and Korea, this paper found that most of the evaluations are based on perceptual surveys. Although this method of data acquisition is simple and easy to implement, it has inherent defects, such as strong subjectivity, lack of details, prone to cause sample deviation, etc. Based on the goal of improving government capacity in integrity, this paper put forward a new idea of government integrity evaluation based on big data: Firstly, collect the behavior data and complaint reporting data in the government business systems, combine with the public opinion data on the Internet. Then, use the big data technology to get the evaluation indicator data of government integrity. Finally, construct a scientific evaluation model through the credit scorecard method. Compared with the traditional measure of evaluation, the government integrity evaluation based on big data in this paper not only can obtain more accurate evaluation results, but also has great value in its evaluation process. A large amount of objective and quantitative - rather than subjective and perceived data - can better support government decision-making and reform. The scientific evaluation indicator system can also guide the specific work of the government.

Siamese Multiplicative LSTM for Semantic Text Similarity
Learning the Semantic Textual Similarity (STS) is a critical issue for many NLP tasks such as question answering, document summarization and etc.. In this paper, we combine the Multiplicative LSTM structure with a Siamese architecture which learn to project word embeddings of each sentence into a fixed-dimensional embedding space to represent this sentence. Then these sentence embeddings can be used to evaluate the STS task. We compare with several similar architectures and the proposed method has achieved better results and is competitive with the best state-of-the-art siamese neural network architecture.

Descriptive analysis of emotion and feeling in voice
Abstract:Recognition of human “emotions” or “feelings” from voice is important to research on human communications. Although there has been much research on emotions or feelings in voice, definitions of these terms have been inconsistent. We reviewed previous papers in linguistics, brain science, information science, etc. and developed specific definitions for these term. In our paper, “emotion” is defined as an involuntary reaction in the human brain; it has two states: pleasure and displeasure. “Feeling” (e.g., anger, enjoyment, sadness, fear, and distress) is defined as a state voluntarily resulting from an emotion. Here, we should notice that the pleasure-displeasure direction does not always correspond to the feeling. So, our objective is to obtain sufficient amount of voice data and to analyze the relationship between emotions and feelings. In voice recording experiments, the voice database from about 100 participants with various natural feelings was constructed. A result of descriptive analysis showed that pleasure-displeasure direction did not correspond to the each feeling in 5% of voice data. This result suggested that, if an experimental situation is constructed that tends to arouse various feelings, data with less variability can be obtained. Further analysis of the characteristics of the data obtained to identify situations in which the pleasure-displeasure direction does not necessarily correspond to the basic feeling should lead to improved accuracy of voice emotion recognition.


Computational Semantics: How to solve the suspense of supersense

Abstract:Augmented reality (AR) is an iconic topic that can be applied in different domains in modern world technology. With the rapid development of technologies, eCommerce (Online Shopping) has become closer to human life. As a result, AR was started implemented with eCommerce platforms by the developers. With the busy lives and the pandemic situation, people are limited to visiting toy stores while providing a solution. An AR-based virtual toy store is proposed with 3D Toy generation for visualizing selected toys, a Virtual tour for enhancing the remote virtual shopping experience, and an Indoor navigation system visualizing the path within large scale shopping malls are new features of the proposed system. The majority of the existing eCommerce platforms are missing image search features. As a solution, “KidLand” has implemented an image search engine, suggesting add-on-related items and nearest branches using machine learning algorithms. An intelligent chatbot uses a reinforcement learning algorithm and Natural Language Understanding (NLU) to give possible solutions regarding the toy store. As a solution to the language literacy problem, developed a chatbot that can chat both English and Sinhala languages. “Kidland” was developed to provide the users the next level of shopping experience with attractive features of AR technology with marketing and use advanced technologies overcoming the issues of ordinary eCommerce platforms. In Sri Lanka, this system has been identified as a solution for the issues with ordinary shopping platforms.



Partial Facial Identification using Transfer Learning Technique

Overview of the fourth message understanding evaluation and conference
The Fourth Message Understanding Conference (MUC-4) is the latest in a series of conferences that concern the evaluation of natural language processing (NLP) systems. These conferences have reported on progress being made both in the development of systems capable of analyzing relatively short English texts and in the definition of a rigorous performance evaluation methodology. MUC-4 was preceded by a period of intensive system development by each of the participating organizations and blind testing using materials prepared by NRaD and SAIC that are described in this paper, other papers in this volume, and the MUC-3 proceedings [1].

Abstract:Nowadays, social media has become a massive real-time source of fresh information. Which gather a large volume of data produced every day to analysts who wish to discover new opportunities in the emerging research area of mining the noisy corpus present in social media. However, the noisy and short nature of this kind of text hamper this desire: unlike structured news content, social media users often prefer communicating unconventionally with informal, and ungrammatical language using abbreviations, slang, misspelled words, or non-standard short-forms: noisy words. Under those purposes, it becomes a challenge to present new methods to boost the performance of existing systems which convert this noisy text to Standard English. Most previous work didn't encompass all kinds of casual English noise, or made a strong assumption that the best canonical candidate is the one present in the correction dictionaries. This is not realistic because one noisy word can have different meanings considering the context, the area of interest and the time period of those noisy words extraction. In this paper, we target all kinds of casual English noise neglected by state-of-art by proposing a novel weakly supervised approach. This one takes into account the context, the type of interest of those noisy words and their time period of extraction: Once the informal word is identified, we recognize its type of interest by surrounding its context. Then, we generate the best and highly ranked canonical form depending on the extraction time period of the noisy word. Finally, we normalize the noisy word. This multi-faceted, context aware, and interest sensitive approach is expected to yield more accurate results than existing state-of-art solutions.



Real-Time Sign Language Converter for Mute and Deaf People

Hands-on NLP for an interdisciplinary audience
The need for a single NLP offering for a diverse mix of graduate students (including computer scientists, information scientists, and linguists) has motivated us to develop a course that provides students with a breadth of understanding of the scope of real world applications, as well as depth of knowledge of the computational techniques on which to build in later experiences. We describe the three hands-on tasks for the course that have proven successful, namely: 1) in-class group simulations of computational processes; 2) team posters and public presentations on state-of-the-art commercial NLP applications, and; 3) team projects implementing various levels of human language processing using open-source software on large textual collections. Methods of evaluation and indicators of success are also described.


Acquiring applicable common sense knowledge from the Web

Towards applying text mining and natural language processing for biomedical ontology acquisition
The use of text mining and natural language processing can extend into the realm of knowledge acquisition and management for biomedical applications. In this paper, we describe how we implemented natural language processing and text mining techniques on the transcribed verbal descriptions from retinal experts of biomedical disease features. The feature-attribute pairs generated were then incorporated within a user interface for a collaborative ontology development tool. This tool, IDOCS, is being used in the biomedical domain to help retinal specialists reach a consensus on a common ontology for describing age-related macular degeneration (AMD). We compare the use of traditional text mining and natural language processing techniques with that of a retinal specialist's analysis and discuss how we might integrate these techniques for future biomedical ontology and user interface development.


A Deep Learning Approach for Part-of-Speech Tagging in Nepali Language

Enhancing retrieval and novelty detection for arabic text using sentence level information pattern
Abstract:Novelty detection is already used in many Natural Processing Language (NLP) applications, such as information retrieval systems, Web search engines, text summarization, question answering systems …etc. This study aims to detect novel Arabic sentence level information patterns. The Length Adjusted (LA) model is based on sentence level information patterns is used, which depends on the sentence length. Test results show a significant improvement in the performance of novelty detection for Arabic texts in terms of precision at top ranks.

Integrating Text Embedding with Traditional NLP Features for Clinical Relation Extraction
Abstract:Recently, text embedding techniques such as Word2Vec and BERT have produced state-of-the-art results in a wide variety of NLP tasks. As a result, traditional NLP features frequently used in Information Extraction (IE) such as POS tags, dependency relations and semantic types have received less attention. In this paper, we investigate whether traditional NLP features can be combined with word and sentence embeddings to improve relation extraction. We have explored diverse feature sets and different neural network architectures and evaluated our models on a benchmark clinical text dataset. Our new models significantly outperformed all the baselines on the same dataset.


A synopsis of learning to recognize names across languages

Abstract:Users are shifting to voice over message-based communications rather than typing. There exists a gap in evaluating emotional aspects of the phrases uttered. There are software analytics for sentimental analysis and other ways of measuring emotional aspects of the spoken phrases. But there is a lack of a system that evaluates secondary and primary emotions that can be used in various areas to detect human behavior. In order to facilitate this, there is a need to decipher the primary and secondary emotions of the user. This project addresses a novel idea of creating an emotional phrase map that identifies secondary and primary emotions, in addition to using machine language to train the system to identify the phrases and classify the emotions via a case study and the results analyzed.



Word mover's distance for affect detection


A Robust Methodology for Building an Artificial Intelligent (AI) Virtual Assistant for Payment Processing

Multi-class confidence weighted algorithms
The recently introduced online confidence-weighted (CW) learning algorithm for binary classification performs well on many binary NLP tasks. However, for multi-class problems CW learning updates and inference cannot be computed analytically or solved as convex optimization problems as they are in the binary case. We derive learning algorithms for the multi-class CW setting and provide extensive evaluation using nine NLP datasets, including three derived from the recently released New York Times corpus. Our best algorithm out-performs state-of-the-art online and batch methods on eight of the nine tasks. We also show that the confidence information maintained during learning yields useful probabilistic information at test time.

Tourist review analytics using complex networks
A number of techniques for Natural Language Processing (shortly, NLP) based on graph representations were developed. Usually they target a specific NLP task, such as: text summarisation, syntactic parsing, word sense disambiguation, ontology construction, sentiment and subjectivity analysis, or text clustering. In this paper we explore complex network representation of tourist reviews for extracting lexical and quantitative features of the review text. The most important contribution of our proposal consists of defining a new method for keywords extraction using Complex Network ranking metrics.


A Novel Framework for Neural Machine Translation of Indian-English Languages


Clinical Sublanguage Trend and Usage Analysis from a Large Clinical Corpus

Abstract:Automated Services composition is an active research area nowadays, most of the approaches are based in Artificial intelligence techniques. However, most of these approaches focus on specific steps of Web services composition and lacks of details for general application in broader fields such as Convergent Services. In the present work, an architecture based on Natural Language analysis for AI planning processing and automatic deployment in JSLEE is presented. The preliminary experiments show promising results.



End-to-End Joint Opinion Role Labeling with BERT

From submit to submitted via submission: on lexical rules in large-scale lexicon acquisition
This paper deals with the discovery, representation, and use of lexical rules (LRs) during large-scale semi-automatic computational lexicon acquisition. The analysis is based on a set of LRs implemented and tested on the basis of Spanish and English business- and finance-related corpora. We show that, though the use of LRs is justified, they do not come cost-free. Semi-automatic output checking is required, even with blocking and preemtion procedures built in. Nevertheless, large-scope LRs are justified because they facilitate the unavoidable process of large-scale semi-automatic lexical acquisition. We also argue that the place of LRs in the computational process is a complex issue.

Using the web as an implicit training set: application to structural ambiguity resolution
Recent work has shown that very large corpora can act as training data for NLP algorithms even without explicit labels. In this paper we show how the use of surface features and paraphrases in queries against search engines can be used to infer labels for structural ambiguity resolution tasks. Using unsupervised algorithms, we achieve 84% precision on PP-attachment and 80% on noun compound coordination.

Automatic text classification using modified centroid classifier
Abstract:This work proposes an approach to address the problem of inductive bias or model misfit incurred by the centroid classifier assumption to enhance the automatic text classification task. This approach is a trainable classifier, which takes into account tfidf as a text feature. The main idea of the proposed approach is to take advantage of the most similar training errors to the classification model to successively update it based on a certain threshold. The proposed approach is simple to implement and flexible. The proposed approach performance is measured at several threshold values on the Reuters -21578 text categorization test collection. The experimental results show that the proposed approach can improve the performance of centroid classifier.

Incremental construction of minimal acyclic finite-state automata
In this paper, we describe a new method for constructing minimal, deterministic, acyclic finite-state automata from a set of strings. Traditional methods consist of two phases: the first to construct a trie, the second one to minimize it. Our approach is to construct a minimal automaton in a single phase by adding new strings one by one and minimizing the resulting automaton on-the-fly. We present a general algorithm as well as a specialization that relies upon the lexicographical ordering of the input strings. Our method is fast and significantly lowers memory requirements in comparison to other methods.

Abstract:Emotion analysis has been attracting researchers’ attention. Most previous works in the artificial-intelligence field focus on recognizing emotion rather than mining the reason why emotions are not or wrongly recognized. The correlation among emotions contributes to the failure of emotion recognition. In this article, we try to fill the gap between emotion recognition and emotion correlation mining through natural language text from Web news. The correlation among emotions, expressed as the confusion and evolution of emotion, is primarily caused by human emotion cognitive bias. To mine emotion correlation from emotion recognition through text, three kinds of features and two deep neural-network models are presented. The emotion confusion law is extracted through an orthogonal basis. The emotion evolution law is evaluated from three perspectives: one-step shift, limited-step shifts, and shortest path transfer. The method is validated using three datasets: 1) the titles; 2) the bodies; and 3) the comments of news articles, covering both objective and subjective texts in varying lengths (long and short). The experimental results show that in subjective comments, emotions are easily mistaken as anger. Comments tend to arouse emotion circulations of love–anger and sadness–anger. In objective news, it is easy to recognize text emotion as love and cause fear–joy circulation. These findings could provide insights for applications regarding affective interaction, such as network public sentiment, social media communication, and human–computer interaction.


Abstract:The sentiment analysis is an emerging research area where vast amount of data are being analyzed, to generate useful insights in regards to a specific topic. It is an effective tool which can serve governments, corporations and even consumers. Text emotion recognizing lays a key role in this framework. Researchers in the fields of natural language processing (NLP) and machine learning (ML) have explored a variety of methods to implement the process with highest accuracy possible. In this paper the Long Short-Term Memory (LSTM) classifier is used for analyzing sentiments of the IMDb movie reviews. It is based on the Recurrent Neural Network (RNN) algorithm. The data is effectively preprocessed and partitioned to enhance the post classification performance. The classification performance is studied in terms of accuracy. Results show a best classification accuracy of 89.9%. It confirms the potential of integrating the designed solution in modern text based sentiments analyzers.



Spatial nonlinear effects with higher order modes in LiNbO3 waveguide arrays

Predicting System for the Behavior of Consumer Buying Personal Car Decision by Using SMO
Abstract:Current consumer behavior traders are used to create decision-making tools for entrepreneurs to produce products or services that meet consumer needs. For this reason, therefore focusing on data analysis using SMO techniques for predict system for the behavior of consumer buying personal car decision based on a total of 1,110 data obtained, with a total of 6 relevant car trading information features, consisting of income customers, type of car, down payment/ cash booking, decision results requires that the answer in the forecast is divided into two classes, namely the class of buying cars and not buying cars When dividing the data into 50% training set and using 50% test set, 555 training set will be used and 555 test sets can be used to enter the learning and testing process. By random 50/50 in the selection of algorithms to be tested and the accuracy rate is 95.13%.


A Natural Language Processing Method of Chinese Instruction for Multi-legged Manipulating Robot


An error driven method to improve rules for the recognition of Chinese modality “LE”

Abstract:Parts-of-Speech(POS) tagging is the technique to assign each word in a sentence as an individual part of speech. POS tagging is the first important step in Natural Language Processing applications (NLP). In some languages, POS tagging works well with higher accuracy, but in the Bengali language, it is still an unsolved problem. The Bengali language is much ambiguous and inflectional, where every word has many more variants based on their suffixes and prefixes. Although developing POS tagging is not new for the Bengali language, we aim to make a highly accurate model with a minimal dataset. Here we developed a deep learning model, and it is mainly based on suffixes, which are parts of Bengali grammar. Moreover, we did experiment with a Bengali corpus that contains 2927 words with their corresponding parts of speech tags. The accuracy of our proposed POS tagging deep learning model is 93.90%. We also included this model as a python package to our open-source Bengali Natural language processing toolkit (BNLTK), which is now live on pipy.org.


IR meets NLP: On the Semantic Similarity between Subject-Verb-Object Phrases
Measuring the semantic similarity between phrases and sentences is an important task in natural language processing (NLP) and information retrieval (IR). We compare the quality of the distributional semantic NLP models against phrase-based semantic IR. The evaluation is based on the correlation between human judgements and model scores on a distributional phrase similarity task. We experiment with four NLP and two IR model variants. On the NLP side, models vary over normalization schemes and composition operators. On the IR side, models vary with respect to estimation of the probability of a term being in a document, namely P(t|d) where only term co-occurrence information is used and P(t|d, sim) which incorporates term distributional similarity. A mixture of the two methods is presented and evaluated. For both methods, word meanings are derived from large corpora of data: the BNC and ukWaC. One of the main findings is that grammatical distributional models give better scores than the IR models. This suggests that an IR model enriched with distributional linguistic information performs better in the long standing problem in IR of document retrieval where there is no direct symbolic relationship between query and document concepts.

A Web-based unsupervised algorithm for learning transliteration model to improve translation of low-frequency proper names
Abstract:In machine translation, cross-language information retrieval, and cross-language question answering, the problems of unknown term translation are difficult to be solved. Although we have proposed several effective Web-based term translation extraction methods exploring Web resources to deal with translation of frequent Web query terms. However, many low-frequency unknown terms are still difficult to be translated by using our previous Web-based term translation extraction methods. Therefore, in this paper we propose a two-stage hybrid translation extraction method, which is composed of our pervious Web-based term translation extraction method and a new Web-based transliteration method to improve translation of low-frequency unknown proper names. Additionally, to construct a good quality transliteration model, we also present a Web-based unsupervised learning algorithm to automatically collect diverse English-Chinese transliteration pairs from the Web. Experimental results showed that our new method can make great improvements for translation of unknown proper names.


Introducing inference-driven OWL ABox enrichment

An approximate approach for training polynomial kernel SVMs in linear time
Kernel methods such as support vector machines (SVMs) have attracted a great deal of popularity in the machine learning and natural language processing (NLP) communities. Polynomial kernel SVMs showed very competitive accuracy in many NLP problems, like part-of-speech tagging and chunking. However, these methods are usually too inefficient to be applied to large dataset and real time purpose. In this paper, we propose an approximate method to analogy polynomial kernel with efficient data mining approaches. To prevent exponential-scaled testing time complexity, we also present a new method for speeding up SVM classifying which does independent to the polynomial degree d. The experimental results showed that our method is 16.94 and 450 times faster than traditional polynomial kernel in terms of training and testing respectively.

Abstract:In this paper we describe a very preliminary conversational system focused on the automated diagnosis of heart conditions. The system uses a relational Ngram model to extract meaning from user queries and answers, and maintains a belief network consisting of 48 different cardiac conditions, symptoms, and other internal system parameters. The end result is a system that can process an initial user query, ask pertinent questions, and to deduce reasonable medical conclusions. Although several example output conversations are shown, the medical accuracy of the diagnosis is not the focus of this paper. Instead this paper focuses on the system level infrastructure and methodology for building a medical conversation system.



Natural Language Processing for Social Media: Second Edition

Abstract:This paper suggests a methodology which is aimed to translate the Chinese terms into Korean. Our basic idea is to use a method which combines domain-tuned dictionary and target language corpus. The translation system used in our work operated at three steps: (1) tokenization of the term candidate; (2) translation of term candidates using domain-tuned bilingual lexicon; (3) ranking the translation candidate to recommend the best translation according to the frequency of the target word cooccurrence in the corpus. And we have demonstrated the effectiveness of our approach by showing a high degree of accuracy evaluation. Our experiments indicate a significant improvement when measured against un-tuned Chinese-Korean MT system.


Medical Concept Extraction using Smartphone and Natural Language Processing Techniques (poster)
Over the past few decade, smartphone technology has played a vital role in the healthcare domain. In this paper, we proposed a methodology to allow end users to automatically process the medical text image using a camera or enter text manually. Then the system output allows to extract medical concepts, its semantic type and entity type from medical text apply Natural Processing Language techniques from UMLS medical dictionary. The medical text can be a health report, a clinical case, or other kinds of a text containing medically related information. The aim of this kind of methodology of the mobile application is to contribute in the area of natural language processing (NLP), intelligent system and to increase the medical students and bioinformatics researchers interest to quickly accessing information about medical data.

Web as Corpus Supporting Natural Language Generation for Online River Information Communication
Web as corpus for NLP has been popular, and we now employed web as corpus for NLG, and made the online communication of tailored river information more effective and efficient. Evaluation and analysis shows that our generated texts were comparable to those written by domain experts and experienced users.

A novel dependency based word-level reordering model for phrased-based translation
Abstract:Phrase based statistic MT (SMT) is an important milestone in MT. However, the translation model in the phrase based SMT is structure free which limits its reordering capacity to some extent. In order to enhance the reordering capacity of phrase based SMT, in this paper we propose a head-modifier relation based reordering model, which exploits the way how to utilize the structured linguistic analysis information in source language. Within very small size of reordering model, we enhance the performance of the phrase based SMT significantly.

Abstract:Currently, the topic of natural language processing, morphological analysis, is a new emerging topics in the world. The main idea of morphological analysis may allow the identification of the literal origin of two words either root or seed and achieve the comparison to the original word. This paper describes a new idea related to standard miniaturization approach for Arabic morphological analysis. This approach based on regular noun morphological automaton technique (RNMT). The proposed algorithm focuses on a morphological regular abstractive noun (singular masculine) with length 3, 4 and 5 letters, which in turn, produce a special type of morphological miniaturization that is based on the concept of world analysis and restructure according to morphine scales. This aspect contribute to develop the Arabic morphological miniaturization, by developing the standardization aspect. It can be exploited by NLP applications like syntactic and semantic analysis, information retrieval, machine translation and orthographical correction. The result reveals that the proposed algorithm is anew attempt in Arabic language) Morphological analysis (and it is perfectly work on research sample, however, doesn't recognize between the enter words if it noun or verb.


Abstract:The prediction task in Natural Language Processing intends to figure out the missing characters, letter, word, expression, or sentence that conceivable follows in a given fragment of a book. Since the beginning of NLP, numerous frameworks with various techniques were produced for various dialects. The missing content prediction is one among the significant concerns of Natural Language Processing. Moreover, most of the text prediction related tasks are conducted in different dialects but not in Malayalam. Over these years though having some of the best classics, historical records and many more to keep the world interested with, the Malayalam was lacking many of the advantages that any other language process in the digital world. This is because there are only a few standard models that could fit into the Malayalam. In this paper, an attempt is made to fit the BERT into Malayalam. A Malayalam pre-trained model will be creating and implementing some of its applications with the model and discussing its scope and areas of development to be addressed and future research scope.


Multilingual document clustering: an heuristic approach based on cognate named entities
This paper presents an approach for Multilingual Document Clustering in comparable corpora. The algorithm is of heuristic nature and it uses as unique evidence for clustering the identification of cognate named entities between both sides of the comparable corpora. One of the main advantages of this approach is that it does not depend on bilingual or multilingual resources. However, it depends on the possibility of identifying cognate named entities between the languages used in the corpus. An additional advantage of the approach is that it does not need any information about the right number of clusters; the algorithm calculates it. We have tested this approach with a comparable corpus of news written in English and Spanish. In addition, we have compared the results with a system which translates selected document features. The obtained results are encouraging.

Grammatical analysis in the OVIS spoken-dialogue system
We argue that grammatical processing is a viable alternative to concept spotting for processing spoken input in a practical dialogue system. We discuss the structure of the grammar, the properties of the parser, and a method for achieving robustness. We discuss test results suggesting that grammatical processing allows fast and accurate processing of spoken input.

The role of argumentation in online epistemic communities: the anatomy of a conflict in Wikipedia
Motivation -- This research aims to investigate the processes by which knowledge objects --- in this case Wikipedia pages on astronomy --- are elaborated, in online communities, focussing on the role of argumentative interactions. Research approach -- We articulate qualitative multidimensional analysis of online discussions, in relation to elaboration of Wikipedia pages, with automatic semantic and syntactic Natural Language Processing (NLP) analysis focussed on identifying the roles of dialogical argumentation processes. Findings/Design -- Knowledge objects in online communities are jointly shaped by socio-relational and epistemic processes. Research limitations/Implications -- Our analysis method, based on previous research, is presently restricted to in-depth analysis of a small number of discussions. In ongoing work, our objective is to apply the method to the whole corpus of the Wikipedia astronomy online epistemic community. Originality/Value -- Our qualitative analysis approach distinguishes multiple functions of dialogue applying to diverse contents (task, interlocutor-related), in relation to automatic NLP analysis. Take away message -- The way that online epistemic communities function goes beyond knowledge-based discussion and argumentation, to involve negotiation of competencies of so-called 'experts' and 'amateurs'.


COCLUBERT: Clustering Machine Learning Source Code

Abstract:Here we discuss our initial efforts towards unsupervised clustering of a large-scale Web tables dataset. We improve our previous approach of weakly-supervised clustering, where an operator would provide a few descriptive keywords to generate an entity-identifying classifier, which is applied to the corpora to form a cohesive entity-centric cluster [1]. Here, we make a next step towards fully unsupervised algorithm by automatically generating these descriptive keywords. These keywords then can be used to generate high-precision training data and train a classifier to form a cluster. Here, we describe and evaluate this new unsupervised keyword generation algorithm and apply it to a large-scale Web tables corpus to form initial small high-precision clusters.


Abstract:Human psychology has always influenced by others suggestion and reviews. Our reviews about something are very much influenced by other's reviews, and whenever we need to make a decision or solution, we often seek out other's reviews, so people are excited to know other's reviews for their profit that is why automated sentiment analysis systems are required. This Survey Paper is describes basics about sentiment analysis, polarity types, sentiment types, sentiment documentation types, levels of sentiment, General Flow Diagram of System, and also different Approaches and Related Work for better understanding about the system.


Abstract:Social media is gaining huge attention in today’s era due to its increased accessibility by individuals. Wherein, a mere tweet can reflect upon an individual’s right to say and express any sort of political, emotional, intellectual views disregarding the fact of how impactful of an impression it leads on the minds of society. Social media nowadays has become a solid platform for young minds to make an interaction with the outside world without having to understand the consequences. Therefore, it can have its pros and cons. Twitter is one such social media site used by people to ventilate their sentiments by posting messages popularly called "Tweets". The sentiments expressed over tweets can be categorized into positive, negative, and neutral. Thus, our model reflects on the sentiment analysis using NLP and various Machine Learning classifiers. Herein, we have compared various ML classifiers using the dynamic dataset which brings novelty in our findings.


Construction of English composition support system based on conveying emotion
Abstract:With the increased demand for English communication, various styles of learning support methods have been proposed and provided to the Japanese learners. However, there are still many learners finding it hard to read, write and speak in English. Regardless of language difference, understanding the other's intention and emotional status accurately and expressing what they think or feel to the others appropriately are the important abilities for actual communication. This paper focuses on the linguistic performance ability especially to communicate emotion and introduce our prototype system that help describe the emotion in English.


Structuralization of Digestive Endoscopic Report Based on NLP

Evaluating Surprise Adequacy for Question Answering
With the wide and rapid adoption of Deep Neural Networks (DNNs) in various domains, an urgent need to validate their behaviour has risen, resulting in various test adequacy metrics for DNNs. One of the metrics, Surprise Adequacy (SA), aims to measure how surprising a new input is based on the similarity to the data used for training. While SA has been evaluated to be effective for image classifiers based on Convolutional Neural Networks (CNNs), it has not been studied for the Natural Language Processing (NLP) domain. This paper applies SA to NLP, in particular to the question answering task: the aim is to investigate whether SA correlates well with the correctness of answers. An empirical evaluation using the widely used Stanford Question Answering Dataset (SQuAD) shows that SA can work well as a test adequacy metric for the question answering task.

A Web Metric Collection and Reporting System
The web genre classification distinguishes between pages by means of their features such as style, presentation layout, etc rather than on the topic; improving search results returned to the user by providing genre class of a web page apart from topic. Hence, if a user is able to specify the genre of search like Help, FAQ, Wikipedia etc, chances of getting results in accordance to his interest are high. The classification of web pages into genre is a challenging task as the information is semi-structured, heterogeneous and dynamic. Therefore it is required to find appropriate features which describes web page in the context of genre to increase the genre classification and accuracy of the search result. In this paper we propose a Metric Collection and Reporting System (MCRS) for Web Application, an automated tool designed to collect 126 significant attributes of web pages for genre identification. MCRS collects and reports important style, presentation layout, form, linguistic, lexical and meta-content features of web page. It collects 88 HTML tags metrics clustered in five groups namely Text formatting tags, Document Structure Tags, External object tags, instruction tags and Navigation Tags. MCRS also reports thirty-eight text metrics including punctuation metrics to describe the lexical attributes of the web page. The NLP module has also been integrated into the system to identify linguistic properties of the web content. The MRCS can be used in parallel with topic search to increase the quality of information retrieval through web genre identification.

Fourth workshop on exploiting semantic annotations in information retrieval (ESAIR)
There is an increasing amount of structure on the Web as a result of modern Web languages, user tagging and annotation, and emerg- ing robust NLP tools. These meaningful, semantic, annotations hold the promise to significantly enhance information access, by enhancing the depth of analysis of today's systems. Currently, we have only started exploring the possibilities and only begin to un- derstand how these valuable semantic cues can be put to fruitful use. Unleashing the potential of semantic annotations requires us to think outside the box, by combining the insights of natural lan- guage processing (NLP) to go beyond bags of words, the insights of databases (DB) to use structure efficiently even when aggregating over millions of records, the insights of information retrieval (IR) in effective goal-directed search and evaluation, and the insights of knowledge management (KM) to get grips on the greater whole. This workshop aims to bring together researchers from these dif- ferent disciplines and work together on one of the greatest chal- lenges in the years to come. The desired result of the workshop will be to gain concrete insight into the potential of semantic an- notations, and in concrete steps to take this research forward; to synchronize related research happening in NLP, DB, IR, and KM, in ways that combine the strengths of each discipline; and to have a lively, interactive workshop where every participant contributes actively and which inspires attendees to think freely and creatively, working towards a common goal.


A semi-automatic extraction of the SERB in machine translation based on SL


Feature vector quality and distributional similarity

Abstract:Text classification plays a vital role today especially with the intensive use of social networking media. Recently, different architectures of convolutional neural networks have been used for text classification in which one-hot vector, and word embedding methods are commonly used. This paper presents a new language independent word encoding method for text classification. The proposed model converts raw text data to low-level feature dimension with minimal or no preprocessing steps by using a new approach called binary unique number of word “BUNOW.” BUNOW allows each unique word to have an integer ID in a dictionary that is represented as a k-dimensional vector of its binary equivalent. The output vector of this encoding is fed into a convolutional neural network (CNN) model for classification. Moreover, the proposed model reduces the neural network parameters, allows faster computation with few network layers, where a word is atomic representation the document as in word level, and decrease memory consumption for character level representation. The provided CNN model is able to work with other languages or multi-lingual text without the need for any changes in the encoding method. The model outperforms the character level and very deep character level CNNs models in terms of accuracy, network parameters, and memory consumption; the results show total classification accuracy 91.99% and error 8.01% using AG's News dataset compared to the state of art methods that have total classification accuracy 91.45% and error 8.55%, in addition to the reduction in input feature vector and neural network parameters by 62% and 34%, respectively.


Abstract:Natural Language processing (NLP) is a field of computer science and linguistics concerned with the interactions between computers and human (natural) languages. Ambiguity is one of these problems which have been a great challenge for computational linguists. This paper concentrates on the problem of target word selection in Myanmar to English machine translation, for which the approach is directly applicable. However this system can only solve the ambiguities of verbs in Myanmar-English translations. This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naïve Bayesian classifiers, each of which based on lexical features. Moreover nouns are detail classified in our system. In this paper, we propose a framework to solve ambiguous verb problems. Our system will support to improve the accuracy of the Myanmar to English translation.


Underspecification of 'meaning': the case of Russian imperfective aspect
One main problem for NLP applications is that natural language expressions are underspecified and require enrichments of different sorts to get a truth-conditional interpretaton in context. Underspecification applies on two levels: what is said underdetermines what is meant, and linguistic meaning underspecifies what is said. One instance of this phenomenon is aspect in Russian, especially the imperfective one. It gives rise to a variety of readings, which are difficult to capture by one invariant meaning. Instead, the imperfective aspect is sense-general; its meaning has to be specified in the course of interpretation by contextual cues and pragmatic inferences. This paper advocates an account of the different imperfective readings in terms of pragmatic principles and inferential heuristics based on, and supplied by, a semantic skeleton consisting of a 'selectional theory' of aspect. This framework might serve as basis for a rule-guided derivation of aspectual readings in Russian.


Semantic-based Relationship between Objective Interestingness Measures in Association Rules Mining

TIEVis: a Visual Analytics Dashboard for Temporal Information Extracted from Clinical Reports
Clinical reports, as unstructured texts, contain important temporal information. However, it remains a challenge for natural language processing (NLP) models to accurately combine temporal cues into a single coherent temporal ordering of described events. In this paper, we present TIEVis, a visual analytics dashboard that visualizes event-timelines extracted from clinical reports. We present the findings of a pilot study in which healthcare professionals explored and used the dashboard to complete a set of tasks. Results highlight the importance of seeing events in their context, and the ability to manually verify and update critical events in a patient history, as a basis to increase user trust.

Abstract:Social media platforms have become one of the most powerful tools for organizations and individuals to publish news and express thoughts or feelings. With the increasingly enormous number of internet users in Saudi Arabia, the need raised to analyze Arabic posts. Since the emergence of COVID-19 in the latest 2019, it lefts economies and businesses counting the cost while governments fight the spread of the virus with new compartmentalization measures. Keeping in view the importance of quick and timely data analysis and sharing for policy actions, Artificial intelligence (AI) has played a crucial role in facilitating the exchange of views and information between scientists and decision-makers during the Coronavirus pandemic, and they continue to do so. This work mined to these content-related tweets to see how people’s feelings and expressions are changing. The results of this analysis can be used with integration with several IoT technologies to reduce the impact of covid-19 and drive new decisions in this field. For this goal, we proposed a Machine Learning (ML) models that can classify both of the sentiment and topic of Modern Standard Arabic (MSA) tweets and achieve high accuracy results.


An investigation into the validity of some metrics for automatically evaluating natural language generation systems
There is growing interest in using automatically computed corpus-based evaluation metrics to evaluate Natural Language Generation (NLG) systems, because these are often considerably cheaper than the human-based evaluations which have traditionally been used in NLG. We review previous work on NLG evaluation and on validation of automatic metrics in NLP, and then present the results of two studies of how well some metrics which are popular in other areas of NLP (notably BLEU and ROUGE) correlate with human judgments in the domain of computer-generated weather forecasts. Our results suggest that, at least in this domain, metrics may provide a useful measure of language quality, although the evidence for this is not as strong as we would ideally like to see; however, they do not provide a useful measure of content quality. We also discuss a number of caveats which must be kept in mind when interpreting this and other validation studies.

Distributed parse mining
We describe the design and implementation of a system for data exploration over dependency parses and derived semantic representations in a large-scale NLP-based search system at powerset.com. Because of the distributed nature of the document repository and the processing infrastructure, and also the complex representations of the corpus data, standard text analysis tools such as grep or awk or language modeling toolkits are not applicable. This paper explores the challenges of extracting statistical information and of building language models in such a distributed NLP environment, and introduces a corpus analysis system, Oceanography, that simplifies the writing of analysis code and transparently takes advantage of existing distributed processing infrastructure.

Researches on algorithm for confidence evaluation and decision modification of SVM
Abstract:In this paper, a new algorithm to estimate confidence measure of support vector machine (SVM) is presented. The algorithm computes the distance from testing sample to the optimal hyperplane of SVM, and the probability that the testing sample and its k nearest neighbors belong to the same class as the decision of Libsvm for the testing sample. The algorithm rejects the classification results of samples whose confidence measures are smaller than the threshold corresponding to a given rejection rate. Experiments show that the performance of the SVM classifier has been improved using this algorithm.


A clustering retrieval system of Chinese information

T-LQG: Closed-loop belief space planning via trajectory-optimized LQG
Abstract:Planning under motion and observation uncertainties requires the solution of a stochastic control problem in the space of feedback policies. In this paper, by restricting the policy class to the linear feedback polices, we reduce the general (n

Abstract:Alzheimer's Disease (AD) is one of the most common forms of neuropsychological disorder in elderly people. It is a slow progressive disease affecting the brain cells. This affects the cognitive abilities of people and their daily activities. During the course of the disease, memory gets brutally affected too. Working as well as long-term declarative memory deteriorates in AD patients. Due to this deterioration of the memory, AD patients tend to show a decline in their communicative skills as well. This decline is reflected in their speech. AD patients usually have poor grammar along with very low coherent ideas. Also, they tend to repeat the words very often and hence become unclear on the message they are trying to convey. As the disease progresses, the speech is completely impaired, and the patients are left to sing or utter words that are totally out of context. Stopwords are the words that are most commonly used in language and it is often hypothesized that AD patients use them much often as compared to Control Normal (CN) subjects. It is seen that due to the degeneration of brain cells in AD patients, they have a tendency to use a lot of stopwords to fill their perplexities in their statements. In this paper, the usefulness of the stopwords in capturing the linguistic information of the patients suffering from AD are discussed. Learning algorithms are evaluated by including stopwords and dropping stopwords at preprocessing to draw comparisons.


A tool for automated revision of grammars for NLP systems
We present an algorithm and a tool for automatically revising grammars for natural language processing (NLP) systems to disallow specifically identified sentences or sets of sentences. We also outline an approach for automatically revising attribute value grammars using counter-examples. Developing grammars for NLP systems that are both general enough to accept most sentences about a domain, but constrained enough to disallow other sentences is very tedious. Our approach of revising grammars automatically using counter-examples greatly simplifies the development and revision of tightly constrained grammars. We have successfully used our tool to constrain over-generalizing grammars of speech understanding systems and obtained higher recognition accuracy.

Twitter in mass emergency: what NLP techniques can contribute
We detail methods for entity span identification and entity class annotation of Twitter communications that take place during times of mass emergency. We present our motivation, method and preliminary results.


The GENIA corpus: an annotated research abstract corpus in molecular biology domain

Ontological semantics, formal ontology, and ambiguity
Ontological semantics is a theory of meaning in natural language

Function-as-a-Service Application Service Composition: Implications for a Natural Language Processing Application
Serverless computing platforms provide Function-as-a-Service (FaaS) to end users for hosting individual functions known as microservices. In this paper, we describe the deployment of a Natural Language Processing (NLP) application using AWS Lambda. We investigate and study the performance and memory implications of two alternate service compositions. First, we evaluate a switchboard architecture, where a single Lambda deployment package aggregates all of the NLP application functions together into a single package. Second, we consider a service isolation architecture where each NLP function is deployed as a separate FaaS function decomposing the application to run across separate runtime containers. We compared the average runtime and processing throughput of these compositions using different pre-trained network weights to initialize our neural networks to perform inference. Additionally, we varied the workload dataset sizes to evaluate implications of inferencing throughput for our NLP application deployed to a FaaS platform. We found our switchboard composition, that shares FaaS runtime containers for all application tasks, produced a 14.75% runtime performance improvement, and also a 17.3% improvement in NLP processing throughput (samples/second). These results demonstrate the potential for careful application service compositions to provide notable performance improvements and ultimately cost savings for application deployments to serverless FaaS platforms.

Abstract:This research paper is intended to propose a technique to solve the task of image-based question answering using EfficientDet and Bidirectional LSTM (BiLSTM). Visual Question Answering (VQA) requires a combination of Image recognition and Natural Language Processing (NLP) techniques. The proposed technique uses EfficientDet for image processing and BiLSTM for question processing. The model acts efficiently because of efficient image processing. The model takes an image and a question as input processes them independently, then fuses the processed versions and predicts the answer to the question. It outputs the result in an open domain based on input questions on the image. This study intends to help the visually impaired person. The model achieves similar performance as its contemporary models on the benchmark VQA dataset.


Why NLP should move into IAS
The paper introduces the ways in which methods and resources of natural language processing (NLP) can be fruitfully employed in the domain of information assurance and security (IAS). IAS may soon claim a very prominent status both conceptually and in terms of future funding for NLP, alongside or even instead of established applications, such as machine translation (MT). After a brief summary of theoretical premises of NLP in general and of ontological semantics as a specific approach to NLP developed and/or practiced by the authors, the paper reports on the interaction between NLP and IAS through brief discussions of some implemented and planned NLP-enhanced IAS systems at the Center for Education and Research in Information Assurance and Security (CERIAS). The rest of the paper deals with the milestones and challenges in the future interaction between NLP and IAS as well as the role of a representational, meaning-based NLP approach in that future.

IR, NLP, AI and UFOS: or IR-relevance, natural language problems, artful intelligence and user-friendly online systems
User Friendly Online Searching is examined in the context of Natural Language Processing in Information Retrieval and Artificial Intelligence. Opportunities for synergetic R & D are identified as the basis for Intelligent Information Retrieval and Artificial Retrieval Intelligence.


3D Foreground Point Segmentation from Background using Centroid-based Min-Cut Method

Survey on Text-Based Sentiment Analysis of Bengali Language
Abstract:Digital contents in Bengali text are increasing with the advancements of World Wide Web (WWW) to express the sentiment of content originators from mixed perspectives. These high voluminous unstructured web contents can be utilized to build smarter tools to assist people through Natural Language Processing (NLP). Though Bengali NLP tools are still inadequate due to its inherent complexities, research on Sentiment Analysis in Bengali is flourishing as a challenging area and is getting researcher’s attention at a rapid pace. To do research in this area, researchers spend a lot of their time to investigate previously published works to understand the advancements for further enhancement; one of the most stressful and challenging parts of research. In this regard, we are going to demonstrate domain specific available pre-processing steps along with existing research methodologies, respective datasets and evaluation metrics of notable works related to sentiment analysis of Bengali language as our research finding. Moreover, we also incorporate a series of directions for future researchers to augment their research in this exciting domain.

A maximum entropy-based sentence simplifier for machine translation
Abstract:We present a method for removing unnecessary words from sentences to expedite automatic machine translation. Our hypothesis is that the resulting simplified sentences are easier to automatically translate, giving improved translation performance. We evaluate the sentence simplifier in two ways. Firstly the system is tested directly against humans in the word deletion task. The output of our system is evaluated against a set of reference sentences and its performance compared to a test set of human-shortened sentences. We show the system is able to perform at close to human performance on this task. Secondly we evaluate the system when used as a preprocessor to two different machine translation systems. We show that we are able to significantly improve the performance of a machine translation (MT) system based on the publicly available GIZA++ software by pre-processing the input, and make a small improvement to the performance of the more capable ATR translation system.

Enriching Electronic Program Guides using semantic technologies and external resources
Abstract:Electronic Program Guides (EPGs) describe broadcast programming information provided by TV stations. However, users may obtain more information when these guides have been enriched. The main contribution of this work is to present an automation process for EPG's information enrichment through the use of semantic technologies and external resources. Among the several resources involved in the enrichment process, the following can be mentioned : ontologies, web services, semantic repositories and natural language processing techniques.

Text Imbalance Handling and Classification for Cross- platform Cyber-crime Detection using Deep Learning
Abstract:Cyberbullying has become a very prevalent issue in recent times. Not just qualifying this as a women issue, starting from politicians to a ten-year-old kid, every person is being bullied on any social platform. It is highly essential to build an artificial intelligence based model that detects the presence of bullying in cross-platform posts. However, textual datasets which are useful for model generation are highly imbalanced in nature. In this paper, we propose two main methods to handle textual data imbalancing which are synonym replacement and artificial data generation using generative adversarial neural networks. We present a systematic analysis of our approaches using a convolutional neural network classifier. Our work shows how removing data imbalance with generative adversarial network techniques before classification improves the overall performance of the model.

Sentence Meaning Representations Across Languages: What Can We Learn
                    from Existing Frameworks?

Stanford parser based approach for extraction of Link- Context from non-descriptive Anchor-Text
Abstract:Link Context Analysis has been widely explored for determining the context of the target web page. But most of the researchers have only considered descriptive or meaningful anchor text and left the undiscriptive anchor text. By researching the World Wide Web it is analyzed that a good percentage of web pages can be reached by following the undescriptive anchor text. So an algorithm has been proposed and implemented for Link context determination (LCD) to determine the context of non-descriptive anchor text in this paper. In this work non-descriptive anchor text are mainly considered for Link Context determination. A corpus of different web pages belonging to a common domain has been considered first. Then the pages were manually analyzed and relation between the anchor text and the words in its vicinity were discovered. Certain numbers of rules were formed and represented in the form of a tree, based upon these relationships. In our proposed and implemented architecture for LCD we have used three components(1) Stanford parser (2) Rules (3) Link Context Determination. The input sentence is given to the Stanford parser which creates a parse tree for the read sentence. This tree is then used by the link context determiner along with the appropriate rules tree to determine the link context. The proposed approach has been implemented and validated by considering limited samples of non-descriptive ATs. The results have shown that, the proposed LCD has extracted 100% actual link-context of each considered non-descriptive Anchor Text (AT's).

Abstract:We describe computational treatments of archival collections through a case study of World War II Japanese-American Incarceration Camps. Camp staff and police officers compiled so-called "internal security" reports relating to alleged cases of "disorderly conduct, assault, theft, loss of property, and accidents" in the camps, and an index to these reports comprising over 25,000 index cards to the reports. The sheer size of these collections is pushing archivists and researchers to consider new forms of processing for collections at scale. We discuss novel digital treatments that involve (1) processing incident cards at scale in an automated way, (2) preserving and documenting the record series and their operations using an Archival Information Package (AIP), and (3) developing social network-based archiving, analysis, and visualization of the col-lection. Since this type of project involves multiple partnerships, agreements, and social interactions with community groups and public-sector organizations, we discuss the possibility of the concept of sociotechnical archival systems as an implication of our approach.



Adding External Features to Convolutional Neural Network for Aspect-based Sentiment Analysis

A reranking method for syntactic parsing with heterogeneous treebanks
Abstract:In the field of natural language processing (NLP), there often exist multiple corpora with different annotation standards for the same task. In this paper, we take syntactic parsing as a case study and propose a reranking method which is able to make direct use of disparate treebanks simultaneously without using techniques such as treebank conversion. The method proceeds in three steps: 1) build parsers on individual treebanks; 2) use parsers independently to generate n-best lists for each sentence in test set; 3) rerank individual n-best lists which correspond to the same sentence by using consensus information exchanged among these n-best lists. Experimental results on two open Chinese treebanks show that our method significantly outperforms the baseline system by 0.84% and 0.53% respectively.

Citizen Participation and Machine Learning for a Better Democracy
The development of democratic systems is a crucial task as confirmed by its selection as one of the Millennium Sustainable Development Goals by the United Nations. In this article, we report on the progress of a project that aims to address barriers, one of which is information overload, to achieving effective direct citizen participation in democratic decision-making processes. The main objectives are to explore if the application of Natural Language Processing (NLP) and machine learning can improve citizens’ experience of digital citizen participation platforms. Taking as a case study the “Decide Madrid” Consul platform, which enables citizens to post proposals for policies they would like to see adopted by the city council, we used NLP and machine learning to provide new ways to (a) suggest to citizens proposals they might wish to support; (b) group citizens by interests so that they can more easily interact with each other; (c) summarise comments posted in response to proposals; and (d) assist citizens in aggregating and developing proposals. Evaluation of the results confirms that NLP and machine learning have a role to play in addressing some of the barriers users of platforms such as Consul currently experience.CCS concepts: • Human-centred computing→Collaborative and social computing • Computing methodologies→Artificial intelligence→Natural language processing

Fast, scalable and reliable generation of controlled natural language
In this paper we describe a natural language generation system which takes as its input a set of assertions encoded as a semantic graph and outputs a data structure connecting the semantic graph to a text which expresses those assertions, encoded as a TAG syntactic tree. The scope of the system is restricted to controlled natural language, and this allows the generator to work within a tightly restricted domain of locality. We can exploit this feature of the system to ensure fast and efficient generation, and also to make the generator reliable by providing a rapid algorithm which can exhaustively test at compile time the completeness of the linguistic resources with respect to the range of potential meanings. The system can be exported for deployment with a minimal build of the semantic and linguistic resources that is verified to ensure that no run-time errors will result from missing resources. The framework is targeted at using natural language generation technology to build semantic web applications where machine-readable information can be automatically expressed in natural language on demand.

DocEng'19 Competition on Extractive Text Summarization
The DocEng'19 Competition on Extractive Text Summarization assessed the performance of two new and fourteen previously published extractive text sumarization methods. The competitors were evaluated using the CNN-Corpus, the largest test set available today for single document extractive summarization.


Learning in Text Streams: Discovery and Disambiguation of Entity and Relation Instances


Universal information retrieval system in semantic Web environment

Abstract:Word Sense Disambiguation (WSD) aims to identify the correct sense of a word in a given sentence. WSD is considered to be an open and AI-hard problem in Natural Language Processing (NLP). WSD is most important in many applications like Machine Translation (MT), Information Retrieval (IR), Information Extraction (IE), Text mining, and Lexicography etc. Supervised, Semi-supervised and Unsupervised Approaches to WSD are important and successful learning approaches. In this paper, we proposed the graph-based Unsupervised Word Sense Disambiguation Algorithm to resolve the ambiguity of a word in a given HINDI Language sentence. Finding the proper meaning of a word here implies identification of the most important node from the set of graph nodes which are representing the senses. We make use of HINDI wordnet developed at IIT Bombay as reference library of words to form the sense graph.


Report on the first summer school on NLP and IR in Beijing
The first Summer School on NLP and IR has been organized in China in July 2011 just before SIGIR conference. This report describes the event.


Bringing active learning to life

Abstract:In common machine learning methods, there is a basic assumption that training data and test data are sampled from the same distribution. However, this assumption is commonly violated in practical fields. The situation where the training and test data are generated from different distributions is so-called covariate shift. In natural language processing, it is highly possible to occur covariate shift due to the size of sample space. Natural language data have theoretically infinite size, which causes that the distribution of training data can not reflect that of entire data. In this paper, we try to verify that the performance of methods on natural language processing can be improved by reducing error from covariate shift. For this purpose, we propose the importance weighted k-NN for base noun detection. In the proposed method, the weights are set as a difference between the training and test distribution. Theoretically, the performance under covariate shift can be improved using importance weight method. In the experiment, the proposed method shows better performance than normal k-NN.


Syntactic correlations of prosodic phrase in broadcasting news speech
Abstract:The interrelation between prosody and syntax becomes more and more important in speech processing. This paper is intended to analyze the syntactic correlations of prosodic phrase in broadcasting news speech. The research results in the followings: Firstly, the C-PP, which there is a stable prosodic pattern of pitch contour within its rhythmic chunking, has a flexible syntactic structure and stable semantic expression. Secondly, we find that the syntactic structure is more complex than the prosodic structure, and some conjunction and particle more likely attached to the end of left structure or the beginning of right one and form a prosodic word. If it has just four lexical words including the conjunction or particle they form a prosodic word by itself. That is to say, it has very great flexibility in prosodic structures for conjunctions and particles.

Abstract:The main drawback of unsupervised approaches in Natural Language Processing (NLP) is often their low accuracy. Nevertheless, they remain a practical shortcut to accommodate a language that lacks theorization and/or computerization. The present article reports an unsupervised identification of morphemes boundaries. Based on an intuitive yet formal definition of event dependence, the approach's input is no more than a plain text. Tests on two languages of totally different families of morphology: Arabic and English show a very acceptable precision and recall. A deeper refinement of the output allowed 89% precision and 78% recall on Arabic. While most of the NLP research works published on the Arabic, focus on the Modern Standard Arabic (MSA), we managed to build a traditional Standard Arabic data set to test our approach.


A web based Punjabi to Hindi Statistical Machine Translation System
Abstract:Machine translation (MT) system translates one natural language into another language with the help of computers. MT is a key application in the field of natural language processing. In proposed system, Statistical Machine Translation (SMT) approach has been used for developing Punjabi to Hindi Machine Translation System. In SMT, every sentence in the target language i.e. Hindi is a translation of the source language i.e. Punjabi with some probability and the best translation will be of high probability which the system will attain in the form of sentence. The key activities involved during translation process are pre-processing, translation engine and post processing. Unigram algorithm, N-Gram string matching algorithm etc formed the basis for solving these issues. The accuracy of the system has been evaluated using subjective tests i.e. intelligibility test and accuracy test. A proposed system is found to perform better than the existing system.

A prototype application for real-time recognition and disambiguation of clinical abbreviations
To save time, healthcare providers frequently use abbreviations while authoring clinical documents. Nevertheless, abbreviations that authors deem unambiguous often confuse other readers, including clinicians, patients, and natural language processing (NLP) systems. Most current clinical NLP systems "post-process" notes long after clinicians enter them into electronic health record systems (EHRs). Such post-processing cannot guarantee 100% accuracy in abbreviation identification and disambiguation, since multiple alternative interpretations exist. In this paper, authors describe a prototype system for real-time Clinical Abbreviation Recognition and Disambiguation (CARD) -- i.e., a system that interacts with authors during note generation to verify correct abbreviation senses. The CARD system design anticipates future integration with web-based clinical documentation systems to improve quality of healthcare records. The prototype application embodies three word sense disambiguation (WSD) methods. We evaluated the accuracy and response times of the prototype CARD system in a simulated study. Using an existing test data set of 25 commonly observed, highly ambiguous clinical abbreviations the evaluation demonstrated that the best WSD method had an accuracy of 88.8%, and a reasonable average response time of 1.6 milliseconds per each abbreviation. The study indicates potential feasibility of real-time NLP-enabled abbreviation disambiguation within clinical documentation systems.

Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2019)
The deluge of scholarly publication poses a challenge for scholars find relevant research and policy makers to seek in-depth information and understand research impact. Information retrieval (IR), natural language processing (NLP) and bibliometrics could enhance scholarly search, retrieval and user experience, but their use in digital libraries is not widespread. To address this gap, we propose the 4th Joint Workshop on BIRNDL and the 5th CL-SciSumm Shared Task. We seek to foster collaboration among researchers in NLP, IR and Digital Libraries (DL), and to stimulate the development of new methods in NLP, IR, recommendation systems and scientometrics toward improved scholarly document understanding, analysis, and retrieval at scale.


Trajectory planning for automated driving based on ordinal optimization

Abstract:With the advancement of natural language understanding (NLU), coreference resolution, as a main part of information extraction (IE), has received much more attention. For this task, either rule-based or statistics-based methods cannot meet the needs of large-scale text processing. An integrated method based on decision tree for Chinese pronominal coreference is proposed. The basic idea is filter rules could, to some degree compensate the drawback of decision tree that ignoring the relationship between attributes. The performance of the proposed method is evaluated on Chinese Treebank. In our experiments, the attributes and coreferences are manually labeled, and then the filter rules are utilized to feature vectors following the decision tree of C4.5 algorithm. The success rate is 82.59%, in which the rate of personal pronouns and demonstrative pronouns are 87.60% and 75.21% respectively.


Rule-based extraction of experimental evidence in the biomedical domain: the KDD Cup 2002 (task 1)
Below we describe the winning system that we built for the KDD Cup 2002 Task 1 competition. Our system is a Rule-based Information Extraction (IE) system. It combines pattern matching, Natural Language Processing (NLP) tools, semantic constraints based on the domain and the specific task, and a post-processing stage for making the final curation decision based on the various evidence (positive and negative) found within the document. Development and implementation were made using the DIAL IE language and the ClearLab development environment. The results achieved were significantly superior than those achieved using categorization approaches.

Abstract:The precision of the answer is now essential for a question answering system, because of the large amount of free texts on the Internet. Attempting to achieve a high precision, we propose a question answering system supported by case grammar theory and based on VerbNet frames. It extracts the syntactic, thematic and semantic information from the question to filter out unmatched sentences in semantic level and to extract answer chunk (a phrase or a word that can answer the question) from the answer sentence. VerbNet is applied in our system to detect the verb frames in question and candidate sentences, so that the syntactic and thematic information as well as semantic information can be therefore obtained. Our question answering system works well especially for answering factoid questions. The experiments show that our approach is able to filter out semantically unmatched sentences effectively and therefore rank the correct answer (s) higher in the result list.


Deep Learning on Graphs for Natural Language Processing
There are a rich variety of NLP problems that can be best expressed with graph structures. Due to the great power in modeling non-Euclidean data like graphs, deep learning on graphs techniques (i.e., Graph Neural Networks (GNNs)) have opened a new door to solving challenging graph-related NLP problems, and have already achieved great success. Despite the success, deep learning on graphs for NLP (DLG4NLP) still faces many challenges (e.g., automatic graph construction, graph representation learning for complex graphs, learning mapping between complex data structures).   This tutorial will cover relevant and interesting topics on applying deep learning on graph techniques to NLP, including automatic graph construction for NLP, graph representation learning for NLP, advanced GNN based models (e.g., graph2seq, graph2tree, and graph2graph) for NLP, and the applications of GNNs in various NLP tasks (e.g., machine translation, natural language generation, information extraction and semantic parsing). In addition, hands-on demonstration sessions will be included to help the audience gain practical experience on applying GNNs to solve challenging NLP problems using our recently developed open source library -- Graph4NLP, the first library for researchers and practitioners for easy use of GNNs for various NLP tasks.


Generic and domain-specific aspects of the Waxholm NLP and dialog modules


Variational inference for grammar induction with prior knowledge

#mytweet via Instagram: Exploring User Behaviour across Multiple Social Networks
We study how users of multiple online social networks (OSNs) employ and share information by studying a common user pool that use six OSNs -- Flickr, Google+, Instagram, Tumblr, Twitter, and YouTube. We analyze the temporal and topical signature of users' sharing behaviour, showing how they exhibit distinct behaviorial patterns on different networks. We also examine cross-sharing (i.e., the act of user broadcasting their activity to multiple OSNs near-simultaneously), a previously-unstudied behaviour and demonstrate how certain OSNs play the roles of originating source and destination sinks.

Abstract:Language transliteration is one of the important area in natural language processing. Accurate transliteration of named entities plays an important role in the performance of machine translation and cross-language information retrieval processes. The transliteration model must be design in such a way that the phonetic structure of words should be preserve as closely as possible. This paper addresses the problem of transliterating English to Kannada language using a publicly available structured output Support Vector Machines (SVM). The proposed transliteration scheme uses sequence labeling method to model the transliteration problem. This transliteration technique was demonstrated for English to Kannada Transliteration and achieved exact Kannada transliterations for 87.28% of English names.


Applying Restrained Likelihood and Floating TMR to Multi-Speaker Identification for Co-channel Speech
Abstract:In this paper, a multi-speaker identification system for co-channel speech is proposed. By using constrained likelihood and floating TMR method, this system can identify two speakers on co-channel speech with high accuracy.

Analysis and parsing of unstructured cyber-security incident data: poster
The latest threat intelligence platforms use structured protocols to share and analyze cyber-security data. However, most of this data is reported to the platform in the form of unstructured text such as social media posts, emails, and news articles, which then require manual conversion to structured form. In order to bridge the gap between unstructured and structured data, we propose to implement a natural-language-processing-(NLP)-based information extraction (IE) system that takes texts within the cyber-security domain and parses them into structured format. Our approach targets the VERIS format and makes use of the VERIS Community Database as a source of unstructured texts---primarily consisting of news articles-and their structured counterparts (VERIS reports). We propose first to use a supervised machine learning (ML) classifier to discriminate between cyber-related and non-cyber-related texts, and then to use ML classifiers decide which VERIS parameters are relevant in a given text. Then, we propose to use NLP and IE techniques to extract tuples of grammatically co-dependent words. Finally, these tuples will be passed to a domain- and field-specific IE components to fill in different fields of an output VERIS report.


Arabic Question Answering System for Information Retrieval on Large-scale Image Objects


Empirically Analyzing and Evaluating Security Features in Software Requirements

Exploring variations across biomedical subdomains
Previous research has demonstrated the importance of handling differences between domains such as "newswire" and "biomedicine" when porting NLP systems from one domain to another. In this paper we identify the related issue of subdomain variation, i.e., differences between subsets of a domain that might be expected to behave homogeneously. Using a large corpus of research articles, we explore how subdomains of biomedicine vary across a variety of linguistic dimensions and discover that there is rich variation. We conclude that an awareness of such variation is necessary when deploying NLP systems for use in single or multiple subdomains.

Identifying idealised vectors for emotion detection using CMA-ES
Detecting the emotional content of text is one of the most popular NLP tasks. In this paper, we propose a new methodology based on identifying "idealised" words that capture the essence of an emotion; we define these words as having the minimal distance (using some metric function) between a word and the text that contains the relevant emotion (e.g. a tweet, a sentence). We look for these words through searching the space of word embeddings using CMA-ES. Our method produces state of the art results, surpassing classic supervised learning methods.

Abstract:Yellow journalism has become a buzzword for everyone nowadays. Increasing use of internet and social media makes people more vulnerable to fake news. To gain popularity and to have profit through clickbait news publisher and social media circulate fake news to deceive people by creating interesting content of a specific topic. The spread of falsified news has become severe in recent times throughout the world. Though recently some existing system is made to classify and to detect fake news for English news article, not much work has been reported for Bengali news. In this work, we consider Bengali fake news classification considering South Asian Context. More than 200 million people speak Bengali and their way of communication is Bengali. In our Bengali fake news classification system, data mining algorithm is used to classify fake and real news. We have also introduced web interface based on our classifier to check whether a news article written in Bengali language fake or real. The classification model has 85% accuracy with random forest classifier.



Computational Stylometry and Machine Learning for Gender and Age Detection in Cyberbullying Texts

Abstract:Background: Qualitative technical communication research often produces datasets that are too large to manage effectively with hand-coded approaches. Text-mining methods, used carefully, may uncover patterns and provide results for larger datasets that are more easily reproduced and scaled. Research questions: 1. To what degree can hand collection results be replicated by automated data collection? 2. To what degree can hand-coded results be replicated by machine coding? 3. What are the affordances and limitations of each method? Literature review:We introduce the stages of data collection and analysis that researchers typically discuss in the literature, and show how researchers in technical communication and other fields have discussed the affordances and limitations of hand collection and coding versus automated methods throughout each stage. Research methodology: We utilize an existing dataset that was hand-collected and hand-coded. We discuss the collection and coding processes, and demonstrate how they might be replicated with web scraping and machine coding. Results/discussion: We found that web scraping demonstrated an obvious advantage of automated data collection: speed. Machine coding was able to provide comparable outputs to hand coding for certain types of data; for more nuanced and verbally complex data, machine coding was less useful and less reliable. Conclusions: Our findings highlight the importance of considering the context of a particular project when weighing the affordances and limitations of hand collecting and coding over automated approaches. Ultimately, a mixed-methods approach that relies on a combination of hand coding and automated coding should prove to be the most productive for current and future kinds of technical communication work, in which close attention to the nuances of language is critical, but in which processing large amounts of data would yield significant benefits as well.



Investigating the Use of Code Analysis and NLP to Promote a Consistent Usage of Identifiers

Abstract:Media bias can strongly impact the individual and public perception of news events. One difficult-to-detect, yet powerful form of slanted news coverage is bias by word choice and labeling (WCL). Bias by WCL can occur when journalists refer to the same concept, yet use different terms, which results in different sentiments being sparked in the readers, such as the terms "economic migrants" vs. "refugees." We present an automated approach to identify bias by WCL that employs models and manual analysis approaches from the social sciences, a research domain in which media bias has been studied for decades. This paper makes three contributions. First, we present NewsWCL50, the first open evaluation dataset for the identification of bias by WCL consisting of 8,656 manual annotations in 50 news articles. Second, we propose a method capable of extracting instances of bias by WCL while outperforming state-of-the-art methods, such as coreference resolution, which currently cannot resolve very broadly defined or abstract coreferences used by journalists. We evaluate our method on the NewsWCL50 dataset, achieving an F1=45.7% compared to F1=29.8% achieved by the best performing state-of-the-art technique. Lastly, we present a prototype demonstrating the effectiveness of our approach in finding frames caused by bias by WCL.



OntoBAeval: Ontology Based Automatic Evaluation of Free-Text Response

Re-ranking algorithms for name tagging
Integrating information from different stages of an NLP processing pipeline can yield significant error reduction. We demonstrate how re-ranking can improve name tagging in a Chinese information extraction system by incorporating information from relation extraction, event extraction, and coreference. We evaluate three state-of-the-art re-ranking algorithms (MaxEnt-Rank, SVMRank, and p-Norm Push Ranking), and show the benefit of multi-stage re-ranking for cross-sentence and cross-document inference.

BE: a search engine for NLP research
Many modern natural language-processing applications utilize search engines to locate large numbers of Web documents or to compute statistics over the Web corpus. Yet Web search engines are designed and optimized for simple human queries---they are not well suited to support such applications. As a result, these applications are forced to issue millions of successive queries resulting in unnecessary search engine load and in slow applications with limited scalability.

Effective Distributed Representation of Code-Mixed Text
Abstract:As an increasing number of people embrace social media, mining data generated from the same has become an important task. Possible applications range from opinion mining, sentiment analysis to hate speech detection. More importantly, analyzing code-mixed multilingual text has gained popularity due to the reason that it holds important socio-cultural clues that may be lost in translation. Methods to effectively analyse code-mixed Hindi/English(Hinglish) text have been explored in this paper. Firstly, we generate a large scale code-mixed corpus that would aid in further research of code mixed text on social media. High-quality word embeddings are trained on this code-mixed text. Finally, we demonstrate the efficacy of our proposed method by training machine learning models that improve upon the previous state-of-the-art using a much lighter and explainable architecture. Our main intention behind training the classifier model was not only high performance but also good model explainability and speed.

The "whiteboard" architecture: a way to integrate heterogeneous components of NLP systems
We present a new software architecture for NLP systems made of heterogeneous components, and demonstrate an architectural prototype we have built at ATR in the context of Speech Translation.

Towards terascale knowledge acquisition
Although vast amounts of textual data are freely available, many NLP algorithms exploit only a minute percentage of it. In this paper, we study the challenges of working at the terascale. We present an algorithm, designed for the teraxale, for mining is-a relations that achieves similar performance to a state-of-the-art linguistically-rich method. We focus on the accuracy of these two systems as a function of processing time and corpus size.

Abstract:The project aims for a machine translation system for augmenting Kannada educational video by providing a translation to Indian Sign Language (ISL) using virtual animation approach along with the subtitles. The system accepts Kannada educational video clips as input and output is delivered as 3D character animation. The first phase approach of the proposed system makes use of SpeechRecognition like kaldi an open source for converting the input video files to their respective text which are used to generate the subtitles and these texts are further pre-processed in order to meet the sign language norms. The right SiGML file for these pre-processed words is picked from the database. These SiGML files consist of XML tags which correspond to the respective HamNoSys Notations. HamNoSys is an alphabetic system that mostly phonetically describes signs. A synthetic animation of a human in the tool in the URL APP, JA SiGML takes input as these SiGML files and generates signs of the respective pre-processed words. Synchronization of inputted video with the sign language and the subtitle generation is vital for the clear and clean flow. This system is Tested by professional sign language trainer/interpreter for the sign language generated by the avatar and is found effective. The accuracy of sign language shown was found out to be 63%.



An intelligent searching model based on data integration and its query algorithms


Which portland is it?: a machine learning approach

∗
 lattice search algorithm and m-gram probability estimation. When full-form words are out-of-vocabulary (OOV), the system utilizes a second layer, which factorizes each Arabic word into its possible morphological constituents (prefix, root, pattern and suffix), then uses m-gram probability estimation and A


Sentiment Analysis using Feature Extraction and Dictionary-Based Approaches

Abstract:This paper reports the development and evaluation of a natural language generation system which generates a variety of language expressions from visual information taken by a CCD camera. The feature of this system is to generate a variety of language expressions from combinations of different syntactic structures and different sets of vocabulary, while managing the generation process based on the user-designated viewpoints. The system converts the visual information into a concept dependency structure using a semantic representation framework proposed by Takagi and Itoh. The system then transforms the structure and divides it into a set of words, deriving a word dependency structure, which is later arranged into a sentence. The transformation of a concept dependency structure and the variation in word segmentation allow the system to generate a variety of sentences from the same visual information. In this paper, we employ user-designated viewpoints to scenes containing more than one object. We designed the parameters of the user-designated viewpoints which enable the system to manage the generation process and to generate a variety of expressions. An evaluation has confirmed that the system generates certain variations according to parameter values set by the user. The variations include expressions referring to attribute values of the objects in the scenes and relative expressions denoting the relations between the targeted object and others.



A Stochastic Arabic Diacritizer Based on a Hybrid of Factorized and Unfactorized Textual Features

Augmenting the automated extracted tree adjoining grammars by semantic representation
Abstract:MICA is a fast and accurate dependency parser for English that uses an automatically LTAG derived from Penn Treebank (PTB) using the Chen's approach. However, there is no semantic representation related to its grammar. On the other hand, XTAG grammar is a hand crafted LTAG that its elementary trees were enriched with the semantic representation by experts. The linguistic knowledge embedded in the XTAG grammar caused it to being used in wide variety of natural language applications. However, the current XTAG parser is not as fast and accurate as well as the MICA parser. Generating an XTAG derivation tree from a MICA dependency structure could make a bridge between these two notions and gets the benefits of both models. Also, by having this conversion, the applications that use the XTAG parser, may get the helps from MICA parser too. In addition, it can enrich the MICA's grammar by semantic representation of XTAG grammar. In this paper, an unsupervised sequence tagger that maps any sequence of MICA elementary trees onto an XTAG elementary trees sequence is presented. The proposed sequence tagger is based on a Hidden Markov Model (HMM) proceeded by an EM-based algorithm for setting its initial parameters values. The trained model is tested on a part of PTB and about 82% accuracy for the detected sequences is achieved.

Abstract:The E-learning system is a virtual environment; hence, its supporting tools of study supervision are not strong. Although previous researchers strove for the enhancement of learning activities and collaborations but the achieved results were often sparse. In this paper we propose a method to enhance the efficiency of activities of learners. Namely, we are proposing an automatic multi-dimensional assessment (M-DA) on free text answers. In addition, we created an online environment in such a way that the learners can assess and collaborate with others. The results obtained are reassessed by the system; therefore, learners always actively study through the proposed system. The approach was applied to assess two groups of learners at the same level in an e-course. The experiment indicates that the proposed system overcome the system without M-DA.


Leveraging hidden dialogue state to select tutorial moves
A central challenge for tutorial dialogue systems is selecting an appropriate move given the dialogue context. Corpus-based approaches to creating tutorial dialogue management models may facilitate more flexible and rapid development of tutorial dialogue systems and may increase the effectiveness of these systems by allowing data-driven adaptation to learning contexts and to individual learners. This paper presents a family of models, including first-order Markov, hidden Markov, and hierarchical hidden Markov models, for predicting tutor dialogue acts within a corpus. This work takes a step toward fully data-driven tutorial dialogue management models, and the results highlight important directions for future work in unsupervised dialogue modeling.

Amazighe Named Entity Recognition using a A rule based approach
Abstract:The Named Entity Recognition (NER) is very important task revolving around many natural language processing applications. However, many studies have achieved the maturity stage in a variety of languages such as English and French, but a few limited research efforts have attacked the named entity recognition problem in Amazighe script. This is due to the resources scarcity for Amazighe named entities and the limited work made in Amazighe natural language processing in general. In this paper, we describe our attempt at the development and implementation of a named entity recognition system for the Amazighe Language using a rule based approach. Our system has been effectively evaluated using our own corpus; it reached acceptable results in terms of precision, recall, and F-measure.

 waveguides fabricated by the combined proton and copper exchange. It has been shown, that holographic recording with guided beams can be more effectively used, comparatively to recording with external beams, for fabrication of micro-Bragg reflectors in the photorefactive LiNbO
3


A web service for automatic word class acquisition


Might a semantic lexicon support hypertextual authoring?

Abstract:Word embedding has gained a lot of attention in recent years. To learn the systematic relation between words from a large junk of unlabeled text data is quite popular these days while using in Machine Translation, Question Answering or even in Text summarization. Computer understand only 0’s and 1’s now can understand text by simply using word embedding and making vectors in corpus i.e. word vector to represent them in numerical form which are used to perform linear algebra on the words which are now in numerical form in vectors. Word embedding is very effective tool in natural language processing (NLP), both CBOW (Continuous bag of words) and SKIP GRAM have increased the efficiency of word embedding along with it both models have reduced the training time. This paper is divided into two parts. In the first part we have discussed what is word embedding along with two models to implement word embedding, these models are continuous bag of word and skip gram. Our objective is to implement these two-model using shallow neural networks. In second part we have compared the efficiency of both the model while using different size of data sets.


An ontology-based approach for key phrase extraction
Automatic key phrase extraction is fundamental to the success of many recent digital library applications and semantic information retrieval techniques and a difficult and essential problem in Vietnamese natural language processing (NLP). In this work, we propose a novel method for key phrase extracting of Vietnamese text that exploits the Vietnamese Wikipedia as an ontology and exploits specific characteristics of the Vietnamese language for the key phrase selection stage. We also explore NLP techniques that we propose for the analysis of Vietnamese texts, focusing on the advanced candidate phrases recognition phase as well as part-of-speech (POS) tagging. Finally, we review the results of several experiments that have examined the impacts of strategies chosen for Vietnamese key phrase extracting.


Applying Deep Learning Approach to Targeted Aspect-based Sentiment Analysis for Restaurant Domain

Abstract:Ontological representation of knowledge has the advantage of being easy to reason with, but ontology construction with knowledge facts, automatically acquiring them from open domain text is often challenging. This research introduces a novel approach to infer new ontological knowledge in a fully automated manner. Such ontological knowledge can be utilized in both constructing new ontologies and extending existing ontologies. Basic level triples that can be extracted from open domain text are used as the data source for this study. A simple mechanism has been introduced to convert the triple into an ontological knowledge fact and such ontological knowledge facts are further processed to infer new ontological knowledge. The main focus of this research is to infer new ontological knowledge using an advanced term-clustering mechanism followed by an intra-cluster permutation generation task. Generated permutations are potential to be selected as good ontological knowledge facts. Inferred ontological knowledge was tested with inter-rater agreement method with high reliability and variability. Results demonstrated that, out of 43,103 triples, this method inferred 127,874 ontological knowledge (approximately 3 times) of which 66% were estimated to be effective. Finally, this research contributes a reliable approach which requires a single pass over the corpus of triples to infer a large number of ontological knowledge facts that can be used to construct/extend ontologies.


Understanding the Representational Power of Neural Retrieval Models Using NLP Tasks
The ease of constructing effective neural networks has resulted in a large number of varying architectures iteratively improving performance on a task. Due to the nature of these models being black boxes, standard weight inspection is difficult. We propose a probe based methodology to evaluate what information is important or extraneous at each level of a network. We input natural language processing datasets into a trained answer passage neural network. Each layer of the neural network is used as input into a unique classifier, or probe, to correctly label that input with respect to a natural language processing task, probing the internal representations for information. Using this approach, we analyze the information relevant to retrieving answer passages from the perspective of information needed for part of speech tagging, named entity retrieval, sentiment classification, and textual entailment. We show a significant information need difference between two seemingly similar question answering collections, and demonstrate that passage retrieval and textual entailment share a common information space, while POS and NER information is used only at a compositional level in the lower layers of an information retrieval model. Lastly, we demonstrate that incorporating this information into a multitask environment is correlated to the information retained by these models during the probe inspection phase.


Program synthesis using natural language

Abstract:Textual entailment is a generic task aiming to capture semantic inference between two text fragments. It has been applied so far to improve many natural language applications such as Question Answering and Information Extraction for the English language. This work aims to exploit results obtained for Textual Entailment in machine translation evaluation to Arabic. We show that the output of a system built for textual entailment recognition for Arabic can be used to serve as a metric to evaluate machine translation into Arabic. Despite the simplicity of the concepts and techniques., this metric has realized competitive results in comparison to state-of-the-art tools in terms of correlation with human judgments.



Continuous natural language processing pipeline strategy


A Low-Latency and Low-Cost Montgomery Modular Multiplier Based on NLP Multiplication


Specific appearing of stimulated Raman scattering in dyed multiple scattering media

The Wisdom of the Gaming Crowd
In this paper, we report on three projects in which we are applying natural language processing techniques to analyse video game reviews. We present our process, techniques, and progress for extracting and analysing player reviews from the gaming platform Steam. Analysing video game reviews presents great opportunity to assist players to choose games to buy, to help developers to improve their games, and to aid researchers in further understanding player experience in video games. With limited previous research that specifically focuses on game reviews, we aim to provide a baseline for future research to tackle some of the key challenges. Our work shows promise for using natural language processing techniques to automatically identify features, sentiment, and spam in video game reviews on the Steam platform.

Abstract:In this paper, the trajectory optimization of alpine skiing is studied based on Radau pseudospectral method (RPM). First, the kinetic model of alpine skiing is established by Newton's second law, then the boundary constraints are considered, and the trajectory optimization of alpine skiing is converted into the optimal control problem which takes the minimum time consumption as the cost function. Finally, the optimal control problem is transcribed to a nonlinear programming problem (NLP) by RPM, and the optimal solution is obtained. Simulation results show that RPM can effectively generate a continuous optimal path meet the requirement of accuracy.


Natural language processing at Battelle-Columbus
Natural Language Processing (NLP) research has evolved into a strong research thrust at Battelle's Columbus Laboratories. Backed by a Battelle-wide corporate commitment to Artificial Intelligence (Al) technology and supported by a substantial volume of contract research, Battelle's NLP research is actively seeking solutions to man-machine interface problems through the use of innovative methodologies and technologies.

Dialogue strategies for improving the usability of telephone human-machine communication
Interactions with spoken language systems may present breakdowns that are due to errors in the acoustic decoding of user utterances. Some of these errors have important consequences in reducing the naturalness of human-machine dialogues. In this paper we identify some typologies of recognition errors that cannot be recovered during the syntactico-semantic analysis, but that may be effectively approached at the dialogue level. We will describe how non-understanding and the effects of misrecognition are dealt with by Dialogos, a realtime spoken dialogue system that allows users to access a database of railway information by telephone. We will discuss the importance of supporting confirmation turns, and clarification and correction sub-dialogues. We will show the positive effects of robust dialogue management and dialogue state dependent language modeling, by taking into account both the recognition and understanding performance, and the success rate of dialogue transactions.


Use relative weight to improve the kNN for unbalanced text category

taka at the FinSBD-3 task: Tables and Figures Extraction using Object Detection Techniques
FinSBD-3 is a shared task organized in the context of the 1st workshop on Financial Technology on the Web. The task focuses on extracting the entire structure of noisy PDF financial documents that include 1) sentences, lists, items, and organization of lists and items; 2) figures and tables; 3) headers and footers. This paper describes the approach that allows us to extract the figures and tables using their visual cues. We applied the object segmentation techniques in image processing to detect the location of figures and tables in the PDF files. A post-processing method is then executed in order to find exact content. The result shows the potential of this approach.

A new yardstick and tool for personalized vocabulary building
The goal of this research is to increase the value of each individual student's vocabulary by finding words that the student doesn't know, needs to, and is ready to learn. To help identify such words, a better model of how well any given word is expected to be known was created. This is accomplished by using a semantic language model, LSA, to track how every word changes with the addition of more and more text from an appropriate corpus. We define the "maturity" of a word as the degree to which it has become similar to that after training on the entire corpus. An individual student's average vocabulary level can then be placed on the word-maturity scale by an adaptive test. Finally, the words that the student did or did not know on the test can be used to predict what other words the same student knows by using multiple maturity models trained on random samples of typical educational readings. This detailed information can be used to generate highly customized vocabulary teaching and testing exercises, such as Cloze tests.

NLP Based on Twitter Information: A Survey Report
Abstract:With the further development of Natural language processing (NLP), the analysis of text has been widely applied in a variety of industries. Among them, Twitter, as an online comment publishing platform, has great research potentials. Predictions on Twitter in various industries have become increasingly relevant to scholars. However, the combination of NLP and Twitter still lacks detailed descriptions and reviews. In this paper, NLP and Twitter are studied, and their combinations in a wide range of applications are also analyzed. Twitter and its advantages as NLP corpus are introduced as well. Besides, the history of NLP development as well as its recent tendency of application and development are fully discussed. Meanwhile, the recent applications of NLP and its combination with Twitter, as well as the classification of the combination, are listed. The detailed reports have shown that the application and advantage of the combination of Twitter and NLP are very significant. In future work, the combination of Twitter data analysis and deep learning methods should be taken into consideration.


SVM Based Part of Speech Tagger for Malayalam


Research on Text Classification Method Based on PTF-IDF and Cosine Similarity

Teaching NLP/CL through games: the case of parsing
This paper advocates the use of games in teaching NLP/CL in cases where computational experiments are impossible because the students lack the necessary skills. To show the viability of this approach, three games are described which together teach students about the parsing process. The paper also shows how the specific game formats and rules can be tuned to the teaching goals and situations, thus opening the way to the creation of further teaching games.

Supporting annotation layers for natural language processing
We demonstrate a system for flexible querying against text that has been annotated with the results of NLP processing. The system supports self-overlapping and parallel layers, integration of syntactic and ontological hierarchies, flexibility in the format of returned results, and tight integration with SQL. We present a query language and its use on examples taken from the NLP literature.

Abstract:Self-Organizing Maps (SOMs) are a good method to cluster and visualize large collections of text documents, but they are computationally expensive. In this paper, we investigate ways to use natural language parsing of the texts to remove unimportant terms from the usual bag-of-words representation, to improve efficiency. We find that reducing the document representation to just the heads of noun and verb phrases does indeed reduce the heavy computational cost without degrading the quality of the map, while more severe reductions which focus on subject and object noun phrases degrade map quality.



Social Media Analytics for Smart Health

Introduction to special issue on machine learning approaches to shallow parsing
This article introduces the problem of partial or shallow parsing (assigning partial syntactic structure to sentences) and explains why it is an important natural language processing (NLP) task. The complexity of the task makes Machine Learning an attractive option in comparison to the handcrafting of rules. On the other hand, because of the same task complexity, shallow parsing makes an excellent benchmark problem for evaluating machine learning algorithms. We sketch the origins of shallow parsing as a specific task for machine learning of language, and introduce the articles accepted for this special issue, a representative sample of current research in this area. Finally, future directions for machine learning of shallow parsing are suggested.

Summarising legal texts: sentential tense and argumentative roles
We report on the SUM project which applies automatic summarisation techniques to the legal domain. We pursue a methodology based on Teufel and Moens (2002) where sentences are classified according to their argumentative role. We describe some experiments with judgments of the House of Lords where we have performed automatic linguistic annotation of a small sample set in order to explore correlations between linguistic features and argumentative roles. We use state-of-the-art NLP techniques to perform the linguistic annotation using XML-based tools and a combination of rule-based and statistical methods. We focus here on the predictive capacity of tense and aspect features for a classifier.

An Empirical Study on Assessing the Quality of Use Case Metrics
Use cases are generally meant to describe the functional requirements of a software system. However, the use of some Natural Language (NL) text may inherently introduce language and interpretation related issues. Several tools and techniques have been proposed and available to assess the quality of use case specification, however, often performed manually. The precise and automated way of analyzing the quality of use cases in different aspects is a need due to volatile functionalities and rapid change in requirements. In this paper, we report the results of two separate experimental studies conducted, a replication of one another, to evaluate the significance and relevance of the use case quality assessment metrics. Our results revealed redundancies among the parameters associated with the quality measures and suggested modifications on the formulation of use case metrics which in turn make them complete, correct and consistent. Subsequently, we develop a tool support to automatically analyze the quality of use case specification on the basis of experimentally validated metrics.

Abstract:The study aims to employ sentiment analysis on Twitter data to determine the behavioral aspects of the ICC(International Cricket Council)’ s top-ranked ODI male all-rounders. The majority of the sportsmen share their thoughts and personal life events on social media platforms like Twitter. We identify some similarities and differences on several sentiment parameters. In addition, we present some insights into sentiments via some multidimensional analyses. Furthermore, the dispositions of those players are being studied and retained for future references, and a systematic way is being explored with a view to examining cricketers’ Twitter activities. This research includes a total of 56889 tweets from the verified Twitter account of 26 ODI all-rounders. We use the Big 5 personality attributes as one of the sentiment analysis methodologies that provide us with some pertinent findings of the pro-social sentiments of top-ranked all-rounders.


Testing the effect of morphological disambiguation in dependency parsing of Basque
This paper presents a set of experiments performed on parsing Basque, a morphologically rich and agglutinative language, studying the effect of using the morphological analyzer for Basque together with the morphological disambiguation module, in contrast to using the gold standard tags taken from the treebank. The objective is to obtain a first estimate of the effect of errors in morphological analysis and disambiguation on the parsers. We tested two freely available and state of the art dependency parser generators, MaltParser, and MST, which represent the two dominant approaches in data-driven dependency parsing.

Discriminative learning over constrained latent representations
This paper proposes a general learning framework for a class of problems that require learning over latent intermediate representations. Many natural language processing (NLP) decision problems are defined over an expressive intermediate representation that is not explicit in the input, leaving the algorithm with both the task of recovering a good intermediate representation and learning to classify correctly. Most current systems separate the learning problem into two stages by solving the first step of recovering the intermediate representation heuristically and using it to learn the final classifier. This paper develops a novel joint learning algorithm for both tasks, that uses the final prediction to guide the selection of the best intermediate representation. We evaluate our algorithm on three different NLP tasks -- transliteration, paraphrase identification and textual entailment -- and show that our joint method significantly improves performance.

A Study On Malayalam Machine Translation
Malayalam is one of the classical Dravidian languages in India, mainly a spoken language in the state of Kerala. As an interaction between computers and the Malayalam language, a number of works have been published. This paper describes the various works in the field of Malayalam machine translation and also deals with the issue associated with the same. Surely the paper will be beneficial to the new researchers in the area of NLP especially in Malayalam machine translation.

Abstract:The HIV/AIDS (Human Immunodeficiency Virus/Acquired Immuno-Deficiency Syndrome) pandemic is still prominent in the global South. However, the proper messaging concerning this pandemic is still a challenge; especially in resource-limited countries. Fortunately, the use of Information and Communication Technology (ICT) can assist in resolving this problem. In this paper, we propose a Natural Language Processing (NLP) tool called BERTina, which can be used as an HIV/AIDS question answering tool. The utility of the BERTina tool is underlined by the robustness of this tool against adversarial questions.


 vowel in classical Arabicrdquo. The first, second, third, and fourth formant values in this vowel are investigated by using more than one Quranic recitation and more than one narrator among those people who recited The Holly Quran perfectly. This vowel is analyzed in both time and frequency domains and acoustically compared with basic Arabic vowels. The result of this analysis and investigation will facilitate Arabic speech processing tasks such as vowel and speech recognition and classification.


Evaluating the meaning of answers to reading comprehension questions a semantics-based approach
There is a rise in interest in the evaluation of meaning in real-life applications, e.g., for assessing the content of short answers. The approaches typically use a combination of shallow and deep representations, but little use is made of the semantic formalisms created by theoretical linguists to represent meaning. In this paper, we explore the use of the underspecified semantic formalism LRS, which combines the capability of precisely representing semantic distinctions with the robustness and modularity needed to represent meaning in real-life applications. We show that a content-assessment approach built on LRS outperforms a previous approach on the CREG data set, a freely available corpus of answers to reading comprehension exercises by learners of German. The use of such a formalism also readily supports the integration of notions building on semantic distinctions, such as the information structuring in discourse, which we show to be useful for content assessment.


A Research on Length Based Sentence Alignment for Chinese-English Parallel Corpus

Detailed Study of Deep Learning Models for Natural Language Processing
Abstract:Natural Language Processing involves computational processing, and understanding of human languages. With the increase in computation power, deep learning models are being used for various NLP tasks. Further availability of large datasets of various languages enables the training of deep learning models. Multiple processing layers are used by the deep learning methods for learning representations of data which are hierarchical in nature and which gives excellent results for different NLP tasks. This paper reviews the important models and methods in deep learning which are applied to natural language problems. In particular, Convolutional Neural Network, Recurrent Neural Network, Long Short-Term Memory, Gated Recurrent Unit, Recursive Neural Network have been described. Also, their advantages and suitability to various natural language processing applications such as text classification, sentiment analysis, etc. have been reviewed.

Abstract:Lone wolf terrorists pose a large threat to modern society. The current ability to identify and stop these kind of terrorists before they commit a terror act is limited since they are very hard to detect using traditional methods. However, these individuals often make use of Internet to spread their beliefs and opinions, and to obtain information and knowledge to plan an attack. Therefore, there is a good possibility that they leave digital traces in the form of weak signals that can be gathered, fused, and analyzed. In this work we present an analysis method that can be used to analyze extremist forums to profile possible lone wolf terrorists. This method is conceptually demonstrated using the FOI Impactorium fusion platform. We also present a number of different technologies that can be used to harvest and analyze information from Internet, serving as weak digital traces that can be fused using the suggested analysis method, in order to discover possible lone wolf terrorists.


Building effective queries in natural language information retrieval
In this paper we report on our natural language information retrieval (NLIR) project as related to the recently concluded 5th Text Retrieval Conference (TREC-5). The main thrust of this project is to use natural language processing techniques to enhance the effectiveness of full-text document retrieval. One of our goals was to demonstrate that robust if relatively shallow NLP can help to derive a better representation of text documents for statistical search. Recently, we have turned our attention away from text representation issues and more towards query development problems. While our NLIR system still performs extensive natural language processing in order to extract phrasal and other indexing terms, our focus has shifted to the problems of building effective search queries. Specifically, we are interested in query construction that uses words, sentences, and entire passages to expand initial topic specifications in an attempt to cover their various angles, aspects and contexts. Based on our earlier results indicating that NLP is more effective with long, descriptive queries, we allowed for long passages from related documents to be liberally imported into the queries. This method appears to have produced a dramatic improvement in the performance of two different statistical search engines that we tested (Cornell's SMART and NIST's Prise) boosting the average precision by at least 40%. In this paper we discuss both manual and automatic procedures for query expansion within a new stream-based information retrieval model.


Novel Technique for Script Translation using NLP: Performance Evaluation


NLP-based error analysis and dynamic motivation techniques in mobile learning

Deep Learning on Graphs for Natural Language Processing
This tutorial of Deep Learning on Graphs for Natural Language Processing (DLG4NLP) will cover relevant and interesting topics on applying deep learning on graph techniques to NLP, including automatic graph construction for NLP, graph representation learning for NLP, advanced GNN based models (e.g., graph2seq, graph2tree, and graph2graph) for NLP, and the applications of GNNs in various NLP tasks (e.g., machine translation, natural language generation, information extraction and semantic parsing). In addition, a handson demonstration session will be included to help the audience gain practical experience on applying GNNs to solve challenging NLP problems using our recently developed open source library - Graph4NLP, the first library for researchers and practitioners for easy use of GNNs for various NLP tasks.


NLP chatbot for Discharge Summaries

Abstract:The task of finding data files related to an information need from a group of information resources is known as Information Retrieval. In this work, the author propose a multi-lingual information retrieval system using deep learning. Input to the system is a question in sentencing form that can be processed by NLP tools. In the preprocessing phase, part-of-speech tagging of the input sentence is performed. A three layer neural network is used for creating word to vector representation. The word2vec model continuous-bag-of-words (CBOW) is used for this purpose. Then related words are obtained via word-2-vec using deep learning RNN. RNN is the recurrent neural network. Finally, results are obtained by calculating the cosine similarity score. For multi-lingual results, bilingual mapping is performed using CFILTs bilingual corpus. The tourism dataset is used for experimentation purposes.



MetaExtract: an NLP system to automatically assign metadata


Text sentiment polarity classification based on TextCNN-SVM combination model

Confronting Abusive Language Online: A Survey from the Ethical and Human Rights Perspective
The pervasiveness of abusive content on the internet can lead to severe psychological and physical harm. Significant effort in Natural Language Processing (NLP) research has been devoted to addressing this problem through abusive content detection and related sub-areas, such as the detection of hate speech, toxicity, cyberbullying, etc. Although current technologies achieve high classification performance in research studies, it has been observed that the real-life application of this technology can cause unintended harms, such as the silencing of under-represented groups. We review a large body of NLP research on automatic abuse detection with a new focus on ethical challenges, organized around eight established ethical principles: privacy, accountability, safety and security, transparency and explainability, fairness and non-discrimination, human control of technology, professional responsibility, and promotion of human values. In many cases, these principles relate not only to situational ethical codes, which may be context-dependent, but are in fact connected to universal human rights, such as the right to privacy, freedom from discrimination, and freedom of expression. We highlight the need to examine the broad social impacts of this technology, and to bring ethical and human rights considerations to every stage of the application life-cycle, from task formulation and dataset design, to model training and evaluation, to application deployment. Guided by these principles, we identify several opportunities for rights-respecting, socio-technical solutions to detect and confront online abuse, including ‘nudging’, ‘quarantining’, value sensitive design, counter-narratives, style transfer, and AI-driven public education applications.evaluation, to application deployment. Guided by these principles, we identify several opportunities for rights-respecting, socio-technical solutions to detect and confront online abuse, including 'nudging', 'quarantining', value sensitive design, counter-narratives, style transfer, and AI-driven public education applications.

Partnering enhanced-NLP with semantic analysis in support of information extraction
Information extraction using Natural Language Processing (NLP) tools focuses on extracting explicitly stated information from textual material. This includes Named Entity Recognition (NER), which produces entities and some of the relationships that may exist among them. Intelligent analysis requires examining the entities in the context of the entire document. While some of the relationships among the recognized entities may be preserved during extraction, the overall context of a document may not be preserved. In order to perform intelligent analysis on the extracted information, we provide an ontology, which describes the domain of the extracted information, in addition to rules that govern the classification and interpretation of added elements. The ontology is at the core of an interactive system that assists analysts with the collection, extraction, organization, analysis and retrieval of information, with the topic of "terrorism financing" as a case study. User interaction provides valuable assistance in assigning meaning to extracted information. The system is designed as a set of tools to provide the user with the flexibility and power to ensure accurate inference. This case study demonstrates the information extraction features as well as the inference power that is supported by the ontology.

Semantic Relation Extraction for Herb-Drug Interactions from the Biomedical Literature Using an Unsupervised Learning Approach
Abstract:Sharing principles of drug-drug interaction, herb-drug interaction (HDI) investigates the impacts of herb-based products on activities of other conventional drugs when combining them in certain medical treatments. For years, patients using herb-based medications have built a misconception about the absolute safety of products derived from natural sources. The current fact revealed that patients had intentionally combined herb-based products and prescription drugs for any certain illnesses without safety concerns to enhance the efficiencies. Incapability of non-experts in reviewing the biomedical literature of potential HDIs may be considered as one of the most reasonable explanations for this issue. In this study, text mining techniques are applied to provide users with a novel approach to save time when looking for information of HDIs. Since constructing an annotated corpus for herb-based products in traditional manner requires a high demand for human resources and financial support, an unsupervised learning model for relation extraction which eliminates to the crucial role of an annotated training set is quite suitable. The relations connecting the entity pairs were discovered and labeled by their most significant features. The obtained result proposes a promising method for the HDIs extraction challenge.


Content-based recommendation for podcast audio-items using natural language processing techniques

The language that gets people to give: phrases that predict success on kickstarter
Crowdfunding sites like Kickstarter--where entrepreneurs and artists look to the internet for funding--have quickly risen to prominence. However, we know very little about the factors driving the 'crowd' to take projects to their funding goal. In this paper we explore the factors which lead to successfully funding a crowdfunding project. We study a corpus of 45K crowdfunded projects, analyzing 9M phrases and 59 other variables commonly present on crowdfunding sites. The language used in the project has surprising predictive power accounting for 58.56% of the variance around successful funding. A closer look at the phrases shows they exhibit general persuasion principles. For example, also receive two reflects the principle of Reciprocity and is one of the top predictors of successful funding. We conclude this paper by announcing the release of the predictive phrases along with the control variables as a public dataset, hoping that our work can enable new features on crowdfunding sites--tools to help both backers and project creators make the best use of their time and money.

Abstract:At a big data level there are voluminous amounts of posts in Arabic that can be found on social media and on various product/service review sites. This provides a rich source of information at a number of levels that can be useful both commercially and at government level. Analyzing media in English, which uses sentiment lexicons, is well established. Posts written in Arabic is a challenging task primarily due to Arabic’s rich morphology. There have been a number of efforts to build Arabic sentiment lexicons. However, they suffer from being of a limited size, unclear usability plan or publicly not available. This project, ASAM, aims to develop tools that will opinion mine media written in Gulf Arabic, Modern Standard Arabic and Arabizi (Arabish) or a mixture of these. In addition to mining opinion, the project will aim at demographically profiling the opinions.


Speech recognition using syllable and pseudo articulatory features modeling
Abstract:This paper presents an account of the use of syllable structure as the basis for a novel approach to speech recognition. It is demonstrated that working with syllables provides the basis for linguistically motivated speech recognition using the previously reported notion of the pseudo-articulatory representation (PAR). Speech recognition experiments show that a recognition model developed on the basis of a single speaker can be successfully applied to recognition of speech produced by a different speaker. Therefore, the approach reported demonstrates the potential for speaker independent recognition. In general, this research leads to a more plausible style of automatic speech recognition and contributes to knowledge of phonetics and speech behavior.

An annotation type system for a data-driven NLP pipeline
We introduce an annotation type system for a data-driven NLP core system. The specifications cover formal document structure and document meta information, as well as the linguistic levels of morphology, syntax and semantics. The type system is embedded in the framework of the Unstructured Information Management Architecture (UIMA).


Beyond Text and Back Again

Cross-Demographic Portability of Deep NLP-Based Depression Models
Abstract:Deep learning models are rapidly gaining interest for real-world applications in behavioral health. An important gap in current literature is how well such models generalize over different populations. We study Natural Language Processing (NLP) based models to explore portability over two different corpora highly mismatched in age. The first and larger corpus contains younger speakers. It is used to train an NLP model to predict depression. When testing on unseen speakers from the same age distribution, this model performs at AUC=0.82. We then test this model on the second corpus, which comprises seniors from a retirement community. Despite the large demographic differences in the two corpora, we saw only modest degradation in performance for the senior-corpus data, achieving AUC=0.76. Interestingly, in the senior population, we find AUC=0.81 for the subset of patients whose health state is consistent over time. Implications for demographic portability of speech-based applications are discussed.


AutoEval: A NLP Approach for Automatic Test Evaluation System

Abstract:During the unprecedented of COVID-19 pandemic, numbers of research had been conducted on mental health in social media worldwide. Past research has shown interest in Twitter sentiment analysis by using keywords, geographical area, and range of ages. Up to the authors’ analysis, there is no research conducted on mental health using keyword in the case of Malaysia. A Malay Tweet dataset was built for analysing mental health tweets during the first Movement Control Order period using unique keywords. Machine learning algorithms namely, Naïve Bayes classifier and Support Vector Machine were used to predict the sentiment of tweets. The classifiers were evaluated using 10-fold cross-validation, accuracy, precision, and F1-score. The data then visualized in charts and WordCloud. The results shows that Support Vector Machine performed better than Naïve Bayes classifier for both test set and 10-fold cross-validation in terms of performances in n-gram TF-IDF. The visualized data could provide insights to the authority pertaining the mental health issues, in which it relates to local news and situations during the periods.



Optimum Coordination of Directional Overcurrent Relays Using the Hybrid GA-NLP Approach

NLP-guided Video Thin-slicing for Automated Scoring of Non-Cognitive, Behavioral Performance Tasks
We propose a novel and practical approach to model non-cognitive, behavioral performance tasks, by using natural language processing (NLP) to guide the selection of “thin-slices” that are representative of holistic video performances and demonstrate its promise.

A practical message-to-speech strategy for dialogue systems
In this paper, we present a Message-to-Speech system for Natural Language Generation that is to be integrated in a dialogue system. As the system has to function in a very restrictive environment with respect to computational resources, a compromise between concept based and template based generation systems had to be found. Still, the approach aims at achieving linguistic flexibility for the utterances and attaining a natural sounding prosody.


Survey on Mathematical Word Problem Solving Using Natural Language Processing

Major–Minor Long Short-Term Memory for Word-Level Language Model
Abstract:Language model (LM) plays an important role in natural language processing (NLP) systems, such as machine translation, speech recognition, learning token embeddings, natural language generation, and text classification. Recently, the multilayer long short-term memory (LSTM) models have been demonstrated to achieve promising performance on word-level language modeling. For each LSTM layer, larger hidden size usually means more diverse semantic features, which enables the LM to perform better. However, we have observed that when a certain LSTM layer reaches a sufficiently large scale, the promotion of overall effect will slow down, as its hidden size increases. In this article, we analyze that an important factor leading to this phenomenon is the high correlation between the newly extended hidden states and the original hidden states, which hinders diverse feature expression of the LSTM. As a result, when the scale is large enough, simply lengthening the LSTM hidden states will cost tremendous extra parameters but has little effect. We propose a simple yet effective improvement on each LSTM layer consisting of a large-scale Major LSTM and a small-scale Minor LSTM to break the high correlation between the two parts of hidden states, which we call Major-Minor LSTMs (MMLSTMs). In experiments, we demonstrate the LM with MMLSTMs surpasses the existing state-of-the-art model on Penn Treebank (PTB) and WikiText-2 (WT2) data sets and outperforms the baseline by 3.3 points in perplexity on WikiText-103 data set without increasing model parameter counts.

Abstract:The paper propose a part of image processing for applying on security and authentication. This system looks to be about image processing techniques applied to work in security and data access authentication information through application on mobile. In regard to financial transactions by using Eigen face for face recognition to process and verify its validity. The paper is an idea for prototype applications, the system may not be complete and correct, it can be used practically. However, our work will provide the technical know-only.


Disease Modeling with a Forest Deep Neural Network Utilizing NLP and a Virtualized Clinical Semantic Network
Abstract:We present a novel classifier, the Forest Deep Neural Network (fDNN), and apply it to disease modeling. The fDNN architecture leverages a supervised forest for feature detection, enabling the algorithm to build complex neural networks with a large number of independent features from sparse feature representations while limiting overfitting. We apply the fDNN to model influenza—more specifically, to predict the occurrence of influenza in a cohort of anonymized real-world patients, aided by Natural Language Processing (NLP) and a virtualized Clinical Semantic Network (vCSN). We report accuracy rates in the 90% range, which is extremely high for the problems of disease modeling and applications of NLP.


Automatic Detection of Arabic Non-Anaphoric Pronouns for Improving Anaphora Resolution

Abstract:Currently, during holidays and weekends in urban areas, a huge number of shoppers can be seen crowding into shopping malls and supermarkets. When there are special offers and huge discounts, the number of people increases even more. In supermarkets these days, consumers need a shopping cart or basket when making their purchases of various items. In addition, the process of procuring the products is rather complicated as the shoppers must carry a basket every time in order to find the products they want and place them into the basket, followed by needing to deal with the calculation of the expenses. After completing their shopping list, the consumers must wait in a long line so that they can check out and pay the bill. As a result, in order to solve this problem, this research presents the development of a smart basket that is used for shopping. All of the products in the supermarket normally have a barcode tag. Therefore, a barcode reader of android smart phone will be included on the smart basket. In this way, the name and the cost of each item will be shown on the display of the mobile phone when the customer scans and places the products in the basket. The total bill will be added up to give the total cost of all of the purchases and stored to the memory of the microcontroller. The product information regarding the items contained in the basket will be transferred to the main computer via a transmitter. Moreover, validation of the accuracy of the shopping process will be performed by a weight sensor system built into the basket. Thus, waiting in line to check out and pay the bill while having to estimate the total cost will be avoided.


Abstract:In this world of modern technologies and media, online news publications and portals are increasing at a high speed. That is why, nowadays, it has become almost impossible to check out the traditional fact of news headlines and examine them due to the increase in the number of content writers, online media portals, and news portals. Mostly, fake headlines are filled with bogus or misleading content. They attract the commoners by putting phony words or misleading fraudulent content in the headlines to increase their views and share. But, these fake and misleading headlines create havoc in the commoner's life and misguide them in many ways. That is why we took a step so that the commoners can differentiate between fake and real news. We proposed a model that can successfully detect whether the story is fake or accurate based on the news headlines. We created a novel data set of Bengali language and achieved our aim and reached the target using the Gaussian Naive Bayes algorithm. We have used other algorithms, but the Gaussian Naive Algorithm has performed well in our model. This algorithm used a text feature dependent on TF-IDF and an Extra Tree Classifier to choose the attribute. In our model, using Gaussian Naive Bayes we got 87% accuracy which is comparatively best than any other algorithm we used in this model.


Survey on Automatic Text Summarization and Transformer Models Applicability
This survey talks about Automatic Text Summarization. Information explosion, the problem caused by the rapid growth of the internet, increased more and more necessity of powerful summarizers. This article briefly reviews different methods and evaluation metrics. The main attention is on the applications of the latest trends, neural network-based, and pre-trained transformer language models. Pre-trained language models now are ruling the NLP field, as one of the main down-stream tasks, Automatic Text Summarization is quite an interdisciplinary task and requires more advanced techniques. But there is a limitation of input and context length results in that the whole article cannot be encoded completely. Motivated by the application of recurrent mechanism in Transformer-XL, we build an abstractive summarizer for long text and evaluate how well it performs on dataset CNN/Daily Mail. The model is under general sequence to sequence structure with a recurrent encoder and stacked Transformer decoder. The obtained ROUGE scores tell that the performance is good as expected.

Unsupervised cross-lingual lexical substitution
Cross-Lingual Lexical Substitution (CLLS) is the task that aims at providing for a target word in context, several alternative substitute words in another language. The proposed sets of translations may come from external resources or be extracted from textual data. In this paper, we apply for the first time an unsupervised cross-lingual WSD method to this task. The method exploits the results of a cross-lingual word sense induction method that identifies the senses of words by clustering their translations according to their semantic similarity. We evaluate the impact of using clustering information for CLLS by applying the WSD method to the SemEval-2010 CLLS data set. Our system performs better on the 'out-of-ten' measure than the systems that participated in the SemEval task, and is ranked medium on the other measures. We analyze the results of this evaluation and discuss avenues for a better overall integration of unsupervised sense clustering in this setting.

Extraction of ingredient names from recipes by combining linguistic annotations and CRF selection
Nutrition and diet have direct and considerable impact on our well-being and health. This field attracts researchers from different areas, such as medicine, nutrition and epidemiology, computer sciences, artificial intelligence and natural language processing (NLP).We process the recipes with NLP methods in order to automatically identify ingredient names within recipes. We propose a hybrid system based on linguistic enrichment of the recipes and selection of the relevant ingredient names with a CRF method. Semantic resources have been specifically built for processing two kinds of information: exact (e.g. quantity expressed in grams or liters, durations expressed in minutes or days) and fuzzy (e.g. quantities expressed in chouilla (smidgeon) and louche (ladle), durations sequenced with aprèes, ensuite, alors que (the, after that, while)). The experiments are performed with French-language textual data. The results demonstrate that the proposed method is useful for searching and managing the recipes.


An Automated Tool for Analysis and Design of MVL Digital Circuits


Improvement in Semantic Address Matching using Natural Language Processing

Abstract:In this paper we investigate if the bigram graph-based analysis can be applied to identify the most common subjects of discussion in English - spoken social media. We will present the construction of directed graph of bigrams, data filtration model and we will perform validation on three real (not artificially created)datasets, each containing 1 000 000 microblogging posts from Twitter platform. We will compare our findings with our earlier researches that used hashtag - based analysis and simple NLP method namely lemmatization. Those methods were evaluated on the same dataset as we use in this paper. With bigrams network it is possible to obtain some limited information about subjects the users are talking about however less than when the analysis uses hashtags. Bigrams also work better than simple lemmatization that in our previous experiment did not produce any useful data. The approach that supplies us with most useful data describing content of microblogging posts seems to be hashtag filtration. However this last type of analysis requires the presence of additional annotation data in user messages. Terms generated by bigrams and hashtag analysis might not be identical and sometimes one of them might be valuable complement of the other. Bigrams network might show the general idea of the content of the information gathered in microblogging posts. It is recommended to visualize a filtered part of this graph (for example limited by weights of edges)to see the general idea of data consisted in this network.



An Approach Towards Identification and Prevention of Riots by Analysis of Social Media Posts in Real-Time

Abstract:Recently, natural language processing tasks are more frequently conducted over online content. This poses a special problem for applications over Arabic language. Online Arabic content is usually written in informal colloquial Arabic, which is characterized to be ill-structured and lacks specific linguistic standardization. In this paper, we investigate a preliminary step to conduct successful NLP processing which is the problem of sentence boundary detection. As informal Arabic lacks basic linguistic rules, we establish a list of commonly used punctuation marks after extensively studying a large amount of informal Arabic text. Moreover, we evaluated the correct usage of these punctuation marks as sentence delimiters; the result yielded a preliminary accuracy of 70%.


Structured sparsity in natural language processing: models, algorithms and applications
This tutorial will cover recent advances in sparse modeling with diverse applications in natural language processing (NLP). A sparse model is one that uses a relatively small number of features to map an input to an output, such as a label sequence or parse tree. The advantages of sparsity are, among others, compactness and interpretability; in fact, sparsity is currently a major theme in statistics, machine learning, and signal processing. The goal of sparsity can be seen in terms of earlier goals of feature selection and therefore model selection (Della Pietra et al., 1997; Guyon and Elisseeff, 2003; McCallum, 2003). This tutorial will focus on methods which embed sparse model selection into the parameter estimation problem. In such methods, learning is carried out by minimizing a regularized empirical risk functional composed of two terms: a "loss term," which controls the goodness of fit to the data (e.g., log loss or hinge loss), and a "regularizer term," which is designed to promote sparsity. The simplest example is L1-norm regularization (Tibshirani, 2006), which penalizes weight components individually, and has been explored in various NLP applications (Kazama and Tsujii, 2003; Goodman, 2004; Gao, 2007). More sophisticated regularizers, those that use mixed norms and groups of weights, are able to promote "structured" sparsity: i.e., they promote sparsity patterns that are compatible with a priori knowledge about the structure of the feature space. These kind of regularizers have been proposed in the statistical and signal processing literature (Yuan and Lin, 2006; Zhao et al., 2009; Kim et al., 2010; Bach et al., 2011) and are a recent topic of research in NLP (Eisenstein et al., 2011; Martins et al, 2011, Das and Smith, 2012). Sparsity-inducing regularizers require the use of specialized optimization routines for learning (Wright et al., 2009; Xiao, 2009; Langford et al., 2009). The tutorial will consist of three parts: (1) how to formulate the problem, i.e., how to choose the right regularizer for the kind of sparsity pattern intended; (2) how to solve the optimization problem efficiently; and (3) examples of the use of sparsity within natural language processing problems.

Abstract:Sentiment analysis (SA) is a modern text mining disciplinary that gained notable position due its various application in social networks (SN) and many internet domains. Since, it is used for discovering audience directions, and impressions about products or any subjects discussed in the internet via social media. Personal opinions availability in SN gave SA a significant attention to discussions makers on modern corporation. In this paper, we uses machine learning techniques via many features and same corpus states in Arabic language, comparing their results, and illustrating the significance of terms merging and pruning in various figures, which helps in the field of SA performance increasing purposes.


Abstract:Fake or fraudulent news is coming into existence in large numbers for various political and commercial causes, which has become common in internet community. People can easily get tainted by any of these fraudulent news for their falsified words that have tremendous effects on the offline community. Therefore interest has increased in research on this topic. Notable work on the identification of false news in English texts as well as other languages except a few in Bangla Language has been carried out. Our work demonstrates the experimental investigation of detecting fake news from Bangla social media, as this area still requires a lot of concentrate. We have utilized two supervised machine learning techniques throughout this research study, Support Vector Machine (SVM) and Multinomial Naive Bayes (MNB) classifiers to recognize Bangla fake news. Term Frequency - Inverse Document Frequency Vectorizer and CountVectorizer has been used as feature extraction. Our suggested system recognizes fake news according to polarity of the related post. Eventually, our research suggests SVM with linear kernel gives a 96.64 percent accuracy overperforming MNB with a 93.32 percent accuracy.


Research and development of speech technology & applications for Mexican Spanish at the Tlatoa group
Thanks to the advances in today's technology in terms of processing speed of computers, storage space and the management of sound and video devices, speech technology is a reality in almost any kind of computerized system. Speech applications are being used in personal computers, cellular phones, etc. This makes this interesting technology accessible to almost anyone. Among it's most useful applications we can find telephone-based information services, banking and computer assisted language learning systems.There exist already a large number of commercial products that use speech interfaces, developed mainly for English, German and Japanese. That is why we at TLATOA have focused our efforts on making this technology available in the Spanish spoken in Mexico.To this effect we perform basic research in the different speech processing techniques, trying to improve the performance of speech recognition and synthesis (artificial neural networks, hidden Markov Models (HMM's), Unit Selection, etc.), as well as, the Spanish language, dialogue structure, perception and human-computer interaction approaches, for the development of speech applications.

Abstract:Arabic stemming is a technique that finds the stem or lexical root for Arabic words through the process of eliminating affixes attached to their roots. Many studies have been conducted to generate the stem of Arabic words according to the desirable level of analysis, i.e., rootbased approach, stem-based approach and statistical approach. Arabic language, which is the focus of this paper, is a Semitic language which means that it is a derivational rather than a concatinative language. In this paper we implemented an Arabic triliteral Morphological Analyser that is capable of analysing the classical and modern standard Arabic (MSA) effectively and able to analyse vowelised, semi-vowelised and nonvowelised text. the system is integratable with other applications. one shortcomming for the developed system is that the output obtained from the morphological analyser may contain several alternative solutions which leads to extraction ambiguity.


A cascaded classification approach to semantic head recognition
Most NLP systems use tokenization as part of preprocessing. Generally, tokenizers are based on simple heuristics and do not recognize multi-word units (MWUs) like hot dog or black hole unless a precompiled list of MWUs is available. In this paper, we propose a new cascaded model for detecting MWUs of arbitrary length for tokenization, focusing on noun phrases in the physics domain. We adopt a classification approach because -- unlike other work on MWUs -- tokenization requires a completely automatic approach. We achieve an accuracy of 68% for recognizing non-compositional MWUs and show that our MWU recognizer improves retrieval performance when used as part of an information retrieval system.

Text chunking using regularized Winnow
Many machine learning methods have recently been applied to natural language processing tasks. Among them, the Winnow algorithm has been argued to be particularly suitable for NLP problems, due to its robustness to irrelevant features. However in theory, Winnow may not converge for non-separable data. To remedy this problem, a modification called regularized Winnow has been proposed. In this paper, we apply this new method to text chunking. We show that this method achieves state of the art performance with significantly less computation than previous approaches.

unintentionally to introduce ambiguity, as opposed to pervasive
uncontrolled and uncontrollable ambiguity in natural language. The

Abstract:In this paper, we suggest a methodology that builds personality `quirks' into A.I. using Lukasiewicz-like three (or more) valued logic. In multi-valued logic, the expanding set of operators provide an opportunity for representing nearly consistent logic, but with a personal preference; we present a suite of nearly logical operators that reflect individual preferences, provide a machine with an artificial logic-based personality (ALP).


A Stance Detection Approach Based on Generalized Autoregressive pretrained Language Model in Chinese Microblogs
Timely identification of Chinese Microblogs users' stance and tendency is of great significance for social managers to understand the trends of online public opinion. Traditional stance detection methods underutilize target information, which affects the detection effect. This paper proposes to integrate the target subject information into a Chinese Microblogs stance detection method based on a generalized autoregressive pretraining language model, and use the advantages of the generalized autoregressive model to extract deep semantics to weaken the high randomness of Microblogs self-media text language and lack of grammar. The impact of norms on text modeling. First carry out microblog data preprocessing to reduce the influence of noise data on the detection effect; then connect the target subject information and the text sequence to be tested into the XLNet network for fine-tuning training; Finally, the fine-tuned XLNet network is combined with the Softmax regression model for stance classification. The experimental results show that the value of the proposed method in the NLPCC2016 Chinese Microblogs detection and evaluation task reaches 0.75, which is better than the existing public model, and the effect is improved significantly.

Abstract:We are increasingly reliant on cellular data services for many types of day-to-day activities, from hailing a cab, to searching for nearby restaurants. Geo-location has become a ubiquitous feature that underpins the functionality of such applications. Network operators can also benefit from accurate mobile terminal localization in order to quickly detect and identify location-related network performance issues, such as coverage holes and congestion, based on mobile measurements. Current implementations of mobile localization on the wildly-popular Android platform depend on either the Global Positioning System (GPS), Android's Network Location Provider (NLP), or a combination of both. In this paper, we extensively study the performance of such systems, in terms of its localization accuracy. We show through real-world measurements that the performance of GPS+NLP is heavily dependent on the mobility of the user, and its gains on localization performance is minimal, and often even detrimental, especially for network round-trip delays up to 1s. Building upon these findings, we evaluate the efficacy of using Tattle, a cooperative local measurement-exchange system, and propose Delay-Adjusted U-CURE, a clustering algorithm that greatly improves the localization performance of both GPS-only, and GPS+NLP techniques, without keeping expensive system states, nor requiring any location anchors nor additional instrumentation, nor any external knowledge that is not available programmatically to application designers. Our results are promising, demonstrating that median location accuracy improvements of over 30% is achievable with just 3 co-located devices, and close to 60% with just 6 co-located devices. These findings can be used by operators to better manage their networks, or by application designers to improve their location-based services.


Abstract:The development of Arabic WordNet (AWN) has provided the Arabic Natural Language Processing (NLP) community with a lexical resource. However, due to the considerable lack of AWN Synsets compared to other WordNets, the use of this resource in Arabic NLP applications cannot be considered as an option yet. In this paper, we propose an automatic approach for AWN enrichment, based on three features: (1) the translation of Princeton WordNet (PWN) words and their direct Hypernyms; (2) an automatic validation of the hypernymy relation that is suggested as existing between words using Wikipedia articles; and (3) the integration of validated Synsets in AWN including their definitions, examples and synonyms that get extracted using dictionaries and other resources. The proposed approach has been implemented and a preliminary evaluation shows that the obtained results are promising.


Session details: NLP applications
No abstract available.

Abstract:Semantic similarity (SS) analysis is a technique for finding similarities between words/sentences/documents based on their meaning. In natural language processing (NLP), SS is an important element to find a suitable mail from a bulk inbox. As the number of mails and mail content increases, it becomes difficult to get the matches with keywords and nearly impossible for many cases. This paper presents a method to find SS between query statements and mail content using BERT (Bidirectional Encoder Representations from Transformers). BERT is a pre-trained unsupervised NLP model developed by Google. The results are presented and compared with the existing keyword-based search to prove the efficiency of the proposed approach.



Enhancing Bilevel Optimization for UAV Time-Optimal Trajectory using a Duality Gap Approach

Semantic keywords-based duplicated web pages removing
Abstract:Because of many duplicated web pages existing on the web, search engines need to find and remove them, not only for saving process time and hardware resource, but also for ensuring that users can get the result information without many replicas. In this paper, we propose a method to find and remove duplicated Chinese Web pages for search engine. First we describe a scheme based on semantic keywords combined with sentence overlapping, and then show an implemented prototype, with the experimental results that suggest the prototype work well under a proper setting.


An Automatic Approach to Detect Traceability Links Using Fuzzy Logic

Language identification at word level in Sinhala-English code-mixed social media text
Abstract:Automatic analyzing and extracting useful information from the noisy social media content are currently getting attention from the research community. It is common to find people easily mixing their native language along with the English language to express their thoughts in social media, using Unicode characters or the Unicode characters written in Roman Scripts. Thus these types of noisy code-mixed text are characterized by a high percentage of spelling mistakes with phonetic typing, wordplay, creative spelling, abbreviations, Meta tags, and so on. Identification of languages at word level become a necessary part for analyzing the noisy content in social media. It would be used as an intimidate language identifier for chatbot application by using the native languages. For this study we used Sinhala- English code-mixed text from social media. Natural Language Processing (NLP) and Machine Learning (ML) technologies are used to identify the language tags at the word level. A novel approach proposed for this system implemented is machine learning classifier based on features such as Sinhala Unicode characters written in Roman scripts, dictionaries, and term frequency. Different machine learning classifiers such as Support Vector Machines (SVM), Naive Bayes, Logistic Regression, Random Forest and Decision Trees were used in the evaluation process. Among them, the highest accuracy of 90.5% was obtained when using Random Forest classifier.

Architecture of Knowledge Extraction System based on NLP
Knowledge extraction is to extract useful structured text information from messy free text. Under the current massive information background, it has attracted extensive attention. This paper analyzes the concept of NLP and the application process of NLP algorithm, discusses web information retrieval system, information extraction based on natural language processing and text relationship extraction, and tests the pipeline performance. The results show that the pipeline time is not linearly correlated with the size of the novel, but positively correlated.

A parallel adaptive tabu search approach for traveling salesman problems
Abstract:TSP (traveling salesman problem) is one of the typical combinatorial optimization problems, which is NP-hard. It is widely believed that there is no efficient polynomial time algorithm that can solve it accurately. On the other hand, this problem is very important since it has many applications in practice. It has been verified that TS (tabu search) is one of the meta-heuristic algorithms that can solve this problem satisfied. With the requirement of solving large-scale problems, we presented a new parallel tabu search (PTS) approach, which was cooperated with genetic crossover operation, for TSPs. In addition, a novel adaptive search strategy of intensification and diversification in TS was proposed to improve the solving quality and efficiency. Through computational experiment, it is showed that our proposed PTS is feasible and effective.


Corpus Development for Dzongkha Automatic Speech Recognition

Clustering sentence level-text using fuzzy hierarchical algorithm
Abstract:Clustering is a popular technique for unsupervised text analysis, often used to explore the content of large amounts of sentences. It is performed based on the similarity of sentences. Sentences may contain interrelated concepts and implementing flat clustering algorithms allows one sentence to be present only in one cluster. Also, sentences are semantically related to each other and so word co-occurrence is not a valid measure for sentence level flat clustering. So, WordNet based semantic similarity measure along with fuzzy sentence clustering algorithm is proposed. The existing system uses the Fuzzy C-Means algorithm where the cluster size should be specified as an input. Due to the rigorous convergence criteria, the time complexity is much larger. Most of the NLP documents are hierarchical in nature and so fuzzy hierarchical sentence clustering algorithm is used here. Labeling is performed for each cluster depending on the hierarchy formed and instead of considering all the terms in a sentence, only the verbs and nouns are considered for the similarity computation. Agglomerative clustering based on the verbs and divisive clustering based on nouns is proposed. This methodology is validated through various performance measures like Purity, Entropy and Time. Upon comparing the results for various datasets, it was observed that the overall improvement in purity is 36.6% and entropy is 31%. The time complexity of the hierarchical algorithm is very much less than the EM algorithm. Thus better quality clusters are formed in comparatively less time by using the Fuzzy Hierarchical Sentence Clustering Algorithm.


Building New Field Association Word Candidates Automatically Using Search Engine

Exploring the effects of diacritization on Arabic frequency counts
Abstract:Statistical natural language processing relies on corpora as a source of token frequency counts. Obtaining reliable frequency counts is challenging in Arabic due to omitted diacritics in almost all writings. In this paper, we explore how severely this situation affects the resulting language models. For this purpose, we analyze the properties of the few available manually diacritized corpora and use them to evaluate automatic diacritization tools. We then apply the best performing tool on non-diacritized texts and investigate the properties of the resulting language models.

GraphRep: Boosting Text Mining, NLP and Information Retrieval with Graphs
Graphs have been widely used as modeling tools in Natural Language Processing (NLP), Text Mining (TM) and Information Retrieval (IR). Traditionally, the unigram bag-of-words representation is applied; that way, a document is represented as a multiset of its terms, disregarding dependencies between the terms. Although several variants and extensions of this modeling approach have been proposed, the main weakness comes from the underlying term independence assumption; the order of the terms within a document is completely disregarded and any relationship between terms is not taken into account in the final task. To deal with this problem, the research community has explored various representations, and to this direction, graphs constitute a well-developed model for text representation. The goal of this tutorial is to offer a comprehensive presentation of recent methods that rely on graph-based text representations to deal with various tasks in Text Mining, NLP and IR.

Evaluating the effects of treebank size in a practical application for parsing
Natural language processing modules such as part-of-speech taggers, named-entity recognizers and syntactic parsers are commonly evaluated in isolation, under the assumption that artificial evaluation metrics for individual parts are predictive of practical performance of more complex language technology systems that perform practical tasks. Although this is an important issue in the design and engineering of systems that use natural language input, it is often unclear how the accuracy of an end-user application is affected by parameters that affect individual NLP modules. We explore this issue in the context of a specific task by examining the relationship between the accuracy of a syntactic parser and the overall performance of an information extraction system for biomedical text that includes the parser as one of its components. We present an empirical investigation of the relationship between factors that affect the accuracy of syntactic analysis, and how the difference in parse accuracy affects the overall system.

Abstract:This paper proposes, effective method for replay attack detection used in an automatic speaker verification system. The replay attack is of interest because it is the most straightforward and effective attack and is challenging to detect. It is a playback of the recording of the voice of a target speaker. From the literature, no speech features work well with all classifiers, and there is no investigation of using ResNet-based model, called ResNeWt, with linear frequency cepstral coefficient (LFCC). Therefore, a replay attack detection model based on 18-layer ResNeWt that takes LFCCs as the input, was constructed in this paper. The proposes method was tested on a dataset provided by ASVspoof 2019 competition. In terms of the equal error rate (EER), the proposed method is the best in all existing methods, with an EER of 0.29%. The comparison in terms of replay attack detection was also made in detail. The performance of the proposed method in terms of the balanced accuracy, precision, recall, and F1-score was considerably better than existing methods.


Do User Entrepreneurs Speak Different?: Applying Natural Language Processing to Crowdfunding Videos
In this work, we analyze video pitches of creators in crowdfunding campaigns and focus on linguistic particularities of "lead user" entrepreneurs. Theory suggests that lead users sense needs long before they become known to the broader public and would benefit greatly from finding a solution to these needs. For our study, we consider 537 video pitches of creators on Kickstarter, distinguishing lead users and regular campaigners. The study employs natural language processing (NLP) analysis of (video-to-)speech-to-text content. Initial results indicate that lead users are more oriented towards product and problem-solving rather than focusing on pecuniary motives.

Abstract:Sign Language is the language used by the deaf and dumb people throughout the world. The structure of sign languages are varies from country to country. This paper presents English text to SiGML file translation model where the system will accept the input text and then convert into HamNoSys representation. The HamNoSys is further translated into xml file called SiGML. The whole translation is based on corpus. There is direct mapping between English word and HamNoSys string. The corpus is prepared by collecting the word used in daily life and corresponding HamNoSys string. The translation algorithm is executed by using python programming language in Windows 8 operating system with the help of NLTK and produce good results about speed.


Automatic NLP-based enrichment of E-learning content for English language learning
Abstract:The creation of quality content for E-learning resources is a time-consuming task. To simplify the process of content creation for language learning and enable easy adaptability for different requirements and language levels we strive to add as much automation as possible. In order to still obtain high quality, we present in this paper our approaches to enrich E-learning-based English vocabulary tests, which support blended learning and improve direct user feedback. We integrate openly available language resources for selecting and appending usage example sentences for a given vocabulary corpus. Furthermore we discuss our results and suggest to acquire natural language processing (NLP) based techniques to improve the generation of language related contents in general and to overcome some of the weaknesses of our current solution.

Abstract:Chinese named entity recognition (NER) is studied in two directions: inner structure and outer surroundings. Inner structural analyses induce constitutions of person, location and organization name from the point of linguistics. However inner structural rules for named entities only provide necessary conditions for a sequence of Chinese characters being an entity name but not sufficient. Whether a string being a proper name or not is also determined by its contextual information or sometimes common sense. We build Chinese NER system based on supervised machine learning using features induced from simple inner structure and contextual information. We compare some NER approaches. The experimental results indicate complicated cases of various NER strategies. Then this paper turns to explore contextual features of named entities on large scale corpus, seeking for contextual evidence for different strategies of NER and mark words giving clues to the occurrence of NE. Finally, we apply some conclusions to improve NER system by enriching features in model and enhance the performance distinctly.


Abstract:Quantification of cerebral white matter hyperintensities (WMH) of presumed vascular origin is of key importance in many neurological research studies. Currently, measurements are often still obtained from manual segmentations on brain MR images, which is a laborious procedure. The automatic WMH segmentation methods exist, but a standardized comparison of the performance of such methods is lacking. We organized a scientific challenge, in which developers could evaluate their methods on a standardized multi-center/-scanner image dataset, giving an objective comparison: the WMH Segmentation Challenge. Sixty T1 + FLAIR images from three MR scanners were released with the manual WMH segmentations for training. A test set of 110 images from five MR scanners was used for evaluation. The segmentation methods had to be containerized and submitted to the challenge organizers. Five evaluation metrics were used to rank the methods: 1) Dice similarity coefficient; 2) modified Hausdorff distance (95th percentile); 3) absolute log-transformed volume difference; 4) sensitivity for detecting individual lesions; and 5) F1-score for individual lesions. In addition, the methods were ranked on their inter-scanner robustness; 20 participants submitted their methods for evaluation. This paper provides a detailed analysis of the results. In brief, there is a cluster of four methods that rank significantly better than the other methods, with one clear winner. The inter-scanner robustness ranking shows that not all the methods generalize to unseen scanners. The challenge remains open for future submissions and provides a public platform for method evaluation.


Approach for multiword expression recognition & annotation in urdu corpora
Abstract:Multiword Expression (MWE) is an open challenging task in NLP because of its compositional behavior. In this paper, we have presented the classification of multiword expressions and also designed some MWE tags for annotation of Urdu corpora. If MWE annotation is proper then it can be accurately processed through parsing phase and can improve the accuracy of machine translator. For annotation of MWEs, initially extracted the compound type MWE on the basis of automatic approach and also extracted different type of MWEs using semiautomatic and manual approach. These extracted MWEs are pre-processed and after that got the correct multiword expressions. Using these MWEs, we have annotated the Urdu corpora of 50,000 sentences.


Learning Context Using Segment-Level LSTM for Neural Sequence Labeling


Research on Sports Game News Information Extraction

Social class and taste in the context of US cities: Validating Bourdieu's theory of distinction using restaurant reviews
In this study, we intend to validate Bourdieu's theory of distinction in the context of United States by using crowdsourced restaurant reviews from Yelp. Bourdieu argues that cultural capital was employed and reinforced by people of higher class to distinguish themselves from the people of lower social standing. This difference emerges from the fact that people of different groups have access to different resources depending on a multitude of variables such as race, class, gender, sexuality, religion, and age. Bourdieu's argument has been critiqued by many scholars on the grounds that it may not be generalized to the current social condition of the United States. Generally, critiques argue that this theory is too deterministic, diminish the influence of individual's choice and variability, data used in Bourdieu's study is outdated and doesn't take new societal changes into consideration, and some argue that individualism in the modern age has added more intricacy to one's cultural choice which cannot be only explained by social hierarchy. To validate Bourdieu's theory, we investigate the association between foods and drinks mentioned in different restaurants and a set of demographic factors which are used to define social class. The results of this study show that different food and drink types are associated with certain demographic features. Based on this observation we conclude that Bourdieu's theory of distinction is still evident in the context of US cities. This theory helps us better understand the structure of social class in the American cities.

Human emotion model based on discourse sentence for expression generation of conversation agent
Abstract:There was a conversation agent on the generation method of facial expression. It is necessary for the conversation system like the human for communication. In the previous method, at first a word which could influence the feeling was defined. Facial expression was changed according to the word which influences the feeling in discourse. Whereas, facial expression could not be changed if there was not a word that was defined in the discourse. Hence, we proposed a human emotion model for the expression generation of the conversation agent. The method based on the human emotion model can solve problem of the previous method and may make a more humanity conversation agent. In this study, we put a human emotion tag to discourse of talks scenarios and model to conversation agent of human emotion. There were two kinds of methods that put the human emotion tag to discourse. We make the human emotion model by scenarios that adopts the human emotion tag. Used the human emotion model to create facial expression of conversation agent. The assessment experiment was performed by using the systems of previous method and two human emotion models, and compared the results between the three methods.

Abstract:The increase in free trade will also amplify the exchange of goods between countries and islands, especially in the seaports. The manual operation of the gantry-crane at the seaports has a risk due to human negligence. For that reason, automation is strongly needed in the loading and unloading of the container. Camera-based object detection is implemented to achieve this goal. In the recent years, CNN gives better accuracy in term of image classification, recognition and detection compared to the traditional method that use handcrafted features. In this paper, several advanced detection methods using CNN-based object detection, namely MobileNet, ResNet, and Faster RCNN are compared to detect and track the movement of containers. The results of the experiment show that the SSD ResNet and Faster RCNN methods are superior in terms of detection accuracy based on the Precision, Recall, Average Precision, and IoU values. MobileNet v3 excels in speed detection compared to the other methods.



A Markov Logic Network Learning Algorithm From Relational Missing Data

Semantic crawling: an approach based on named entity recognition
Law Enforcement Agencies (LEAs) are increasingly more reliant on information and communication technologies and affected by a society shaped by the Internet. The richness and quantity of information available from open sources, if properly gathered and processed, can provide valuable intelligence and help in drawing inferences from existing closed source intelligence. Today the intelligence cycle is characterized by manual collection and integration of data. Named Entity Recognition (NER) plays a fundamental role in Open Source Intelligence (OSINT) solutions when fighting crime. This paper describes the implementation of a NER-based focused web crawler under the EU FP7 Security Research Project CAPER (Collaborative information, Acquisition, Processing, Exploitation and Reporting for the prevention of organized crime). The crawler allows 1. to look for documents starting from a URL until a parametric depth of levels - also specifying a keyword that has to be contained in the page and in the related links - and 2. to look for a parametric number of documents starting from a keyword (entrusting the keyword search to one of the principal search engines, thus behaving as a meta-search engine). In addition, the crawler is able to retrieve only those documents that contain the information semantically relevant to the query (in other words: the required keyword with the required sense). This is achieved through the use of NER technologies. In this paper we present the CAPER NER-based Semantic Crawler, which has been proven to be a suitable tool for focused crawling, allowing LEAs to drastically reduce data collection and integration efforts.


Identification of Symptoms Based on Natural Language Processing (NLP) for Disease Diagnosis Based on International Classification of Diseases and Related Health Problems (ICD-11)

Legal Intelligence: Algorithmic, Data, and Social Challenges
In the digital era, information retrieval, text/knowledge mining, and NLP techniques are playing increasingly vital roles in legal domain. While the open datasets and innovative deep learning methodologies provide critical potentials, in the legal-domain, efforts need to be made to transfer the theoretical/algorithmic models into the real applications to assist users, lawyers, judges and the legal professions to solve the real problems. The objective of this workshop is to aggregate studies/applications of text mining/retrieval and NLP automation in the context of classical/novel legal tasks, which address algorithmic, data and social challenges of legal intelligence. Keynote and invited presentations from industry and academic will be able to fill the gap between ambition and execution in the legal domain.

Abstract:Manual acquisition of high-quality, broad-coverage knowledge needed by knowledge-based NLP systems is commonly considered too expensive a procedure, and has been known to cause "the knowledge acquisition bottleneck". The use of the web as a corpus to support automating knowledge acquisition has been gaining in popularity in the recent years. This approach tends to introduce noise at the early stages of learning which can have a compounding impact on the quality of the final results. If the goal is to alleviate the expense of manual knowledge acquisition in the short term, a combination of automatic knowledge learning and human validation/correction must be considered. People can either post-edit automatically produced candidate knowledge elements or intervene at various stages in the acquisition process to facilitate high-quality automatic output. In this paper, we report on a sequence of experiments analyzing the utility of the latter methodology in the framework of a mutual-bootstrapping environment in which new knowledge resources are acquired both for and through the operation of an automatic meaning extraction system.


Automatic semantic sequence extraction from unrestricted non-tagged texts
Mophological processing, syntactic parsing and other useful tools have been proposed in the field of natural language processing (NLP). Many of those NLP tools take dictionary-based approaches. Thus these tools are often not very efficient with texts written in casual wordings or texts which contain many domain-specific terms, because of the lack of vocabulary.In this paper we propose a simple method to obtain domain-specific sequences from unrestricted texts using statistical information only. This method is language-independent.We had experiments on sequence extraction on email texts in Japanese, and succeeded in extracting significant semantic sequences in the test corpus. We tried morphological parsing on the test corpus with ChaSen, a Japanese dictionary-based morphological parser, and examined our system's efficiency in extraction of semantic sequences which were not recognized with ChaSen. Our system detected 69.06% of the unknown words correctly.


Sentiment Analysis in Turkish with Deep Learning

Abstract:In this paper a method for expanding the limited vocabulary of neural-network based language systems is introduced. The proposed method draws on developmental constraints observed in human language acquisition, to generate increasingly specialist feature maps in linked orthogonal spaces. Each space acts as a semantic filter, channelling words to more specialist spaces. The resultant trace through each space corresponds to a full feature list for the word, which can be manipulated symbolically or by another network. This approach allows arbitrary feature accuracy for any word, whilst limiting input dimensionality to the minimum required to uniquely specify the word in the relevant specialist space. Consequently crossover between unrelated words is also minimised, so avoiding the n-squared relation between computation and vocabulary size found in fully connected networks. The resultant topology of spaces also suggests that complex inferences are possible, and the use of a perception-based feature set allows a common knowledge base to be shared between languages.


A Comparative Study of Fuzzy Topic Models and LDA in terms of Interpretability
Abstract:In many domains that employ machine learning models, both high performing and interpretable models are needed. A typical machine learning task is text classification, where models are hardly interpretable. Topic models, used as topic embeddings, carry the potential to better understand the decisions made by text classification algorithms. With this goal in mind, we propose two new fuzzy topic models; FLSA-W and FLSA-V. Both models are derived from the topic model Fuzzy Latent Semantic Analysis (FLSA). After training each model ten times, we use the mean coherence score to compare the different models with the benchmark models Latent Dirichlet Allocation (LDA) and FLSA. Our proposed models generally lead to higher coherence scores and lower standard deviations than the benchmark models. These proposed models are specifically useful as topic embeddings in text classification, since the coherence scores do not drop for a high number of topics, as opposed to the decay that occurs with LDA and FLSA.

A hybrid-strategy method combining semantic analysis with rule-based MT for patent machine translation
Abstract:This paper presents a hybrid method combining semantic analysis with rule-based MT for patent machine translation. Based on the theory of Hierarchical Network of Concepts, the semantic analysis used the lv principle to deal with the ambiguity of multiple verbs and the boundary of long NP. The determination of main verb can help to select the right syntax tree, and the boundary detection of long NP can help to schedule the process of syntax. From the result of the experiments, we can see that this hybrid-strategy method can effectively improve the performance of Chinese-English patent machine translation.


Based on sentence similarity and emotion conversation for spoken dialogue system


Text Mining Strategy of Power Customer Service Work Order Based on Natural Language Processing Technology

Generating Text to Realistic Image using Generative Adversarial Network
Abstract:Generative Adversarial Network (GANs) has become one of the most interesting ideas in the last years in Machine Learning. Generative Adversarial Network is a very exciting area and that’s why researchers are so excited about building generative models as they are set to vary what machines can do for humans. This paper proposes the generation of realistic images according to their semantics based on text description using a Knowledge Graph alongside Knowledge Guided Generative Adversarial Network (KG-GAN) that comes with the embeddings generated from the Knowledge Graph (KG) into GAN. The Knowledge Graph is made from the text description by making the machine understand from the Natural Language Processing (NLP) techniques. The Knowledge Graph produced from the text description is converted to its embeddings by utilizing a Graph Convolutional Networks (GCN) and is fed into the GAN for generating realistic images by training the generators and discriminators and also the performance is evaluated. The experimental study is completed on a Caltech-UCSD Birds 200-2011 (CUB-200-2011) dataset and results that the approach using the knowledge graph for image generation using GAN has performed well and with high accuracy in comparison to the other established techniques generated in the past years for text to image generation in GAN.

Learning Fine-Grained Fact-Article Correspondence in Legal Cases
Abstract:Automatically recommending relevant law articles to a given legal case has attracted much attention as it can greatly release human labor from searching over the large database of laws. However, current researches only support coarse-grained recommendation where all relevant articles are predicted as a whole without explaining which specific fact each article is relevant with. Since one case can be formed of many supporting facts, traversing over them to verify the correctness of recommendation results can be time-consuming. We believe that learning fine-grained correspondence between each single fact and law articles is crucial for an accurate and trustworthy AI system. With this motivation, we perform a pioneering study and create a corpus with manually annotated fact-article correspondences. We treat the learning as a text matching task and propose a multi-level matching network to address it. To help the model better digest the content of law articles, we parse articles in form of premise-conclusion pairs with random forest. Experiments show that the parsed form yielded better performance and the resulting model surpassed other popular text matching baselines. Furthermore, we compare with previous researches and find that establishing the fine-grained fact-article correspondences can improve the recommendation accuracy by a large margin. Our best system reaches an F1 score of 96.3%, making it of great potential for practical use. It can also significantly boost the downstream task of legal decision prediction, increasing the F1 score by up to 12.7%. Code and dataset are available at 

Rebuilding the Oxford Dictionary of English as a semantic network
This paper describes a project to develop a lexicon for use both as an electronic dictionary and as a database for a range of NLP tasks. It proposes that a lexicon for such open-ended application may be derived from a human-user dictionary, retaining and enhancing the richness of its editorial content but abandoning its entry-list structure in favour of networks of relationships between discrete lexical objects, where each object represents a discrete lexeme-meaning unit.

Multi-mode Natural Language Processing for Extracting Open Knowledge
As more and more open knowledge sources become available, it is interesting to explore opportunities of enhancing autonomous agents' capacities by utilizing the knowledge in these sources, instead of hand-coding knowledge for agents. A major challenge towards this goal lies in the translation of the open knowledge organized in multiple modes, unstructured or semi-structured, into the internal representations of agents. In this paper we present a set of multi-mode NLP techniques to formalize the open knowledge for autonomous agents. Two case studies are reported in which our robot KeJia, equipped with the multi-mode NLP techniques, succeeded in acquiring knowledge from the microwave oven manual and from the open knowledge database, OMICS, and solving problems that could not be solved before the robot acquired the knowledge.

Abstract:Situational Awareness (SA) is paramount to ensuring operational safety in Nuclear Power Plant (NPP) and Commercial aviation industry. An increase in Human-in-the-loop (HITL) error rate may be indicative of reduced operator SA while undermining safety. In this paper, natural language processing (NLP) is applied for modelling industrial Human Machine Interface (HMI) state transitions as a means to detect operator HITL error precursors in real-time. A custom seq2seq encode-decoder deep-learning model design is implemented and evaluated using real-plant scenario dataset obtained from a NPP Operator training simulator. Results support NLP HMI state model may be employed to detect HITL error precursor within the desired N time-steps prior to an accident event.


#mytweet via Instagram: Exploring user behaviour across multiple social networks
Abstract:We study how users of multiple online social networks (OSNs) employ and share information by studying a common user pool that use six OSNs - Flickr, Google+, Instagram, Tumblr, TWitter, and YouTube. We analyze the temporal and topical signature of users' sharing behaviour, showing how they exhibit distinct behaviorial patterns on different networks. We also examine cross-sharing (i.e., the act of user broadcasting their activity to multiple OSNs near-simultaneously), a previously-unstudied behaviour and demonstrate how certain OSNs play the roles of originating source and destination sinks.


An Analysis of Interaction and Engagement in YouTube Live Streaming Chat

Clustering of words using dictionary-learnt word representations
Abstract:Language is the way of communication through words. This will help to get a better insight of the world. Natural Language Processing (NLP) mainly concentrate on expanding systems that allow computers to communicate with people using everyday language. One of the challenges inherent in NLP is teaching computers to recognize the way humans learn and use language. Word representations give rise to capture syntactic and semantic properties of words. So the main purpose of this work is to find out the set of words which have similar semantics by matching the context in which the words occur. In this work we explore a new method for learning word representations using sparse coding, a technique usually done on signals and images. We present an efficient sparse coding algorithm, Orthogonal Matching Pursuit to generate the sparse code. Based on the input given, sparse codes are generated for the input. The input term vectors are classified based on the sparse code by grouping the terms which have same sparse code into one class. K-Means algorithm is also used to classify the input terms vectors which have semantic similarities. Finally, this paper makes a comparison that gives the best representation from the sparse code and K-Means. The result shows an improved set of similar words using sparse code when compared to K-Means. This is because SVD is used as a part of dictionary learning which captures the latent relationship that exists between the words.

Towards agile and test-driven development in NLP applications
c-rater® is the Educational Testing Service technology for automatic content scoring for short free-text responses. In this paper, we contend that an Agile and test-driven development environment optimizes the development of an NLP-based technology.

Abstract:Music has serious effects on children's development. Music lyrics have become more violent and sexual over the years. However, the system for filtering explicit contents in music often does not work properly, not to mention that it takes a lot of time and effort to do it properly. In this study, we propose several machine learning models that automatically detect explicit contents in Korean lyrics and compare their performances. The proposed Bagging with selective vocabulary model outperformed not only the other competing models we designed, but also the filtering method that used the man-made profanity dictionary, which is a widely-used method to detect explicit contents in the industry. The proposed automated lyrics screening approach makes practical contributions to music industry, helping it significantly save time and effort for censoring harmful contents for the youths. The proposed approach is generalizable to other language settings as long as the same kinds of data used in the study are available.


Usefulness of temporal information automatically extracted from news articles for topic tracking
Temporal information plays an important role in natural language processing (NLP) applications such as information extraction, discourse analysis, automatic summarization, and question-answering. In the topic detection and tracking (TDT) area, the temporal information often used is the publication date of a message, which is readily available but limited in its usefulness. We developed a relatively simple NLP method for extracting temporal information from Korean news articles, with the goal of improving performance of TDT tasks. To extract temporal information, we make use of finite state automata and a lexicon containing timerevealing vocabulary. Extracted information is converted into a canonicalized representation of a time point or a time duration. We first evaluated and investigated the extraction and canonicalization methods for their accuracy and the extent to which temporal information extracted as such can help TDT tasks. The experimental results show that time information extracted from the text does indeed help to significantly improve both precision and recall.

Abstract:A recent research trend has emerged to identify developers' emotions, by applying sentiment analysis to the content of communication traces left in collaborative development environments. Trying to overcome the limitations posed by using off-the-shelf sentiment analysis tools, researchers recently started to develop their own tools for the software engineering domain. In this paper, we report a benchmark study to assess the performance and reliability of three sentiment analysis tools specifically customized for software engineering. Furthermore, we offer a reflection on the open challenges, as they emerge from a qualitative analysis of misclassified texts.


Abstract:This paper proposes method for aspect identification and sentiment analysis. This paper deals with the problem of sentence level. We use semantic syntax tree to extract NP nodes, a method based on slack to identify aspect and mini-support function to narrow aspect; to analyze the sentiment for the aspect we use dictionary-based method. Experimental results show that the proposed approach achieves the state-of-the-art.


Abstract:One of the NLP’s big problems (natural language processing) is a sentimental interpretation or opinion mining. Market analytics is playing a crucial role in the present scenario, perceiving that people are keen to increase their businesses. These people rely in particular on input from products used by the consumers to survive in the market and mining of information gives them an excellent picture of what they can expect in the future. Few words or sentences will choose consequences or effects. So, these people try to improve their business by selling luxury products to provide maximum benefit for their customers. Hence, sentimental analysis in the current years has acquired much attention. SA is an NLP research field used to categorize the perspective or the view inside a text of a particular feature. In addition, the data collection contains a variety of machine learning algorithms and the outcomes are correlated with the Decision Tree classifiers, Naive Bayes which are tested based on parameters such as recall, precision, and F- score. This paper is based on the various classification approaches for deciding whether or not the general sentimentality of a person is undesirable, positive, or impersonal according to the views expressed by the consumers and also predicts the star rating of a mobile. The two advanced methods such as feature classification followed by polarization classification along with the experimental results are also considered. In conclusion, in this paper a comparative analysis is performed among 3 classification techniques 1) Decision Tree, 2) Hybrid-Bag Boost algorithm, 3) Naive-Bayer’s where the hybrid algorithm is of high precision in comparison with the other two algorithms in machine learning. The key goal of the proposed method is to establish a standard for the assessment and the classification based on the analysis text.



Text-clustering based deep neural network for prediction of occupational accident risk: A case study

Local and Global Feature Based Hybrid Deep Learning Model for Bangla Parts of Speech Tagging
Abstract:Parts of Speech (pos) tagging corresponds to techniques of tagging accurate pos tags of each word of a sentence. It is a pivotal and indispensable part of sentiment analysis, synthesizing text to speech, analyzing textual data and many more Natural Language Processing (NLP) tasks. Though there are many researches done in the field of languages like English, Chinese etc., it is still comparatively unexplored for Bangla. An excellent pos tagger in Bangla language can accelerate NLP of Bangla textual data. In this study, we have developed a novel hybrid Deep Learning (DL) model with two parallel input networks with Convolutional Neural Network (CNN), and Bidirectional Long Short Term Memory (BiLSTM), and an output network of CNN. CNN works well regarding the local dependencies of words in sentences, and BiLSTM works well with the global dependencies of words regarding the context in a sentence. No other work has been done with local features of textual data along with long term word dependencies in Bangla pos tagging studies. The model can work with any external Word Embeddings. In this study, it is implemented with both Embedding Layer and Word2Vec layer. In the evaluation process, Mathews Correlation Coefficient (MCC), Precision-Recall curve are used along with accuracy and F1-score due to the dataset's imbalanced characteristic. Our proposed model accompanied by Word2Vec layer has outperformed Vanilla Recurrent Neural Network (RNN), Gated Recurrent Units (GRU), Long Short Term Memory (LSTM), BiLSTM networks, and also one recent study using the same dataset with the highest accuracy of 0.974, F1-score of 0.883, and MCC value of 0.936.

Lemmatization Algorithm Development for Bangla Natural Language Processing
Abstract:Natural language processing (NLP) finds enormous applications in autonomous communication, while lemmatization is an essential preprocessing technique for simplification of a word to its origin-word in NLP. However, there is scarcity of effective algorithms in Bangla NLP. This leads us to develop a useful Bangla language lemmatization tool. Usually, some rule base stemming processes play the vital role of lemmatization in Bangla language processing as there is lack of Bangla lemmatization tool. In this paper, we propose a Bangla lemmatization framework using three effective lemmatization techniques based on data structures and dynamic programming. We have used Trie algorithm and developed a mapping algorithm named “Dictionary Based Search by Removing Affix (DBSRA)” based on data structure. We have applied both Trie and DBSRA lemmatization and selected the better one by considering the Levenshtein distance between the lemma and the original word. Eventually, we have experimented with Bangla language lemmatization among all three techniques and the framework. Among the three proposed techniques, the DBSRA performed better compared to others with an accuracy of 93.1 percent. The framework, developed by fusing three algorithms, came out with the highest efficiency of 95.89 percent. Contribution-This paper presents the development of three lemmatization algorithms and their fusion to develop a framework for Bangla Natural Language Processing.

Finding a line between trusted and untrusted information on tweets through sequence classification
Abstract:The Internet has long since established itself as an indispensable source of information for both organizations and individuals. The lack of social responsibility of many digital platforms, however, offers many incentives for various forms of abuse. Disinformation, propaganda and fake news are just a few examples. Among the actors of information campaigns, we find not only individuals but also state actors with a clear agenda. Often, such information campaigns make use of psychological and rhetorical methods to achieve their goals. The manipulation of information is a major challenge for our democracies. It also presents us with major technical problems to identify and assess risks arising from the dissemination of such information. The following system description presents our approach to the detection of misinformation on social media data, which is twofold. Initially, we subjected the given training data to an exploratory analysis to get an overview of the general structure. Then we framed the given task as a simpler classification problem. In order to distinguish between trusted and untrusted information, using BERT (Bidirectional Encoder Representations from Transformers) as a neural network architecture for sequence classification, we started with a pre-trained model for language representation. In a supervised training step we fine-tuned this model on the given classification task with the provided annotated data. In this paper we would like to discuss both the quality of the training data and the performance of the trained classifier to derive promising directions for future work.

Short text similarity computation method based on feature expansion and Siamese network
Text similarity computation issues is a widely studied problem in natural language processing (NLP). Short text similarity computation is a new and more challenging problem, which cannot be effectively solved by using previous regular text similarity computation approach. The main reason is that, a short text generally contains limited number of words and fewer features can be extracted. In this paper, we propose a short text similarity computation method based on feature expansion and Siamese neural network. Firstly, a latent Dirichlet allocation (LDA) based model is constructed to expand the features of a short text. Then, deep features are extracted by using Siamese neural networks model which contains both convolutional neural networks (CNN) and Bi-directional long short-term memory (BiLSTM). Finally, the similarity of two short texts can be achieved by computing the Manhattan distance between generated feature vectors of these two texts. Experimental results show that, based on the data set of Ant Financial NLP Challenge, our method achieves higher accuracy and F1 score.

Abstract:The last few years have seen a tremendous surge in the information that is being dumped online. In this digital world every organization have their respective website which gives a detailed knowledge about them to the public. Considering organizations like educational institutions, their official websites provide all the necessary information from admissions to research works carried out. Prospective students, parents, researchers and academicians refer the website to in order to get relevant answers to their query. In order to retrieve an answer for their query one to has to spend a lot of time searching through the website, reading through several subpages available and consolidating the relevant information. This paper presents a model to retrieve concise and irredundant answers to various questions / queries regarding an educational institution -Amrita School of Engineering. Our model uses various Natural Language Processing (NLP) based techniques for text summarization to give appropriate results. Hybrid similarity measure and clustering algorithm are used for retrieving relevant data and removing redundancy respectively. Our model was tested by many users and the results were accurate in 86% of the cases.


Abstract:Named-Entity Recognition (NER) is a fundamental task in Natural Language Processing (NLP) and information extraction. NER is used to extract information such as names of the people, organizations, and places. NER has been used in many fields of work, one of which is in chatbot development. NLP and machine learning approaches enable a smarter chatbot with better personal analysis to users. This research builds a NER model in Indonesian Language using Bidirectional Long Short-Term Memory (BiLSTM) and Convolutional Neural Networks (CNNs) model architecture. Unlike the former research, this model only uses word-level embedding in the CNNs layer to keep the model simple. The Named-Entities (NEs) used in this study are limited to the name of the person, organization, location, quantity, and time using the BILOU labeling format. The performance of the model built is measured using micro-averaged f1 score evaluation metric. The BiLSTM-CNNs + pretrained word2vec embedding model provides good performance compared to other models with an f1 score of 71.37%.


Optimal target aiming maneuver for an advanced aircraft via timescale pseudospectral method
Abstract:A two-timescale Chebyshev pseudospectral optimal control method, which can reduce the converted nonlinear programming (NLP) significantly, is proposed. The optimal aiming maneuver problem of an advanced aircraft, which is a six-degree-of-freedom trajectory problem, is solved by this two-timescale method. It is the first time that the six-degree-of-freedom equations are used to solve optimization of the flight trajectory problems.

Cascading XSL filters for content selection in multilingual document generation
Content selection is a key factor of any successful document generation system. This paper shows how a content selection algorithm has been implemented using an efficient combination of XML/XSL technology and the framework of RST for discourse modeling. The system generates multilingual documents adapted to user profiles in a learning environment for the web. This CourseViewGenerator applies simplified RST schemes to the elaboration of a master document in XML from which content segments are chosen to suit the user's needs. The personalisation of the document is achieved through the application of a sequence of filtering levels of text selection based on the user aspects given as input. These cascading filters are implemented in XSL.

Abstract:This paper focuses majorly on the design of the web application which will be used to screen resumes (Curriculum Vitae) for a particular job posting. In the proposed system, a web application will encourage the job applicant candidates as well as the recruiters to use it for job applications and screening of resumes. Recruitment is a tedious process wherein the first task for any recruiter is to screen the resumes. The proposed web application is designed in such a way that job applicant as well as recruiters can use it with ease for applying for job openings and screening respectively. The recruiters from various companies can post the details of the job openings available in their respective companies. The interactive web application will allow the job applicants to submit their resume and apply for their job postings they may still be interested in. The resumes submitted by the candidates are then compared with the job profile requirement posted by the company recruiter by using techniques like machine learning and Natural Language Processing (NLP). Scores can then be given to the resumes and they can be ranked from highest match to lowest match. This ranking is made visible only to the company recruiter who is interested to select the best candidates from a large pool of candidates.



A method and application of automatic term extraction using conditional random fields


An analogical learner for morphological analysis


Latent Dirichlet Allocation (LDA) for Anomaly Detection in Avionics Networks

Automatic identification of non-compositional phrases
Non-compositional expressions present a special challenge to NLP applications. We present a method for automatic identification of non-compositional expressions using their statistical properties in a text corpus. Our method is based on the hypothesis that when a phrase is non-composition, its mutual information differs significantly from the mutual informations of phrases obtained by substituting one of the word in the phrase with a similar word.


Light-matter interactions mediated by nanoscale confinement in plasmonic resonators

Research on automatic recognition of Tibetan personal names based on multi-features
Abstract:Tibetan name has strong religious and cultural connotations, and the construction is different from Chinese name. So far, there is limited research on Tibetan name recognition, and current method for recognition of Chinese names does not work on Tibetan names. Therefore, through the analysis of Tibetan names characteristics, this paper proposes an automatic recognition method of Tibetan name based on multi-features. This method uses the internal features of names, contextual features and boundary features of names, and establishes the dictionary and feature base of Tibetan names. Finally, an experiment is conducted, and the results prove the algorithm is effective.

Answering learners' questions by retrieving question paraphrases from social Q&A sites
Information overload is a well-known problem which can be particularly detrimental to learners. In this paper, we propose a method to support learners in the information seeking process which consists in answering their questions by retrieving question paraphrases and their corresponding answers from social Q&A sites. Given the novelty of this kind of data, it is crucial to get a better understanding of how questions in social Q&A sites can be automatically analysed and retrieved. We discuss and evaluate several pre-processing strategies and question similarity metrics, using a new question paraphrase corpus collected from the WikiAnswers Q&A site. The results show that viable performance levels of more than 80% accuracy can be obtained for the task of question paraphrase retrieval.

100 things you always wanted to know about linguistics but were afraid to ask
Many NLP tasks have at their core a subtask of extracting the dependencies---who did what to whom---from natural language sentences. This task can be understood as the inverse of the problem solved in different ways by diverse human languages, namely, how to indicate the relationship between different parts of a sentence. Understanding how languages solve the problem can be extremely useful in both feature design and error analysis in the application of machine learning to NLP. Likewise, understanding cross-linguistic variation can be important for the design of MT systems and other multilingual applications. The purpose of this tutorial is to present in a succinct and accessible fashion information about the structure of human languages that can be useful in creating more linguistically sophisticated, more language independent, and thus more successful NLP systems. While many kinds of linguistic structure can be relevant to different NLP tasks, the focus of this tutorial will be on morphosyntax. The tutorial will take an explicitly typological perspective as an understanding of cross-linguistic variation can facilitate the design of more portable (language-independent) NLP systems. In order to help participants retain the information better, the tutorial will be structured interactively. I will ask participants for examples of tasks and data sets they work with, and then as a group we will brainstorm ways in which each of the linguistic properties discussed can related to feature design and/or error analysis for those tasks.

Abstract:Among the operational shortfalls that hinder law enforcement from achieving greater success in preventing terrorist attacks is the difficulty in dynamically assessing individualized violent extremism risk at scale given the enormous amount of primarily text-based records in disparate databases. In this work, we undertake the critical task of employing natural language processing (NLP) techniques and supervised machine learning models to classify textual data in analyst and investigator notes and reports for radicalization behavioral indicators. This effort to generate structured knowledge will build towards an operational capability to assist analysts in rapidly mining law enforcement and intelligence databases for cues and risk indicators. In the near-term, this effort also enables more rapid coding of biographical radicalization profiles to augment a research database of violent extremists and their exhibited behavioral indicators.


Abstract:Automatic summarization plays an important role in document processing system and information retrieval system. Generation of summary of a text document is a very important part of NLP. There are a number of scenarios where automatic construction of such summaries is useful. Text summarization is that process which convert a larger text into its shorter form maintaining its information. Summary of a longer text saves the reading time as it contain lesser number of lines but all important information of the original text document. In this paper we present a novel approach for text summarization of Hindi text document based on some linguistic rules. Dead wood words and phrases are also removed from the original document to generate the lesser number of words from the original text. Proposed system is tested on various Hindi inputs and accuracy of the system in form of number of lines extracted from original text containing important information of the original text document.


Speculation and negation annotation in natural language texts: what the case of BioScope might (not) reveal
In information extraction, it is of key importance to distinguish between facts and uncertain or negated information. In other words, IE applications have to treat sentences / clauses containing uncertain or negated information differently from factual information that is why the development of hedge and negation detection systems has received much interest -- e.g. the objective of the CoNLL-2010 Shared Task was also to develop hedge detection systems (Farkas et al., 2010). For the training and evaluation of such systems, corpora annotated for negation and speculation are necessary.


AdaReNet: Adaptive Reweighted Semi-supervised Active Learning to Accelerate Label Acquisition

Question Answering Chatbot using Deep Learning with NLP
Abstract:In spite of the number of techniques, models and datasets, Question Answering is still an exacting problem because of the issues in understanding the question and extracting the correct answer. It refers to creating platforms that when given a question in a natural language by humans, can automatically answer it. While many information retrieval chatbots achieve the task, recently, deep learning has earned a lot of attention to question answering due to its capability to learn optimal representation for the given task. This paper aims to build a closed domain, factoid Question Answering system. We recruit NLP methods of pattern matching and information retrieval to create an answer candidate pool. Before scoring similarities between the question and answers, we map them into some feature space. Our approach solves this task through distributional representations of the words and sentences wherein encodings store their lexical, semantic, and syntactic aspects. We use a convolutional neural network architecture to rank these candidate answers. Our model learns an optimal representation for the input question and answer sentences and a matching function to relate each such pair in a supervised manner from training data. Our model does not require any manual feature engineering or language sensitive data; hence can be extended to various domains. Training and testing on TREC QA, a Question Answering dataset, showed very promising metrics for our model.

Spatial load forecasting based on unstructured information processing and multi — Attribute deep learning
Abstract:A spatial load forecasting method based on unstructured information processing and multi - attribute depth learning is proposed. In order to solve the problem that the unstructured attributes have great influence on the load density but can't be directly put into calculation, the natural language processing (NLP) technique is used to structure those attributes. In view of the on characterization of the high dimension attributes by traditional method, Stacked Denoising Auto Encoder (SDAE) deep learning network is used to forecast the spatial load density. And the Rectified Linear Unit (ReLU) function is used as the excitation function of the network as well as the network structure gets improved to overcome the gradient disappearance and over-fitting. The results of case study show that the method of spatial load forecasting is effective and feasible.


2nd Workshop on Patent Text Mining and Semantic Technologies (PatentSemTech2021)

Efficient Large Scale NLP Feature Engineering with Apache Spark
Abstract:Feature engineering is a computationally time-consuming process in the end-to-end machine learning pipeline. Large amounts of text data are being generated on many heterogeneous sources and platforms on the internet. The compute resources needed to extract valuable features from these big datasets are increasing significantly. In this research, we evaluate the runtime of the RDD and the Spark-SQL APIs of the Apache Spark framework to extract text features from the corpus of english Wikipedia. As a result, we demonstrate the significant runtime performance of the SparkSQL compared to RDD API.

Towards an Automated Classification Approach for Software Engineering Research
The rapid growth of software engineering research publications forces an amount of scholarly knowledge that needs to be managed, organized and communicated in digital libraries and scientific search engines. Thus, there is a need for classified papers to accomplish these tasks, but the classification process is cumbersome. Moreover, in case of new schemas, one would need to reclassify previously published research. We propose to automate the classification and present different possible techniques for doing so: Using natural language models, a rule-based approach, or an approach based on topic-labeling. In this proposal paper, we initially implemented a prototype for text classification of software engineering research papers.

An n-gram frequency database reference to handle MWE extraction in NLP applications
The identification and extraction of Multiword Expressions (MWEs) currently deliver satisfactory results. However, the integration of these results into a wider application remains an issue. This is mainly due to the fact that the association measures (AMs) used to detect MWEs require a critical amount of data and that the MWE dictionaries cannot account for all the lexical and syntactic variations inherent in MWEs. In this study, we use an alternative technique to overcome these limitations. It consists in defining an n-gram frequency data-base that can be used to compute AMs on-the-fly, allowing the extraction procedure to efficiently process all the MWEs in a text, even if they have not been previously observed.


COVID19 Tweeter Dataset Sentiment Analysis


Information retrieval by text summarization for an Indian regional language

Abstract:Identification of non-anaphoric anaphora is an important step towards a full anaphora resolution. In this paper, we present an automatic identification approach for this task. In our work, some novel features are proposed, which are based on dependency grammars, surrounding words and their POS tags. All the features are automatically extracted using a part-of-speech (POS) tagger and a dependency parser. Our experiments are on a commonly available dialogue corpus, Trains-93. Several machine learning algorithms are used in the experiments, including CME, CRF and SVM. Results show that compared to the approaches used in the previous work, our algorithm is simpler and achieves a higher accuracy.


Abstract:in this paper; we present a closed domain factoid question answering system focusing on a why type Questions that can generate the answers of subjective type questions. The reasons we work on this research is that we want to show the system on what bases the why type Answers are generated this is because the strong requirement to satisfied in the early phase of system development. we follow the knowledge-based approach using TF-IDF algorithm and TF-IDF Vectorizer module we find that using the four parameter on the bases of because, neither, either, thoughts, and thus parameter is considered and lastly we present an evaluation of our system by giving precision, recall, & Accuracy of our system for why type questions.


Estimating emotion changes using electroencephalographic activities and its clinical application
Abstract:In the care, the communication based empathetic understanding is important. We have examined how to grasp empathetic understanding. We presume that the measurement of electroencephalographic (EEG) changes, those activities that are considered physiological indicators, enables an objective understanding of changes in emotions of those who have difficulty in expressing these through facial expression or physical action. Generally, EEG is used in the hospital to examine encephalopathy and brain disorder. Using an electroencephalograph device to acquire digital data we propose a method to objectively capture changes in the recognition state of people from changes in EEG activities (action potential), and a way to apply it into a clinical situation.


Trajectory Optimization of Aircraft Tactical Engagement Based on Model Prediction Control


A modified particle swarm optimizer with roulette selection operator

Annotating the semantic web using natural language
Because the ultimate purpose of the Semantic Web is to help users better locate, organize, and process content, we believe that it should be grounded in the information access method humans are most comfortable with---natural language. However, the Resource Description Framework (RDF), the foundation of the Semantic Web, was designed to be easily processed by computers, not humans. To render RDF more friendly to humans, we propose to augment it with natural language annotations, or metadata written in everyday language. We argue that natural language annotations, parsed into computer-readable representations, are not only intuitive and effective, but can also accelerate the pace with which the Semantic Web is being adopted. We believe that our technology can facilitate a happy marriage between natural language technology and the Semantic Web vision.


Application of Convolutional Neural Network in Natural Language Processing

Abstract:In today's world recruitment has become a tedious process. It is very difficult for the recruiters to read each and every resume manually. If there is an opening for a position in the firm, millions of people will send their resumes to the company to apply for that position. It is a tough task for a company's recruiters to manually look through hundreds of resumes and choose the best prospects for the position. The suggested web application is developed in such a way that it makes the hiring process considerably easier and simpler. This web application helps us to screen and rank the candidates resumes. The submitted resumes are then compared with the job description and finds the best profiled resumes using the technique like Natural Language Processing (NLP). Finally, the resumes are scored and ranked from best to worst match. This ranking is only available to the corporate recruiter who is looking to choose the best profiled prospects from a wide pool of applications. NLP offers a data-driven approach to resume screening that not only saves you time per candidate but also helps you make better recruiting decisions. NLP also allows us to rate and categorize applicant profiles while eliminating human biases.


Abstract:One of the roles of a natural language processing (NLP) model in continuous speech recognition (CSR) systems is to find the best sentence hypothesis by ranking all n-best sentences according to the grammar. This paper describes a robust parsing algorithm for spoken language recognition (SLR) which utilizes a technique that improves the efficiency of parsing. This technique integrates grammatical and statistical approaches, and by using a best-first parsing strategy improves the accuracy of recognition. Preliminary experimental results using a Persian continuous speech recognition system show effective improvements in accuracy with little change in recognition time. The word error rate was also reduced by 18%.


Abstract:Seeking bits of useful information from a large amount of data on the Web still remains a difficult and time consuming task for a wide range of people such as students, reporters, and many other types of professionals. This problem requires to investigate new ways to handle and process information, that has to be delivered in a rather small space, retrieved in a short time, and represented as accurately as possible. This is surely one of the most important reasons for searching suitable and efficient summarization techniques capable of "distilling" the most important information from a variety of logically related sources, as the one returned from classic search engines, in order to produce a short, concise and grammatically meaningful version of information spread out in pages and pages of texts. In this paper we present a summarizer system, named iWIN (information on the Web In a Nutshell), that is able to perform an automatic summarization of multiple documents through: a semantic analysis of the text, a ranking method used to evaluate the relevance of the information for the specific user, a clustering method based on the document representation in terms of set of triplets (subject, verb, object) and a sentences' selection/ordering process to make the final summary as much readable as possible. Some preliminary results about system performances obtained using the ROUGE evaluation software are presented and discussed.


A Short Survey of Pre-trained Language Models for Conversational AI-A New Age in NLP
Building a dialogue system that can communicate naturally with humans is a challenging yet interesting problem of agent-based computing. The rapid growth in this area is usually hindered by the long-standing problem of data scarcity as these systems are expected to learn syntax, grammar, decision making, and reasoning from insufficient amounts of task-specific dataset. The recently introduced pre-trained language models have the potential to address the issue of data scarcity and bring considerable advantages by generating contextualized word embeddings. These models are considered counterpart of ImageNet in NLP and have demonstrated to capture different facets of language such as hierarchical relations, long-term dependency, and sentiment. In this short survey paper, we discuss the recent progress made in the field of pre-trained language models. We also deliberate that how the strengths of these language models can be leveraged in designing more engaging and more eloquent conversational agents. This paper, therefore, intends to establish whether these pre-trained models can overcome the challenges pertinent to dialogue systems, and how their architecture could be exploited in order to overcome these challenges. Open challenges in the field of dialogue systems have also been deliberated.

Abstract:Theoretical - Machine learning is the subset of manmade thinking that goes under data science. Without explicitly redid, getting PCs to learn is a science known as Machine Learning. The proposition systems present in the market are accepted to be working in mainstream applications like YouTube online media applications like Facebook, Instagram or thing based applications like Flipkart. Basically, these systems help to zero in on information that is concerned or important for a particular customer. One region where such structures can be particularly useful is contamination forewarning framework. Considering an ailment the customer commitments to the structure, that he thinks they are slanted to or they are encountering they will be proposed top 5 or top 3 afflictions they are for the most part slanted to reliant upon the similarity between the contamination customer inputted and the sickness customer is being recommended for the present circumstance being forewarned. As of now, everything is open on the web, every contamination and its information around there. Experts are there but simultaneously the count of ailments, number of patients for a disease are extending. An individual has one disorder then there are chances they will get another. Sickness incorporate among youths in this age bundle is growing at a gigantic rate. There is the fix of afflictions or conceivably not anyway shouldn't something be said about notice. If we alert someone before they are truly encountering a contamination. It will make him/her considerably more careful than beforehand. This paper examines existing recommender systems and besides includes the burdens of such structures. Drawbacks can be adaptability, cold start and sparsity. The proposed system partakes in its advantages anyway isn't yet available watching out. Assessment has been done on how this disease alerted system using content-based idea under AI is eliminating features from dataset and how this structure presents features like customer self-rule, straightforwardness and no infection start.


Grouping strategy using Enneagram typologies
Abstract:Students are often encouraged to work in groups to solve complex tasks and develop collaboration skills. Classical grouping strategies don't take into account the student's personality or the compatibilities of group members. We introduce an alternative grouping method based on the student typologies and their neuro-linguistic programming (NLP) profile. Typologies are determined, according to the Enneagram methodology, based on the RHETI test. The result of such a test is a chief feature (base typology). Groups are composed of members with compatible chief features. Compatibilities are determined according to the Enneagram principle. New grouping correlations were introduced based on MBTI types. The NLP profile is evaluated by an eye-tracking system, based on eye movement patterns. The proposed grouping method was tested using an e-learning environment, which consists of a CMS (Content Management System) and an eye-tracking component. Eye tracking is based on a low-cost solution that uses a regular webcam. Groups created using this method show an increased communication among the members and better practical results. The tests were realized considering Bachelor and Master of Science students in different educational and practical work activities.

Towards an OWL-based framework for extracting information from clinical texts
In this paper, we present our ongoing work towards an OWL-based framework for extracting a variety of information (including patient history) from clinical texts. Our framework integrates a well-known natural language processing (NLP) system by converting its ontology and output logical form interpretation into the Web Ontology Language (OWL). The OWL-based Semantic Query-Enhanced Web Rule Language (SQWRL) is then used as a platform for authoring Semantic Web-aware rules for extracting information of interest from the OWL knowledge based created from parsing a clinical report. We also describe our ongoing work on using this system for extracting a timeline-based patient medical record from the history of present illness section of clinical texts.


A sort Approach for Anaphora Resolution of Chinese Personal Pronoun Based on Machine Learning Method

From character to word level: enabling the linguistic analyses of inputlog process data
Keystroke-logging tools are widely used in writing process research. These applications are designed to capture each character and mouse movement as isolated events as an indicator of cognitive processes. The current research project explores the possibilities of aggregating the logged process data from the letter level (keystroke) to the word level by merging them with existing lexica and using NLP tools. Linking writing process data to lexica and using NLP tools enables researchers to analyze the data on a higher, more complex level. In this project the output data of Inputlog are segmented on the sentence level and then tokenized. However, by definition writing process data do not always represent clean and grammatical text. Coping with this problem was one of the main challenges in the current project. Therefore, a parser has been developed that extracts three types of data from the S-notation: word-level revisions, deleted fragments, and the final writing product. The within-word typing errors are identified and excluded from further analyses. At this stage the Inputlog process data are enriched with the following linguistic information: part-of-speech tags, lemmas, chunks, syllable boundaries and word frequencies.


Application of Arabic language processing in language learning

Predicting Customer Churn in the Telecommunication Industry by Analyzing Phone Call Transcripts with Convolutional Neural Networks
For telecommunication service providers, a principle method for reducing costs and generating revenue is to focus on retaining existing customers rather than acquiring new customers. To support this strategy, it is important to understand customer concerns as early as possible to prevent churn: The customer action of canceling a subscription and moving to a new provider. In this paper, we use actual customer phone call data and develop the convolutional neural network (CNN)-based predictive model to detect churn signals from transcript data of phone calls. Experimental results show that when sufficient training data is provided with our text annotation method, our CNN-based predictive model generates state-of-the-art performance in churn prediction.

Abstract:Online social network such as Twitter, LinkedIn are widely used now a days. In these people lists their personal information and favorite activities which meant to be secure. Private information leakage becomes key issues with social network users. Apart from this user are hampered with various types of malicious data attacks which feel users very embarrassing in a real life. Also manual filtering for such a large data is not feasible at all. So various users stay away from social network sites to avoid such activities the social network architecture should be improved so that normal user can take a relief. Proposed work is an automatic prevention mechanism for such a heavy data using NLP and data mining approach. Objective of work is creation of real time rule sets to filter data using graph theory.


Sentence difficulty evaluation for a learner's dictionary
Abstract:In order to choose the best example sentences for Chinese language learners, we have compared how well different methods of estimating difficulty (reading time, translation time, direct rating) could be approximated from a sentence's superficial features. We have found that direct rating of difficulty by a user is the most promising, and that character-based features allow for better evaluation than word-based ones, all of which bodes well for user-tailored learner's dictionaries.


A Comparative Study of Neural Network for Text Classification

Robust speech recognition technology program summary
The major objective of this program is to develop and demonstrate robust, high-performance continuous speech recognizer (CSR) techniques and systems focused on application in spoken language systems (SLS). A key supporting objective is to develop techniques for integration of CSR and natural language processing (NLP) systems in SLS applications. The CSR techniques are based on a continuous-observation Hidden Markov Model (HMM) approach. Efforts are focused on improved HMM training and recognition for high performance and robustness in advanced SLS environments which include variabilities due to spontaneous speech, noise, and task-induced stress. Robustness is also being addressed through a new effort in comparison and development of auditory model front ends for HMM recognizers. The effort in CSR/NLP integration is focused on development of a structured CSR/NLP interface, which will allow effective collaboration with and between other groups developing NLP and/or CSR systems.

Abstract:Recent years have seen great process in studying English question classification. In our research, we learn Chinese question classification by exploiting the result of lexical, syntactic and semantic parsing on question sentences. Support vector machines are adopted to train a classifier on 6 coarse categories using single and combination of different parsing results as features. We find that even the surface information such as words and parts of speech could lead to a satisfying result, while augmenting the classifier with syntactic and semantic features could give even higher precision. However, the lack of words and incomplete syntactic structures among most questions cause combination of features even sparser than single features in the feature space, with much side effect brought to the performance of Chinese question classification.


Gender Bias and Under-Representation in Natural Language Processing Across Human Languages
Natural Language Processing (NLP) systems are at the heart of many critical automated decision-making systems making crucial recommendations about our future world. However, these systems reflect a wide range of biases, from gender bias to a bias in which voices they represent. In this paper, a team including speakers of 9 languages - Chinese, Spanish, English, Arabic, German, French, Farsi, Urdu, and Wolof - reports and analyzes measurements of gender bias in the Wikipedia corpora for these 9 languages. In the process, we also document how our work exposes crucial gaps in the NLP-pipeline for many languages. Despite substantial investments in multilingual support, the modern NLP-pipeline still systematically and dramatically under-represents the majority of human voices in the NLP-guided decisions that are shaping our collective future. We develop extensions to profession-level and corpus-level gender bias metric calculations originally designed for English and apply them to 8 other languages, including languages like Spanish, Arabic, German, French and Urdu that have grammatically gendered nouns including different feminine, masculine and neuter profession words. We compare these gender bias measurements across the Wikipedia corpora in different languages as well as across some corpora of more traditional literature.

Weakly supervised relevance feedback based on an improved language model
Abstract:Relevance feedback, which traditionally uses the terms in the relevant documents to enrich the user's initial query, is an effective method for improving retrieval performance. This approach has another problem is that Relevance feedback assumes that most frequent terms in the feedback documents are useful for the retrieval. In fact, the reports of some experiments show that it does not hold in reality many expansion terms identified in traditional approaches are indeed unrelated to the query and harmful to the retrieval. In this paper, we propose to select better and more relevant documents with a clustering algorithm. And then we present an improved Language Model to help us identify the good terms from those relevant documents. Ours experiments on the 2008 TREC collection show that retrieval effectiveness can be much improved when the improved Language Model is used.

Abstract:The large scale of scholarly publications poses a challenge for scholars in information-seeking and sensemaking. Bibliometric, information retrieval (IR), text mining and NLP techniques could help in these activities, but are not yet widely used in digital libraries. This workshop is intended to stimulate IR researchers and digital library professionals to elaborate on new approaches in natural language processing, information retrieval, scientometric and recommendation techniques which can advance the state-of-the-art in scholarly document understanding, analysis and retrieval at scale.



Sentiment Classification with Gated CNN for Customer Reviews


Open domain event extraction from twitter

Abstract:We have yet to realise the full capability of social media as an innovative information platform during emergencies and crisis response and management. Sentiment analysis can systematically identify, extract, and scrutinise emotional states and subjective information in social media data. Exploring reactions and perceptions to response messaging is invaluable and proved especially useful for a pandemic response as it can demonstrate general population reaction to the pandemic and governments response actions. This can be further analysed to identify the gap between government response actions and communications and citizens' perceptions. In this paper, an analysis of Twitter data explores population reaction towards COVID-19 health messaging. A Natural Language Processing Python tool is known as TextBlob was used to discover general data sentiment. Data were divided into three sentiments and text extraction of health messages was conducted to explore subsequent tweets in predefined categories. Our findings show the outcome of Tweets analysis could help us to identify the general population concerns and their reactions to COVID-19 to give a better understanding of the situation to governments and support them in implementing appropriate policies.



NLP based Machine Learning Approaches for Text Summarization

Abstract:The given paper describes modern approach to the task of sentiment analysis of movie reviews by using deep learning recurrent neural networks and decision trees. These methods are based on statistical models, which are in a nutshell of machine learning algorithms. The fertile area of research is the application of Google's algorithm Word2Vec presented by Tomas Mikolov, Kai Chen, Greg Corrado and Jeffrey Dean in 2013. The main idea of Word2Vec is the representations of words with the help of vectors in such manner that semantic relationships between words preserved as basic linear algebra operations. The extra advantage of the mentioned algorithm above the alternatives is computational efficiency. This paper focuses on using Word2Vec model for text classification by their sentiment type.



Industrial Requirements Classification for Redundancy and Inconsistency Detection in SEMIOS

A Semantic Approach for Traceability Link Recovery in Aerospace Requirements Management System
Abstract:The efficiency and effectiveness of the recovery of traceability links in requirements management is becoming increasingly important within interdisciplinary industry. Due to the complexity of production development such as automotive, software industry and the aerospace industry, managing requirements is indispensable and challenging. Products in these industries are constantly being updated and modified as the understanding of risks increases with experience and new products are developed in light of such risk. Therefore, the traceability links among the requirements artifacts, which fulfill business objectives, is so critical to reducing the risk and ensuring the success of products. To that end, this paper propose a semantic based traceability link recovery (STLR) architecture. According to best of our knowledge this is the first architectural approach that uses DBpedia knowledge-base and Bablenet 2.5 multilingual dictionary and semantic network for finding similarity among requirements and the automation of the recovery traceability links using our novel Triple extraction, and Triple disambiguation algorithm. Our preliminary results show the effectiveness in term of precision and recall compared to Vector Space Model and Wu Palmer algorithm.

Power Fault Preplan Text Information Extraction Based on NLP
Abstract:A large amount of texts recorded in Chinese exist in power grid enterprises. These texts contain abundant information of power system. Manually mining the text information is inefficient and the accuracy may vary with different dispatchers. In this paper, the power fault countermeasure text is taken as the object to study the power Chinese text information extraction method. Power texts are segmented firstly based on the nature language process (NLP), the ontology lexicon is established according to the power word attribute in the power fault countermeasure text; Based on the syntax structure characteristics of punctuations and the concept of separate parsing phrase are brought in to guide the division of long texts, which can separate the sentence with only one power entity and its related information; The syntax rule template applicable to the separate parsing phrase is established based on the meta-character templates (generalization slot, fixed word-combination, wildcard character, and registry function) used for the power fault preplan text information extraction and the structured output of that information; At last, the generalization ability and the universality of the template are analyzed. Examples show that the rule template applies to the information extraction of most texts with strong universality and high accuracy.

A dynamic content summarization system for opportunistic driver infotainment
The in-vehicle experience offers a unique challenge for delivering the right amount of information to the driver at the right time. The level of attention required to successfully manage the driving task is often in variable. An ideal in vehicle information delivery system would deliver content to the driver only during low task demand times, such as waiting at a stop light, when the driver's safety would be minimally compromised. The system would also have to respond to sudden changes in the situation such as driver interruption or distraction and terminate gracefully, allowing the driver to refocus on the driving task. In this paper, we present an embedded natural language processing (NLP) system that delivers speech synthesized summarized text content into tailored time slices. The system is also designed to respond dynamically to interruptions. We anticipate that this system could safely deliver speech synthesized content to drivers and allow them to make the most of their time on the road. We have implemented this system on an Atom Z530 processor with 1GB of RAM, a processor comparable to those found in factory installed In-Vehicle Infotainment (IVI) systems and have evaluated it in a laboratory test using a standard NLP corpus to demonstrate this potential.

Abstract:The increasing pace of change in languages affects many applications and algorithms for text processing. Researchers in Natural Language Processing (NLP) have been striving for more generalized solutions that can cope with continuous change. This is even more challenging when applied on short text emanating from social media. Furthermore, increasingly social media have been casting a major influence on both the development and the use of language. Our work is motivated by the need to develop NLP techniques that can cope with short informal text as used in social media alongside the massive proliferation of textual data uploaded daily on social media. In this paper, we describe a novel approach for Short Text Topic Modelling using word embeddings and taking into account any informality of words in the social media text with the aim of addressing the challenge of reducing noise in messy text. We present a new algorithm derived from the Term Frequency -Inverse Document Frequency (TF-IDF), named Term Frequency - Inverse Context Term Frequency (TF-ICTF). TF-ICTF relies on a probabilistic relation between words and context with respect to time. Our experimental work shows promising results against other state-of-the-art methods.


Decoding The Style And Bias of Song Lyrics
The central idea of this paper is to gain a deeper understanding of song lyrics computationally. We focus on two aspects: style and biases of song lyrics. All prior works to understand these two aspects are limited to manual analysis of a small corpus of song lyrics. In contrast, we analyzed more than half a million songs spread over five decades. We characterize the lyrics style in terms of vocabulary, length, repetitiveness, speed, and readability. We have observed that the style of popular songs significantly differs from other songs. We have used distributed representation methods and WEAT test to measure various gender and racial biases in the song lyrics. We have observed that biases in song lyrics correlate with prior results on human subjects. This correlation indicates that song lyrics reflect the biases that exist in society. Increasing consumption of music and the effect of lyrics on human emotions makes this analysis important.

Question to Question Similarity Analysis using Morphological, Syntactic, Semantic, and Lexical Features
Abstract:In the digitally connected world that we are living in, people expect to get answers to their questions spontaneously. This fact increased the burden on the Question/Answer platforms such as Stack Overflow and many others. A promising solution to this problem is to detect if a question being asked similar to a question in the database and present the answer of the detected question to the user. To address this challenge, we propose a novel Natural Language Processing (NLP) approach that detects if two Arabic questions are similar or not using their extracted morphological, syntactic, semantic, lexical features. Our approach involves several phases including Arabic text processing, novel feature extraction, and text classifications. To conduct our experiments, we used a real-world questions dataset consisting of 4,000 pairs of Arabic questions in which our approach achieved 78.2% accuracy using XGBoost model on the best features selected by the Random Forest feature selection technique. This high accuracy shows the ability of our approach to correctly detect the similarity between two Arabic questions.

Abstract:The tasks of Natural Language Processing (hereinafter - NLP) are very topical today, in particular in the context of the application of machine learning and artificial intelligence models. In this work the approach allowing to carry out classification of texts with the minimal required manual markup is considered. The problem of prediction of resonance of news in the Kazakhstani media space where resonance is understood as abnormal interest of the public to the publication within the limits of a concrete information source is considered. The proposed model is implemented and tested within an informational system.


High-performance high-volume layered corpora annotation
NLP systems that deal with large collections of text require significant computational resources, both in terms of space and processing time. Moreover, these systems typically add new layers of linguistic information with references to another layer. The spreading of these layered annotations across different files makes them more difficult to process and access the data. As the amount of input increases, so does the difficulty to process it. One approach is to use distributed parallel computing for solving these larger problems and save time. We propose a framework that simplifies the integration of independently existing NLP tools to build language-independent NLP systems capable of creating layered annotations. Moreover, it allows the development of scalable NLP systems, that executes NLP tools in parallel, while offering an easy-to-use programming environment and a transparent handling of distributed computing problems. With this framework the execution time was decreased to 40 times less than the original one on a cluster with 80 cores.

Abstract:Medical and healthcare study programmes are quite complicated in terms of branched structure and heterogeneous content. In logical sequence a lot of requirements and demands placed on students appear there. This paper focuses on an innovative way how to discover and understand complex curricula using modern information and communication technologies. We introduce an algorithm for curriculum metadata automatic processing - automatic keyword extraction based on unsupervised approaches, and we demonstrate a real application during a process of innovation and optimization of medical education. The outputs of our pilot analysis represent systematic description of medical curriculum by three different approaches (centrality measures) used for relevant keywords extraction. Further evaluation by senior curriculum designers and guarantors is required to obtain an objective benchmark.


Does BERT Pay Attention to Cyberbullying?
Social media have brought threats like cyberbullying, which can lead to stress, anxiety, depression, and in some severe cases, suicide attempts. Detecting cyberbullying can help to warn/ block bullies and provide support to victims. However, very few studies have used self-attention-based language models like BERT for cyberbullying detection and they typically only report BERT's performance without examining in depth the reasons for its performance. In this work, we examine the use of BERT for cyberbullying detection on various datasets and attempt to explain its performance by analyzing its attention weights and gradient-based feature importance scores for textual and linguistic features. Our results show that attention weights do not correlate with feature importance scores and thus do not explain the model's performance. Additionally, they suggest that BERT relies on syntactical biases in the datasets to assign feature importance scores to class-related wordsrather than cyberbullying-related linguistic features.


Electrically driven exciton-polariton lasers

Abstract:Treebanks are essential resources for both data-driven approaches to natural language processing (NLP) and empirical linguistic researches. Developing these resources is time- and cost-consuming and requires specialized expertise. Therefore, they should be designed to be reused for different purposes. Currently, there are several dependency treebanks for some languages which are annotated in CoNLL format. For some languages, such as Persian, they are the few available linguistic resources. These treebanks are more suitable for the input of data-driven parsers, and querying linguistic data in them is not easy. In recent years, XML has been widely used for formatting treebanks, and there are various tools available for querying and annotating a linguistic croups in this format. In this paper, we present a tool for converting a dependency treebank in CoNLL format to an appropriate XML format. We designed the XML scheme to be particularly suitable for writing linguistic queries in XQuery syntax.


Representing and querying multi-dimensional markup for question answering
This paper describes our approach to representing and querying multi-dimensional, possibly overlapping text annotations, as used in our question answering (QA) system. We use a system extending XQuery, the W3C-standard XML query language, with new axes that allow one to jump easily between different annotations of the same data. The new axes are formulated in terms of (partial) overlap and containment. All annotations are made using stand-off XML in a single document, which can be efficiently queried using the XQuery extension. The system is scalable to gigabytes of XML annotations. We show examples of the system in QA scenarios.

LayoutLM: Pre-training of Text and Layout for Document Image Understanding
Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the widespread use of pre-training models for NLP applications, they almost exclusively focus on text-level manipulation, while neglecting layout and style information that is vital for document image understanding. In this paper, we propose the LayoutLM to jointly model interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM. To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for document-level pre-training. It achieves new state-of-the-art results in several downstream tasks, including form understanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification (from 93.07 to 94.42). The code and pre-trained LayoutLM models are publicly available at https://aka.ms/layoutlm.

http://affective-sciences.org/home/research/materials-and-onlineresearch/research-material/ (International Survey On Emotion Antecedents And Reactions) dataset and the Vietnamese emotion dataset. The experimental results show that the proposed model is very effective in comparison with previous studies.


Detecting and Correlating Video-Based Event Patterns: An Ontology Driven Approach
Abstract:With increasing amount of information (video, text) being available today, it has become non-trivial to develop techniques to categorize documents into contextually meaningful classes. The information as available in the documents is composed of sequence of events termed as patterns. It is evident to know the important trends as observed from patterns that are emerging over a specific time period and space. For identifying the patterns, we must focus on semantic meaning of documents. Tracing such patterns in videos or texts manually is a time-consuming, cumbersome or an impossible task. So, in this paper we have devised an unsupervised trend discovery approach that detects and correlates event patterns from videos temporally as well as spatially. We begin by building our own document collection on the basis of contextual meaning of documents. This helps in associating an input video with another video or text documents on the basis of their semantic meaning. This approach helps in accumulating variety of information that is scattered over the web thus providing relatively complete information about the video. The highly correlated words are grouped in a topic using Latent Dirichlet Allocation (LDA). To identify topics an E-MOWL based ontology is used. This event ontology helps in discovering associations and relations between the various events. With this kind of representation, the users can infer different concepts as emerged over time. For identifying the various spatial patterns that exist corresponding to an event in a document, we have utilized geographic ontology (Geoontology). We establish validity of our approach using experimental results.

Extending a broad-coverage parser for a general NLP toolkit
With the rapid growth of real world applications for NLP systems, there is a genuine demand for a general toolkit from which programmers with no linguistic knowledge can build specific NLP systems. Such a toolkit should have a parser that is general enough to be used across domains, and yet accurate enough for each specific application. In this paper, we describe a parser that extends a broad-coverage parser, Minipar (Lin, 2001), with an adaptable shallow parser so as to achieve both generality and accuracy in handling domain specific NL problems. We test this parser on our corpus and the results show that the accuracy is significantly higher than a system that uses Minipar alone.

Extracting Precise Link Context Using NLP Parsing Technique
Link context has been exploited extensively ever since the advent of the World Wide Web, but the approach to extracting precise link context has not been fully explored and many state-of-the-art extraction methods are based on simplistic heuristics and require ad-hoc parameters. In this paper, we propose a novel two-step extraction model, which aims to systematically derive link context of quality as high as anchor text. In the macroscopic analysis step, a systematic web page structure analysis is performed to locate the content cohesive text region and potential relevant header or header like tags. In the microscopic extraction step, an English parser is used to extract the relevant sentence fragments in the text region and the nearest heading text is encompassed if the need arises. Preliminary experimental results proved our approach's effectiveness.


Exploiting lexical expansions and Boolean compositions for web querying

WISDOM: a web information credibility analysis system
We demonstrate an information credibility analysis system called WISDOM. The purpose of WISDOM is to evaluate the credibility of information available on the Web from multiple viewpoints. WISDOM considers the following to be the source of information credibility: information contents, information senders, and information appearances. We aim at analyzing and organizing these measures on the basis of semantics-oriented natural language processing (NLP) techniques.

UBC-AS: a graph based unsupervised system for induction and classification
This paper describes a graph-based unsupervised system for induction and classification. The system performs a two stage graph based clustering where a cooccurrence graph is first clustered to compute similarities against contexts. The context similarity matrix is pruned and the resulting associated graph is clustered again by means of a random-walk type algorithm. The system relies on a set of parameters that have been tuned to fit the corpus data. The system has participated in tasks 2 and 13 of the SemEval-2007 competition, on word sense induction and Web people search, respectively, with mixed results.

Making Sense of Subtitles: Sentence Boundary Detection and Speaker Change Detection in Unpunctuated Texts
The rise of deep learning methods has transformed the research area of natural language processing beyond recognition. New benchmark performances are reported on a daily basis ranging from machine translation to question-answering. Yet, some of the unsolved practical research questions are not in the spotlight and this includes, for example, issues arising at the interface between spoken and written language processing.  We identify sentence boundary detection and speaker change detection applied to automatically transcribed texts as two NLP problems that have not yet received much attention but are nevertheless of practical relevance. We frame both problems as binary tagging tasks that can be addressed by fine-tuning a transformer model and we report promising results.

Abstract:This paper addresses the application and integration of coreferences resolution tasks in a legislative corpus by using SpanBERT, which is an improvement of the BERT (Bidirectional Encoder Representations from Transformers) model and semantic extraction by Abstract Meaning Representation (AMR) for reducing text complexity, meaning preservation and further applications. Our main processes are divided into four subparts: legal text pre-processing, coreference resolution, AMR, evaluation for meaning preservation, and complexity reduction. Smatch evaluation tool and Bilingual Evaluation Understudy (BLEU) scores are applied to evaluate overlapped meaning between resolved and unresolved coreference sentences. The AMR graphs after complexity have been reduced can be applied for further processing tasks with Neural Network such as legal inferencing and legal engineering tasks.


Content analysis of MOOC forums: The characteristics of the learners' discourse in forums
Abstract:Considering the importance of social and collaborative learning, this research aims to explore the characteristics of the discourse in forums in order to have insight into learners' needs and interests in massive open online courses. This will help in developing an instructional strategy that will increase the learners' participation in forums, and their involvement in the creation of knowledge; consequently, improving learning processes. An innovative approach was taken in this study, using the Natural Language Processing (NLP) tool and Henri's content analysis model in order to analyze the learners' discourse in forums and identify their types of interactions. We hope that this approach will lead to additional understanding and shed light on the ability to create effective online learning communities, through the learners' behavior.


An Analyze Movement Path of Employees in Fire Drill by Indoor Location System Using Bluetooth

Natural language processing complexity and parallelism
Abstract:This paper reviews the processes involved in Natural Language Processing (NLP). It then demonstrates the various kinds of choices that need be taken during the execution of the word morphology, the syntactic text analysis, or text generation components. It compares the time complexity of traditional serial algorithms and examines the possible expected gain in some corresponding parallel counterparts.

Ontology-based WOM extraction service from weblogs
In this paper, we introduce a web-based service that extracts reputations of a product from the internet. If a user inputs the product name, the service first collects articles reviewing the product from weblogs, bbs, and so on. Also, it analyzes their contents using metadata and ontologies with conventional NLP techniques. Then, it indicates the reputations (positive or negative) from the overall and several pre-defined aspects, and other related products that are the subject of much discussion in the articles. This paper illustrates two technical points regarding use of metadata and ontologies with NLP, and summarizes evaluations in a case that we applied it to a market research for a vehicle.

Inferring tutorial dialogue structure with hidden Markov modeling
The field of intelligent tutoring systems has seen many successes in recent years. A significant remaining challenge is the automatic creation of corpus-based tutorial dialogue management models. This paper reports on early work toward this goal. We identify tutorial dialogue modes in an unsupervised fashion using hidden Markov models (HMMs) trained on input sequences of manually-labeled dialogue acts and adjacency pairs. The two best-fit HMMs are presented and compared with respect to the dialogue structure they suggest; we also discuss potential uses of the methodology for future work.


Rule-based extraction of family history information from clinical notes

Modeling case marking system of Urdu-Hindi languages by using semantic information
Abstract:Rule based modeling of the grammar of a language is important for various natural language processing chores. In this paper, formulation of Urdu/Hindi case marking system based on lexical functional grammar (LFG) is presented. In this paper, semantic information associated with nouns is incorporated to better classify grammatical roles adopted by each case. Especially, the versatile case marker 'sey' has found to adopt different roles depending upon semantic information associated with nouns. The agent role of 'sey' found by such classification helped identifying subject and indirect subject roles of tetravalent causative verbs found in Urdu/Hindi languages.

Towards intelligent arabic text-to-speech application for disabled people
Abstract:Assistive technology customizes speech technology to offer a new communication channel for disabled people such as blind or having speech difficulties. Converting written text into natural speech has been addressed in the last decades for some languages such as English, hence, used in many applications such as voice answering machines, reading articles and exploring software for blind people. Other languages such as Arabic are still not fully served to have high quality Text-To-Speech applications. This paper describes our effort in developing an intelligent Text-To-Speech mobile application for Arabic. We use a set of statistical language models n-gram for word prediction and auto-completion for easy typing. A large new Arabic corpus for daily communication in different domains is constructed which could be used for other purposes. A serious of normalization processing, including spelling correction, is applied to the corpus to maintain the consistency and unify the occurrence of the same words. We use outsource Sakhr Arabic Text-To-Speeh voices as one of the best speech synthesizer exist for Arabic. To ensure a high usability of the application, we use simple graphical user interface and easy access libraries to favorite phrases with an ability of adding pictures with recorded speech. Our experiments shows that word prediction using global and local corpus decries 50% of keystroke of typing desired sentences with a high prediction of 84% of bigram model.

Semantic search extension based on polish wordnet relations in business document exploration
This paper addresses the problem of building a specialized semantic search engine for documents collected in small or medium-sized enterprises. It presents the results of a project that brought together computer scientists and entrepreneurs for the purpose of providing a common perspective regarding the implementation in company practice of a search engine based on the Polish version of Word-Net semantic relations. The core functionality of the search engine module is provided along with a discussion on how to arrange semantic similarity structures so as to ensure the efficient generation of relevant search engine results. Some patterns and similarity coefficients for hyperonymy, hyponymy, holonymy and meronymy relations are presented and analyzed for the purpose of producing relationship structures. Finally, the architecture of the system that can be implemented in a company is outlined.

Abstract:Recent development and enhancement in the field of soft computing which is the cutting edge of Neural network and fuzzy logic. Here by we are proposing a solution using Deep Neural Network (DNN) and Fuzzy logics for Natural Language Processing (NLP) with is the sub part of Artificial Intelligence (AI). If we look India we have 22 constitutional human languages and in the world around 6909 living human languages. Building cross Machine Translation (MT) system using these all language which currently not possible because of lack of resources, knowledge and etc. If we have huge corpus of source and target language this propose system will automatically find out the grammatical structure of source sentence matching with the target language using Neural network and fuzzy logic and it is flexible machine learning technique it deals with uncertainly or vagueness existing in a system and formulating fuzzy rules to find grammar structure to a given new sentences.



Challenges and Barriers in Applying Natural Language Processing to Medical Examiner Notes from Fatal Opioid Poisoning Cases

Adversarial Attacks on Deep Models for Financial Transaction Records
Machine learning models using transaction records as inputs are popular among financial institutions. The most efficient models use deep-learning architectures similar to those in the NLP community, posing a challenge due to their tremendous number of parameters and limited robustness. In particular, deep-learning models are vulnerable to adversarial attacks: a little change in the input harms the model's output. In this work, we examine adversarial attacks on transaction records data and defenses from these attacks. The transaction records data have a different structure than the canonical NLP or time-series data, as neighboring records are less connected than words in sentences, and each record consists of both discrete merchant code and continuous transaction amount. We consider a black-box attack scenario, where the attack doesn't know the true decision model and pay special attention to adding transaction tokens to the end of a sequence. These limitations provide a more realistic scenario, previously unexplored in the NLP world. The proposed adversarial attacks and the respective defenses demonstrate remarkable performance using relevant datasets from the financial industry. Our results show that a couple of generated transactions are sufficient to fool a deep-learning model. Further, we improve model robustness via adversarial training or separate adversarial examples detection. This work shows that embedding protection from adversarial attacks improves model robustness, allowing a wider adoption of deep models for transaction records in banking and finance.

Abstract:The following topics are dealt with: learning (artificial intelligence); natural language processing; text analysis; feature extraction; Internet; pattern classification; support vector machines; neural nets; mobile computing; convolutional neural nets.


Agent-Based Model Characterization Using Natural Language Processing
Abstract:This paper reports on Natural Language Processing (NLP) as a technique to analyze phenomena towards specifying agent-based models (ABM). The objective of the ABM NLP Analyzer is to facilitate non-simulationists to actively engage in the learning and collaborative designing of ABMs. The NLP model identifies candidate agents, candidate agent attributes, and candidate rules all of which non-simulationists can later evaluate for feasibility. IBM's Watson Natural Language Understanding (NLU) and Knowledge Studio were used in order to annotate, evaluate, extract agents, agent attributes, and agent rules from unstructured descriptions of phenomena. The software, and related agent-attribute-rule characterization, provides insight into a simple but useful means of conceptualizing and specifying baseline ABMs. Further, it emphasizes on how to approach the design of ABMs without the use of NLP by focusing on the identification of agent, attributes and rules.

Abstract:Natural Language Processing (NLP) is a computerized way of analyzing texts. NLP involves the acquisition of knowledge on how a person understands and uses language. This is for the development of appropriate tools and techniques that allow computer systems to understand and manipulate natural languages to perform various desirable tasks. This article reviews the literature in the NLP. It also provides information about NLP history. Additionally it provides architectural design for semantic search tool using NLP, and discussed architectural modules, domain and constraints. This work proposes future possibilities for research and its application.


Identification of Noun Phrase with Various Granularities
Abstract:Since noun phrases are the most popular phrases in texts, noun phrase identification is one of vital subtasks of natural language processing. Generally Chinese noun phrases have hierarchical inner structures. This paper proposes an approach of defining various levels of granularity for noun phrases, catering for different application demands. Three levels of granularity noun phrases are proposed, that is, concept noun phrase, base noun phrase and entire noun phrase. The task of noun phrase identification is to label word sequences with phrase tags. All granularity noun phrase identifications are cast as classification problem under certain encoding schemes. The experimental dataset is acquired empirically from Chinese Penn Treebank 5.1. F, measure of concept noun phrase, base noun phrase and entire noun phrase identification reaches 92.12%, 84.13% and 85.32% respectively.

Low-Rank and Locality Constrained Self-Attention for Sequence Modeling
Self-attention mechanism becomes more and more popular in natural language processing (NLP) applications. Recent studies show the Transformer architecture which relies mainly on the attention mechanism achieves much success on large datasets. But a raised problem is its generalization ability is weaker than CNN and RNN on many moderate-sized datasets. We think the reason can be attributed to its unsuitable inductive bias of the self-attention structure. In this paper, we regard the self-attention as matrix decomposition problem and propose an improved self-attention module by introducing two linguistic constraints: low-rank and locality. We further develop the low-rank attention and band attention to parameterize the self-attention mechanism under the low-rank and locality constraints. Experiments on several real NLP tasks show our model outperforms the vanilla Transformer and other self-attention models on moderate size datasets. Additionally, evaluation on a synthetic task gives us a more detailed understanding of working mechanisms of different architectures.

