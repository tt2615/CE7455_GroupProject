adaptive natural - language targeting for student feedback
in tutoring software , targeting feedback to students ' natural - language inputs is a promising avenue for making the software more effective . as a case study , we built such a system using natural language processing ( nlp ) to provide adaptive feedback to students in an online learning task . we found that the nlp targeting mechanism , relative to more traditional multiple - choice targeting , was able to provide optimal feedback from fewer student interactions and generalize to previously unseen prompts . 

annotating indirect anaphora for hindi : a corpus based study
natural language processing requires a lot of analysis and information regarding words and segment of sentence . almost all nlp applications such as machine translation , information extraction , automatic summarization , question answering system , natural language generation , etc . , require successful identification and resolution of anaphora . information regarding word using pos tagger , parser and other tool can be gathered . hindi is language of free word order as compare to english . this enforces additional constraints on different nlp task . in this working paper we present an analysis of hindi genre . we used ten tags from literature . out of ten tags seven are annotated using botley ' s annotation scheme manually . we annotated 1540 demonstrative pronoun from twelve files of emilee corpus . input file is emilee file and output is fully annotated unicode file . 

cross - lingual perspectives about crisis - related conversations on twitter
the role of social networks during natural disasters is becoming crucial to share relevant information and coordinate relief actions . with the reach of the social networks , any user around the world has the possibility of interact in crisis - events as these unfold . a large part of the information posted during a disaster uses the native language where the disaster occurred . however , there are also users from other parts of the world who can comment about the event , often in another language . in this work , we conducted a study of crisis - related tweets about the earthquake that occurred in ecuador in april 2016 . to that end , we introduce a new annotated dataset in both spanish and english languages with approximately 8k tweets ; half of them belong to conversations . we evaluate several neural architectures to identify crisis - related tweets in a multi - lingual setting , and we found that deep contextual multi - lingual embeddings outperform other strong baseline models . we then explore the type of conversations that occur from the perspective of different languages . the results show that certain types of conversations occur more in the native language and others in a foreign language . conversations from foreign countries seek to gather situation awareness and give emotional support , while in the affected country the conversations aim mainly to humanitarian aid . 

self - supervised extractive text summarization for biomedical literatures
in this study , we propose a self - supervised approach to extractive text summarization for biomedical literature . the approach uses abstracts to find the most informative content in the article , then generate a summary for training a classification model . the sentences in the abstract and literature were first embedded using bert . a similarity - based model was then applied to label the informative sentences for training the classifier . we used logistic regression as our classification model and used the features of sentence embedding for the classification . the results showed the feasibility of employing the abstract to perform self - supervised training of a classification model to generate extractive summarization . this approach can enable automatic generation of one or two - page executive summaries of biomedical literature to keep clinicians and biomedical researchers up to date with the latest developmentview less

tectomt : highly modular mt system with tectogrammatics used as transfer layer
we present a new english → czech machine translation system combining linguistically motivated layers of language description ( as defined in the prague dependency treebank annotation scenario ) with statistical nlp approaches . 

evaluating the application of nlp tools in mainstream participatory budgeting processes in scotland
in recent years participatory budgeting ( pb ) in scotland has grown from a handful of community - led processes to a movement supported by local and national government . this is epitomized by an agreement between the scottish government and the convention of scottish local authorities ( cosla ) that at least 1 % of local authority budgets will be subject to pb . this ongoing research paper explores the challenges that emerge from this ‘ scaling up ’ or ‘ mainstreaming ’ across the 32 local authorities that make up scotland . the main objective is to evaluate local authority use of the digital platform consul , which applies natural language processing ( nlp ) to address these challenges . this project adopts a qualitative longitudinal design with interviews , observations of pb processes , and analysis of the digital platform data . thematic analysis is employed to capture the major issues and themes which emerge . longitudinal analysis then explores how these evolve over time . the potential for 32 live study sites provides a unique opportunity to explore discrete political and social contexts which materialize and allow for a deeper dive into the challenges and issues that may exist , something a wider cross - sectional study would miss . initial results show that issues and challenges which come from scaling up may be tackled using nlp technology which , in a previous controlled use case - based evaluation , has shown to improve the effectiveness of citizen participation . 

an efficient meta heuristic algorithm for pos - tagging
tagging is an increasingly important task in natural language processing domains . as there are many natural language processing tasks which can be improved by applying disambiguation to the text , fast and high quality tagging algorithms are a crucial task in information retrieval and question answering . tagging aims to assigning to each word of a text its correct tag according to the context in which the word is used . part of speech ( pos ) tagging is a difficult problem by itself , since many words has a number of possible tags associated to it . in this paper we present a novel algorithm that deals with pos - tagging problem based on harmony search ( hs ) optimization method . this paper analyzes the relative advantages of hs metaheuristic approache to the well - known natural language processing problem of pos - tagging . in the experiments we conducted , we applied the proposed algorithm on linguistic corpora and compared the results obtained against other optimization methods such as genetic and simulated annealing algorithms . experimental results reveal that the proposed algorithm provides more accurate results compared to the other algorithms . 

pruning deficiency of big data analytics using cognitive computing
since past few years the size of the data is growing extremely at fast rates 10 times faster in growth . this will include all the responsibilities to make smart decisions streaming from the browsing patterns and produce extra supplements which aids in the decision - making progress . as the size of the data is recorded from a variety of devices like mobile sensors , remote sensing and data is recorded from everywhere ; huge amount of data gets stored which is sometimes even never analysed . big data is a very “ big ” thing which is getting stored and increasing the volume of raw data sometimes 90 % of the raw data sets are never analysed and are just discarded from the memory . out of it just 10 % gets analysed sometimes and are converted into information from those raw data sets . so , analysis of data by human beings could be time consuming but processing enormous amount of data at a large scale using cognitive can be done . in this paper , we tried to focus on the areas where the cognitive computing can be used in order to lessen the shortcomings of the big data analytics , principles from which cognitive computing came . we also focused on the urgent need on how the language processing of data is done to understand the meaning of the rough data and process it to useful information . 

new words discovery method based on word segmentation result
a kind of new words discovery method based on word segmentation result is presented in this paper . word segmentation is an important part of many chinese natural language processing ( nlp ) tasks . improving the accuracy of chinese word segmentation is a matter of great concern . with the increasing number of web text , more and more chinese nlp tasks need to use micro - blog , movie review and other web text . the content of web text changes very fast and often contains a large number of new words . it is an important factor affecting the accuracy of word segmentation that word segmentation tools can not identify these new words . one way to solve this problem is to discover new words in the text to use and add these new words to the dictionaries on which the word segmentation tool depends . the traditional method of new words discovery can only find the words that do not exist in word segmentation tool ' s dictionary . but these words do not necessarily affect the result of the word segmentation . that is , the words may be correctly segmented even if they are not added to the word segmentation tool ' s dictionary . to address this issue , we propose to build a collection of candidate new words based on segmentation result . all the new words discovered in this way segmented by the word segmentation tool by mistake . adding these new words to the word segmentation tool ' s dictionary can improve the accuracy of the word segmentation more than traditional methods . experiments on the douban movie review dataset show that our method can get better new words to improve the accuracy on movie review sentiment classification . 

a novel nlp - fuzzy system prototype for information extraction from medical guidelines
medical guidelines have a significant role in the field of evidence - based medical treatment . the content of a medical guideline is based on a systematic review of clinical evidence with instructions and recommendations that clinicians can refer to . most of the guidelines are available in an unstructured text format . hence , clinicians must take a considerable time to search and find relevant recommendations in their semantic context . using machine learning algorithms , automatic information extraction from medical guidelines has recently become possible . we present a novel system for information extraction and a fuzzy rule database developed for clinical guidelines . the proposed system , dubbed nlp - fuzzy , combines capabilities of natural language processing ( nlp ) and fuzzy logic approaches . first , the nlp - fuzzy performs a semantic extraction of medical guidelines using a bi - directional long short - term memory ( lstm ) . subsequently , using the extracted semantic , it creates fuzzy rules , which are able to recognize new cases in a learning domain while predicting and extract the grade of recommendation . in order to test the nlp - fuzzy system , we compared its performance with state - of - the - art nlp approaches for clinical information extraction . 

research on the key technology based - nlp of chinese medicine pulse ' s mathematical quantifying
in this paper , chinese medicine pulse is dealt in view of measuration and information extraction . we analyze the research status and problems in quantifying pulse objectively . a mathematical model of ancient pulse is constructed by utilizing the technique and method of natural language processing ( nlp) , according to traditional chinese medicine theory and pulse - symptom knowledge . and the new description of pulse in modern mathematical quantization level will be established by mining the modern biology and biochemistry correlation index , so as to form the open system of pulsepsila theory , and to realize the development of it . 

nerd : a framework for unifying named entity recognition and disambiguation extraction tools
named entity extraction is a mature task in the nlp field that has yielded numerous services gaining popularity in the semantic web community for extracting knowledge from web documents . these services are generally organized as pipelines , using dedicated apis and different taxonomy for extracting , classifying and disambiguating named entities . integrating one of these services in a particular application requires to implement an appropriate driver . furthermore , the results of these services are not comparable due to different formats . this prevents the comparison of the performance of these services as well as their possible combination . we address this problem by proposing nerd , a framework which unifies 10 popular named entity extractors available on the web , and the nerd ontology which provides a rich set of axioms aligning the taxonomies of these tools . 

multi - mode natural language processing for extracting open knowledge
as more and more open knowledge sources become available , it is interesting to explore opportunities of enhancing autonomous agents ' capacities by utilizing the knowledge in these sources , instead of hand - coding knowledge for agents . a major challenge towards this goal lies in the translation of the open knowledge organized in multiple modes , unstructured or semi - structured , into the internal representations of agents . in this paper we present a set of multi - mode nlp techniques to formalize the open knowledge for autonomous agents . two case studies are reported in which our robot kejia , equipped with the multi - mode nlp techniques , succeeded in acquiring knowledge from the microwave oven manual and from the open knowledge database , omics , and solving problems that could not be solved before the robot acquired the knowledge . 

vocabchecker : measuring language abilities for detecting early stage dementia
recently , dementia patients have been increasing in number worldwide , necessitating the development of techniques to detect dementia as early as possible . considering that a typical symptom of dementia , especially alzheimer ' s disease , is language impairment , speech - based dementia detection approaches have drawn much attention . this paper presents a smartphone - based dementia screening application , vocabchecker , which measures language abilities from a speech narrative via automatic speech recognition ( asr) . it measures four language abilities related to dementia : number of tokens ( token) , number of types ( type) , type token ratio ( ttr) , and potential vocabulary size ( pvs) . we also reported that the use of vocabchecker has distinguished dementia patients from elderly people . 

unlocking super bowl insights : weighted word embeddings for twitter sentiment classification
sentiment classification plays an important role in sentiment analysis . it is challenging to develop an automatic method for classification problems without annotated training data . in this paper , we present a wwe ( weighted word embeddings ) method , which uses a continuous word representations algorithm ( word2vec ) to train a vector model . according to the cosine similarity between the vector of a word and the vectors of seed words , a polarity score of this word can be calculated . we then use the weighted polarity scores of words to compute a polarity score of the whole tweet . unlike the previous learning - based approaches , our method does not require annotated data gathered for the purpose of training models . we collected the super bowl 50 related tweets to demonstrate the wwe classification method . experiments are performed with promising outcomes . 

event - event relation identification : a crf based approach
temporal information extraction is a popular and interesting research field in the area of natural language processing ( nlp ) . the main tasks involve the identification of event - time , event - document creation time and event - event relations in a text . in this paper , we take up task c that involves identification of relations between the events in adjacent sentences under the timeml framework . we use a supervised machine learning technique , namely conditional random field ( crf ) . initially , a baseline system is developed by considering the most frequent temporal relation in the task ' s training data . for crf , we consider only those features that are already available in the tempeval - 2007 training set . evaluation results on the task c test set yield precision , recall and f - score values of 55 . 1 % , 55 . 1 % and 55 . 1 % , respectively under the strict evaluation scheme and 56 . 9 % , 56 . 9 and 56 . 9 % , respectively under the relaxed evaluation scheme . results also show that the proposed system performs better than the baseline system . 

nlp - supported decision - making
one way to think about what decision - making will be like in the future is to look at it in terms of the potential combinatorial power of adaptive , intelligent agents with the significant accomplishments of the burgeoning natural language processing ( nlp ) capabilities of today in technologies that will significantly impact human decision - making , decision support systems , and potentially fully automated decision making . 

a novel approach to categorize news articles from headlines and short text
over the last few years the world has experienced a surge in the number of online news portals . this has caused the volume of news articles to reach an all time high ; which will only get higher with time . thus , an efficient system of categorization and organization of the articles has become a necessity for various information systems like - news aggregation and association in search engines . it is impractical to employ humans to label this expansive volume of text data , prompting the growth of automated text categorization systems . and so , we devised a deep learning model that effectively categorizes news articles from the headlines and short text descriptions . the prime foci of our work were to design , develop , and measure the performance metrics of our proposed model . 

a local alignment kernel in the context of nlp
this paper discusses local alignment kernels in the context of the relation extraction task . we define a local alignment kernel based on the smith - waterman measure as a sequence similarity metric and proceed with a range of possibilities for computing a similarity between elements of sequences . we propose to use distributional similarity measures on elements and by doing so we are able to incorporate extra information from the unlabeled data into a learning task . our experiments suggest that a la kernel provides promising results on some biomedical corpora largely outperforming a baseline . 

an evaluation of tweet sentiment classification methods
in this paper , we present the result of our research in predicting sentiment from twitter data derived from a kaggle competition . our goal was to determine the efficacy of different supervised classification methods to predict twitter sentiment to be positive , neutral or negative . we evaluated four different classification statistical models : 1 . logistic regression ( lr ) , 2 . linear support vector machine ( lsv ) , 3 . multinomial na ï ve bayesian ( nb ) , and 4 . random forest ( rf ) . we also evaluated two different tokenization methods 1 . document term matrix ( dtm ) and 2 . term frequency – inverse document frequency ( tf - idf ) . we combined this with three text extraction methods 1 . original tweet text , 2 . rapid automatic keyword extraction ( rake ) and 3 . hand curated " selected text " . furthermore , various neural networks were applied to the tweet text and bert extracted data that reduced the original 1000 features to be 768 that were applied to different models . our experiment shows rf and lr gives the best results and there is little difference between dtm and tf - idf . fully connected neural network ( fcnn ) performed the best for the bert extracted data with a test score of 0 . 75 . 

an interactive voice controlled humanoid smart home prototype using concepts of natural language processing and machine learning
currently available interactive technologies and infrastructure such as open source hardware , increased processing power , artificial intelligence ( ai ) , machine learning ( ml ) , natural language processing ( nlp ) and connectivity through the internet , have led to a new definition of human living . smart homes , smart streets , smart cities and eventually a smarter planet , is what humans are progressing towards . the boon in things being smart is that it involves automation and interactive technologies , something that we as humans prefer over introvert and monotonous characteristic of machines . however , most present systems have a physical entity like a remote or a personal device assistant ( pda ) which is used to control our homes . when the access to these physical devices is lost , you can no longer control them . this paper addresses this very issue and proposes a solution which eliminates the need for any physical device . it proposes a multi - functional ` smart home automation system ' ( shas ) controlled using an interactive and humanoid central processing unit which is powered by features of ai and nlp . voice - commands to control the devices of the house such as fans , lights , doors , etc . can be given by the residents . the system is also intelligent enough to search things which it doesn ' t quite understand using the internet . the system has a camera integrated for continuously monitoring the home and also uses image processing techniques to identify the people trying to access the house . the system also has an automated mailing system which e - mails photos captured by the camera based on valid / invalid users . the system is aimed at being easy , cost - effective and non - physical . 

investigating deep learning word2vec model for sentiment analysis in arabic and english languages for user ’ s reviews
in this paper , we explore natural language processing ( nlp ) methods to perform sentiment analysis or opinion mining . in addition , we show an application on english and arabic sentiment analysis by implementing sentiment classification for three datasets which are booking hotel dataset , food fine amazon dataset and arabic movie review dataset . we applied word2vec model followed by random forest classifier ( rf ) for arabic movie dataset . the results show that the word2vec model shows highly effective performance in sentiment analysis for english language datasets but it does not work for arabic language as arabic language need different mechanism . 

hybrid framework for information extraction for geographical terms in hindi language texts
a hybrid information extraction ( ie ) framework based on geographical term detection approach has been developed to extract geographical information from an unrestricted hindi text . the relationship between geographical entities extracted with the adjacent text is shown graphically so that information about these entities can be related . the system , a combination of statistically and linguistically motivated techniques , identifies single geographical names and multiple geographical names as well . the method is applied on hindi language text , but the approach can be adapted for other languages also . the paper presents some experiments illustrating the accuracy of the method . the system being developed is in a prototype stage and will be extended to include relation mark - up as well . 

multilingual extraction and mapping of dictionary entry names in business schema integration
being a research field for many years , natural language processing ( nlp ) has gained a lot of attention in recent times due to the quality of translation software like google translator or babylon . in the context of the igreen project , we are developing a schema integration service ( working title : warp 10 ) , which helps to generate business transformations from individual business schemata . to match the different schemas natural language processing plays an important role . information coming from user input or files is extracted and mapped to a canonical data model ( cdm ) based on the ccts standard . in this paper , we illustrate the use of nlp for the extraction of dictionary entry names ( dens ) and indicate some of the problems of nlp and term extraction . furthermore , we describe the nlp - supported mapping of dens and outline the problems and approaches in a multilingual setting . 

an nlp based information extraction system for patents
patents are rich source of information related to new technologies , products and methods . patents contain valuable information which can be used for researchers , business people , scientists etc . in this paper an information extraction system using nlp is implemented for patents . users can extract interesting information easily using nlp and rule ensemble approach . since there is need for important information from the available data , a classification based information extraction is considered here . 

comparing insights from inductive qualitative analysis versus automated nlp algorithms for analyzing feedback in digital randomized controlled trials
randomized controlled trials ( i . e . a / b testing ) is the gold standard for evaluating improvements and accelerating innovations in the digital space . prior work has shown that qualitative user feedback is an effective and important tool in the analysis of a / b tests . however , manual inductive qualitative analysis of feedback - best practices today - is expensive and unscalable , which may lead to the omission of important insights about quality and user experiences . prior work has shown various automated nlp algorithms to be effective in extracting insights from feedback in the digital domain . but we lack understanding of the differences in insights gained from a manual inductive qualitative analysis versus automated nlp algorithms , for analyzing digital randomized controlled trials ( where a key objective is understanding differences between control and treatment conditions ) . in this paper , we compare insights from manual inductive qualitative analyses and from six automated nlp algorithms , using data from large - scale real - world digital randomized controlled trials . we find that collocation algorithms ( notable , trigrams ) are promising , providing similar insights as manual analyses with substantially lower cost ; however , issues remain and improvements are needed . we discuss implications for future research and for operationalization . 

an approach to detect abusive bangla text
maximum research work on abusive text detection is in the english language and some of these are to detect offensive text . but in bangla language , a few work is found . detecting abusive text in bangla language will be useful for preventing cybercrime such as online harassment , blackmailing and cyber bullying which are becoming the main concern in bangladesh nowadays . our goal is to detect abusive bangla comments which are collected from various social sites where people share their sentiment , opinions , views etc . in this paper . we proposed a root level algorithm to detect abusive text and also proposed unigram string features to get a better result . 

finite state lazy operations in nlp
finite state networks can represent dictionaries and lexical relations . traditional finite - state operations like composition can produce huge networks with prohibitive computation space and time . for a subset of finite state operations , these drawbacks can be avoided by using virtual networks , which rely on structures that are partially built on demand . this paper addresses the implementation of virtual network operations in xfst ( xerox finite state technology software) . the example of " priority union " which is particularly useful in nlp , is developed . 

research and implementation english morphological analysis and part - of - speech tagging
english morphological analysis ( ma ) and part - of - speech ( pos ) tagging are key task in natural language processing ( nlp ) and computational linguistics . this research and application are of great theoretical and practical significance . english morphological analysis ( ma ) , part - of - speech ( pos ) tagging and phrase dictionary retrieval ( pdr ) are essential steps in the course of nlp . and they are difficult in nlp . their results are decisive to the accuracy of next processing , such as information searching , information filtration . as separate problems of english , ma , pos , pdr can be considered independent with each other . in a practical research system , however , they are dependent : solution of the prior one forms the base for processing the next one . considering different features of these problems in this thesis , after a comprehensive study , a divide - and - conqueror strategy is proposed and resolves them separately . first , a knowledge - based method is put forward for the solution of ma . the whole ma processing is completed by many subordinate functions dealing with different particular marks of english words . a strategy of combining the word length with statistic enumeration is developed to distinguish between the periods and abbreviations . 

compression of deep learning models for nlp
in recent years , the fields of nlp and information retrieval have made tremendous progress thanks to deep learning models like rnns and lstms , and transformer [ 35 ] based models like bert [ 9 ] . but these models are humongous in size . real world applications however demand small model size , low response times and low computational power wattage . we will discuss six different types of methods ( pruning , quantization , knowledge distillation , parameter sharing , matrix decomposition , and other transformer based methods ) for compression of such models to enable their deployment in real industry nlp projects . given the critical need of building applications with efficient and small models , and the large amount of recently published work in this area , we believe that this tutorial is very timely . we will organize related work done by the ' deep learning for nlp ' community in the past few years and present it as a coherent story . 

from submit to submitted via submission : on lexical rules in large - scale lexicon acquisition
this paper deals with the discovery , representation , and use of lexical rules ( lrs ) during large - scale semi - automatic computational lexicon acquisition . the analysis is based on a set of lrs implemented and tested on the basis of spanish and english business - and finance - related corpora . we show that , though the use of lrs is justified , they do not come cost - free . semi - automatic output checking is required , even with blocking and preemtion procedures built in . nevertheless , large - scope lrs are justified because they facilitate the unavoidable process of large - scale semi - automatic lexical acquisition . we also argue that the place of lrs in the computational process is a complex issue . 

bangla document categorisation using multilayer dense neural network with tf - idf
document categorization is a classic nlp task which includes allocating documents based on their content into one or more predefined classes . in this paper , we proposed a model using multilayer dense neural network with tf - idf as feature selection technique in terms of bangla text document categorization . the proposed system is distributed into three consecutive steps : i ) preprocessing raw text data and extracting feature using tf - idf , ii ) designing the model architecture and fitting the model to train set , and iii ) evaluating model performance on test set based on accuracy and f1 score . it is observed from experiments that the proposed method exhibits higher accuracy ( 84 . 58% ) and f1 score ( 0 . 84 ) compared to the other well - known classification algorithms ( support vector machine , multinomial naive bayes and so on ) for bangla text document classification . 

phis ( protected health information ) identification from free text clinical records based on machine learning
to preserve patient confidentiality , there is a need to identify phis ( protected health information ) from free text text clinical records , and such sensitive information must either be removed or replaced . identification of the phi ' s are normally performed manually on large sets of structured ehr databases , which is time - consuming , prohibitively expensive and error - prone . hence , methods for automatic or semi - automatic identification of personal health information are of significant scientific and commercial interest . in this paper , we propose an innovative computational framework based on novel text mining and machine learning algorithms for automatic identification of phis from massive , unstructured free text clinical records , discharge summaries and other care documents . the experimental evaluation of the proposed algorithmic framework development , for several publicly available i2b2 challenge datasets from informatics for integrating biology & the bedside ( i2b2 ) shared tasks , has shown promising outcomes . 

question matching based on fuzzy set
sentence similarity computing plays an important role in the question answering ( qa ) system . because there are many question expressions for one meaning , we present a new approach to match question based on fuzzy set . in this paper , we establish a library of standard questions . each standard question is relative with a series of keywords . the main focus of this paper lies with matching of standard questions and questions asked by users . an experimental system based on the proposed method has been built , and the results of our experiments shows the proposed method is effective for question matching . 

a hybrid statistical model to generate pronunciation variants of words
generating pronunciation variants of words is an important applicable subject in speech researches and is used extensively in automatic speech segmentation and recognition systems . in this way , decision trees are extremely used to model pronunciation variants of words and sub - word unites . in the case of word unites and very large vocabulary , to train necessary decision trees we need a huge amount of speech utterances which contains all of the needed words with a sufficient number of each one . this approach besides demanding very large data , for new words needs some new extra corpus . to solve these problems we have used generalized decision trees , that each tree is trained for a group of words with similar phonemic structure instead of a single word . these trees can predict regions of the words in which substitution , deletion and insertion of phonemes would occur . next to this step , appropriate statistical contextual rules , which are extracted from a large speech corpus , is applied to these regions in order to generate words variants . this new hybrid d - tree / c - rule approach takes into account word phonological structures , stress , and phone context information simultaneously and an ordinary size speech corpus is sufficient to train its models . by using the word variants obtained by this method in the lexicon of " shenava " , a persian acsr , a relative wer % reduction of as high as 6 % was obtained . 

question answering system with indic multilingual - bert
question answering system is a natural language processing ( nlp ) task that is very important when it comes to search useful information on websites or large documents . many language models have been evolved to provide language processing capabilities in various languages . the mbert is the popular model used to deal with multilingual datasets throughout the world . the research focus was centric to the high resource languages such as english , hindi , etc . due to the availability of the dataset and many other local languages are overlooked for sentiment analysis . in the present research , a question answering system is focused with indic bidirectional encoder representations from transformers ( bert ) . the performance of indic bert with mbert was analyzed along with the equivalent models . 

home automation using iot and a chatbot using natural language processing
home automation - controlling the fans , lights and other electrical appliances in a house using internet of things is widely preferred in recent days . in this paper , we propose a web application using which the fans , lights and other electrical appliances can be controlled over the internet . the important features of the web application is that firstly , we have a chatbot algorithm such that the user can text information to control the functioning of the electrical appliances at home . the messages sent using the chatbot is processed using natural language processing techniques . secondly , any device connected to the local area network of the house can control the devices and other appliances in the house . thirdly , the web application used to enable home automation also has a security feature that only enables certain users to access the application . and finally , it also has a functionality of sending an email alert when intruder is detected using motion sensors . 

conditional random fields in speech , audio , and language processing
conditional random fields ( crfs ) are probabilistic sequence models that have been applied in the last decade to a number of applications in audio , speech , and language processing . in this paper , we provide a tutorial overview of crf technologies , pointing to other resources for more in - depth discussion ; in particular , we describe the common linear - chain model as well as a number of common extensions within the crf family of models . an overview of the mathematical techniques used in training and evaluating these models is also provided , as well as a discussion of the relationships with other probabilistic models . finally , we survey recent work in speech , audio , and language processing to show how the same crf technology can be deployed in different scenarios . 

the comparison of the proposed recommended system with actual data
recommendation systems for the museum have been active in the past decade . it used to be a difficult task to make the personalized recommended list for museum - goer . however , with the current technology , research can provide the list for visitors via technology such as mobile applications . we have proposed a recommendation system based on social filtering and statistical methods in the previous paper . this paper applies the f1 - score to evaluate our recommendation methods on the actual visitor loggers from chao sampradaya national museum . we compare the social filtering method with the statistical method and benchmark with the random recommendation . in comparison , the statistical method gives the same result as social filtering when the time is limited . the longer time the visitor spends in the museum , the better result from the social filtering . however , in terms of calculation complexity , the statistical method outperforms social filtering . 

ontology - based venous thromboembolism risk factors mining and model developing from medical records
padua linear model is widely used for the risk assessment of venous thromboembolism ( vte ) , which is a common and preventable complication for inpatients . however , differences of race , genetics and environment between western and chinese population limit padua model ' validity in chinese patients . extracting vte risk factors from unstructured medical records in chinese hospital can help to understand vte events and develop efficient risk assessment model . in this study , we proposed an ontology - based method to mine vte risk factors combining natural language processing ( nlp ) and machine learning ( ml ) methods . medical records of 3106 inpatients were processed and terms in multiple ontologies from various sections of records enriched in vte patients were sorted automatically . then ml methods were used to estimate terms ' importance and terms within admitting diagnosis and progress notes showed better vte prediction performance than other sections . finally a novel vte prediction model was built based on selected terms and showed higher auc score ( 0 . 815 ) than the padua model ( 0 . 789 ) . 

detecting racist and bad words using text mining in social media
the use of social media has grown rapidly in the last decades . many of these social media platforms , i . e . , twitter , do not screen or remove potentially offensive / harmful tweets . it can cause discomfort for the users , especially if minors encounter these words while they are surfing the media . we came up with a profanity detection model with text mining that uses bag of words algorithm . then we applied four different classifiers to see which one gets the highest accuracy score . the results show a high accuracy performance , with random forest classification being the highest with 97 . 5% . 

session details : poster presentations
we are very pleased to welcome the 100 posters of this www 2012 poster track . we wish to thank all program committee members for the quality ( and the quantity ) of their work . and naturally , we also congratulate authors of the selected contributions that reflect the variety of the www series conferences : all posters present high quality work in various domains , poster authors come from various countries and represent both academic and industrial research teams . the poster track provides an opportunity for authors and conference attendees to interact and exchange about innovative theoretical and technical work in progress around , about and on the web . in addition to being displayed during three whole days from 4 / 18 to 4 / 20 , www ' 2012 posters will also be shortly presented on thursday 4 / 19 pm . seven " spotlight events " are organized to give conference attendees a 5 - minute glimpse on each poster content , in the following areas : 1 - web search / web mining : www contains rich data , yet how to effectively use such rich data is challenging . web search is mainly about searching information with a proper query , while web mining has a boarder scope of knowledge exaction from the web . 2 - natural language processing / information search and retrieval : nlp and ir are two major foundations of web search . with more and more applications on the web , the boundary between nlp and ir is getting smaller . how to efficiently and effectively handle large scale online data is a challenge for nlp and ir communities . 3 - web engineering / performance / security / privacy : this session copes with the everyday challenges of web development : how to improve contents and applications design processes , how to serve them efficiently and securely to users and how to face privacy issues . 4 - recommender systems / semantic web : recommender systems allow web communities access the best of users ' opinions . semantic web and linked data rely on communities ' data to harvest and process complex information . this session presents posters that cope with graph algorithms and semantic / social data . 5 - social web / human factors / accessibility : this session deals with human - oriented topics . the presented posters mainly concern analysis and prediction of social behavior and influence , as well as of individual behavior , affective computing and web accessibility . 6 - microblogging / social media / catch up tv : social media and microblogging provide rich online information with a different perspective from traditional static web page data . how to effectively search , mine and utilize such valuable information is a very stimulating research area . 7 - monetization : posters in this session explore efficient ways to spread ads on the web or analyze the efficiency of existing algorithms on actual e - commerce activity . this year ' s edition aims at " augmenting posters " in several ways : poster views are also available in virtual worlds , and attendees can access poster metadata and additional information and even chat with authors using their smartphones and tabs . numerous efforts have been made to encourage discussions about posters for the largest audience , inside and outside the conference . so come , see and participate ! 

the finsim - 2 2021 shared task : learning semantic similarities for the financial domain
the finsim - 2 is a second edition of finsim shared task on learning semantic similarities for the financial domain , colocated with the finweb workshop . finsim - 2 proposed the challenge to automatically learn effective and precise semantic models for the financial domain . the second edition of the finsim offered an enriched dataset in terms of volume and quality , and interested in systems which make creative use of relevant resources such as ontologies and lexica , as well as systems which make use of contextual word embeddings such as bert [ 4] . going beyond the mere representation of words is a key step to industrial applications that make use of natural language processing ( nlp) . this is typically addressed using either unsupervised corpus - derived representations like word embeddings , which are typically opaque to human understanding but very useful in nlp applications or manually created resources such as taxonomies and ontologies , which typically have low coverage and contain inconsistencies , but provide a deeper understanding of the target domain . finsim is inspired from previous endeavours in the semeval community , which organized several competitions on semantic / lexical relation extraction between concepts / words . this year , 18 system runs were submitted by 7 teams and systems were ranked according to 2 metrics , accuracy and mean rank . all the systems beat our baseline 1 model by over 15 points and the best systems beat the baseline 2 by over 1 ∼ 3 points in accuracy . 

a dynamic weighted method with support vector machines for chinese word segmentation
we explore how a dynamic weighted method with svm works well for a chinese word segmentation task . we set up two systems , system1 uses the uniform weight and we take it as our baseline system , system1 uses the dynamic weighted method as we proposed . we compare the performance of the two systems under different experiments conditions , and experiments results show that system2 got an outstanding performance in every condition we tested . it indicates that the dynamic weighted method we proposed has a stronger performance and can be used in many other svm task such as chunking , pos and so on . at last , we describe a trick that can calculate the segmentation precision ratio and recall ratio for a segmentation task as soon as the end of the training or test process of a svm procedure at essentially no extra cost . 

social robot interactions for social engineering : opportunities and open issues
the field of information security is receiving greater attention in recent years . it is well - known that social engineering attacks exploit psychological manipulation techniques to trick people in providing private information without being aware . robots are entwined in our daily lives and humans tend to trust them more and more . this leads to a kind of " overtrust " in robots that in turn may expose humans to security , fraud and privacy issues . this paper aims to provide insights into how interaction with social robots could be exploited for the purpose of social engineering . in particular , the paper presents the ability of robots to gather information during an interaction / conversation with humans and how this information could be integrated and enriched with emotion recognition techniques . authors show some preliminary experimental results that are encouraging since the interaction between the robot and the users resembles a humanto - human conversation . in particular , for emotion recognition has been obtained two different results : an accuracy of 74 . 38 % fusing two biometric traits ( voice and face ) and a tolerance of 5 bpm to estimate heart - rate . 

light in disordered nonlinear photonic structures
we present selected wave propagation phenomena in disordered nonlinear photonic structures ranging from high harmonic generation in disordered quadratic media to anderson localization in optically induced refractive index distributions . 

a bidirectional lstm model for classifying chatbot messages
online channels , e . g . , facebook messenger and line , are widely used especially in covid - 19 pandemic . to quickly respond to their customer , chatbot system are implemented in many companies or organizations , connected to those channels . the office of registrar , thammasat university also implements a chatbot to answer questions from students . an important step in the chatbot system is to know an intention of a question message . a bidirectional lstm model is employed to classify a question message from the chatbot system into five intention classes . the experimental results shows that the obtained model yields an accuracy of 0 . 80 on our validation dataset . 

feature vector quality and distributional similarity
we suggest a new goal and evaluation criterion for word similarity measures . the new criterion - meaning - entailing substitutability - fits the needs of semantic - oriented nlp applications and can be evaluated directly ( independent of an application ) at a good level of human agreement . motivated by this semantic criterion we analyze the empirical quality of distributional word feature vectors and its impact on word similarity results , proposing an objective measure for evaluating feature vector quality . finally , a novel feature weighting and selection function is presented , which yields superior feature vectors and better word similarity performance . 

a novel part of speech tagging framework for nlp based business process management
natural language processing ( nlp ) is a key technique to automate business process management ( bpm ) at different levels . the performance of existing nlp based bpm methods suffer from the limited accuracy of part of speech ( pos ) tagging , which is a key step in nlp pipelines . note that the performance of pos tagging highly depends on the domain of annotated training data . however , most state - of - the - art pos taggers are trained from corpus in newswire domain which usually have different syntax features with business process description ( bpd ) . the syntax features of bpd domain include usually starting with an imperative verb and containing numerous out - of - vocabulary ( oov ) words . in this paper , we propose a novel pos tagging framework to tackle these problems . the main idea is that syntax feature of starting with imperative verb could be studied by enhancing the proportion of correctly pos - annotated imperative sentences in the training data . the trained pos tagger could reduce the overall pos tagging error by nearly 12 % compared with newswire trained pos tagger . for verbs which are key words in bpd , the tagging precision could be increased by 27 % . the lexical ambiguity caused by oov words is solved by extracting local contextual knowledge out of images which are attached to help users understand the process better . experimental results show that the overall pos tagging accuracy could be increased by nearly 10 % with contextual oov knowledge . 

punjabi stop words : a gurmukhi , shahmukhi and roman scripted chronicle
with advent of unicode encoding , punjabi language content , written using gurmukhi script as well as in shahmukhi script , is increasing day by day on internet . processing textual information involves passing it to various pre - processing phases . stop - word elimination is one such sub phase . 256 gurmukhi stop words had been identified from poetry , stories and online material and passed to punjabi stemmer . after stemming , 184 stemmed stop words were generated and these stemmed stop words were passed to transliteration phase . this led to generation of stop words in shahmukhi script . for the first time in scientific community dealing with computational linguistics and literature processing using nlp techniques , the list of 184 stop words of punjabi language is released for public usage and further nlp applications . the presented list consists of stop words of punjabi language with their gurmukhi , shahmukhi as well as roman scripted forms . 

westlaw edge ai features demo : keycite overruling risk , litigation analytics , and westsearch plus
westlaw edge , a new legal research platform from westlaw , was launched in 2018 . the three ai - enabled features that launched with westlaw edge are keycite overruling risk , litigation analytics , and westsearch plus . keycite overruling risk uses nlp and machine learning to warn users when a point of law in a case may have been implicitly undermined based on a prior decision , when that prior citation has no direct citation relationship to the at - risk case . litigation analytics allows users to access valuable metadata extracted from legal dockets about the legal actions carried out by parties , lawyers , law firms , and judges presiding over cases . westsearch plus is a non - factoid question answering system that provides legally correct , jurisdictionally relevant , and conversationally responsive answers to user - entered questions in the legal domain . added ai - enabled features are set to launch on the westlaw edge platform in 2019 . it is available for demo at icail 2019 . 

automated context aware composition for convergent services
automated services composition is an active research area nowadays , most of the approaches are based in artificial intelligence techniques . however , most of these approaches focus on specific steps of web services composition and lacks of details for general application in broader fields such as convergent services . in the present work , an architecture based on natural language analysis for ai planning processing and automatic deployment in jslee is presented . the preliminary experiments show promising results . 

an word2vec based on chinese medical knowledge
introducing a large amount of external prior domain knowledge will effectively improve the performance of the word embedded language model in downstream nlp tasks . based on this assumption , we collect and collate a medical corpus data with about 36m ( million ) characters and use the data of ccks2019 as the test set to carry out multiple classifications and named entity recognition ( ner ) tasks with the generated word and character vectors . compared with the results of bert , our models obtained the ideal performance and efficiency results . 

reducing sentiment polarity for demographic attributes in word embeddings using adversarial learning
the use of word embedding models in sentiment analysis has gained a lot of traction in the natural language processing ( nlp ) community . however , many inherently neutral word vectors describing demographic identity have unintended implicit correlations with negative or positive sentiment , resulting in unfair downstream machine learning algorithms . we leverage adversarial learning to decorrelate demographic identity term word vectors with positive or negative sentiment , and re - embed them into the word embeddings . we show that our method effectively minimizes unfair positive / negative sentiment polarity while retaining the semantic accuracy of the word embeddings . furthermore , we show that our method effectively reduces unfairness in downstream sentiment regression and can be extended to reduce unfairness in toxicity classification tasks . 

deep - confidentiality : an iot - enabled privacy - preserving framework for unstructured big biomedical data
due to the internet of things evolution , the clinical data is exponentially growing and using smart technologies . the generated big biomedical data is confidential , as it contains a patient ’ s personal information and findings . usually , big biomedical data is stored over the cloud , making it convenient to be accessed and shared . in this view , the data shared for research purposes helps to reveal useful and unexposed aspects . unfortunately , sharing of such sensitive data also leads to certain privacy threats . generally , the clinical data is available in textual format ( e . g . , perception reports ) . under the domain of natural language processing , many research studies have been published to mitigate the privacy breaches in textual clinical data . however , there are still limitations and shortcomings in the current studies that are inevitable to be addressed . in this article , a novel framework for textual medical data privacy has been proposed as deep - confidentiality . the proposed framework improves medical entity recognition ( mer ) using deep neural networks and sanitization compared to the current state - of - the - art techniques . moreover , the new and generic utility metric is also proposed , which overcomes the shortcomings of the existing utility metric . it provides the true representation of sanitized documents as compared to the original documents . to check our proposed framework ’ s effectiveness , it is evaluated on the i2b2 - 2010 nlp challenge dataset , which is considered one of the complex medical data for mer . the proposed framework improves the mer with 7 . 8 % recall , 7 % precision , and 3 . 8 % f1 - score compared to the existing deep learning models . it also improved the data utility of sanitized documents up to 13 . 79 % , where the value of the   k is 3 . 

web service integration for next generation localisation
developments in natural language processing technologies promise a variety of benefits to the localization industry , both in its current form in performing bulk enterprise - based localization and in the future in supporting personalized web - based localization on increasingly user - generated content . as an increasing variety of natural language processing services become available , it is vital that the localization industry employs the flexible software integration techniques that will enable it to make best use of these technologies . to date however , the localization industry has been slow reap the benefits of modern integration technologies such as web service integration and orchestration . based on recent integration experiences , we examine how the localization industry can best exploit web - based integration technologies in developing new services and exploring new business models

devise sparse compression schedulers to enhance fasttext methods
in natural language processing ( nlp ) , the general way to understand the meaning of a word is via word embedding . the word embedding training model can convert words into multidimensional vectors and make the words that do not know “ meaning ” into vectors with “ meaning ” . famous word embedding training models , include models such as fasttext , word2vec , and glove . they can train words into vectors and then they are used for further semantic classifications . in this paper , we work on the efficient support for the fasttext . fasttext is an open source library created by facebook ( fair ) lab that allows users to learn word embedding and text classification . we focus on the word representation application in fasttext , in which general matrix - vector multiplication ( gemv ) is one of the most computationally intensive operations . in this paper , we adjust the software architecture of fasttext , and pre - process the pre - trained model offline . in addition , we introduce a new accelerating method with sparse matrix compression in halide , which improves performance by compressing the matrix . our support with halide sparse compression schedulers include hybrid compression schemes and re - ordering methods to improve the performance . 

unsupervised nlp and human language acquisition : making connections to make progress
natural language processing and cognitive science are two fields in which unsupervised language learning is an important area of research . yet there is often little crosstalk between the two fields . in this talk , i will argue that considering the problem of unsupervised language learning from a cognitive perspective can lead to useful insights for the nlp researcher , while also showing how tools and methods from nlp and machine learning can shed light on human language acquisition . i will present two case examples , both of them models inspired by cognitive questions . the first is a model of word segmentation , which introduced new modeling and inference techniques into nlp while also yielding a better fit than previous models to human behavioral data on word segmentation . the second is more recent work on unsupervised grammar induction , in which prosodic cues are used to help identify syntactic boundaries . preliminary results indicate that such cues can be helpful , but also reveal weaknesses in existing unsupervised grammar induction methods from nlp , suggesting possible directions for future research . 

semantic structure and interpretability of word embeddings
dense word embeddings , which encode meanings of words to low - dimensional vector spaces , have become very popular in natural language processing nlp research due to their state - of - the - art performances in many nlp tasks . word embeddings are substantially successful in capturing semantic relations among words , so a meaningful semantic structure must be present in the respective vector spaces . however , in many cases , this semantic structure is broadly and heterogeneously distributed across the embedding dimensions making interpretation of dimensions a big challenge . in this study , we propose a statistical method to uncover the underlying latent semantic structure in the dense word embeddings . to perform our analysis , we introduce a new dataset semcat that contains more than 6500 words semantically grouped under 110 categories . we further propose a method to quantify the interpretability of the word embeddings . the proposed method is a practical alternative to the classical word intrusion test that requires human intervention . 

sciencesearch : enabling search through automatic metadata generation
scientific facilities are increasingly generating and handling large amounts of data from experiments and simulations . next - generation scientific discoveries rely on insights derived from data , especially across domain boundaries . search capabilities are critical to enable scientists to discover datasets of interest . however , scientific datasets often lack the signals or metadata required for effective searches . thus , we need formalized methods and systems to automatically annotate scientific datasets from the data and its surrounding context . additionally , a search infrastructure needs to account for the scale and rate of application data volumes . in this paper , we present sciencesearch , a system infrastructure that uses machine learning techniques to capture and learn the knowledge , context , and surrounding artifacts from data to generate metadata and enable search . our current implementation is focused on a dataset from the national center for electron microscopy ( ncem ) , an electron microscopy facility at lawrence berkeley national laboratory sponsored by the department of energy which supports hundreds of users and stores millions of micrographs . in this paper , we describe ( a ) our search infrastructure and model , ( b ) methods for generating metadata using machine learning techniques , and ( c ) optimizations to improve search latency , and deployment on an hpc system . we demonstrate that sciencesearch is capable of producing valid metadata for ncem ' s dataset and providing low - latency good quality search results over a scientific dataset . 

usefulness of temporal information automatically extracted from news articles for topic tracking
temporal information plays an important role in natural language processing ( nlp ) applications such as information extraction , discourse analysis , automatic summarization , and question - answering . in the topic detection and tracking ( tdt ) area , the temporal information often used is the publication date of a message , which is readily available but limited in its usefulness . we developed a relatively simple nlp method for extracting temporal information from korean news articles , with the goal of improving performance of tdt tasks . to extract temporal information , we make use of finite state automata and a lexicon containing timerevealing vocabulary . extracted information is converted into a canonicalized representation of a time point or a time duration . we first evaluated and investigated the extraction and canonicalization methods for their accuracy and the extent to which temporal information extracted as such can help tdt tasks . the experimental results show that time information extracted from the text does indeed help to significantly improve both precision and recall . 

creating a novel semantic video search engine through enrichment textual and temporal features of subtitled youtube media fragments
semantic video annotation is an active research zone within the field of multimedia content understanding . with the stable increase of videos published on the famous video sharing platforms such as youtube , more efforts are spent to automatically annotate these videos . in this paper , we propose a novel framework to annotating subtitled youtube videos using both textual features such as all of portions extracted from web natural language processors in relation to subtitles , and temporal features such as the duration of the media fragments where particular entities are spotted . we implement sy - vse ( subtitled youtube video search engine ) as an efficient framework to cruising on the subtitled youtube videos resident in the linked open data ( lod ) cloud . for realizing this purpose , we propose unifier module of natural language processing ( nlp ) tools results ( um - nlptr ) for extracting main portions of the 10 nlp web tools from subtitles associated to youtube videos in order to generate media fragments annotated with resources from the lod cloud . then , we propose unifier module of popular api ' s results ( um - par ) containing the seven favorite web apis to boost results of named entities ( ne ) obtained from um - nlptr . we will use dotnetrdf as a powerful and flexible api for working with resource description framework ( rdf ) and sparql in . net environments . 

microcontroller based automatic sun tracking solar panel
to learn a hierarchal representation of data , deep learning techniques can be used that use multiple processing layers , and produce state of art results . many models and methods are designed in deep learning for classification in natural language processing ( nlp) . various classification algorithms have been used for arabic documents classification , but they have two problems high dimensional feature representation and the low accuracy of the classification . in this work , an important experiment is made by using deep related models and methods for classifying arabic text also compare our model with various models . also to forward a full understand , present and future of deep learning in arabic text classification and have obtained encouraging results . 

vnlp : an open source framework for vietnamese natural language processing
natural language processing ( nlp ) for vietnamese has been researched for more than a decade but still lacks of an open - source nlp pipeline . as the result , researchers have to spend a lot of time on various fundamental tasks before working on the task of interest . besides , the circumstance holds back text processing technology in vietnam because an application costs much more money and time to reach a deliverable state . this work is an attempt to solve this issue . by incorporating available open - source software packages and implementing new ones , we have created an open - source , production - ready solution for vietnamese text processing . via three experiments , we demonstrated its effectiveness and efficiency . the software has helped us to develop our solution for vietnamese sentiment analysis and online reputation management and we hope that it will also facilitate research in vietnamese nlp . 

an unsupervised approach to interpreting noun compounds
this paper proposes an unsupervised approach to automatically interpret noun compounds using semantic similarity . our proposed unsupervised method is based on obtaining a large amount of robust evidence for nc interpretation . in order to obtain evidence sentences for semantic relations ( srs ) , we first acquired sentences containing both a head noun and its modifier in the form of sr definitions . then we determined the semantic relations represented in the sentences by looking at the nouns in the test instances ( noun mapping ) and verbs in the sr definitions ( verb mapping ) . in the noun mapping , we measured the similarity between nouns in test instances and nouns in the collected sentences . in the verb mapping , we mapped the verbs of sentences onto those in the sr definitions . finally , we built a statistical classifier to interpret noun compounds and evaluated it over 17 srs defined in . 

a morphological analysis of arabic language based on multicriteria decision making : taghit system
in this paper , we present our work on arabic morphology and especially the mechanisms for resolving the morphological ambiguity in arabic text . these researches , which have given birth to taghit system which is a morphosyntactic tagger for arabic , where the originality of our work lies in the implementation of our internal system of a new approach to disambiguation different from those that currently exist , which is based on the principles and techniques issued from multicriteria decision making . 

research on sentiment classification of blog based on pmi - ir
development of blog texts information on the internet has brought new challenge to chinese text classification . aim to solving the semantics deficiency problem in traditional methods for chinese text classification , this paper implements a text classification method on classifying a blog as joy , angry , sad or fear using a simple unsupervised learning algorithm . the classification of a blog text is predicted by the max semantic orientation ( so ) of the phrases in the blog text that contains adjectives or adverbs . in this paper , the so of a phrase is calculated as the mutual information between the given phrase and the polar words . then the so of the given blog text is determined by the max mutual information value . a blog text is classified as joy if the so of its phrases is joy . two different corpora are adopted to test our method , one is the blog corpus collected by monitor and research center for national language resource network multimedia sub - branch center , and the other is chinese dataset provided by coae2008 task . based on the two datasets , the method respectively achieves a high improvement compared to the traditional methods . 

representing and accessing multilevel linguistic annotation using the meaning format
we present an xml annotation format ( meaning annotation format , maf ) specifically designed to represent and integrate different levels of linguistic annotations and a tool that provides flexible access to them ( meaning browser ) . we describe our experience in integrating linguistic annotations coming from different sources , and the solutions we adopted to implement efficient access to corpora annotated with the meaning format . 

hmm - based phonemic distance in different speaking styles and its influence on substitutions in mandarin speech recognition
statistical confusability between different acoustic models is important to character substitution error rate in large vocabulary continuous speech recognition . in this paper , we take factors of gender and speaking styles into consideration in mandarin speech recognition . we modeled phonemes in different speaking styles , including read speech of female , male , and spontaneous dialogue . then minimum gaussian distances between chinese initial / final model pairs are given and average phoneme distances are calculated which denote the pronunciation varieties . the effect of different style to average phonemic distance is studied and relative articulation is given for three databases . qualitative relationship between phone size and error rate in recognition is analytical researched , showing that for a particular phoneme , pronunciation variety is one of reasons for misidentification in recognizing process , which provides us a novel mind to reduce substitution errors . 

automatic evaluation of translation quality using expanded n - gram co - occurrence
bleu and nist as official machine translation evaluation metrics are widely used to assess system translation quality . these n - gram co - occurrence algorithms are applied to evaluate language learners ' translations in this paper . subtle differences of evaluation on machine translation and learners ' translation are discussed . dependent on n - gram matching between target translation and references , bleu and nist evaluate translation quality completely disregarding the source language . based on sense overlapping in original language , we make pseudo translations for bleu and nist by substituting words or phrases in target translation for synonymous words and phrases in references . pseudo translations expand n - gram co - occurrence between target translation and references . evaluation experiments on learners ' translation and machine translation corpus with expanded n - gram co - occurrence outperform pure bleu and nist evaluation in higher correlation with human assessments . 

clinical decision support systems : a survey of nlp - based approaches from unstructured data
clinical decision support on patients health outcomes can be performed from free text with natural language processing techniques . however , it becomes a computational challenge due to the complexity of natural language . in recent years , several nlp - based approaches have been proposed to consider clinical decisions support . this paper presents a survey of natural language processing approaches to support clinical decisions on patient health outcomes . the presented approaches are emphasized on the use of free text as input for diverse languages . an analysis of clinical decision support systems based on natural language processing in terms of their performance results is presented . 

frequency based named entity recognition system for under resource language
this paper tries to study the issues and challenges for developing a named entity recognition ( ner ) system for a resource scarce language of north east india . kokborok a language spoken in the state of tripura is taken as the target language in developing our ner system . kokborok is an under resource language and not much digital work is available . we have used the frequency based approach to test our work which gave us a satisfactory result . as this is the first ner system being studied upon for this language we consider this to be our baseline ner system for future research in this area . 

toward a noun morphological analyser of standard amazigh
despite its position as second official language of morocco , amazigh language still suffers from the lack of studies from computational point of view . to achieve this goal , and given that the noun classification system in amazigh is computationally interesting and constitutes a major word category which plays a major role in syntactic analysis , we have undertaken to develop a component of a noun morphological analysis for standard amazigh of morocco using finite state technology , within the linguistic developmental environment nooj . 

towards aligning multi - concern models via nlp
the design of large - scale complex systems requires their analysis from multiple perspectives , often through the use of requirements models . diversely located experts with different backgrounds ( e . g . , safety , security , performance ) create such models using different requirements modeling languages . one open challenge is how to align these models such that they cover the same parts of the domain . we propose a technique based on natural language processing ( nlp ) that analyzes several models included in a project and provides suggestions to modelers based on what is represented in the models that analyze other concerns . unlike techniques based on meta - model alignment , ours is flexible and language agnostic . we report the results of a focus group session in which experts from the air traffic management domain discussed our approach . 

detecting predatory behavior in game chats
while games are a popular social media for children , there is a real risk that these children are exposed to potential sexual assault . a number of studies have already addressed this issue , however , the data used in previous research did not properly represent the real chats found in multiplayer online games . to address this issue , we obtained real chat data from moviestarplanet , a massively multiplayer online game for children . the research described in this paper aimed to detect predatory behaviors in the chats using machine learning methods . in order to achieve a high accuracy on this task , extensive preprocessing was necessary . we describe three different strategies for data selection and preprocessing , and extensively compare the performance of different learning algorithms on the different data sets and features . 

automatic question - answer pair generation using deep learning
automatic question - answer pair generation is gaining importance because of the time being saved when compared to manual creation of questions . it not only saves time but also reduces the effort involved in the manual question generation process . it is useful in many fields such as school assignments , law practicing , self - assessments and many more . our main objective is to create wh - type questions from a paragraph and find its accurate answer as well . this research work introduces a question answer generation ( qag ) system by using a deep learning approach for combining answer extraction ( ae ) , question generation ( qg ) and question answering ( qa ) models . this research work aims to use a pre - trained language model - bidirectional encoder representation from transformers ( bert ) and fine - tune it as per the proposed research objective . 

an interpretable deep learning system for automatically scoring request for proposals
the managed care system within medicaid ( us healthcare ) uses request for proposals ( rfp ) to award contracts for various healthcare and related services . rfp responses are very detailed documents ( hundreds of pages ) submitted by competing organisations to win contracts . subject matter expertise and domain knowledge play an important role in preparing rfp responses along with analysis of historical submissions . automated analysis of these responses through natural language processing ( nlp ) systems can reduce time and effort needed to explore historical responses , and assisting in writing better responses . our work draws parallels between scoring rfps and essay scoring models , while highlighting new challenges and the need for interpretability . typical scoring models focus on word level impacts to grade essays and other short write - ups . we propose a novel bi - lstm and a transformer based regression model , and provide deeper insight into phrases which latently impact scoring of responses . we contend the merits of our proposed methodology using extensive quantitative experiments . we also qualitatively assess the impact of important phrases using human evaluators . finally , we introduce a novel problem statement that can be used to further improve the state of the art in nlp based automatic scoring systems . 

stimulated brillouin scattering in kerr filamentation regimes
the coupling between kerr filamentation and stimulated brillouin scattering ( sbs ) is investigated for nanosecond laser pulses propagating in bulk silica . phase modulated pulses with moderate spectral bandwidth may not prevent backscattering . we demonstrate the existence of a critical bandwidth above which sbs is efficiently suppressed at any pump power . 

uml - based representation for textual objects
this paper shows how the unified modeling language ( uml ) can be used to represent a textual object or sentence of a natural language . precisely , the representation builds on the uml class . that is data of a textual object - which are component types or " word types " , lemmas of components , morphological information , syntactic dependency relations between components , and meanings - take up the compartments of the uml class - which are the compartment of the name , attributes , operations , and responsibilities . in addition to existent uml concepts , the representation introduces new ones to denote syntactic dependency relations and their types , gender and some morphological information of object components . 

legalbert - th : development of legal q & a dataset and automatic question tagging
tagging questions according to their topics is useful for internet forum management . in this paper , we use the bidirectional encoder representations from transformers ( bert ) model to categorize posts from thai legal internet forums . first , we construct our new legal q & a dataset by scraping the internet , cleaning the data , and annotating the data . second , we perform transfer learning to let our model learn about the legal language model in general and then fine - tune the model for the law topic classification task . as a result , we have developed a legal q & a dataset of 12 , 695 question / answer pairs and a law topic classification model based on bert with 92 % accuracy . finally , we build a prototype legal internet forum which equipped with the automatic tagging function , law topic classification , to provide a concrete example of how to apply the model in the real situation . 

turbotransformers : an efficient gpu serving system for transformer models
the transformer is the most critical algorithm innovation of the nature language processing ( nlp ) field in recent years . unlike the recurrent neural network ( rnn ) models , transformers are able to process on dimensions of sequence lengths in parallel , therefore leads to better accuracy on long sequences . however , efficient deployments of them for online services in data centers equipped with gpus are not easy . first , more computation introduced by transformer structures makes it more challenging to meet the latency and throughput constraints of serving . second , nlp tasks take in sentences of variable length . the variability of input dimensions brings a severe problem to efficient memory management and serving optimization . to solve the above challenges , this paper designed a transformer serving system called turbotransformers , which consists of a computing runtime and a serving framework . three innovative features make it stand out from other similar works . an efficient parallel algorithm is proposed for gpu - based batch reduction operations , like softmax and layernorm , which are major hot spots besides blas routines . a memory allocation algorithm , which better balances the memory footprint and allocation / free efficiency , is designed for variable - length input situations . a serving framework equipped with a new batch scheduler using dynamic programming achieves the optimal throughput on variable - length requests . the system can achieve the state - of - the - art transformer model serving performance on gpu platforms and can be seamlessly integrated into your pytorch code with a few lines of code . 

translating multi word terms into korean from chinese documents
this paper suggests a methodology which is aimed to translate the chinese terms into korean . our basic idea is to use a method which combines domain - tuned dictionary and target language corpus . the translation system used in our work operated at three steps : ( 1 ) tokenization of the term candidate ; ( 2 ) translation of term candidates using domain - tuned bilingual lexicon ; ( 3 ) ranking the translation candidate to recommend the best translation according to the frequency of the target word cooccurrence in the corpus . and we have demonstrated the effectiveness of our approach by showing a high degree of accuracy evaluation . our experiments indicate a significant improvement when measured against un - tuned chinese - korean mt system . 

fiction sentence expansion and enhancement via focused objective and novelty curve sampling
we describe the task of sentence expansion and enhancement , in which a sentence provided by a human is expanded in some creative way . the expansion should be understandable , believably grammatical , and highly related to the original sentence . sentence expansion and enhancement may serve as an authoring tool , or integrate in dynamic media , conversational agents , and advertising . we implement a neural sentence expander , which is trained on sentence compressions generated from a corpus of modern fiction . we modify the objective loss function to support the task by focusing on new words , and decode at test time with controlled curve - like novelty sampling . we run the sentence expander on sentences provided by human subjects and have humans evaluate these expansions . the generation methods are shown to be comparable to , and as well liked as , subjects ' original input sentences , and preferred over baselines . 

a study on a joint deep learning model for myanmar text classification
text classification is one of the most critical areas of research in the field of natural language processing ( nlp ) . recently , most of the nlp tasks achieve remarkable performance by using deep learning models . generally , deep learning models require a huge amount of data to be utilized . this paper uses pre - trained word vectors to handle the resource - demanding problem and studies the effectiveness of a joint convolutional neural network and long short term memory ( cnn - lstm ) for myanmar text classification . the comparative analysis is performed on the baseline convolutional neural networks ( cnn ) , recurrent neural networks ( rnn ) and their combined model cnn - rnn . 

enhanced sentiment classification using geo location tweets
information shared by millions of people in social networks . it is an important way to know the views on political issues , the vision of social problems , the latest trends , commercial websites , etc . in particular , twitter is an important source of knowledge . it is useful to perform a moral assessment of the service . the classification of emotions on tweets has given a financial and effective form to determine public sentiment . in this research , more than 30 , 000 location - based tweets are collected from the twitter page also emotions ( emoji ‘ s ) and the abbreviated form of words are used for the sentiment classification by replacing in top form , then perform the opinion classification in 5 categories , extreme positive , positive , neutral , negative and extreme negative classes . suppliers and shareholders help to take into account the opinions of modes , services , transactions , etc from a particular geographical location . it also performs a logical analysis of many products or services on the same tool . 

sentence boundary disambiguation for indonesian language
sentence boundary detection is essential for natural language processing ( nlp ) . sentence boundary detection in the indonesian language has lots of problems , which includes punctuation , abbreviation , and character in the bracket . the disambiguation should be detected as sentence boundary . thus the sentence boundary system can divide the sentences accurately . this study presents the development of a training dataset for the existing model to optimize supervised sentence boundary detection for the indonesian language . indonesian translation of the quran ( itq ) data set was used in this study by using the supervised method . the following is the process briefly : create the training data , apply sentence detection to separate sentences on itq , and calculate precision , recall , and f - measure . the result is quite promising , it gives as follows : precision of 91 . 7% , recall 81 . 6% , and f - measure 86 . 4 % , respectively . 

plagiarism detection in malayalam language text using a composition of similarity measures
plagiarism refers to the actof using someone else ' s work without permission or without due acknowledgement . it has become a main concern in educational and research organisations . though plagiarism existed from the very old times , the fast technological changes due to the internet has increased the problem to a very high level . numerous plagiarism detection systems have been developed for dealing with this problem , especially for the english language . for a south indian language like malayalam which is both inflectional and agglutinative , developing an efficient plagiarism detection system is a challenging task . this paper focuses on the effect of using shallow natural language processing techniques along with a combination of similarity metrics for extrinsic plagiarism detection in malayalam text documents . 

satire detection from web documents using machine learning methods
satire exposes humanity ' s vices and foibles through the use of irony , wit , and sometimes sarcasm too . it is also frequently used in online communities . recognition of satire can help in many nlp applications like dialogue system and review summarization . in this paper we filter online news articles as satirical or true news documents using svm ( support vector machine ) classification method combined with machine learning techniques . with ample training documents svm tends to give good classification results . for obtaining promising results with svm an understanding of its working and ways to influence its accuracy is required . we also use various feature extraction strategies and conclude that tf - idf - bns feature extraction gives maximum accuracy for detection of satire in web content . 

unsupervised software - specific morphological forms inference from informal discussions
informal discussions on social platforms ( e . g . , stack overflow ) accumulates a large body of programming knowledge in natural language text . natural language process ( nlp ) techniques can be exploited to harvest this knowledge base for software engineering tasks . to make an effective use of nlp techniques , consistent vocabulary is essential . unfortunately , the same concepts are often intentionally or accidentally mentioned in many different morphological forms in informal discussions , such as abbreviations , synonyms and misspellings . existing techniques to deal with such morphological forms are either designed for general english or predominantly rely on domain - specific lexical rules . a thesaurus of software - specific terms and commonly - used morphological forms is desirable for normalizing software engineering text , but very difficult to build manually . in this work , we propose an automatic approach to build such a thesaurus . our approach identifies software - specific terms by contrasting software - specific and general corpuses , and infers morphological forms of software - specific terms by combining distributed word semantics , domain - specific lexical rules and transformations , and graph analysis of morphological relations . we evaluate the coverage and accuracy of the resulting thesaurus against community - curated lists of software - specific terms , abbreviations and synonyms . we also manually examine the correctness of the identified abbreviations and synonyms in our thesaurus . we demonstrate the usefulness of our thesaurus in a case study of normalizing questions from stack overflow and codeproject . 

bridging the gap of dimensions in distillation : understanding the knowledge transfer between different - dimensional semantic spaces
in recent years , knowledge distillation has been widely used in the field of deep learning in order to reduce the model size and save time and space . the student - teacher paradigm is a framework for knowledge distillation , and knowledge distillation proposed to minimize the kl divergence between the probabilistic outputs of a teacher and student network . however , apart from the probabilistic outputs , there are much valuable information contained in the middle layers of the teacher network . as for nlp tasks , the hidden vectors from different layers of a model have different semantic information , but the vectors ' dimension of the student network is different from that of the teacher network in many cases , which makes hidden layer distillation hard to be performed directly . we propose to simply use a transition matrix to project the student ' s vector to a space of the same dimension as the teacher ' s vector , and we theoretically prove the effectiveness of this method . our analysis shows how the transition matrix preserve important semantic information , which is closely related to the vector ' s characteristic in euclidean space . we provide a geometric method for the interpretability of shared knowledge space for student - teacher architectures . our experiments show that this method can significantly improve the performance of a small model in different tasks with different models . 

improving english subcategorization acquisition with diathesis alternations as heuristic information
automatically acquired lexicons with subcategorization information have already proved accurate and useful enough for some purposes but their accuracy still shows room for improvement . by means of diathesis alternation , this paper proposes a new filtering method , which improved the performance of korhonen ' s acquisition system remarkably , with the precision increased to 91 . 18 % and recall unchanged , making the acquired lexicon much more practical for further manual proofreading and other nlp uses . 

study of text classification natural language processing algorithms for four european areas ’ english dialects
in the era of big data , the text data on the network is growing day by day . it is very important to organize and manage the massive data scientifically by using text classification technology . as the most widely distributed and the largest data carrier , how to organize and manage these data effectively is a problem to be solved . text classification is a basic work in the task of natural language processing . its purpose is to organize and classify text resources , and it is also the key link to solve the problem of text information overload . this article will use data mining technology to evaluate several nlp algorithms for text classification and use weka to classify and predict english dialects in four different european countries . the research method used in this paper is the crisp - dm method , including business understanding , data understanding , data preparation , modelling and evaluation for algorithms . five different models were used for text classification : zeror , j48 , naivebayes , randomforest and multilayerperceptron . among them , randomforest performed best for text classification . it can process high dimensional data without feature selection ( because feature subset is randomly selected) . also , during training , trees are independent of each other , and the training speed is fast , so it is easy to make parallel method . 

prototype machine translation system from text - to - indian sign language
this paper presents a prototype text - to - indian sign language ( isl ) translation system . the system will help dissemination of information to the deaf people in india . this paper also presents the sl - dictionary tool , which can be used to create bilingual isl dictionary and can store isl phonological information . 

the development of a model to predict marbling score for fattening kamphaeng saen beef breed using data mining
the purpose of this research was to create a model for beef marbling score in kamphaeng saen beef breed using data mining techniques . the data of the fattening cattle were transformed from kamphaeng saen beef co - operative co . , ltd . during 2015 - 2019 . for 5 years , 1 , 568 head , including the fattening period ( month ) , the age of the cattle from the shedding and wear of the teeth the weight is alive when slaughter , the carcass weight into a fresh , carcass weight , cool carcass percentage and the marbling score in the meat . to create a forecast model by using data classification techniques with decision tree method with c4 . 5 algorithm , then test the predictive model with percentage split method by dividing the data into training set 30 % and test set 70 % . it is found that the accuracy is 57 . 1949 % both because there are many factors related to the formation of fat in the muscles such as varieties , age , sex and feed , especially feed is the main factor affecting the accumulation of fat in the muscles . the growth of fat that is controlled by stearoyl - coa desaturase ( scd ) shows that the prediction of the marbling score in fattening cow muscle with data mining techniques can be used as a means of forecasting without having to wait . the cattle were finished and assessed from the remains of the cattle . 

a study of chinese document representation and classification with word2vec
word2vec is a neural network language model which can convert words and phrases into a high - quality distributed vector ( called word embedding ) with semantic word relationships , so it offers a unique perspective to the text classification and other natural language processing ( nlp ) tasks . in this paper , we propose to combine improved tfidf algorithm and word embedding as a way to represent documents and conduct text classification experiments on the sogou chinese classification corpus . our results show that the combination of word embedding and improved tf - idf algorithm can outperform either individually . 

enriched dashboard : an integration and visualization tool for distributed nlp systems on heterogeneous platforms
dashboard is an integration , validation , and visualization tool for natural language processing ( nlp ) applications . it provides infrastructural facilities using which individual nlp modules may be evaluated and refined , and multiple nlp modules may be combined to build a large end - user nlp system . it helps the system integration team to integrate and validate the nlp system . the tool provides a visualization interface that helps the developers to profile ( time and memory ) each module . it helps researchers to evaluate and compare their module with the earlier versions of same module . the tool promotes reuse of existing nlp modules . enriched dashboard supports execution of modules on heterogeneous platforms . it supports execution of modules that are distributed i . e . , located on different machines . it has a powerful notation to define the runtime properties of the nlp modules . it provides an easy - to - use graphical interface that is developed using eclipse rcp . the user can choose a perspective ( view ) that allows him to perform his task better . additionally , eclipse rcp provides a plugin architecture , hence additional end - user functionalities can be easily added to the tool . 

nptas : a novel platform for text annotation and service
natural language processing ( nlp ) is a critical research area in artificial intelligence , which has spawned a variety of useful applications . however , creating an online nlp service from scratch is still challenging for non - expert users , which has to go through complex steps such as corpus annotation , model training and deployment . existing tools mainly focus on corpus annotation , which rarely support collaborative annotation or online model training and deployment . to meet the requirement of rapid deployment of nlp services , we develop a novel full process platform nptas , which supports collaborative annotation , online model training and model deploying with restful interfaces . to validate the effectiveness of the platform , we build a clinical entities recognition service from chinese electronic medical records on it . the experimental results show that nptas improves the efficiency and quality of corpus annotation and greatly reduces the effort to build online nlp services . the platform is available at http :// nptas . c2cloud . cn . 

coclubert : clustering machine learning source code
nowadays , we can find machine learning ( ml ) applications in nearly every aspect of modern life , and we see that more developers are engaged in the field than ever . in order to facilitate the development of new ml applications , it would be beneficial to provide services that enable developers to share , access , and search for source code easily . a step towards making such a service is to cluster source code by functionality . in this work , we present coclubert , a bert - based model for source code embedding based on their functionality and clustering them accordingly . we build coclubert using cubert , a variant of bert pre - trained on source code , and present three ways to fine - tune it for the clustering task . in the experiments , we compare coclubert with a baseline model , where we cluster source code using cubert embedding without fine - tuning . we show that coclubert significantly outperforms the baseline model by increasing the dunn index metric by a factor of 141 , the silhouette score metric by a factor of two , and the adjusted rand index metric by a factor of 11 . 

ptokenizer : pos tagger tokenizer
by the advent of new information sources and the expansion of text data , natural language processing ( nlp ) has become one of the key parts of all the systems dealing with human written texts , and part of speech ( pos ) tagging is an inseparable part of all nlp tasks . as a result , it is of the paramount importance to enhance the accuracy of pos tagging . in this paper , applying language model and statistical information , we introduce a new approach to tokenize sentences and prepare them to be labeled by pos taggers . an evaluation shows that the proposed method yields a precision of 98 percent for tokenizing , and applying it to a maximum likelihood and tnt pos taggers achieve improvement in the accuracy of persian pos tagging . 

recipegm : a hierarchical recipe generation model
this paper demonstrates the application of hierarchical convolutional neural networks using self - attention mechanisms for the task of generating recipes given a set of ingredients the recipe should contain . we compare this model , recipegm , to an lstm baseline and recipegpt using several metrics and show that our model is able to outperform even recipegpt in some cases . furthermore , this work discusses suitable evaluation techniques for recipe generation and highlights weak points of some current in use metrics . 

smart recommendation system based on product reviews using random forest
social network , e - commerce sites , blogs are new emerging platforms for people to express their opinion . these sites contain huge amount of text which can be used for different purpose like sentiment analysis . sentiment analysis is a growing field in natural language processing . sentiment analysis is major focused on company ' s improvement . but sentiment analysis can be useful in recommendation system also . based on various performance measures , this paper compares the results of machine learning algorithms like multinomial naive bayes algorithm , logistic regression , svm classifier , decision tree and random forest . these algorithms are used for sentiment analysis of reviews and in turn for product recommendation . in proposed system , random forest shows outstanding performance . to create suitable recommendations using the analysis of emotions , there is a need to use polarity obtained through the reviews . 

understanding the representational power of neural retrieval models using nlp tasks
the ease of constructing effective neural networks has resulted in a large number of varying architectures iteratively improving performance on a task . due to the nature of these models being black boxes , standard weight inspection is difficult . we propose a probe based methodology to evaluate what information is important or extraneous at each level of a network . we input natural language processing datasets into a trained answer passage neural network . each layer of the neural network is used as input into a unique classifier , or probe , to correctly label that input with respect to a natural language processing task , probing the internal representations for information . using this approach , we analyze the information relevant to retrieving answer passages from the perspective of information needed for part of speech tagging , named entity retrieval , sentiment classification , and textual entailment . we show a significant information need difference between two seemingly similar question answering collections , and demonstrate that passage retrieval and textual entailment share a common information space , while pos and ner information is used only at a compositional level in the lower layers of an information retrieval model . lastly , we demonstrate that incorporating this information into a multitask environment is correlated to the information retained by these models during the probe inspection phase . 

speech2health : a mobile framework for monitoring dietary composition from spoken data
diet and physical activity are known as important lifestyle factors in self - management and prevention of many chronic diseases . mobile sensors such as accelerometers have been used to measure physical activity or detect eating time . in many intervention studies , however , stringent monitoring of overall dietary composition and energy intake is needed . currently , such a monitoring relies on self - reported data by either entering text or taking an image that represents food intake . these approaches suffer from limitations such as low adherence in technology adoption and time sensitivity to the diet intake context . in order to address these limitations , we introduce development and validation of speech2health , a voice - based mobile nutrition monitoring system that devises speech processing , natural language processing ( nlp) , and text mining techniques in a unified platform to facilitate nutrition monitoring . after converting the spoken data to text , nutrition - specific data are identified within the text using an nlp - based approach that combines standard nlp with our introduced pattern mapping technique . we then develop a tiered matching algorithm to search the food name in our nutrition database and accurately compute calorie intake values . we evaluate speech2health using real data collected with 30 participants . our experimental results show that speech2health achieves an accuracy of 92 . 2 % in computing calorie intake . furthermore , our user study demonstrates that speech2health achieves significantly higher scores on technology adoption metrics compared to text - based and image - based nutrition monitoring . our research demonstrates that new sensor modalities such as voice can be used either standalone or as a complementary source of information to existing modalities to improve the accuracy and acceptability of mobile health technologies for dietary composition monitoring . 

knowledge distillation application technology for chinese nlp
at this stage , the popular deep neural network models often encounter problems of high latency , difficult deployment and high hardware requirements in practical applications . knowledge distillation is a good approach to solve these problems . we adopted an innovative knowledge distillation approach and formulated data augmentation strategies for the tasks , and obtained a lightweight model with 6 . 7 × acceleration ratio and 13 . 6 × compression ratio compared to the baseline model bert - base , and the average performance of the lightweight model reached 95 % of bert - base for each task . we continue to conduct in - depth research to investigate some of the issues that remain in the knowledge distillation phase . to address the problems in distillation model selection and model fine - tuning , we propose a teacher model and student model selection strategy and a two - stage model fine - tuning strategy before and after the knowledge distillation stage . these two strategies further improve the average performance of the models to 98 % of bert - base . finally , we developed a lightweight model evaluation scheme based on different types of downstream tasks , which provides a reference for subsequent practical applications when encountering similar tasks . 

natural language processing in the undergraduate curriculum
the paper has two purposes : first , we argue that natural language processing , and particularly those aspects of that field often referred to as language technology , should play an important role in the computer science curriculum ; second , we describe in broad terms the content of an undergraduate program we have developed at macquarie university that covers this material . we question the industrial relevance of much that is taught in nlp courses , and emphasize the need for a practical orientation as a means to growing the size of the field . we argue that a more evangelical approach , both with regard to students and industry , is required . the paper provides an overview of the material we cover , and makes some observations for the future on the basis of our experiences so far . 

converting mikrokosmos frames into description logics
mikrokosmos contains an ontology plus a number of lexicons in different languages that were originally developed for machine translation . the underlying representation formalism for these resources is an ad - hoc frame - based language which makes it difficult to inter - operate mikrokosmos with state - of - the - art knowledge - based systems . in this paper we propose a translation from the frame - based representation of mikrokosmos into description logics . this translation allows us to automatically transform mikrokosmos sources into owl and thus provide a powerful ontology in the formalism of the semantic web . furthermore , the reasoning mechanisms of description logics may also support knowledge acquisition and maintenance as well as its application in natural language processing systems . 

fedbert : when federated learning meets pre - training
the fast growth of pre - trained models ( ptms ) has brought natural language processing to a new era , which has become a dominant technique for various natural language processing ( nlp ) applications . every user can download the weights of ptms , then fine - tune the weights for a task on the local side . however , the pre - training of a model relies heavily on accessing a large - scale of training data and requires a vast amount of computing resources . these strict requirements make it impossible for any single client to pre - train such a model . in order to grant clients with limited computing capability to participate in pre - training a large model , we propose a new learning approach fedbert that takes advantage of the federated learning and split learning approaches , resorting to pre - training bert in a federated way . fedbert can prevent sharing the raw data information and obtain excellent performance . extensive experiments on seven glue tasks demonstrate that fedbert can maintain its effectiveness without communicating to the sensitive local data of clients . 

applying pomdp to moving target optimization
diversity maintains security by making the computing environment less standard and less predictable . recent studies show that many randomization techniques , e . g . address space layout randomization ( aslr ) significantly enhance system security simply through reducing the number of return to libc exploits [ 14] . however , " diversity " may incur significant overhead on the computing platforms . we study the problem of implementing diversity to trade off security performance with diversity implementation costs . we address this problem by formulating it as a partially observable markov decision process ( pomdp ) . an optimal solution considering a fixed amount of history can be obtained by transforming the pomdp optimization problem into a nonlinear programming ( nlp ) problem . simulation results for a set of benchmark problems illustrate the effectiveness of the proposed method . 

information retrieval in kannada using ontology
as internet technology has become a part of the lifestyle of the common man , research efforts are extensively made in the fields of natural language processing ( nlp ) and information retrieval . studying regional languages for developing the system to store , retrieve , extract the information from the database has gained lots of prominence nowadays . case studies show that ontological information retrieval has many advantages over keyword - based approach . in this paper we have focused on the general architecture of ontology - based information retrieval used for kannada . 

a robust dialogue system with spontaneous speech understanding and cooperative response
a spoken dialogue system that can understand spontaneous speech needs to handle extensive range of speech in comparision with the read speech that has been studied so far . the spoken language has looser restriction of the grammar than the written language and has ambiguous phenomena such as interjections , ellipses , inversions , repairs , unknown words and so on . it must be noted that the recognition rate of the speech recognizer is limited by the trade - off between the looseness of linguistic constraints and recognition precision , and that the recognizer may output a sentence as recognition results which human would never say . therefore , the interpreter that receives recognized sentences must cope not only with spontaneous sentences but also with illegal sentences having recognition errors . some spoken language systems focus on robust matching to handle ungrammatical utterances and illegal sentences . 

an ontology - based approach for key phrase extraction
automatic key phrase extraction is fundamental to the success of many recent digital library applications and semantic information retrieval techniques and a difficult and essential problem in vietnamese natural language processing ( nlp ) . in this work , we propose a novel method for key phrase extracting of vietnamese text that exploits the vietnamese wikipedia as an ontology and exploits specific characteristics of the vietnamese language for the key phrase selection stage . we also explore nlp techniques that we propose for the analysis of vietnamese texts , focusing on the advanced candidate phrases recognition phase as well as part - of - speech ( pos ) tagging . finally , we review the results of several experiments that have examined the impacts of strategies chosen for vietnamese key phrase extracting . 

ensemble deep timenet : an ensemble learning approach with deep neural networks for time series
time series classification ( tsc ) is the problem of predicting class labels of time series generated by different signal sources . tsc has been a challenging problem in machine learning and statistics for many decades . tsc has many important applications in bioinformatics , biomedical engineering , and clinical predictions . a large number of classification algorithms have been developed to address tsc problem . however , there is still a lot of room for improving the accuracy of classification . traditional approaches extract discriminative features from the time series data by applying different types of transformations . these features are then fed into standard classifiers for classification . after a tremendous success of deep neural networks in certain areas such as nlp , image processing , and speech recognition , some researchers have applied deep convolutional neural networks and recurrent neural network based approaches for tsc . deep neural network based algorithms have established a new baseline for tsc . in this paper , we propose ensemble deep timenet ( edtnet ) , an ensemble of multiple deep neural networks for tsc . we have compared the accuracy of edtnet with those of the state - of - the - art algorithms on 44 different datasets from ucr time series database . through extensive experiments we show that edtnet outperforms all the competing algorithms in most of the ucr datasets in terms of classification accuracy . specifically , edtnet outperforms the other algorithms on 26 out of the 44 datasets , whereas , the next best algorithm ( fcn ) has a better accuracy than the others in only 10 out of the 44 . a gpu based python implementation of our algorithm will be posted at https : / / github . com / sudiptap / timeseries - classification when the paper is accepted . 

uofl : word sense disambiguation using lexical cohesion
one of the main challenges in the applications ( i . e . : text summarization , question answering , information retrieval , etc . ) of natural language processing is to determine which of the several senses of a word is used in a given context . the problem is phrased as " word sense disambiguation ( wsd) " in the nlp community . this paper presents the dictionary based disambiguation technique that adopts the assumption of one sense per discourse in the context of semeval - 2007 task 7 : " coarse - grained english all - words " . 

automated requirement sentences extraction from software requirement specification document
in the requirement reuse and natural language document - based software product line ( spl ) domain analysis , requirement sentences of the requirement document are the primary concern . most studies conducted in this research area have document preprocessing stage in their methods that is a manual process to separate requirement sentences and non - requirement sentences from the document . this manual labor process might be tedious and error - prone since it will need much time and expert intervention to make this process completely done . in this paper , we present a method to automate requirement sentence extraction from the software requirement specification ( srs ) document by leveraging natural language processing ( nlp ) approach and requirement boilerplate sentence patterns . conducted experiments in this research show this method has such accuracy from 64 % to 100 % on precision value and recall value in the range of 64 % to 89 % . 

salt : an xml application for web - based multimodal dialog management
this paper describes the speech application language tags , or salt , an xml based spoken dialog standard for multimodal or speech - only applications . a key premise in salt design is that speech - enabled user interface shares a lot of the design principles and computational requirements with the graphical user interface ( gui ) . as a result , it is logical to introduce into speech the object - oriented , event - driven model that is known to be flexible and powerful enough in meeting the requirements for realizing sophisticated guis . by reusing this rich infrastructure , dialog designers are relieved from having to develop the underlying computing infrastructure and can focus more on the core user interface design issues than on the computer and software engineering details . the paper focuses the discussion on the web - based distributed computing environment and elaborates how salt can be used to implement multimodal dialog systems . how advanced dialog effects ( e . g . , cross - modality reference resolution , implicit confirmation , multimedia synchronization ) can be realized in salt is also discussed . 

research on semi - automatic construction of domain ontology based on machine learning and clustering technique
in this paper we take the approach that constructed the ontology automatically , which attempted to take a method that extremely beneficial for the knowledge acquisition task was the integration of knowledge acquisition with machine learning techniques to increase the ontology construction effect , including domain concepts acquisition , taxonomy relation recognition , non - taxonomy relation recognition and ontology formalization description . this paper adopted an approach of non - dictionary chinese word segmentation techniques based on n - gram to acquire domain candidate concepts , take the method based of nlp in the recognition of domain concept property relation , extracted subject , predicate and object of sentences . this triangle data can be treated as the triplet of data and object type property . 

dls magician : promoting early - stage collaboration by automating ui design process in an e & p environment
in this work , we present a prototype of an intelligent system that can automate the ui design process via converting text descriptions into interactive design prototypes . we conducted user research in an international oilfield services company , and found that product owners prefer to validate their hypotheses via visual mockups rather than text descriptions ; however , many of them need assistance from designers to produce the visual mockups . based on this finding and after exploring multiple possibilities using design thinking , we chose a solution that uses natural language processing ( nlp ) to automate the visual design process . to validate the answer , we conducted user tests via means and iterated the solution . in the future , we expect the work can be fully deployed in a working environment to help product owners initiate their projects faster . 

event detection in noisy streaming data with combination of corroborative and probabilistic sources
global physical event detection has traditionally relied on dense coverage of physical sensors around the world ; while this is an expensive undertaking , there have not been alternatives until recently . the ubiquity of social networks and human sensors in the field provides a tremendous amount of real - time , live data about true physical events from around the world . however , while such human sensor data have been exploited for retrospective large - scale event detection such as hurricanes or earthquakes , there has been limited to no success in exploiting this rich resource for general physical event detection . prior implementation approaches have suffered from the concept drift phenomenon , where real - world data exhibits continuous , unknown , and unbounded changes in its data distribution , making static machine learning models ineffective in the long term . we propose and implement an end - to - end collaborative drift adaptive system that integrates corroborative and probabilistic sources to deliver real - time predictions . furthermore , our system is adaptive to concept drift and performs automated continuous learning to maintain high performance . we demonstrate our approach in a real - time demo available online for landslide disaster detection , with extensibility to other real - world physical events such as flooding , wildfires , hurricanes , and earthquakes . 

multi - level mining and visualization of scientific text collections : exploring a bi - lingual scientific repository
we present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles . the text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper ' s contents . in addition to the extraction and enrichment of documents with metadata ( titles , authors , affiliations , etc) , the deep analysis performed comprises semantic interpretation , rhetorical analysis of sentences , triple - based information extraction , and text summarization . the visualization components allow geographical - based exploration of collections , topic - evolution interpretation , and collaborative network analysis among others . the paper presents a case study of a bi - lingual collection in the field of natural language processing ( nlp) . 

power fault preplan text information extraction based on nlp
a large amount of texts recorded in chinese exist in power grid enterprises . these texts contain abundant information of power system . manually mining the text information is inefficient and the accuracy may vary with different dispatchers . in this paper , the power fault countermeasure text is taken as the object to study the power chinese text information extraction method . power texts are segmented firstly based on the nature language process ( nlp ) , the ontology lexicon is established according to the power word attribute in the power fault countermeasure text ; based on the syntax structure characteristics of punctuations and the concept of separate parsing phrase are brought in to guide the division of long texts , which can separate the sentence with only one power entity and its related information ; the syntax rule template applicable to the separate parsing phrase is established based on the meta - character templates ( generalization slot , fixed word - combination , wildcard character , and registry function ) used for the power fault preplan text information extraction and the structured output of that information ; at last , the generalization ability and the universality of the template are analyzed . examples show that the rule template applies to the information extraction of most texts with strong universality and high accuracy . 

nlp based chatbot for multiple restaurants
errand arranged chatbots can be utilized to robotize a specific task , for example , finding a restaurant and reserving a spot . carrying out such a conversational framework can be difficult , requiring space information and handmade principles . restaurant chatbot is an automated system for carrying out day to day work in a restaurant such as ordering , reservations , faqs at reception , and many more tasks which are not that much important but take much time of the receptionist while working . chatbot is a system backed by an artificial intelligence technology to learn , adapt and work according to the surrounding environment . the main focus of this proposal was to assess the chance of utilizing a neural organization based model to make a start to finish teachable chatbot that can mechanize an eatery reservation administration . the reason for this review was to investigate clients ’ insights and practices when utilizing chatbots in cafe takeout orders . based on the social presence hypothesis , this review led a lab examination to look at and think about three requesting techniques in speedy help and full - administration restaurant . 

a multi - sentiment classifier based on gru and attention mechanism
previous sentiment analysis studies have focused on monolingual texts , and are basically multi - category tasks ( ie , a sentence belongs to only one category ) . however , in practice , a sentence often expresses multiple sentiments , and the text often contains multiple languages . this paper proposes a multi - label sentiment classifier based on gru and attention mechanism , which has achieved good results in the data set provided by nlp & cc share task 1 . 

toward multiperiod ac - based contingency constrained optimal power flow at large scale
this work presents a scaling study of a parallel nonlinear , nonconvex optimization approach applied to a multiperiod contingency constrained alternating current optimal power flow . we propose a “ variable duplication ” model for efficient parallelization of the numerical optimization on massively parallel hpc hardware . the model is expressed as a two - stage nonlinear programming problem , where the first stage captures time - dependent constraints and the second stage reflects the system changes in response to contingencies . the parallel interior - point optimization solver for nonlinear programming ( pips - nlp ) enables us to leverage the dual - block angular structure specific to the formulation by applying the schur complement for efficient parallelization of the linear solves . the julia modelling package structjump , allows us to compactly and conveniently express the model ' s algebraic components . structjump uses automatic differentiation to provide the first - and second - order derivatives to pips - nlp . aiming at a strategy for computations at petascale , numerical experiments conducted on theta , an intel knl - based system at the argonne leadership computing facility are presented . 

determining physical location of wireless access point using smart devices
indoor position system ( ips ) is a method for locating objects in an enclosed space . this paper applies the concept of ips to locate the physical location of wireless access point . the proposed system can be used for locating the suspected access point in the building . the system uses at least three smartphones to form the points in two - dimensional geometry . these smartphones will gather wi - fi ` s information and send them to the server . then , trilateration is applied to calculate the distance and the location of the targeted access point . the physical location will then send back to each smartphone . the smartphone can use this position to navigate the user to the targeted access point . the result shows that , in the enclosed space , the error of the proposed method is around 3 . 5 meters . 

a feature based fitness cautionary system for heavy internet users by using deep learning
theoretical - machine learning is the subset of manmade thinking that goes under data science . without explicitly redid , getting pcs to learn is a science known as machine learning . the proposition systems present in the market are accepted to be working in mainstream applications like youtube online media applications like facebook , instagram or thing based applications like flipkart . basically , these systems help to zero in on information that is concerned or important for a particular customer . one region where such structures can be particularly useful is contamination forewarning framework . considering an ailment the customer commitments to the structure , that he thinks they are slanted to or they are encountering they will be proposed top 5 or top 3 afflictions they are for the most part slanted to reliant upon the similarity between the contamination customer inputted and the sickness customer is being recommended for the present circumstance being forewarned . as of now , everything is open on the web , every contamination and its information around there . experts are there but simultaneously the count of ailments , number of patients for a disease are extending . an individual has one disorder then there are chances they will get another . sickness incorporate among youths in this age bundle is growing at a gigantic rate . there is the fix of afflictions or conceivably not anyway shouldn ' t something be said about notice . if we alert someone before they are truly encountering a contamination . it will make him / her considerably more careful than beforehand . this paper examines existing recommender systems and besides includes the burdens of such structures . drawbacks can be adaptability , cold start and sparsity . the proposed system partakes in its advantages anyway isn ' t yet available watching out . assessment has been done on how this disease alerted system using content - based idea under ai is eliminating features from dataset and how this structure presents features like customer self - rule , straightforwardness and no infection start . 

backdoor pre - trained models can transfer to all
pre - trained general - purpose language models have been a dominating component in enabling real - world natural language processing ( nlp ) applications . however , a pre - trained model with backdoor can be a severe threat to the applications . most existing backdoor attacks in nlp are conducted in the fine - tuning phase by introducing malicious triggers in the targeted class , thus relying greatly on the prior knowledge of the fine - tuning task . in this paper , we propose a new approach to map the inputs containing triggers directly to a predefined output representation of the pre - trained nlp models , e . g . , a predefined output representation for the classification token in bert , instead of a target label . it can thus introduce backdoor to a wide range of downstream tasks without any prior knowledge . additionally , in light of the unique properties of triggers in nlp , we propose two new metrics to measure the performance of backdoor attacks in terms of both effectiveness and stealthiness . our experiments with various types of triggers show that our method is widely applicable to different fine - tuning tasks ( classification and named entity recognition ) and to different models ( such as bert , xlnet , bart ) , which poses a severe threat . furthermore , by collaborating with the popular online model repository hugging face , the threat brought by our method has been confirmed . finally , we analyze the factors that may affect the attack performance and share insights on the causes of the success of our backdoor attack . 

deriving verbal and compositional lexical aspect for nlp applications
verbal and compositional lexical aspect provide the underlying temporal structure of events . knowledge of lexical aspect , e . g . , ( a ) telicity , is therefore required for interpreting event sequences in discourse ( dowty , 1986 ; moens and steedman , 1988 ; passoneau , 1988 ) , interfacing to temporal databases ( androutsopoulos , 1996 ) , processing temporal modifiers ( antonisse , 1994 ) , describing allowable alternations and their semantic effects ( resnik , 1996 ; tenny , 1994 ) , and selecting tense and lexical items for natural language generation ( ( dorr and olsen , 1996 ; klavans and chodorow , 1992 ) , cf . ( slobin and bocaz , 1988 ) ) . we show that it is possible to represent lexical aspect --- both verbal and compositional --- on a large scale , using lexical conceptual structure ( lcs ) representations of verbs in the classes cataloged by levin ( 1993 ) . we show how proper consideration of these universal pieces of verb meaning may be used to refine lexical representations and derive a range of meanings from combinations of lcs representations . a single algorithm may therefore be used to determine lexical aspect classes and features at both verbal and sentence levels . finally , we illustrate how knowledge of lexical aspect facilitates the interpretation of events in nlp applications . 

evaluating educational intervention fidelity with transcriptsim , a replicable nlp technique
pedagogical implementation research at large identifies and addresses factors affecting adoption and sustainability of evidence - based practices . part of this work is elaborating and testing implementation strategies which prescribe educator intervention techniques and processes to adopt and integrate into educational settings . however , in practice , implementation research is slowed and constrained by the temporal and monetary costs of conducting manual evaluations . in this study , we use an automated , low - cost , and scalable natural language processing ( nlp ) approach we call transcriptsim to assess intervention fidelity in a teacher coaching study ( teachsim ) . transcriptsim quantifies similarity between the intervention protocol and intervention transcripts as an approximation of coaches ’ fidelity to the intervention protocol . 

ai - nlp analytics : a thorough comparative investigation on india - usa universities branding on the trending social media platform “ instagram ” 
the branding of the universities through social media has enormous influence among individuals . social media has prospered through different networking sites such as twitter , facebook , instagram , etc . however , in recent times , instagram has become popular due to its unique features , which includes posting images , writing description alongside these images , and many more . people can even comment on these posts . the emotions expressed by people through their posts represent their real feelings and can be used for the analysis of public opinion regarding any topic . in our paper , we are applying the concepts of nlp on these textual descriptions and comments to perform a comparative investigation on the branding of the colleges / universities of india and usa . our method provides a completely innovative way to view social media data . our analysis can help the colleges / universities of both the country in providing an insight into their recognition among students through instagram considering that the younger generation is largely influenced by instagram . it can also help them in their rapid development and future goals . 

obtaining knowledge from the web using fusion and summarization techniques
nowadays , information and knowledge are fundamental in our society . this has induced an information overload problem in the internet . for this reason , we propose to create an automatic system to retrieve , select , and extract information from the web whose methodology is based on fusion techniques . the system , called diana , facilitates and improves the identification of interesting contents , and it allows to extract the most relevant information about a certain topic from the web as a summary . to do this , we have developed algorithms that use semantic tools , natural language processing ( nlp ) techniques , statistics , a generic gazetteer , and fusion methods . the development of the system is undergoing , but the preliminary results that we have obtained so far are very promising and show the interest of our proposal . 

annotated corpus of comments and basic semantic analysis
this article presents an annotated corpus of turkish comment texts gathered from employees . special attention is given to neutrality of paragraphs in the corpus and quality of the annotation . we employ the majority voting of the annotators . we describe the details of the dataset , the annotation methodology and the experiments with basic methods to investigate the corpus . the corpus has three classes , positive , negative and neutral . 

wikipedia revision toolkit : efficiently accessing wikipedia ' s edit history
we present an open - source toolkit which allows ( i ) to reconstruct past states of wikipedia , and ( ii ) to efficiently access the edit history of wikipedia articles . reconstructing past states of wikipedia is a prerequisite for reproducing previous experimental work based on wikipedia . beyond that , the edit history of wikipedia articles has been shown to be a valuable knowledge source for nlp , but access is severely impeded by the lack of efficient tools for managing the huge amount of provided data . by using a dedicated storage format , our toolkit massively decreases the data volume to less than 2 % of the original size , and at the same time provides an easy - to - use interface to access the revision data . the language - independent design allows to process any language represented in wikipedia . we expect this work to consolidate nlp research using wikipedia in general , and to foster research making use of the knowledge encoded in wikipedia ' s edit history . 

hyponymy acquisition from chinese text by svm
hyponymy as one of semantic relation taxonomies provides a fundamental knowledge for natural language processing applications . in this paper , we propose a method for automatically learning hyponymy terms by machine learning technique from text for chinese . our method relies on hand - crafted hyponymy patterns , and uses the syntactic features to build a multiple classifier to identify novel hyponymy pairs ( hyponym / hypernym or hypernym / hyponym ) in a sentence by svm . experimental results show that the method is effective in acquiring hyponymy from chinese free text . 

a data viz platform as a support to study , analyze and understand the hate speech phenomenon
in this paper we present a data visualization platform designed to support the natural language processing ( nlp ) scholar to study and analyze different corpora collected with the purpose to understand the hate speech phenomenon in social media . the project started with the creation of a corpus which collects tweets addressed to specific groups of ethnic minorities considered very controversial in the italian public debate . each tweet has been manually tagged with a series of attributes in order to capture the different features used to characterize the hate speech phenomenon . this corpus is mainly built to be used for training an automatic classifier and helping us in its testing and validation , before being it adopted to detect tweets targeted as hate speech on larger scale datasets . as opposed as many other traditional machine learning tasks , to build a good classifier achieving high scores in terms of accuracy is very challenging in such scenario , because of the intrinsic ambiguity of the language , the lack of a proper and explicable context in social media , and the attitude of on line users of being sarcastic and ironical . therefore , in order to properly validate an effective feature selection process , correlations between selected attributes must be studied and analyzed . this motivated us to build an interactive platform to explore data in our corpora across the dimensions that have been used to characterize collected tweets . in our paper , after a brief introduction of the hate speech dataset , we will show how the dashboard can fit into the nlp pipeline , and how its architecture can be structured . finally , we will present some of the challenges we have faced to visualize data with spatial , temporal and numerical attributes . 

sreyantra : automated software requirement inter - dependencies elicitation , analysis and learning
requirements elicitation is a cognitively difficult task . rich semantics in natural language based requirements impose challenges in elicitation , analysis and maintenance of requirement inter - dependencies . the challenges intensify further when dependency types and strengths are considered . ignoring inter - dependencies can adversely impact the design , development and testing of software products . this phd research proposal addresses three main challenges . first , natural language processing ( nlp ) is studied to automatically extract dependencies from textual documents . further verb classifiers are utilized to automate elicitation and analysis of different types of dependencies ( e . g : requires , coupling etc ) . second , representation and maintenance of changing requirement dependencies from designing graph theoretic algorithms will be explored . third , the process of providing recommendations of dependencies will be studied . the results are aimed at assisting project managers to evaluate the impact of inter - dependencies and make effective decisions in software development life cycle . 

a natural language processing ( nlp ) framework for embedded systems to automatically extract verification aspects from textual design requirements
embedded systems requirements are significantly different with respect to general purpose systems due to the safety - critical nature and the presence of temporal aspects . particularly , the design requirements of embedded systems , comprise several temporal conditions , are first identified . subsequently , a test engineer / system engineer analyzes the design requirements manually to identify the verification characteristics and develops the verification assertions / constraints accordingly . however , the manual analysis of design requirements for verification is time consuming task . furthermore , high level of domain expertise is required to develop the correct and complete verification assertions from the design requirements . this article presents a novel natural language processing ( nlp ) framework for embedded systems to analyze and automatically extract verification aspects from the textual design requirements . this leads to considerably simplify and accelerate the development of verification assertions . as a part of research , a complete ar2aa ( automated requirements 2 assertions analyzer ) tool is developed in c # by utilizing the sharpnlp and regular expression libraries . the usefulness of proposed framework is demonstrated through car collision and avoidance system ( ccas ) case study . the initial results prove that the proposed framework is highly effective for the analysis and development of verification assertions from the textual design requirements . 

text summarization of hindi documents using rule based approach
automatic summarization plays an important role in document processing system and information retrieval system . generation of summary of a text document is a very important part of nlp . there are a number of scenarios where automatic construction of such summaries is useful . text summarization is that process which convert a larger text into its shorter form maintaining its information . summary of a longer text saves the reading time as it contain lesser number of lines but all important information of the original text document . in this paper we present a novel approach for text summarization of hindi text document based on some linguistic rules . dead wood words and phrases are also removed from the original document to generate the lesser number of words from the original text . proposed system is tested on various hindi inputs and accuracy of the system in form of number of lines extracted from original text containing important information of the original text document . 

updating a name tagger using contemporary unlabeled data
for many nlp tasks , including named entity tagging , semi - supervised learning has been proposed as a reasonable alternative to methods that require annotating large amounts of training data . in this paper , we address the problem of analyzing new data given a semi - supervised ne tagger trained on data from an earlier time period . we will show that updating the unlabeled data is sufficient to maintain quality over time , and outperforms updating the labeled data . furthermore , we will also show that augmenting the unlabeled data with older data in most cases does not result in better performance than simply using a smaller amount of current unlabeled data . 

joint and conditional estimation of tagging and parsing models
this paper compares two different ways of estimating statistical language models . many statistical nlp tagging and parsing models are estimated by maximizing the ( joint ) likelihood of the fully - observed training data . however , since these applications only require the conditional probability distributions , these distributions can in principle be learnt by maximizing the conditional likelihood of the training data . perhaps somewhat surprisingly , models estimated by maximizing the joint were superior to models estimated by maximizing the conditional , even though some of the latter models intuitively had access to " more information " . 

automatic generation of system test cases from use case specifications
in safety critical domains , system test cases are often derived from functional requirements in natural language ( nl ) and traceability between requirements and their corresponding test cases is usually mandatory . the definition of test cases is therefore time - consuming and error prone , especially so given the quickly rising complexity of embedded systems in many critical domains . though considerable research has been devoted to automatic generation of system test cases from nl requirements , most of the proposed approaches re - quire significant manual intervention or additional , complex behavioral modelling . this significantly hinders their applicability in practice . in this paper , we propose use case modelling for system tests generation ( umtg ) , an approach that automatically generates executable system test cases from use case spec - ifications and a domain model , the latter including a class diagram and constraints . our rationale and motivation are that , in many environments , including that of our industry partner in the reported case study , both use case specifica - tions and domain modelling are common and accepted prac - tice , whereas behavioural modelling is considered a difficult and expensive exercise if it is to be complete and precise . in order to extract behavioral information from use cases and enable test automation , umtg employs natural language processing ( nlp ) , a restricted form of use case specifica - tions , and constraint solving . 

human - in - the - loop ai for analysis of free response facial expression label sets
facial expressions ( fes ) communicate a rich variety of social , grammatical , and affective signals . however , the most generally accepted set of recognizable fes remains limited to seven basic displays of emotion : happiness , sadness , fear , anger , disgust , surprise and contempt . to develop intelligent virtual agents capable of interpreting and synthesizing nuanced facial behavior , we need a more complete lexicon . one roadblock has been the limiting nature of forced - choice study designs , the most common paradigm for investigating observer judgements of fes . however , there has been no consensus on an objective way to evaluate alternative free response designs . we present a human - in - the - loop artificial intelligence pipeline for analyzing sets of freely chosen natural language labels . the pipeline , freeres - nlp , makes it possible to automatically identify whether there is consensus on the signal value of an fe and which label best classifies it . freeres - nlp scales to process very large datasets . we validate our approach in two stages : 1 ) comparison between label synonymy scores from ten computer algorithms and human raters across three synonym datasets , and 2 ) examples of pipeline results compared with manual data processing results from emotion and fe recognition studies . the pipeline can potentially improve automated facial expression recognition and procedural modeling of virtual humans . 

word embedding for arabic word sense disambiguation to create a historical dictionary for arabic language
arabic is one of the oldest semitic languages in the world . it is also one of the main languages spoken in the world with thousands of people in various arab countries using it as their mother tongue . however , despite its richness , the arabic language does not yet have a historical dictionary which helps to understand the language , its richness , its origins and its evolution . the historical dictionary aims at overseeing the compilation of a historical lexicon of the arabic language and its development over the last two millennia . to create such a dictionary , we must follow many steps . as part of these steps is the extraction of the appropriate meaning of a given word occurring in a given context , also called word sense disambiguation ( wsd) . indeed , the evolution of the arabic language from antiquity to the present days has given birth to several linguistic registers ascribed to the great periods of the history of the arabic language . they can be classified as : old arabic , classical arabic and modern standard arabic . in this work , we propose a method that aims to disambiguate words in modern standard arabic . 

texture image retrieval based on gray - primitive co - occurrence matrix
the research of texture similarity is very important component of content - based image retrieval system . firstly the rotation invariance of gray - primitive co - occurrence matrix was proved in this paper , then a new texture image retrieval technique based on gray - primitive co - occurrence matrix was presented . the result of experiment indicates that the algorithm proposed has low computational complexity and certain noise resisting ability . 

wikulu : an extensible architecture for integrating natural language processing techniques with wikis
we present wikulu , a system focusing on supporting wiki users with their everyday tasks by means of an intelligent interface . wikulu is implemented as an extensible architecture which transparently integrates natural language processing ( nlp ) techniques with wikis . it is designed to be deployed with any wiki platform , and the current prototype integrates a wide range of nlp algorithms such as keyphrase extraction , link discovery , text segmentation , summarization , or text similarity . additionally , we show how wikulu can be applied for visually analyzing the results of nlp algorithms , educational purposes , and enabling semantic wikis . 

what are they reporting ? examining student cybersecurity course surveys through the lens of machine learning
this paper examines irb - approved student surveys across five cybersecurity and digital forensics courses during spring 2020 for the benefit of continual course improvement for regulatory requirements such as the accreditation board for engineering and technology ( abet ) ( re ) accreditation . there is very little research on qualitative machine - learning based methods for the analysis of student feedback in cybersecurity courses , if any courses . this research fills the literature gap by analyzing the feedback from 114 open - ended surveys across five cybersecurity and digital forensic courses to categorize areas of course improvement . to gain insights into the qualitative survey feedback , we employed term frequency - inverse document frequency ( tf - idf ) with a k - means clustering algorithm on cleaned and pre - processed survey data . this methodology provides more useful insights for curriculum developers than a standard sentiment analysis . the methodology can be further extended to be directly integrated into continual course improvement ( e . g . abet , nsa , dod , etc . ) indicators . 

event based sentence level interpretation of sentiment variation in twitter data
twitter is one of the most popular micro blogging sites used by people to express their opinions . text mining is the area where automatically data is mined for extracting features etc for different purposes . interpretation of public opinion in micro blogging site is a challenging problem since it has noise data and other unnecessary tweets . the current systems focus on removing these challenges along with the sentiment extraction and modeling . also the existing system focus on topic related extraction . we move ahead to the sentence level extraction with the help of existing methods . in this paper we propose a combination of enhanced rcb - lda method , nlp , event based analysis and text summarization . rcb - lda is used to automatically extract the sentiments within a variation period . nlp is used for finding the meaning of sentiments in the tweets . event based analysis analyzes the sentiment related to each other by using text summarization . event based analysis group the sentiment together to relate each other by summarizing tweets . finally a candidate is assigned to which related ones are combined together so that it will be the most important reason behind the sentiment variation . 

detection and correction of preposition and determiner errors in english : hoo 2012
this paper reports on our work in the hoo 2012 shared task . the task is to automatically detect , recognize and correct the errors in the use of prepositions and determiners in a set of given test documents in english . for that , we have developed a hybrid system of an n - gram statistical model along with some rule - based techniques . the system has been trained on the hoo shared task ' s training datasets and run on the test set given . we have submitted one run , which has demonstrated an f - score of 7 . 1 , 6 . 46 and 2 . 58 for detection , recognition and correction respectively before revision and f - score of 8 . 22 , 7 . 59 and 3 . 16 for detection , recognition and correction respectively after revision . 

learning personal human biases and representations for subjective tasks in natural language processing
many tasks in natural language processing like offensive , toxic , or emotional text classification are subjective by nature . humans tend to perceive textual content in their own individual way . existing methods commonly rely on the agreed output values , the same for all consumers . here , we propose personalized solutions to subjective tasks . our four new deep learning models take into account not only the content but also the specificity of a given human . the models represent different approaches to learning the representation and processing data about text readers . the experiments were carried out on four datasets : wikipedia discussion texts labelled with attack , aggression , and toxicity , as well as opinions annotated with ten numerical emotional categories . emotional data was considered as multivariate regression ( multitask) , whereas wikipedia data as independent classifications . all our models based on human biases and their representations significantly improve the prediction quality in subjective tasks evaluated from the individual ’ s perspective . 

description of the unl / usl system used for muc - 3
the muc - 3 task consists of generating a database of filled templates with respect to messages that belong to a general topical domain . in particular , for the current phase , the message collection belongs to the domain of terrorist activities . on the one hand , a decision as to the relevance of a message to a specified class of terrorist events should be made . if relevant , a predefined set of facts are to be extracted and placed as fills for appropriate slots of the template ( s ) created for this message . if not relevant , a template having a '*' as the fill in all but one slot , is created ( see appendix a for details ) . some aspects of the muc - 3 task are amenable to be solved by techniques typically employed in information retrieval ( ir ) . these techniques are especially designed to be applicable to any domain . in contrast , there are other aspects of the problem that may require a great deal of language understanding , thus needing natural language processing ( nlp ) techniques . for the most part , nlp techniques may be considered domain dependent . 

employing auto - annotated data for government document classification
in china , the government documents are documents with legal effect and of standard forms formulated in the process of government administration . with the continuous development of e - government in china , government database size increases hugely . to fully utilize the potential of the database , many applications based on natural language processing ( nlp ) are developed . classification is a fundamental task for many nlp applications such as automatic document archive , intelligent search , and personalized recommendation . presently , in china , the government document classification method which based on issuing departments has very low accuracy . traditional text classifiers based on machine learning or deep learning models rely heavily on human - labeled training data . while there are no open data sets on the government documents , we propose a method to automatically constructing large - scale annotated data set for government document classification based on the information retrieval method . experiment results show that the supervised classification model trained on our automatically constructed data set outperforms the baseline method 15 % on f1 - score . 

a study of machine translation approaches for gujarati to english translation
in the world of computer research , machine translation is a hot topic . it is fundamental in nlp . language translation is very important in india because it is a country with myriad languages . the aim of this paper is to look at several various types of machine translation techniques and see how they are used in different translation systems . we begin with some tried - and - true methods before moving on to more modern and hybrid ways that are still being perfected . we also compare these studies based on the following parameters such as the machine translation approaches used , the language pair taken into research and the papers key features . we then offer a language translation model using hybrid machine translation systems to facilitate us in translating from gujarati to english . 

de - identification in natural language processing
natural language processing ( nlp ) systems usually require a huge amount of textual data but the publication of such datasets is often hindered by privacy and data protection issues . here , we discuss the questions of de - identification related to three nlp areas , namely , clinical nlp , nlp for social media and information extraction from resumes . we also illustrate how de - identification is related to named entity recognition and we argue that de - identification tools can be successfully built on named entity recognizers . 

semantic representation and composition for spatial concepts in extended - hownet
we distinguish two different types of spatial concepts , places and locations in extended - hownet by illustrating the representation of place nouns , place adverbs , and place prepositions . for a lexical knowledge representation system , it is necessary to encode both relational senses and content senses for each word . we add new features to the hownet definitions for function words to represent concepts more precisely such that semantic composition process can be carried out while composing words into a phrase and phrases into a sentence . the new features include fine - grain semantic roles and their taxonomy . new event roles and directional functions are employed to define spatial concepts . 

a cleaning algorithm for noiseless opinion mining corpus construction
this paper presents dycorc , an extractor and cleaner of web forums contents . its main points are that the process is entirely automatic , language - independent and adaptable to all kinds of forum architectures . the corpus is built accordingly to user queries using expressions or item keywords as in research engines , and then dycorc minimizes the boilerplate for further feature - based opinion mining and sentiment analysis , gathering comments and scorings . such noiseless corpora are usually hand made with the help of crawlers and scrapers , with specific containers devised for each type of forum , entailing lots of work and skills . our aim is to cut down this preprocessing stage . our algorithm is compared to state of the art models ( apache nutch , bootcat , justext) , with a gold standard corpus we released . dycorc offers a better quality of noiseless content extraction . its algorithm is based on dom trees with string distances , seven of which have been compared on the reference corpus , and feature - distance has been chosen as the best fit . 

marine literature categorization based on minimizing the labelled data
in marine literature categorization , supervised machine learning method will take a lot of time for labelling the samples by hand . so we utilize co - training method to decrease the quantities of labelled samples needed for training the classifier . in this paper , we only select features from the text details and add attribute labels to them . it can greatly boost the efficiency of text processing . for building up two views , we split features into two parts , each of which can form an independent view . one view is made up of the feature set of abstract , and the other is made up of the feature sets of title , keywords , creator and department . in experiments , the f1 value and error rate of the categorization system could reach about 0 . 863 and 14 . 26% . they are close to the performance of supervised classifier ( 0 . 902 and 9 . 13%) , which is trained by more than 1500 labelled samples , however , the labelled samples used by co - training categorization method to train the original classifier are only one positive sample and one negative sample . in addition we consider joining the idea of the active - learning in co - training method . 

machine learning algorithms in arabic text classification : a review
as the number of electronic resources is rapidly increasing , the need for automatic classification of such resources became very crucial . in the literature , extensive text classification schemes were proposed to classify text documents into predefined categories . even though , most studies in this problem were conducted for english texts while very little studies were conducted for the arabic language . this paper provides a review for text classification process , particularly for arabic documents . as well , it presents the most important techniques used in the process of documents classification . furthermore , the current unresolved issues regarding arabic text classification will be discussed and thus highlight the future recommendations in this context . 

pattern learning for relation extraction with a hierarchical topic model
we describe the use of a hierarchical topic model for automatically identifying syntactic and lexical patterns that explicitly state ontological relations . we leverage distant supervision using relations from the knowledge base freebase , but do not require any manual heuristic nor manual seed list selections . results show that the learned patterns can be used to extract new relations with good precision . 

co - construction of ontology - based knowledge base through the web : theory and practice
ontology - based knowledge base plays an increasingly important role in improving the precision and recall rate of a retrieval system . based on distributed learning theory , a novel approach for the co - construction of ontology - based knowledge base is explored . making use of the platform set up for the co - construction and sharing of domain - specific knowledge through the web , we constructed an ontology - based knowledge base of airborne radar field . this study is expected to contribute to the effective improvement of precision and recall rate of information retrieval in the airborne radar field . hopefully , the mode we designed and adopted for the co - construction and sharing of domain - specific knowledge base could be enlightening for other similar studies . 

comprehensive information based semantic orientation identification
the identification of the texts ' attitude orientation is to identify the opinion on a special issue in a text is supporting or opposing . this paper introduces a method to identify the orientation of a chinese text ' s attitude using the comprehensive information acquired form the text . first , we label the text with information in several layers , which include information of segmentation , pos , semantic role and semantic emotional orientation . we add these information to a text step by step , and make use of them to identify the attitude orientation of the text in a given topic . experiments show that , the orientation identifier which uses above comprehensive information from syntactic , semantic and pragmatic levels has better performance in this task than some others . 

exploring language technologies to provide support to wcag 2 . 0 and e2r guidelines
part of citizenship faces accessibility barriers when they read and understand texts containing long sentences , unusual words , complex linguistic structures , etc . readability and understanding should be considered when texts are created . in order to make online texts more accessible , there are initiatives as easy - to - read ( e2r ) and web content accessibility guidelines ( wcag ) 2 . 0 , however they do not cover every need that arises in this regard . in addition to accessibility guidelines and e2r rules , technology supporting the authorship of texts is required because the transformation of a text to an easier text to read and understandable is impossible . as a solution to the need to have ( semi ) automatic support to comply with accessibility and e2r guidelines , the application of natural language processing ( nlp ) resources and methods is proposed . in this paper , e2r guidelines are introduced and a subset of wcag guidelines regarding readability and understanding has been obtained . in addition , a review on nlp technology concerning accessibility is given . to illustrate this proposal for improving accessibility using pln approaches , a use case for simplifying drug package leaflets in spanish is introduced . 

when harry met harri : cross - lingual name spelling normalization
foreign name translations typically include multiple spelling variants . these variants cause data sparseness problems , increase out - of - vocabulary ( oov ) rate , and present challenges for machine translation , information extraction and other nlp tasks . this paper aims to identify name spelling variants in the target language using the source name as an anchor . based on word - to - word translation and transliteration probabilities , as well as the string edit distance metric , target name translations with similar spellings are clustered . with this approach tens of thousands of high precision name translation spelling variants are extracted from sentence - aligned bilingual corpora . when these name spelling variants are applied to machine translation and information extraction tasks , improvements over strong baseline systems are observed in both cases . 

natural language processing ( nlp ) based text summarization - a survey
the size of data on the internet has risen in an exponential manner over the past decade . thus , the need for a solution emerges , that transforms this vast raw information into useful information which a human brain can understand . one such common technique in research that helps in dealing with enormous data is text summarization . automatic summarization is a renowned approach which is used to reduce a document to its main ideas . it operates by preserving substantial information by creating a shortened version of the text . text summarization is categorized into extractive and abstractive methods . extractive methods of summarization minimize the burden of summarization by choosing from the actual text a subset of sentences that are relevant . although there are a ton of methods , researchers specializing in natural language processing ( nlp ) are particularly drawn to extractive methods . based on linguistic and statistical characteristics , the implications of sentences are calculated . a study of extractive and abstract methods for summarizing texts has been made in this paper . this paper also analyses above mentioned methods which yields a less repetitive and a more concentrated summary . 

a comprehensive nlp system for modern standard arabic and modern hebrew
this paper presents a comprehensive nlp system by melingo that has been recently developed for arabic , based on morfix ™ - an operational formerly developed highly successful comprehensive hebrew nlp system . the system discussed includes modules for morphological analysis , context sensitive lemmatization , vocalization , text - to - phoneme conversion , and syntactic - analysis - based prosody ( intonation ) model . it is employed in applications such as full text search , information retrieval , text categorization , textual data mining , online contextual dictionaries , filtering , and text - to - speech applications in the fields of telephony and accessibility and could serve as a handy accessory for non - fluent arabic or hebrew speakers . modern hebrew and modern standard arabic share some unique semitic linguistic characteristics . yet up to now , the two languages have been handled separately in natural language processing circles , both on the academic and on the applicative levels . this paper reviews the major similarities and the minor dissimilarities between modern hebrew and modern standard arabic from the nlp standpoint , and emphasizes the benefit of developing and maintaining a unified system for both languages . 

sentiment analysis approaches for movie reviews forecasting : a survey
human psychology has always influenced by others suggestion and reviews . our reviews about something are very much influenced by other ' s reviews , and whenever we need to make a decision or solution , we often seek out other ' s reviews , so people are excited to know other ' s reviews for their profit that is why automated sentiment analysis systems are required . this survey paper is describes basics about sentiment analysis , polarity types , sentiment types , sentiment documentation types , levels of sentiment , general flow diagram of system , and also different approaches and related work for better understanding about the system . 

pitch prediction from mfcc vectors using support vector regression
mel - frequency cepstral coefficients ( mfcc ) are proved to be the effective feature for speech recognition and speaker recognition , while pitch frequency is also one of the favorite prosodic features . this work manages to bridge them with hidden markov model ( hmm ) and support vector regression ( svr ) . a set of speaker - independent hmms is used to align the training data , so that the parameters of the local svr models can be trained from the pooled data with the same label . to evaluate the accuracy of the regression functions , mfcc streams are extracted from the test data , and the pitch frequencies are predicted from them with the trained mapping functions . the comparison between the predicted pitch contour and the reference one from praat proves the strong bind of pitch frequency to mfcc vectors . and the hmm and the svr are the proper models to characterize the connection . 

automated mapping of environmental higher education ranking systems indicators to sdgs indicators using natural language processing and document similarity
to evaluate the esherss and determine their efficiency to measure environmental sustainability , we tackle this problem as a classification assignment . this study benchmark three esherss : ui greenmetric , times higher education impact ranking , and stars ( sustainability tracking , assessment rating system ) by aashe ( the association for the advancement of sustainability in higher education ) . next , we recruited a group of experts who mapped the eshers indicators to the sdgs indicators . then , we use nlp techniques to classify ( map ) the eshers indicators to the sdgs indicators . since most of the eshers indicators and the sdgs indicators are in the form of short text , we use the query expansion technique to make the nlp techniques more effective . each eshers indicator and its expanded text represents a document . and , each sdg indicator and its expanded text represents a document . we took the expanded text from the description of the eshers indicators and the description of sdg indicators , forming the corpus for our study . then , we used document similarity to find the similarity between every pair of the corpus documents . we used different similarity measures to see the similarity between the forms . then , we used a voting system to map the esherss indicators to the sdgs indicators . the proposed system was able to automatically map the underlying ranking systems indicators to the un sdgs with 99 % accuracy compared to the experts mapping . 

towards cross - lingual generalization of translation gender bias
cross - lingual generalization issues for less explored languages have been broadly tackled in recent nlp studies . in this study , we apply the philosophy on the problem of translation gender bias , which necessarily involves multilingualism and socio - cultural diversity . beyond the conventional evaluation criteria for the social bias , we aim to put together various aspects of linguistic viewpoints into the measuring process , to create a template that makes evaluation less tilted to specific types of language pairs . with a manually constructed set of content words and template , we check both the accuracy of gender inference and the fluency of translation , for german , korean , portuguese , and tagalog . inference accuracy and disparate impact , namely the biasedness factors associated with each other , show that the failure of bias mitigation threatens the delicacy of translation . furthermore , our analyses on each system and language indicate that the translation fluency and inference accuracy are not necessarily correlated . the results implicitly suggest that the amount of available language resources that boost up the performance might amplify the bias cross - linguistically . 

improvement of an abstractive summarization evaluation tool using lexical - semantic relations and weighted syntax tags in farsi language
in recent years , high increase in the amount of published web elements and the need to store , classify , restore , and process them have intensified the importance of natural language processing and its related tools such as automatic summarizers and machine translators . in this paper , a novel approach for evaluating automatic abstractive summarization system is proposed which can also be used in the other natural language processing and information retrieval applications . by comparing auto - abstracts ( abstracts created by machine ) with human abstracts ( ideal abstracts created by human ) , the metrics introduced in the proposed tool can automatically measure the quality of auto - abstracts . evidently , we can ' t semantically compare texts of abstractive summaries by comparison of just their words ' appearance . so it is necessary to use a lexical database such as wordnet . we use ferdowsnet with a proper idea for farsi language and it notably improves the evaluation results . this tool has been assessed by linguistic experts . this tool contains metric for determining the quality of summaries automatically by comparing them with summaries generated by humans ( ideal summaries ) . evidently , we can ' t semantically compare texts of abstractive summaries by comparison of just their words ' appearance and it is necessary to use a lexical database . we use this database with a proper idea together with farsi parser in order to identify groups forming sentences and the results of evaluation improve significantly . 

design and implementation of chinese words clustering based on atomic - concepts
cluster analysis is an important technique in statistics ; it is an effective way to find hidden knowledge behind huge amounts of data . especially now , in one hand , the amount of all kinds of papers is too huge to read ; in the other hand , the extensive use of search engine technology also provides a huge number of words to find all kinds of information , and thus how to find the information from these words using words cluster become a meaningful issue . the method of words cluster in this paper is based on a certain improvement to the previous methods , which is the idea of putting forward atomic concepts , by calculating the similarity between the source words and the atomic concepts and weighted to obtain the clustering space . then we use the fcm cluster algorithm of matlab to calculate and achieve relatively good results . 

augmenting kannada educational video with indian sign language captions using synthetic animation
the project aims for a machine translation system for augmenting kannada educational video by providing a translation to indian sign language ( isl ) using virtual animation approach along with the subtitles . the system accepts kannada educational video clips as input and output is delivered as 3d character animation . the first phase approach of the proposed system makes use of speechrecognition like kaldi an open source for converting the input video files to their respective text which are used to generate the subtitles and these texts are further pre - processed in order to meet the sign language norms . the right sigml file for these pre - processed words is picked from the database . these sigml files consist of xml tags which correspond to the respective hamnosys notations . hamnosys is an alphabetic system that mostly phonetically describes signs . a synthetic animation of a human in the tool in the url app , ja sigml takes input as these sigml files and generates signs of the respective pre - processed words . synchronization of inputted video with the sign language and the subtitle generation is vital for the clear and clean flow . this system is tested by professional sign language trainer / interpreter for the sign language generated by the avatar and is found effective . the accuracy of sign language shown was found out to be 63% . 

automatic football match event detection from the scoreboard using a single - shot multibox detector
during a football match , the information is manually collected by humans . however , the correctness of the football match data is difficult to check because of the game ' s speed , and thus , human errors can occur . this paper presents an automatic football match event detection from the scoreboard using a deep learning algorithm . the proposed method can reduce human error and performs the detection faster . in this study , the detection was trained with 30 , 000 data of goals , substitutions and cards scoreboard from 68 matches of english premier league 2017 - 2018 broadcast videos . the detection was tested with 80 sub - testing videos . these videos were prepared from 20 full matches broadcast videos , which consisted of 12 full matches from the year 2017 - 2018 and 8 full matches from the year 2018 - 2019 . the proposed method contains three main steps : data gathering and augmentation , object detection for scoreboard visualization forms , and the event classification . the scoreboard detection is performed with a single - shot multibox detector . the event classification employs the majority vote and time frame technique . the experimental results show an accuracy rate of 1 . 00 with the expected event scoreboards , comprised of goal , substitution , and card events . 

a comparison of relational databases and information retrieval libraries on turkish text retrieval
the present work covers a comparison of the text retrieval performances of relational databases and ir systems over a trec - like test collection for turkish . the effects of language specific preprocessing and different query lengths for different information retrieval systems are investigated . showed that language specific preprocessing improves retrieval performance for all systems and also relational databases are slower with longer queries . 

parallel multi - feature attention on neural sentiment classification
the analysis of the review ' s sentiment polarity is a fundamental task in nlp . however , most of the existing sentiment classification models only focus on extracting features but ignore features ' own differences . additionally , these models only pay attention to content information but ignore the user ' s ranking preference . to address these issues , we propose a novel parallel multi - feature attention ( pma ) neural network which concentrates on fine - grained information between user and product level content features . moreover , we use multi - feature , user ' s ranking preference included , to improve the performance of sentiment classification . experimental results on imdb and yelp datasets show that pma model achieves state - of - the - art performance . 

a web service for automatic word class acquisition
in this paper we present a web service for building nlp resources to construct semantic word classes in japanese . the system takes a few seed words belonging to the target class as input and uses automatic class expansion to suggest semantically similar training samples for the user to label . the system automatically generates random negative training samples as well , and then trains a supervised classifier on this labeled data to generate the target word class from 107 candidate words extracted from a corpus of of 108 web documents . this system eliminates the need for expert machine learning knowledge in creating semantic word classes , and we experimentally show that it significantly reduces the human effort required to build them . 

collective intelligence & sentimental analysis of twitter data by using standfordnlp libraries with software as a service ( saas ) 
social networking sites are the resources which contains huge data . for example , twitter produces millions of bytes of the data . these data can be used for business or social purpose . analyzing data from these social networking web sites is one of the new buzzword for many business strategies . election campaigns , world health issues , technical concepts , inventions , entertainment , natural resources can be effectively handle by using sentimental analysis . our proposed work evaluates sentimental analysis of twitter data using standfordnlp libraries implemented in saas ( cloud ) which will handle all current affairs in the world . cloud implementation will give process efficiency , result growth and improvement in time to market . 

the use of arabic language covid - 19 tweets analysis in iot applications
social media platforms have become one of the most powerful tools for organizations and individuals to publish news and express thoughts or feelings . with the increasingly enormous number of internet users in saudi arabia , the need raised to analyze arabic posts . since the emergence of covid - 19 in the latest 2019 , it lefts economies and businesses counting the cost while governments fight the spread of the virus with new compartmentalization measures . keeping in view the importance of quick and timely data analysis and sharing for policy actions , artificial intelligence ( ai ) has played a crucial role in facilitating the exchange of views and information between scientists and decision - makers during the coronavirus pandemic , and they continue to do so . this work mined to these content - related tweets to see how people ’ s feelings and expressions are changing . the results of this analysis can be used with integration with several iot technologies to reduce the impact of covid - 19 and drive new decisions in this field . for this goal , we proposed a machine learning ( ml ) models that can classify both of the sentiment and topic of modern standard arabic ( msa ) tweets and achieve high accuracy results . 

evaluating a statistical ccg parser on wikipedia
the vast majority of parser evaluation is conducted on the 1984 wall street journal ( wsj ) . in - domain evaluation of this kind is important for system development , but gives little indication about how the parser will perform on many practical problems . wikipedia is an interesting domain for parsing that has so far been under - explored . we present statistical parsing results that for the first time provide information about what sort of performance a user parsing wikipedia text can expect . we find that the c & c parser ' s standard model is 4 . 3 % less accurate on wikipedia text , but that a simple self - training exercise reduces the gap to 3 . 8 % . the self - training also speeds up the parser on newswire text by 20 % . 

a semi - supervised method for persian homograph disambiguation
one of the major challenges in the most natural languages processing ( nlp ) tasks such as machine translation , text to speech and text mining is word sense disambiguation ( wsd ) . supervised methods are the most common solutions for wsd . however , they need large tagged corpuses which are not available in some languages such as persian . the semi - supervised methods can solve this problem by using small tagged corpus and large untagged corpus . this paper presents a coarse - grained work in wsd that uses tri - training as the semi - supervised method and decision list as supervised classifier for training . the proposed method was evaluated on a corpus . the results show that the proposed method is more precise than the conventional decision list when the tagged corpus is small . 

practical world modeling for nlp applications
no abstract available . 

a traditional chinese medicine terminology recognition model based on deep learning : a tcm terminology recognition model
although the classics of traditional chinese medicine are extensive and profound , the application and research of the deep learning model in traditional chinese medicine terminology recognition stay scarce . in order to make use of the knowledge in these classics , a traditional chinese medicine terminology recognition model named bert - bilstm - crf is presented and achieve superior performance as shown in the results section . the design process of the bert - bilstm - crf model combines the transfer learning strategy , the pre - training language model , and the classical machine learning model . specifically , the semantic features of tcm sample sequences is first extracted by transferring a bert model that pre - trained on other large - scale chinese corpora . then go to the bilstm module to abstract the semantic features of sequence context . and last , introduce crf to learn the transfer features between context tags . in the experiment , the bert - bilstm - crf model is compared with a variety of benchmark models and outperforms others . 

pre - annotating clinical notes and clinical trial announcements for gold standard corpus development : evaluating the impact on annotation speed and potential bias
in this study our aim was to present a series of experiments to evaluate the impact of pre - annotation : ( 1 ) on the speed of manual annotation of clinical notes and clinical trial announcements ; and ( 2 ) test for potential bias if pre - annotation is utilized . the gold standard was 900 clinical trial announcements from clinicaltrials . gov website and 1655 clinical notes annotated for diagnoses , signs , symptoms , umls cui and snomed ct codes . two dictionary - based methods were used to pre - annotate the text . annotation time savings ranged from 2 . 89 % to 29 . 1 % per entity . the pre - annotation did not reduce the iaa or annotator performance but reduced the time to annotate in every experiment . dictionary - based pre - annotation is a feasible and practical method to reduce cost of annotation without introducing bias in the process . 

joint and conditional estimation of tagging and parsing models
this paper compares two different ways of estimating statistical language models . many statistical nlp tagging and parsing models are estimated by maximizing the ( joint ) likelihood of the fully - observed training data . however , since these applications only require the conditional probability distributions , these distributions can in principle be learnt by maximizing the conditional likelihood of the training data . perhaps somewhat surprisingly , models estimated by maximizing the joint were superior to models estimated by maximizing the conditional , even though some of the latter models intuitively had access to " more information " . 

optimal renewable energy project sequencing with transmission expansion
solving the project sequencing problem ( psp ) by dynamic programming ( dp ) has usually assumed a set of potential projects to be connected to a single bus and has therefore ignored any transmission constraints . this papers reformulates the psp by expanding to simultaneously optimize generation and transmission by taking into account transmission limitations and costs . the problem is generally formulated , by assuming a set of projects , available at each bus , and a set of transmission expansion options for network branches . a solution with dp is proposed , where limited set sizes are assumed to avoid the well known " curse of dimensionality " . this limitation may be acceptable , as the number of expansion options is often limited in practice . for each configuration a nonlinear programming ( nlp ) subproblem can be solved to calculate the appropriate timing for each state and stage of the dp process . finally , the nlp / dp algorithm is used in a case study where a small generation and transmission expansion problem is solved . 

swings and roundabouts : attention - structure interaction effect in deep semantic matching
in the context of deep learning models for semantic matching problems , we propose a novel multi - view progressive attention ( mv - pa ) mechanism general enough to operate on various linguistic structures of text . more importantly , we study the interaction effect between explicit linguistic structures ( e . g . , linear , constituency , and dependency ) and implicit structures elicited by attention mechanisms . empirical results on multiple datasets demonstrate salient patterns of substitutability between the two families of structures ( explicit and implicit ) . our findings not only provide intellectual foundations for the popular use of &# x201c ; linear lstm + attention &# x201d ; architectures in nlp / qa research , but also have implications in other modalities and domains . 

a survey on sentiment analysis
sentiment analysis is one of the recent technologies under nlp ( an application of al and ml ) . sentiment analysis is used in many applications for recommendation and feedback analysis . in this paper , from defining the sentiment analysis to algorithms for sentiment analysis and from the first step of sentiment analysis to evaluating the predictions of sentiment classifiers , additional feature extractions to boost performance are discussed with practical results . a brief description of complex sequence - based neural network sentiment classifiers with reasonable analytics is provided . the practical results declared in this paper are from the implantation of sentiment analysis on the imdb movie reviews dataset . evaluation metrics such as accuracy , precision , recall , and f1 - score are used . this research - based survey has been divided into different sections , each section concerning the stepwise process of sentiment analysis . 

fast : an automatic generation system for grammar tests
this paper introduces a method for the semi - automatic generation of grammar test items by applying natural language processing ( nlp ) techniques . based on manually - designed patterns , sentences gathered from the web are transformed into tests on grammaticality . the method involves representing test writing knowledge as test patterns , acquiring authentic sentences on the web , and applying generation strategies to transform sentences into items . at runtime , sentences are converted into two types of toefl - style question : multiple - choice and error detection . we also describe a prototype system fast ( free assessment of structural tests ) . evaluation on a set of generated questions indicates that the proposed method performs satisfactory quality . our methodology provides a promising approach and offers significant potential for computer assisted language learning and assessment . 

xtragen : a natural language generation system using xml - and java - technologies
in this paper we present xtragen , a xml - and java - based software system for the flexible , real - time generation of natural language that is easily integrated and used in real - world applications . we describe its grammar formalism and implementation in detail , depict the context of how the system was evaluated and finally provide an outlook on future work with the system . 

natural language processing of lyrics
we report experiments on the use of standard natural language processing ( nlp ) tools for the analysis of music lyrics . a significant amount of music audio has lyrics . lyrics encode an important part of the semantics of a song , therefore their analysis complements that of acoustic and cultural metadata and is fundamental for the development of complete music information retrieval systems . moreover , a textual analysis of a song can generate ground truth data that can be used to validate results from purely acoustic methods . preliminary results on language identification , structure extraction , categorization and similarity searches suggests that a lot of profit can be gained from the analysis of lyrics . 

shallow parsing in turkish
in this study , shallow parsing is applied on turkish sentences . these sentences are used to train and test the per - formances of various learning algorithms with various features specified for shallow parsing in turkish . 

ontological knowledge inferring approach based on term - clustering and intra - cluster permutations
ontological representation of knowledge has the advantage of being easy to reason with , but ontology construction with knowledge facts , automatically acquiring them from open domain text is often challenging . this research introduces a novel approach to infer new ontological knowledge in a fully automated manner . such ontological knowledge can be utilized in both constructing new ontologies and extending existing ontologies . basic level triples that can be extracted from open domain text are used as the data source for this study . a simple mechanism has been introduced to convert the triple into an ontological knowledge fact and such ontological knowledge facts are further processed to infer new ontological knowledge . the main focus of this research is to infer new ontological knowledge using an advanced term - clustering mechanism followed by an intra - cluster permutation generation task . generated permutations are potential to be selected as good ontological knowledge facts . inferred ontological knowledge was tested with inter - rater agreement method with high reliability and variability . results demonstrated that , out of 43 , 103 triples , this method inferred 127 , 874 ontological knowledge ( approximately 3 times ) of which 66 % were estimated to be effective . finally , this research contributes a reliable approach which requires a single pass over the corpus of triples to infer a large number of ontological knowledge facts that can be used to construct / extend ontologies . 

nlp - guided video thin - slicing for automated scoring of non - cognitive , behavioral performance tasks
we propose a novel and practical approach to model non - cognitive , behavioral performance tasks , by using natural language processing ( nlp ) to guide the selection of “ thin - slices ” that are representative of holistic video performances and demonstrate its promise . 

shallow nlp techniques for noun phrase extraction
the field of information retrieval plays an important role in searching on the internet . most of the information retrieval systems are limited to the query processing based on keywords . in information retrieval system the matching of the query against a set of text record is the core of the system . retrieval of the relevant natural language text document is of more challenge . today ' s most search engines are based on keyword based ( bag of words ) techniques , which results in some disadvantages . for text retrieval key phrases can help to narrow the search results or rank retrieved documents . we exploit shallow nlp techniques to support a range of nl queries and snippets over an existing keyword - based search . this paper describes a simple system for choosing noun phrases from a document as key phrases . the noun phrase extractor is made up of three modules : tokenization ; part - of - speech tagging ; noun phrase identification using chunking . a preliminary evaluation was conducted to test this technique with the standard ir benchmark collections such as classic test collections and then with the web snippets collection from the search engines results . the experimental results have been encouraging . 

rescribe : authoring and automatically editing audio descriptions
audio descriptions make videos accessible to those who cannot see them by describing visual content in audio . producing audio descriptions is challenging due to the synchronous nature of the audio description that must fit into gaps of other video content . an experienced audio description author will produce content that fits narration necessary to understand , enjoy , or experience the video content into the time available . this can be especially tricky for novices to do well . in this paper , we introduce a tool , rescribe , that helps authors create and refine their audio descriptions . using rescribe , authors first create a draft of all the content they would like to include in the audio description . rescribe then uses a dynamic programming approach to optimize between the length of the audio description , available automatic shortening approaches , and source track lengthening approaches . authors can iteratively visualize and refine the audio descriptions produced by rescribe , working in concert with the tool . we evaluate the effectiveness of rescribe through interviews with blind and visually impaired audio description users who give feedback on rescribe results . in addition , we invite novice users to create audio descriptions with rescribe and another tool , finding that users produce audio descriptions with fewer placement errors using rescribe . 

parsing model for answer extraction in chinese question answering system
this paper presents a novel approach to answer extraction in chinese question answering ( qa ) system . an extended lexicalized context - free grammar parser is used for question and answer understanding , the exact answer is extracted with the clue of named entity ( ne ) and question classification ( qc ) . it shows that the - performance of qa will be greatly improved with more comprehensive language understanding technologies . 

text chunking based on a generalization of winnow
this paper describes a text chunking system based on a generalization of the winnow algorithm . we propose a general statistical model for text chunking which we then convert into a classification problem . we argue that the winnow family of algorithms is particularly suitable for solving classification problems arising from nlp applications , due to their robustness to irrelevant features . however in theory , winnow may not converge for linearly non - separable data . to remedy this problem , we employ a generalization of the original winnow method . an additional advantage of the new algorithm is that it provides reliable confidence estimates for its classification predictions . this property is required in our statistical modeling approach . we show that our system achieves state of the art performance in text chunking with less computational cost then previous systems . 

a speech feature based on bark frequency warping - the non - uniform linear prediction ( nlp ) cepstrum
we propose a new method of obtaining features from speech signals for robust analysis and recognition - the non - uniform linear prediction ( nlp ) cepstrum . the objective is to derive a representation that suppresses speaker - dependent characteristics while preserving the linguistic quality of speech segments . the analysis is based on two principles . first , bark frequency warping is performed on the lp spectrum to emulate the auditory spectrum . while widely used methods such as the mel - frequency and plp analysis use the fft spectrum as its basis for warping , the nlp analysis uses the lp - based vocal - tract spectrum with glottal effects removed . second , all - pole modeling ( lp ) is used before and after the warping . the pre - warp lp is used to first obtain the vocal - tract spectrum , while the post - warp lp is performed to obtain a smoothed , two - peak model of the warped spectrum . experiments were conducted to test the effectiveness of the proposed feature in the case of identification / discrimination of vowels uttered by multiple speakers using linear discriminant analysis ( lda ) , and frame - based vowel recognition with a statistical model . in both cases , the nlp analysis was shown to be an effective tool for speaker - independent speech analysis / recognition applications . 

finding a line between trusted and untrusted information on tweets through sequence classification
the internet has long since established itself as an indispensable source of information for both organizations and individuals . the lack of social responsibility of many digital platforms , however , offers many incentives for various forms of abuse . disinformation , propaganda and fake news are just a few examples . among the actors of information campaigns , we find not only individuals but also state actors with a clear agenda . often , such information campaigns make use of psychological and rhetorical methods to achieve their goals . the manipulation of information is a major challenge for our democracies . it also presents us with major technical problems to identify and assess risks arising from the dissemination of such information . the following system description presents our approach to the detection of misinformation on social media data , which is twofold . initially , we subjected the given training data to an exploratory analysis to get an overview of the general structure . then we framed the given task as a simpler classification problem . in order to distinguish between trusted and untrusted information , using bert ( bidirectional encoder representations from transformers ) as a neural network architecture for sequence classification , we started with a pre - trained model for language representation . in a supervised training step we fine - tuned this model on the given classification task with the provided annotated data . in this paper we would like to discuss both the quality of the training data and the performance of the trained classifier to derive promising directions for future work . 

audio noise filter using cycle consistent adversarial network - cyclegan anf
speech enhance methods base on traditional digital signal processing ( dsp ) algorithms or adaptive filters can effectively suppress stationary noises . however , they don ' t provide viable solution for the variety of non - stationary noises that exist in our everyday life . smart voice assistants such as google home and alexa deteriorate their performance mostly due to non - stationary noises . in this paper we introduce cyclegan anf , a neural network approach that can learn to reduce both stationary and non - stationary noises , totally unsupervised . cyclegan anf is capable of reducing undesired interference by reading in a raw audio sample from a set x ( speech mixed with noises ) and transforming it so that it sound as if it belongs in set y ( clean speech ) . our experiments demonstrate that without labels and when trained on unparalleled ; relatively small vocabulary of speech datasets , cyclegan anf can achieve significant improvements without the ground assumptions of nature and form of the noise . 

automatically determining semantic relations in chinese sentences
after parsing an important task is to determine the semantic structure of sentences . in this paper , we attempt to automatically annotate the penn chinese treebank with semantic dependency structure . initially a small portion of the penn chinese treebank was manually annotated with headword and dependency relations . two supervised machine learning algorithms with varying features were then used to learn the relations . finally , a set of rules were created based on features of chinese to solve some problem patterns that were found in the penn chinese treebank dealing with ambiguous structures . the experimental results show that the algorithms and proposed approach are effective for determining semantic dependency structure automatically . 

multiclass text classification and analytics for improving customer support response through different classifiers
in any industry , customer relationship management ( crm ) is a very important aspect of the business . in a complex business environment , providing an efficient customer support service is always a challenge . customer reports the issues / defects in the system to the vendor by sending emails or by creating a ticket in crm tools like salesforce . com . the content of such reports includes detailed technical problems or complex workflow issues due to system failures . in the industrial automation systems , a commissioning engineer or a field operating engineer generally reports such issues . understanding and responding to the customer issues / defects and providing quick customer support is not an easy task . these crm tools are not sufficiently astute to classify the defects into predefined classes . text classification techniques are used to automatically identify and categorize the defects from the text messages . in this paper , five different machine learning classifiers ( i . e . svm , mnb , decision tree , random forest and k - nearest neighbors ) are applied to perform multiclass text classification . the text messages are classified into predefined twelve technical system defects . the comparative analysis of five different classifiers on customer support dataset shows that the support vector machine ( svm ) has a better accuracy score in identifying the defects . 

biohcdp : a hybrid constituency - dependency parser for biological nlp information extraction
one of the key goals of biological natural language processing ( nlp ) is the automatic information extraction from biomedical publications . most current constituency and dependency parsers overlook the semantic relationships between the constituents comprising a sentence and may not be well suited for capturing complex long - distance dependencies . we propose in this paper a hybrid constituency - dependency parser for biological nlp information extraction called biohcdp . biohcdp aims at enhancing the state of the art of biological text mining by applying novel linguistic computational techniques that overcome the limitations of current constituency and dependency parsers outlined above , as follows : ( 1 ) it determines the semantic relationship between each pair of constituents in a sentence using novel semantic rules , and ( 2 ) it applies semantic relationship extraction models that represent the relationships of different patterns of usage in different contexts . biohcdp can be used to extract various classes of data from biological texts , including protein function assignments , genetic networks , and protein - protein interactions . we compared biohcdp experimentally with three systems . results showed marked improvement . 

computational semantics : how to solve the suspense of supersense
understanding human language is a difficult task , with varied fields of study which aim at explaining and researching the human language principles . linguistics , psychology and computer science all use domain specific tools to describe and model language . natural language processing is the field which aims at using computational mechanisms to process naturally occurring human language . modeling syntax gives language structure . using general sense classes , or " supersenses " one can potentially enrich texts with semantic information . given a sentence with syntactic information , and a closed set of semantic supersenses , can a supersense tagged sentence be derived ? furthermore , can one demarcate boundaries for multiword expressions ? the aim of this research study is to create a multiword expression boundary and supersense labelled sentence by training with word , part - of - speech ( pos ) , multiword expression ( mwe ) and supersense tagged training data . the semantically tagged sentences can be used for many tasks such as question answering systems , information retrieval , discourse and sentiment analysis . 

an automated tool for analysis and design of mvl digital circuits
 in the development of logical circuits projects in multivalued logic , it is necessary to obtain the result of logical expressions that sometimes are very large and complex . a software called elomv , that is capable of calculating the truth - table for expressions in ternary or quaternary level with even three variables of entry is presented . the non - classical algebra of mvl based on post algebra , used in this work is shown , as well as the operators and the synthesis of the logical functions . this software is a tool that can be used in a future work in which synthesis and simplification of logical functions will be performed . 

identifying and analyzing brazilian portuguese complex predicates
semantic role labeling annotation task depends on the correct identification of predicates , before identifying arguments and assigning them role labels . however , most predicates are not constituted only by a verb : they constitute complex predicates ( cps ) not yet available in a computational lexicon . in order to create a dictionary of cps , this study employs a corpus - based methodology . searches are guided by pos tags instead of a limited list of verbs or nouns , in contrast to similar studies . results include ( but are not limited to ) light and support verb constructions . these cps are classified into idiomatic and less idiomatic . this paper presents an in - depth analysis of this phenomenon , as well as an original resource containing a set of 773 annotated expressions . both constitute an original and rich contribution for nlp tools in brazilian portuguese that perform tasks involving semantics . 

trainable , scalable summarization using robust nlp and machine learning
we describe a trainable and scalable summarization system which utilizes features derived from information retrieval , information extraction , and nlp techniques and on - line resources . the system combines these features using a trainable feature combiner learned from summary examples through a machine learning algorithm . we demonstrate system scalability by reporting results on the best combination of summarization features for different document sources . we also present preliminary results from a task - based evaluation on summarization output usability . 

categorization and identification of fragments with shi plus punctuation
studies on chinese sentences with shi ( êç ) as predicate have been profoundly fruitful from the perspective of syntax , semantics and pragmatics . in a broader sense , however , a large number of sentences with shi functioning as other syntactic roles - adverb , conjunction , auxiliary and even interjection - are practically used , and stand as barriers to natural language processing ( nlp ) and machine translation ( mt ) . the special fragments consisting of shi plus punctuation are divided into " shi plus comma " and " comma plus shi " , which are examined and discussed with the instruments of corpora , illustrations and comparison . two exceptional fragments are also briefed to improve the precision in computer identification of these shi - plus - punctuation fragments . 

sms spam detection and comparison of various machine learning algorithms
past few years have seen increase in the number of spam emails and messages . legal , economic and technical measures can be used to tackle spam sms ' s nowadays . a key role is being played by bayesian filters in stopping this problem . in this paper , we analyzed and studied the relative strengths of various machine learning algorithms in order to detect spam messages which are sent on mobile devices . we have acquired the data from on open public dataset and prepared two datasets for our testing and validation purposes . accuracy in detecting spam messages was the first priority in ranking these algorithms . our results clearly demonstrate that different machine learning algorithms under different features tend to perform differently in classifying spam messages . 

the quest for nlp applications and tools : the case of standard arabic and the dialects
the rapid advances of natural language processing ( nlp ) research bring about applications and tools that serve automating arabic at different linguistic levels . however , there is a focus on the modern standard arabic ( msa ) more than the dialects which are used heavily in day - to - day communication and require special handling . this paper introduces research efforts in arabic nlp with emphasis on tools , applications , and resources related to core nlp areas . some research and scholarly work come from researchers working on extended projects to compute arabic . the findings demonstrated that research on arabic nlp can be characterized by emphasis on morphological analysis and processing of msa especially tokenization and tagging . recently there has been a growing interest in processing and identifying arabic dialects . the proposed solutions were adapted from other solutions used for msa or designed mainly for certain dialects . also , advances in developing transformers were applied to arabic nlp in limited research . it appeared that ambiguity is still a big challenge to overcome and there is a need to innovate applications and tools for arabic msa and the dialects in parallel . 

the role of algorithm bias vs information source in learning algorithms for morphosyntactic disambiguation
morphosyntactic disambiguation ( part of speech tagging ) is a useful benchmark problem for system comparison because it is typical for a large class of natural language processing ( nlp ) problems that can be defined as disambiguation in local context . this paper adds to the literature on the systematic and objective evaluation of different methods to automatically learn this type of disambiguation problem . we systematically compare two inductive learning approaches to tagging : mx - post ( based on maximum entropy modeling ) and mbt ( based on memory - based learning ) . we investigate the effect of different sources of information on accuracy when comparing the two approaches under the same conditions . results indicate that earlier observed differences in accuracy can be attributed largely to differences in information sources used , rather than to algorithm bias . 

low delay nearend speech detector for acoustic echo cancellation
double talk detector ( dtd ) also called nearend signal detector ( nsd ) is an important control unit of the acoustic echo cancellers ( aec ) for full duplex communications system . usually , the standard nsds have delay in detection of onset of nearend signal . to reduce this detection delay , new triggering logic is proposed in this paper . this triggering logic can work with any normalized nsd algorithm which has delay in detection of onset of the nearend signal . this detection logic also improves the double talk detection for low or weak nearend signal . the proposed logic is implemented with angle based double talk detector ( abm ) [ 1 ] and tested for many real time signals captured on different hardware platforms . the test results prove the significance of proposed nsd and verify the expected performance . the additional complexity required for the proposed triggering logic is marginal . 

using customers ' online reviews to identify and classify human robot interaction failures in domestic robots
little information is available regarding which types of failures robots experience in domestic settings . to further the community ' s knowledge , we manually classified 3062 customer reviews of robotic vacuum cleaners on amazon . com . the resulting database was analyzed and used to create a natural language processing ( nlp ) model capable of predicting whether a review contains a description of a failure or not . the current work describes the initial analysis and model development process as well as preliminary findings . 

concrete assignments for teaching nlp in an m . s . program
the professionally oriented computer science m . s . students at northern illinois university are intelligent , interested in new ideas , and have good programming skills and a good math background . however , they have no linguistics background , find traditional academic prose difficult and uninteresting , and have had no exposure to research . given this population , the assignments i have found most successful in teaching introduction to nlp involve concrete projects where students could see for themselves the phenomena discussed in class . this paper describes three of my most successful assignments : duplicating kernighan et al . ' s bayesian approach to spelling correction , a study of greenberg ' s universals in the student ' s native language , and a dialogue generation project . for each assignment i discuss what the students learned and why the assignment was successful . 

resco - cc : unsupervised identification of key disinformation sentences
disinformation is often presented in long textual articles , especially when it relates to domains such as health , often seen in relation to covid - 19 . these articles are typically observed to have a number of trustworthy sentences among which core disinformation sentences are scattered . in this paper , we propose a novel unsupervised task of identifying sentences containing key disinformation within a document that is known to be untrustworthy . we design a three - phase statistical nlp solution for the task which starts with embedding sentences within a bespoke feature space designed for the task . sentences represented using those features are then clustered , following which the key sentences are identified through proximity scoring . we also curate a new dataset with sentence level disinformation scorings to aid evaluation for this task ; the dataset is being made publicly available to facilitate further research . based on a comprehensive empirical evaluation against techniques from related tasks such as claim detection and summarization , as well as against simplified variants of our proposed approach , we illustrate that our method is able to identify core disinformation effectively . 

ontology - driven coordination for supply chain system
this paper proposes an ontology - driven coordination framework for information interoperation in supply chain system ( scs ) . to achieve semantic interoperability in heterogeneous enterprise information systems , ontologies have been advised to use . mapping among multiple ontologies is necessary for an across - enterprises supply chain . in some complex supply chain , it highly demands semantic interoperability to bridge heterogeneous enterprise applications at various domain into an integrative system . in this paper , we elaborate application ontologies and their mapping logic in scs , and discuss how to use object - oriented technologies to define primitive ontology and describe ontology coordination logic . we also discuss how to design and implement the coordination framework to solving the semantic interoperability for supply chain information system . 

automatic identification of non - anaphoric anaphora in spoken dialog
identification of non - anaphoric anaphora is an important step towards a full anaphora resolution . in this paper , we present an automatic identification approach for this task . in our work , some novel features are proposed , which are based on dependency grammars , surrounding words and their pos tags . all the features are automatically extracted using a part - of - speech ( pos ) tagger and a dependency parser . our experiments are on a commonly available dialogue corpus , trains - 93 . several machine learning algorithms are used in the experiments , including cme , crf and svm . results show that compared to the approaches used in the previous work , our algorithm is simpler and achieves a higher accuracy . 

data readiness level for unstructured data
when time or computational resources is a constraint , dealing with large amount of data can be painful for many organizations . in this paper , we proposed a new concept called data readiness level ( drl) . it will measures the readiness of a file very quickly . we define readiness as ready for immediate analytical purposes . drl is pair - wised measure , one is pkss value , another is pvi value . one can see pkss as the degree of similarity to the objective . pvi exhibits how much valuable information the file has . in the real word , not every data contains valuable information . even if it is , it is not guaranteed that it will be relevant to the objective . with the aid of drl , users can simply analyze those data with higher drl values when time is a constraint . we collected 40 , 000 pdf files by hand from ieee xplore digital library and spriger . the experimental result was quite encouraging . 

a new algorithm of rule generation for chinese information extraction
learning rules automatically is a hot and difficult topic in information extraction . this paper proposes an algorithm for rule generation in chinese information extraction - rga - cie , which is domain independent for free text of chinese . rga - cie applies supervised learning with bottom - up strategy , which is a rule generalization process with a heuristic method to decide rule generalization path and laplacian * formula to evaluate the performance of rules . the learned rules have been applied to the experimental system - ches ( comprehensive information based chinese information extraction system) , and achieved good result , which proves the feasibility and effectiveness of rga - cie . 

an automated system for tamil named entity recognition using hybrid approach
named entity recognition is the process of identifying and recognizing named entities such as person , organization , location , date , time and money in the text documents . named entity recognition is a subtask of information extraction . information extraction is the process of extracting the relevant data from documents . it is one of the research areas in natural language processing . in this project implement a named entity recognizer using the hybrid approach that uses both rule based and hidden markov model in succession , which identifies only person , location and organization names respectively . input data for proposed named entity recognition system is any text document related to the any domain but limited size corpora respectively in tamil language . in this system are tagging each word by using pos tagger and then imposing certain rules such as lexical features and use some gazetteers . hmm model using e - m algorithm is taken output data from trained as input to recognition system . the main purpose of this system identifies unknown entities and solves the problem of same name entity in different positions in the same document . the system is measuring the recall and precision parameters calculate the f - measure score . goal of this project is to improve the performance of ner system to achieving high f - measure score . 

p - stemmer or nltk stemmer for arabic text classification ? 
natural language processing ( nlp ) is a branch of computer science that focuses on developing systems that allow computers to communicate with people using everyday language . nlp tools are devoted to making computers understand statements written in human language . indexing , text retrieval and word processing are considered as challenges in the classification process . hence , arabic natural language processing anlp tools are needed to achieve the aforementioned tasks . anlp includes preprocessing such as stemming , normalization , stop - word removal , part of speech pos and other processes . in this work , we collected 1 , 000 news articles from alghad . com newspaper , then we classified our dataset using svm and nb algorithms using nltk tool . we compared the results of two stemmers ; p - stemmer and nltk stemmer using the mentioned classification process . the results of the classification for the p - stemmer was better than the nltk stemmer and for the two classifiers . 

natural language processing laboratory plan : an essential for persian language
prevalent use of human language in computer systems and language oriented document processing , especially in the web , caused creating the need of designing mechanisms for developing natural language processing . in this paper , a survey in natural language processing laboratory has been done and a proposed framework for its development is suggested . achieving this goal , the specifications , structures and activities of available natural language processing laboratories have been reviewed and valuable results are obtained . afterward , a proposed framework is established . this framework introduces requirements of laboratory in a structural way that providing solutions for it any clear plan in nlp laboratories . finally , according to national requirements of iran and characteristics of persian language , a natural language processing laboratory in discussed framework is designed . the activities of this laboratory includes basic tasks like creating linguistic resources ( corpora , bilingual corpora , lexicon , etc) , test datasets , developing natural language processing system ' s evaluation framework , development of basic tools ( parser , pos - tagger , etc) , research tasks like defining areas for universities , and services such as application development , natural language processing system evaluation , and granting of the appropriate certificates . 

a cost sensitive part - of - speech tagging : differentiating serious errors from minor errors
all types of part - of - speech ( pos ) tagging errors have been equally treated by existing taggers . however , the errors are not equally important , since some errors affect the performance of subsequent natural language processing ( nlp ) tasks seriously while others do not . this paper aims to minimize these serious errors while retaining the overall performance of pos tagging . two gradient loss functions are proposed to reflect the different types of errors . they are designed to assign a larger cost to serious errors and a smaller one to minor errors . through a set of pos tagging experiments , it is shown that the classifier trained with the proposed loss functions reduces serious errors compared to state - of - the - art pos taggers . in addition , the experimental result on text chunking shows that fewer serious errors help to improve the performance of subsequent nlp tasks . 

extracting statistical data frames from text
we present a framework that bridges the gap between natural language processing ( nlp ) and text mining . central to this is a new approach to text parameterization that captures many interesting attributes of text usually ignored by standard indices , like the term - document matrix . by storing nlp tags , the new index supports a higher degree of knowledge discovery and pattern finding from text . the index is relatively compact , enabling dynamic search of arbitrary relationships and events in large document collections . we can export search results in formats and data structures that are transparent to statistical analysis tools like s - plusid® . in a number of experiments , we demonstrate how this framework can turn mountains of unstructured information into informative statistical graphs . 

first international workshop on e - commerce and nlp ( ecnlp ) chairs ' welcome
no abstract available . 

optimization based motion planning for multi - limbed vertical climbing robots
motion planning trajectories for a multi - limbed robot to climb up walls requires a unique combination of constraints on torque , contact force , and posture . this paper focuses on motion planning for one particular setup wherein a six - legged robot braces itself between two vertical walls and climbs vertically with end effectors that only use friction . instead of motion planning with a single nonlinear programming ( nlp ) solver , we decoupled the problem into two parts with distinct physical meaning : torso postures and contact forces . the first part can be formulated as either a mixed - integer convex programming ( micp ) or nlp problem , while the second part is formulated as a series of standard convex optimization problems . variants of the two wall climbing problem e . g . , obstacle avoidance , uneven surfaces , and angled walls , help verify the proposed method in simulation and experimentation . 

lavinia : an environment for natural language processing
en este art í culo presentamos lavinia , un ambiente para procesamiento de lenguaje natural ( pln) , en donde desarrolladores y usuarios pueden integrar y compartir componentes construidos en la plataforma uima . laviniaintroduce un algoritmo para visualizar los resultados independiente del proceso que los generó , permitiendo adem á s al usuario modificar en forma din á mica la forma en que se muestran , buscando destacar aquellos aspectos del an á lisis que sean de su inter é s . 

generating predicate logic expressions from natural language
formal logic expressions are commonly written in standardized mathematical notation . learning this notation typically requires many years of experience and is not an explicit part of undergraduate academic curricula . constructing and comprehending logical predicates can feel difficult and unintuitive . we hypothesized that this process can be automated using neural machine translation . most machine translation techniques involve word - based segmentation as a preprocessing step . given the nature of our custom dataset , hosts first - order - logic ( fol ) semantics primarily in unigram tokens , the word - based approach does not seem applicable . the proposed solution was to automate the translation of short english sentences into fol expressions using character - level prediction in a recurrent neural network model . we trained four encoder - decoder models ( lstm , bidirectional gru with attention , and two variants of bi - directional lstm with attention ) . our experimental results showed that several established neural translation techniques can be implemented to produce highly accurate machine translators of english sentences to fol formalisms , given only characters as markers of semantics . we also demonstrated that attention - based enhancement to the encoder - decoder architecture can vastly improve translation accuracy . most machine translation techniques involve word - based segmentation as a preprocessing step . given the nature of our custom dataset , hosts first - order - logic ( fol ) semantics primarily in unigram tokens , the word - based approach does not seem applicable . the proposed solution was to automate the translation of short english sentences into fol expressions using character - level prediction in a recurrent neural network model . we trained four encoder - decoder models ( lstm , bidirectional gru with attention , and two variants of bi - directional lstm with attention ) . our experimental results showed that several established neural translation techniques can be implemented to produce highly accurate machine translators of english sentences to fol formalisms , given only characters as markers of semantics . we also demonstrated that attention - based enhancement to the encoder - decoder architecture can vastly improve translation accuracy . we trained four encoder - decoder models ( lstm , bidirectional gru with attention , and two variants of bi - directional lstm with attention ) . our experimental results showed that several established neural translation techniques can be implemented to produce highly accurate machine translators of english sentences to fol formalisms , given only characters as markers of semantics . we also demonstrated that attention - based enhancement to the encoder - decoder architecture can vastly improve translation accuracy . 

an unsupervised method for lexical acquisition based on bootstrapping
in this paper , we present an unsupervised method called mutual screening graph algorithm based on bootstrapping ( msga - bootstrapping ) for lexical acquisition . bootstrapping is a weakly supervised algorithm that has been the focus of attention in many natural language processing ( nlp ) and information extraction ( ie ) fields , especially in learning semantic lexicons . our approach only needs unannotated corpuses to learn new words for each semantic category . msga - bootstrapping hypothesizes the semantic class of a word based on collective information over a large body of extraction pattern contexts and the extraction patterns and words can mutual reinforced . although there are some former algorithms on this task , their precision and stability can be enhanced . by counting on the impact of both the quality information and quantity information of words and patterns when scoring the words and patterns created by them , we improve the former bootstrapping algorithm . we also make msga - bootstrapping run as an unsupervised method by changing the order of its processing . experiments have shown that msga can outperform previous bootstrapping algorithm basilisk and gmr ( graph mutual reinforcement based bootstrapping ) . and the result of using msga - bootstrapping as an unsupervised method is acceptable . 

what ' s hot in software engineering twitter space ? 
twitter is a popular means to disseminate information and currently more than 300 million people are using it actively . software engineers are no exception ; singer et al . have shown that many developers use twitter to stay current with recent technological trends . at various time points , many users are posting microblogs ( i . e . , tweets ) about the same topic in twitter . we refer to this reasonably large set of topically - coherent microblogs in the twitter space made at a particular point in time as an event . in this work , we perform an exploratory study on software engineering related events in twitter . we collect a large set of twitter messages over a period of 8 months that are made by 79 , 768 twitter users and filter them by five programming language keywords . we then run a state - of - the - art twitter event detection algorithm borrowed from the natural language processing ( nlp ) domain . next , using the open coding procedure , we manually analyze 1 , 000 events that are identified by the nlp tool , and create eleven categories of events ( 10 main categories + “ others ” ) . we find that external resource sharing , technical discussion , and software product updates are the “ hottest ” categories . these findings shed light on hot topics in twitter that are interesting to many people and they provide guidance to future twitter analytics studies that develop automated solutions to help users find fresh , relevant , and interesting pieces of information from twitter stream to keep developers up - to - date with recent trends . 

detecting parser errors using web - based semantic filters
nlp systems for tasks such as question answering and information extraction typically rely on statistical parsers . but the efficacy of such parsers can be surprisingly low , particularly for sentences drawn from heterogeneous corpora such as the web . we have observed that incorrect parses often result in wildly implausible semantic interpretations of sentences , which can be detected automatically using semantic information obtained from the web . based on this observation , we introduce web - based semantic filtering - - - a novel , domain - independent method for automatically detecting and discarding incorrect parses . we measure the effectiveness of our filtering system , called woodward , on two test collections . on a set of trec questions , it reduces error by 67% . on a set of more complex penn treebank sentences , the reduction in error rate was 20% . 

transformation from publications to diabetes ontology using topic - based assertion discovery
during the last decade , we have seen an explosive growth in the number of bio - medical publications . in this paper , we present an assertion discovery framework that aims to transform from pubmed publications ( diabetes domain ) to an ontology , called diabetes publication ontology ( dpo ) . the assertions in the dpo ontology were mapped and integrated with ones in existing diabetes ontologies . the assertion discovery framework consists of three main components : ( i ) assertion discovery , ( ii ) assertion alignment , and ( iii ) assertion integration . the proposed approach for ontology generation was based on stanford corenlp for natural language processing , openie ( open information extraction ) for relation extraction , lda ( latent dirichlet allocation ) for topic modeling , and owl api for ontology generation on the spark parallel engine . we presented a web - based application for searching diabetes publications as well as retrieving the assertions from the diabetes publications through the dpo ontology . 

the architecture of speech - to - speech translator for mobile conversation
with competencies and the results of the engineering of natural language processing technology owned by bppt since 1987 , bppt develops an english - bahasa indonesia speech - to - speech translation system ( s2st ) . in this paper , we propose an architecture of speech - to - speech translation system for android - based mobile conversation using separate mobile devices for each language . this architecture applies three leading technologies , namely : websocket , rest , and json . the system utilizes a two - way communication protocol between two users and a simple voice activation detector that can detect a boundary of user ' s utterance . 

improving phrase - based smt model with flattened bilingual parse tree
phrase orders influence much on translation quality . however , general phrase based methods take only the source side information for phrase orderings . we instead propose a bilingual parse structure , flattened bilingual parse tree ( fbpt) , for better describing the inner structure of bilingual sentences and then for better translations . the main idea is to extract phrase pairs with orientation features under the help of fbpt structure . such features can help maintain better sentence generations during translation . furthermore , the fbpt structure can be learned automatically from parallel corpus with lower costs without the need of complex linguistic parsing . evaluations on mt08 translation task indicate that 7 % relative improvement on bleu can be achieved compared to distortion based method ( like pharaoh) . 

bengali fake news detection
yellow journalism has become a buzzword for everyone nowadays . increasing use of internet and social media makes people more vulnerable to fake news . to gain popularity and to have profit through clickbait news publisher and social media circulate fake news to deceive people by creating interesting content of a specific topic . the spread of falsified news has become severe in recent times throughout the world . though recently some existing system is made to classify and to detect fake news for english news article , not much work has been reported for bengali news . in this work , we consider bengali fake news classification considering south asian context . more than 200 million people speak bengali and their way of communication is bengali . in our bengali fake news classification system , data mining algorithm is used to classify fake and real news . we have also introduced web interface based on our classifier to check whether a news article written in bengali language fake or real . the classification model has 85 % accuracy with random forest classifier . 

detection and correction of real - word errors in bangla language
detection of spelling error is not so facile in bangla . to check for real - world error in a sentence , it comes with more difficulties . in this paper , we focus on correcting homophone error in real - word error . we use n - gram model which is used in many purposes like machine translation , speech recognition , to extract syntactic information etc . we have used a combination of bi - gram and tri - gram with candidate word which is going to be detected whether it is a real - word error or not . we have developed corpora which contain : ( i ) one of them is a collection of sets of homophone ( confusing ) word , ( ii ) another two are the collection of bigrams and trigrams using homophone word and ( iii ) other seven are the test sets . a candidate word extracts the set of homophone words from the corpus . in our proposed method , we create tri - gram and bigram using homophone word , then it checks the validity and takes the frequency of bi - gram or tri - gram , and finally calculates the probability for making the final decision about the candidate word . we have used around a million words to inspect our system . our proposed method achieves more than 96 % accuracy in detecting and correcting real - word errors of bangla text . 

passively mode - locked fiber lasers based on nonlinearity at 2 - μm band
2 - μ m fiber lasers are of wide potential applications in the fields of radar , sensing , and free - space communications . we introduced a simple approach to generate passively mode - locked pulse in thulium - doped fiber lasers based on nonlinear polarization rotation ( npr ) in our previous work . the high repetition rate of 1 . 78 ghz is obtained by both npr and semiconductor saturable absorption mirror . by using a polarization - maintaining fiber in the experiment , a birefringence lyot filter as a fiber comb filter can be constructed , and then the tuning range of 94 nm can be obtained for the mode - locked laser . in this paper , we also demonstrated our recent research work , a square - wave mode - locked fiber laser at 2 - μ m band based on nonlinear amplifying loop mirror . the square - wave noiselike pulse ( nlp ) at longer wavelength of 2 - μ m band can be observed . with the increase of the pump power , pulse width can be increased from 0 . 75 to 1 . 3 ns . the polarization vector characteristics of the square - wave nlp were analyzed experimentally . in addition , with the increase of the pump power , a kind of square - wave mode - locked pulse with low - intensity multipulse bunches can be observed , and these multipulse bunches can be operated at the multipulse mode - locked state . the side - mode suppression ratio decreased from 50 to 30 db . 

hate speech detection using ml algorithms
social media is a growing platform where different users share their ideas and sentiments towards different topics because users spend a lot of time expressing their thoughts and views . there are various researches going on in detecting the sentiments of the user ’ s comments but the main sentiment factor remain undiagnosed . in this paper , the aim is to detect hate speeches . the dataset was preprocessed and cleaned and cleaned text was explored to get a better understanding . salient features were extracted from the data to train our model and to identify the hate sentiments of tweets . the vector model is created using genism to learn the relationship between words and based on that sentence are labeled . stop words and port stemmer are used to filter unwanted data to build the vocabulary using countvectorizer before it is used for model building . using various machine algorithms , comparative study is done to check the performance of algorithms and promising results are attained . 

gamified rehabilitation for pain distraction in total - knee - replacement patients
total - knee - replacement ( tkr ) is becoming a prevalent procedure for the treatment of knee osteoarthritis among older adults worldwide . a key to the success of tkr is an effective post - surgical rehabilitation . however , for tkr patients , pain is a major factor that hinders them from taking up the rehabilitation exercise fully and leads to unsatisfactory functional recovery rate . to help tkr patients cope with pain and adhere more to rehabilitation exercises , we design a gamified rehabilitation tool , fun - knee , for effective pain distraction based on the " peak - end effect " theory . we have developed the alpha version of fun - knee and conducted an acceptability and usability test of its hardware and software . the results from a focus group show the general acceptance of end users on fun - knee . 

mining implicit relevance feedback from user behavior for web question answering
training and refreshing a web - scale question answering ( qa ) system for a multi - lingual commercial search engine often requires a huge amount of training examples . one principled idea is to mine implicit relevance feedback from user behavior recorded in search engine logs . all previous works on mining implicit relevance feedback target at relevance of web documents rather than passages . due to several unique characteristics of qa tasks , the existing user behavior models for web documents cannot be applied to infer passage relevance . in this paper , we make the first study to explore the correlation between user behavior and passage relevance , and propose a novel approach for mining training data for web qa . we conduct extensive experiments on four test datasets and the results show our approach significantly improves the accuracy of passage ranking without extra human labeled data . in practice , this work has proved effective to substantially reduce the human labeling cost for the qa service in a global commercial search engine , especially for languages with low resources . our techniques have been deployed in multi - language services . 

evaluating surprise adequacy for question answering
with the wide and rapid adoption of deep neural networks ( dnns ) in various domains , an urgent need to validate their behaviour has risen , resulting in various test adequacy metrics for dnns . one of the metrics , surprise adequacy ( sa ) , aims to measure how surprising a new input is based on the similarity to the data used for training . while sa has been evaluated to be effective for image classifiers based on convolutional neural networks ( cnns ) , it has not been studied for the natural language processing ( nlp ) domain . this paper applies sa to nlp , in particular to the question answering task : the aim is to investigate whether sa correlates well with the correctness of answers . an empirical evaluation using the widely used stanford question answering dataset ( squad ) shows that sa can work well as a test adequacy metric for the question answering task . 

speech - graphics dialogue systems
the central mechanism of a dialogue system must be a planner ( allen et al . , 1994 ; smith et al . , 1995 ; young et al . , 1989 ) that seeks the dialogue goal and organizes all behaviors for that purpose . our project uses a hybrid prolog - like planner ( smith and hipp , 1994 ) which first attempts to prove the top - most goal and then initiates interactions with the user when the proof cannot easily be achieved . specifically , it attempts to discover key missing axioms in the proof that prevent its completion and that may be attainable with the help of the user . the purposes of the interaction are to gather the missing information and to eventually achieve the top - most goal . 

capturing a taxonomy of failures during automatic interpretation of questions posed in natural language
an important problem in artificial intelligence is capturing , from natural language , formal representationsallthat can be used by a reasoner to compute an answer . many researchers have studied this problem by developing algorithms addressing specific phenomena in natural language interpretation , but few have studied ( or cataloged ) the types of failures associated with this problem . knowledgeallof these failures can help researchers by providing a roadallmap of open research problems and help practitioners by providing a checklist of issues to address in order to build systems that can achieve good performance on this problem . allin this paper , we present a study -- conducted in the context of the halo project -- cataloging the types of failures that occur when capturing knowledge from naturallanguage . we identified the categories of failures by examining a corpus of questions posed byallnaive usersallto a knowledge based question answering system and empirically demonstrated the generality of ourallcategorizations . we also describe available technologies that can address some of the failures we have identified . 

part - of - speech ( pos ) tagging using deep learning - based approaches on the designed khasi pos corpus
part - of - speech ( pos ) tagging is one of the research challenging fields in natural language processing ( nlp ) . it requires good knowledge of a particular language with large amounts of data or corpora for feature engineering , which can lead to achieving a good performance of the tagger . our main contribution in this research work is the designed khasi pos corpus . till date , there has been no form of any kind of khasi corpus developed or formally developed . in the present designed khasi pos corpus , each word is tagged manually using the designed tagset . methods of deep learning have been used to experiment with our designed khasi pos corpus . the pos tagger based on bilstm , combinations of bilstm with crf , and character - based embedding with bilstm are presented . the main challenges of understanding and handling natural language toward computational linguistics to encounter are anticipated . in the presently designed corpus , we have tried to solve the problems of ambiguities of words concerning their context usage , and also the orthography problems that arise in the designed pos corpus . the designed khasi corpus size is around 96 , 100 tokens and consists of 6 , 616 distinct words . initially , while running the first few sets of data of around 41 , 000 tokens in our experiment the taggers are found to yield considerably accurate results . when the khasi corpus size has been increased to 96 , 100 tokens , we see an increase in accuracy rate and the analyses are more pertinent . as results , accuracy of 96 . 81 % is achieved for the bilstm method , 96 . 98 % for bilstm with crf technique , and 95 . 86 % for character - based with lstm . concerning substantial research from the nlp perspectives for khasi , we also present some of the recently existing pos taggers and other nlp works on the khasi language for comparative purposes . 

replay attack detection in automatic speaker verification based on resnewt18 with linear frequency cepstral coefficients
this paper proposes , effective method for replay attack detection used in an automatic speaker verification system . the replay attack is of interest because it is the most straightforward and effective attack and is challenging to detect . it is a playback of the recording of the voice of a target speaker . from the literature , no speech features work well with all classifiers , and there is no investigation of using resnet - based model , called resnewt , with linear frequency cepstral coefficient ( lfcc) . therefore , a replay attack detection model based on 18 - layer resnewt that takes lfccs as the input , was constructed in this paper . the proposes method was tested on a dataset provided by asvspoof 2019 competition . in terms of the equal error rate ( eer) , the proposed method is the best in all existing methods , with an eer of 0 . 29% . the comparison in terms of replay attack detection was also made in detail . the performance of the proposed method in terms of the balanced accuracy , precision , recall , and f1 - score was considerably better than existing methods . 

applying natural language processing to analyze customer satisfaction
the aim of this paper is to analyze customer satisfaction by applying natural language processing ( nlp ) . we have collected over 50 , 000 airline reviews from tripadvisor data in the period from 2016 until 2019 . this analysis demonstrates the capability of discovering the pain points of the customers by using data science techniques related to nlp . our study shows that in today ` s world , data - driven decisions must be taken quickly in order to maintain customer satisfaction and prevent customer churn . 

detection of users suspected of using multiple user accounts and manipulating evaluations in a community site
some users in a community site abuse the anonymity and attempt to manipulate communications in a community site . these users and their submissions discourage other users , keep them from retrieving good communication records , and decrease the credibility of the communication site . to solve this problem , we conducted an experimental study to detect users suspected of using multiple user accounts and manipulating evaluations in a community site . in this study , we used messages in the data of yahoo ! chiebukuro for data training and examination . 

short text classification model based on multi - attention
short text classification plays an important role in nlp and its applications span a wide range of activities such as sentiment analysis , spam detection . recently , attention mechanism is widely used in text classification task . inspired by this , a text classification model based on multi - attention network ( man ) is proposed in this study , which perform well in extracting information related to text category . in our model , we combine the textual information based on multi - attention mechanism , which enables model to focus on global information of the sentence . we tested effectiveness of our model using several standard text classification datasets . experiment told that our model achieved state - of - the - art results on all datasets . 

story assembly in the r2aft dyslexia fluency tutor
to overcome their substantial barriers to fluent reading , students with dyslexia need to be enticed to read more , and to read texts with carefully controlled lexical content . we describe and show examples from a prototype of the new r2aft story assembly engine , which generates an interactive text that has a ) variable plot and b ) lexical content which is individualized by decoding pattern . 

privacy - preserving deep learning nlp models for cancer registries
population cancer registries can benefit from deep learning ( dl ) to automatically extract cancer characteristics from the high volume of unstructured pathology text reports they process annually . the success of dl to tackle this and other real - world problems is proportional to the availability of large labeled datasets for model training . although collaboration among cancer registries is essential to fully exploit the promise of dl , privacy and confidentiality concerns are main obstacles for data sharing across cancer registries . moreover , dl for natural language processing ( nlp ) requires sharing a vocabulary dictionary for the embedding layer which may contain patient identifiers . thus , even distributing the trained models across cancer registries causes a privacy violation issue . in this article , we propose dl nlp model distribution via privacy - preserving transfer learning approaches without sharing sensitive data . these approaches are used to distribute a multitask convolutional neural network ( mt - cnn ) nlp model among cancer registries . the model is trained to extract six key cancer characteristics – tumor site , subsite , laterality , behavior , histology , and grade – from cancer pathology reports . using 410 , 064 pathology documents from two cancer registries , we compare our proposed approach to conventional transfer learning without privacy - preserving , single - registry models , and a model trained on centrally hosted data . the results show that transfer learning approaches including data sharing and model distribution outperform significantly the single - registry model . in addition , the best performing privacy - preserving model distribution approach achieves statistically indistinguishable average micro - and macro - f1 scores across all extraction tasks ( 0 . 823 , 0 . 580 ) as compared to the centralized model ( 0 . 827 , 0 . 585 ) . 

implementing a portable clinical nlp system with a common data model -- a lisp perspective
this paper presents a lisp architecture for a portable nlp system , termed lapnlp , for processing clinical notes . lapnlp integrates multiple standard , customized and in - house developed nlp tools . our system facilitates portability across different institutions and data systems by incorporating an enriched common data model ( cdm ) to standardize necessary data elements . it utilizes umls to perform domain adaptation when integrating generic domain nlp tools . it also features stand - off annotations that are specified by positional reference to the original document . we built an interval tree based search engine to efficiently query and retrieve the stand - off annotations by specifying positional requirements . we also developed a utility to convert an inline annotation format to stand - off annotations to enable the reuse of clinical text datasets with inline annotations . we experimented with our system on several nlp facilitated tasks including computational phenotyping for lymphoma patients and semantic relation extraction for clinical notes . these experiments showcased the broader applicability and utility of lapnlp . 

chembl bot - a chat bot for chembl database
chembl is a chemical database that provides curated bioactivity data along with extensive annotations about compounds ( ex . biological relevance , medicinal uses , pharmacology ) . it is of general interest to experts from diverse areas . this makes it imperative that multiple access end - points be made available so it reaches a diverse set of users . chembl already produces various ways to access the resources including web url , rest apis , mysql / xml downloads , etc . this paper introduces a chat bot called “ chembl bot” , which seeks to complement the ways in which chembl may be accessed . the chat bot is an extensible tool that responds to queries around chembl compounds using casual parlance to address questions pertaining to the compounds ' associations with targets , diseases , mechanisms of action , phase of approval ( fda ) and patents . it is a conversational agent that uses semantics deduced from chembl data and relies on natural language processing tools via dialogflow . 

why human translators still sleep in peace ?: ( four engineering and linguistic gaps in nlp ) 
because they will keep their job quite for a few . this paper has been inspired by a recent editorial on the financial times , that gives a discouraging overview of commercial natural language processing systems ( ' the computer that can sustain a natural language conversation . . . is unlikely to exist for several decades ' ) . computational linguists are not so much concerned with applications but computer scientists have the ultimate objective to build systems that can ' increase the acceptability of computers in everyday situations . ' eventually , linguists as well would profit by a significant break - through in natural language processing . this paper is a brief dissertation on four engineering and linguistic issues we believe critical for a more striking success of nlp : extensive acquisition of the semantic lexicon , formal performance evaluation methods to evaluate systems , development of shell systems for rapid prototyping and customization , and finally a more linguistically motivated approach to word categorization . 

a summarizer system based on a semantic analysis of web documents
the availability of web and search engines has made the search easier nowadays . information overload is one of the major problems which require algorithms and tools for faster access . electronic documents are one of the major sources of information for business and academic information . in order to fully utilizing these on - line documents effectively , it is crucial to be able to extract the summary of these documents . summarization system will be one of the solutions to the above problem . this project proposes a summarizer system which will be able to perform summarization of multiple documents . the input text documents are analyzed through a parser which parses the input documents and generates parse tree for each sentence . rdf triples are extracted from each sentence by analyzing the typed dependencies generated from the parser in the form of subject , verb and object . semantic distance is computed between each pair of sentences and a matrix containing the semantic distance for sentences are computed . the measure adopted to compute semantic distance is wu and palmer distance . a clustering algorithm is applied to the extracted subject , verb and object space and the extracted rdf triples are grouped into clusters . the important sentences are selected for final summary are extracted using sentence selection algorithm . 

a comparison between syllabus of aicte and various universities - an nlp based approach
this paper focuses on comparative studies between the syllabus of aicte and various universities across india and will recommend the changes to be done so as to have holistic development . today ' s generation needs to be updated with the technical world . in spite of the growing engineer population , unemployment is on the verge . this is because they have degrees but are not technically skilled . it has become necessary that engineering graduate should possess a set of skills which encompass technical skills and practical knowledge . this paper focuses on nlp based approach and lists the topics which are lagging in those universities as compared to the aicte syllabus . it also lists the topics which are upfront and updated in those universities and together it provides the means to improve the syllabus . each course is designed with specific course outcomes ( cos ) and each co is mapped to the program outcome ( po ) . hence course outcome attainment and mapping of course outcome with program outcome play a vital role in outcome - based education ( obe ) . the attainment of course outcomes is quantified by assessing students . program outcomes need to be achieved so as to measure the attainment of a particular program . hence , the program outcomes need to be linked properly . this paper also focuses that if a particular topic is not covered , it will show the course outcome not attained for that particular topic . this process will surely help in structuring the syllabus to effectively reach the desired outcome based education goals . a dataset of topics for each module for various courses has been collected and processed through punctuation and stop words , tokenization , stemming , and term weighting . 

multi - objective longitudinal trajectory optimization for hypersonic reentry glide vehicle based on pso algorithm
for the purpose of solving the multi - objective reentry trajectory optimization problem with multiple path and terminal constraints for a hypersonic reentry - gliding vehicle , the energy - like dynamic equations are introduced by original time - varying equations so that the problem can be finally transformed to a non - linear program nlp problem with fixed terminal values . after that , a numerical optimization method based on pso algorithm is considered , where the adaptive penalty technique and normalized non - dominated sorting technique are used to handling the multi - constraints and multi - objectives , respectively . the simulation shows that the complete longitudinal multi - objective trajectories can be generated by the proposed algorithm and the optimized angles of attack can provide multiple references for practical engineering . 

cricket sentiment analysis from bangla text using recurrent neural network with long short term memory model
nowadays , people used to express their feelings , thoughts , suggestions and opinions on different social platform and video sharing media . many discussions are made on twitter , facebook and many respective forums on sports especially cricket and football . the opinion may express criticism in different manner , notation that may comprise different polarity like positive , negative or neutral and it is a challenging task even for human to understand the sentiment of each opinion as well as time consuming . this problem can be solved by analyzing sentiment in respective comments through natural language processing ( nlp ) . along with the success of many deep learning domains , recurrent neural network ( rnn ) with long - short - term - memory ( lstm ) is popularly used in nlp task like sentiment analysis . we have prepared a dataset about cricket comment in bangla text of real people sentiments in three categories i . e . positive , negative and neutral and processed it by removing unnecessary words from the dataset . then we have used word embedding method for vectorization of each word and for long term dependencies we used lstm . the accuracy of this approach has given 95 % that beyond the accuracy of previous all method . 

explorations of nlp for information management ( panel ): observations from practice in mono - and multi - lingual applications
no abstract available . 

a study in urdu corpus construction
we are interested in contributing a small , publicly available urdu corpus of written text to the natural language processing community . the urdu text is stored in the unicode character set , in its native arabic script , and marked up according to the corpus encoding standard ( ces ) xml document type definition ( dtd ) . all the tags and metadata are in english . to date , the corpus is made entirely of data from british broadcasting company ' s ( bbc ) urdu web site , although we plan to add data from other urdu newspapers . upon completion , the corpus will consist mostly of raw urdu text marked up only to the paragraph level so it can be used as input for natural language processing ( nlp ) tasks . in addition , it will be hand - tagged for parts of speech so the data can be used to train and test nlp tools . 

spatial load forecasting based on unstructured information processing and multi — attribute deep learning
a spatial load forecasting method based on unstructured information processing and multi - attribute depth learning is proposed . in order to solve the problem that the unstructured attributes have great influence on the load density but can ' t be directly put into calculation , the natural language processing ( nlp ) technique is used to structure those attributes . in view of the on characterization of the high dimension attributes by traditional method , stacked denoising auto encoder ( sdae ) deep learning network is used to forecast the spatial load density . and the rectified linear unit ( relu ) function is used as the excitation function of the network as well as the network structure gets improved to overcome the gradient disappearance and over - fitting . the results of case study show that the method of spatial load forecasting is effective and feasible . 

the building of a cbd - based domain ontology in chinese
this paper describes a method of building a medical ontology prototype in chinese . during the procedure , we explored the following questions , which are crucial for the task of ontology engineering : ( 1 ) are there some more computer - understandable knowledge description models ? we proposed a structural and fine - grained knowledge description model called concept - based description model ( cbd ) to describe the rich knowledge in the ontology . that is , to use other concept or the combination of the related concepts to represent the targeted concept , which is supposed to be more computer - understandable ; ( 2 ) during large scale ontology engineering , how to use nlp technologies to reduce domain expertspsila work to the minimal ? in our work , we used some nlp technologies to try to reduce domain expertspsila work to the minimal as possible as it can . the experiments show the significance of our method . 

exploring the potential of twitter to understand traffic events and their locations in greater mumbai , india
detecting traffic events and their locations is important for an effective transportation management system and better urban policy making . traffic events are related to traffic accidents , congestion , parking issues , to name a few . currently , traffic events are detected through static sensors e . g . , cctv camera , loop detectors . however they have limited spatial coverage and high maintenance cost , especially in developing regions . on the other hand , with web 2 . 0 and ubiquitous mobile platforms , people can act as social sensors sharing different traffic events along with their locations . we investigated whether twitter - a social media platform can be useful to understand urban traffic events from tweets in india . however , such tweets are informal and noisy and containing vernacular geographical information making the location retrieval task challenging . so far most authors have used geotagged tweets to identify traffic events which accounted for only 0 . 1 % - 3 % or sometimes less than that . recently twitter has removed precise geotagging , further decreasing the utility of such approaches . to address these issues , this research explored how ungeotagged tweets could be used to understand traffic events in india . we developed a novel framework that does not only categorize traffic related tweets but also extracts the locations of the traffic events from the tweet content in greater mumbai . the results show that an svm based model performs best detecting traffic related tweets . while extracting location information , a hybrid georeferencing model consists of a supervised learning algorithm and a number of spatial rules outperforms other models . the results suggest people in india , especially in greater mumbai often share traffic information along with location mentions , which can be used to complement existing physical transport infrastructure in a cost - effective manner to manage transport services in the urban environment . 

a lightweight chinese semantic dependency parsing model based on sentence compression
this paper is concerned with lightweight semantic dependency parsing for chinese . we propose a novel sentence compression based model for semantic dependency parsing without using any syntactic dependency information . our model divides semantic dependency parsing into two sequential sub - tasks : sentence compression and semantic dependency recognition . sentence compression method is used to get backbone information of the sentence , conveying candidate heads of arguments to the next step . the bilexical semantic relations between words in the compressed sentence and predicates are then recognized in a pairwise way . we present encouraging results on the chinese data set from conll 2009 shared task . without any syntactic information , our semantic dependency parsing model still outperforms the best reported system in the literature . 

natural spoken instructions understanding for rescue robot navigation based on cascaded conditional random fields
this paper introduces a framework of using natural chinese spoken instructions to navigate a robot at distance in the scenario of disaster rescues . a model of three layers of cascaded conditional random fields ( crfs ) is proposed to process the instructions , which outputs a series of structured commands that are called continuous movements ( cms ) of robot . 12 - type navigation part of speech ( npos ) is defined and tagged through the first layer of crfs , and the sequence of npos tagging is used as the main features ( or observed sequence ) for the other two layers of crfs . feature selection is critical to the degree of accuracy of the model ; experiments with the text corpus we collected in our laboratory , and the results are analyzed with respect to the consideration of features selected . the overall rate of correctly understanding the instructions in our experiment is 70 . 79 % which is still not acceptable in practical use where a confirmation dialog with the operator is needed . the demonstration of this frame is quite well in our lab experiments of navigating a pioneer 3 - at robot . 

confronting abusive language online : a survey from the ethical and human rights perspective
the pervasiveness of abusive content on the internet can lead to severe psychological and physical harm . significant effort in natural language processing ( nlp ) research has been devoted to addressing this problem through abusive content detection and related sub - areas , such as the detection of hate speech , toxicity , cyberbullying , etc . although current technologies achieve high classification performance in research studies , it has been observed that the real - life application of this technology can cause unintended harms , such as the silencing of under - represented groups . we review a large body of nlp research on automatic abuse detection with a new focus on ethical challenges , organized around eight established ethical principles : privacy , accountability , safety and security , transparency and explainability , fairness and non - discrimination , human control of technology , professional responsibility , and promotion of human values . in many cases , these principles relate not only to situational ethical codes , which may be context - dependent , but are in fact connected to universal human rights , such as the right to privacy , freedom from discrimination , and freedom of expression . we highlight the need to examine the broad social impacts of this technology , and to bring ethical and human rights considerations to every stage of the application life - cycle , from task formulation and dataset design , to model training and evaluation , to application deployment . guided by these principles , we identify several opportunities for rights - respecting , socio - technical solutions to detect and confront online abuse , including ‘ nudging’ , ‘ quarantining’ , value sensitive design , counter - narratives , style transfer , and ai - driven public education applications . evaluation , to application deployment . guided by these principles , we identify several opportunities for rights - respecting , socio - technical solutions to detect and confront online abuse , including ' nudging ' , ' quarantining ' , value sensitive design , counter - narratives , style transfer , and ai - driven public education applications . 

predicting the helpfulness of online physician reviews
online physician reviews have recently gained an increasing attention because they can have a significant impact on patients ' choice of physician . a large number of patients consult these reviews before choosing their physician . despite that the helpfulness of product reviews has been widely investigated in the marketing domain , little is known about the helpfulness of online physician reviews . this study aims to analyzes factors that influence the helpfulness of online physician reviews . it uses review ratings , linguistic , psychological and semantic features as input to classify these reviews into helpful or unhelpful categories . the review data have been collected from ratemds . com . the results demonstrate a significant impact of review ratings on the helpfulness of online physician reviews . the findings reveal differences between the reviews of the product and physician domains , which can have significant implications for the design of physician review websites . 

developing a hybrid np parser
we describe the use of energy function optimisation in very shallow syntactic parsing . the approach can use linguistic rules and corpus - based statistics , so the strengths of both linguistic and statistical approaches to nlp can be combined in a single framework . the rules are contextual constraints for resolving syntactic ambiguities expressed as alternative tags , and the statistical language model consists of corpus - based n - grams of syntactic tags . the success of the hybrid syntactic disambiguator is evaluated against a held - out benchmark corpus . also the contributions of the linguistic and statistical language models to the hybrid model are estimated . 

towards better technical debt detection with nlp and machine learning methods
technical debt ( td ) is an economical term used to depict non - optimal choices made in the software development process . it occurs usually when developers take shortcuts instead of following agreed upon development practices , and unchecked growth of technical debt can start to incur negative effects for software development processes . technical debt detection and management is mainly done manually , and this is both slow and costly way of detecting technical debt . automatic detection would solve this issue , but even state - of - the - art tools of today do not accurately detect the appearance of technical debt . therefore , increasing the accuracy of automatic classification is of high importance , so that we could eliminate significant portion from the costs relating to technical debt detection . this research aims to solve the problem in detection accuracy by bringing in together static code analysis and natural language processing . this combination of techniques will allow more accurate detection of technical debt , when compared to them being used separately from each other . research also aims to discover themes and topics from written developer messages that can be linked to technical debt . these can help us to understand technical debt from developers ' viewpoint . finally , we will build an open - source tool / plugin that can be used to accurately detect technical debt using both static analysis and natural language processing methods . 

detecting spam game reviews on steam with a semi - supervised approach
the potential value of online reviews has led to more and more spam reviews appearing on the web . these spam reviews are widely distributed , harmful , and difficult to identify manually . in this paper , we explore and implement generalised approaches for identifying online deceptive spam game reviews from steam . we analyse spam game reviews and present and validate a selection of techniques for detection . in addition , we aim to identify the unique features of game reviews and to create a labelled game review dataset based on different features . we were able to create a labelled dataset that can be used to identify spam game reviews in future research . our method resulted in 5 , 021 of the 33 , 450 unlabelled steam reviews being labelled as spam reviews , or approximately 15 % . this falls within the expected range of 10 - 20 % and maps to the yelp figures of 14 - 20 % of reviews being spam . 

an ensemble based nlp feature assessment in binary classification
text feature selection plays an important role in text mining . terms are the key players in document representation . the document representation can help application in following areas - indexing , summarization , classification , clustering and filtering . text instances come with a challenge of high dimensional feature space and using such features can be extremely useful in text analysis . hence it is important to extract important terms from a document . in this paper , we examine the impact of nlp features ( stop words , stemmer and combination of both ) on predictive performance of base classifiers and ensembles of naive bayesian category . we selected different category of base classifier like nb , svm , knn and j48 as these are frequently used by the researchers in text mining . imbd movie review dataset is used as a standard dataset for experimental work . we prepared ensembles of naive bayesian with base classifiers and found ensemble gives better performance over the base classifiers with entire nlp categorical dataset . ensemble of nb with svm out performed among other ensembles with different categorical dataset . 

semantic frames as an anchor representation for sentiment analysis
current work on sentiment analysis is characterized by approaches with a pragmatic focus , which use shallow techniques in the interest of robustness but often rely on ad - hoc creation of data sets and methods . we argue that progress towards deep analysis depends on a ) enriching shallow representations with linguistically motivated , rich information , and b ) focussing different branches of research and combining ressources to create synergies with related work in nlp . in the paper , we propose sentiframenet , an extension to framenet , as a novel representation for sentiment analysis that is tailored to these aims . 

combining open - source with research to re - engineer a hands - on introductory nlp course
we describe our first attempts to re - engineer the curriculum of our introductory nlp course by using two important building blocks : ( 1 ) access to an easy - to - learn programming language and framework to build hands - on programming assignments with real - world data and corpora and , ( 2 ) incorporation of interesting ideas from recent nlp research publications into assignment and examination problems . we believe that these are extremely important components of a curriculum aimed at a diverse audience consisting primarily of first - year graduate students from both linguistics and computer science . based on overwhelmingly positive student feedback , we find that our attempts were hugely successful . 

text analysis case study : determining word frequency based on azerbaijan top 500 websites . 
word frequency distribution ( wfd ) is one the most important sub - areas of natural language processing ( nlp ) and computational linguistic . the reliability and quality of wfd results are highly dependent on the size and quality of the corpora . in this paper describes the ongoing project with aim to build a corpus azerbaijani text azwebcorpus . top 500 websites in azerbaijan are used as a text source for corpus building . most of essential tools including web crawler , text cleaner , tokenizer have been developed and several opensource tools have been used . moreover , azwebcorpus compared to another corpus azbookcorpus built on text taken from electronic books in terms of word frequency . same approach that used in this paper is applicable for other languages . 

studying the effects of noisy text on text mining applications
text mining aims at deriving high quality information from text in an automated way . text mining applications rely on natural language processing ( nlp ) tools like tagger , parser etc . to locate and retrieve relevant information in an application specific manner . most of these nlp tools however have been designed to work on clean and grammatically correct text . presently , many organizations are interested to derive information from informally written text that is generated as a result of human communication through emails , or blog posts , web - based reviews etc . these texts are highly noisy and often found to contain mixture of languages . in this study we present some analysis on how noise introduced due to incorrect english affects the performance of some of the nlp tools and thereafter the text mining applications . the text mining application that we focus on is opinion mining . opinion mining is the most significant text mining application that has to deal with noisy text generated in an unregulated fashion by users . 

an empirical study on class - based word sense disambiguation
as empirically demonstrated by the last senseval exercises , assigning the appropriate meaning to words in context has resisted all attempts to be successfully addressed . one possible reason could be the use of inappropriate set of meanings . in fact , wordnet has been used as a de - facto standard repository of meanings . however , to our knowledge , the meanings represented by wordnet have been only used for wsd at a very fine - grained sense level or at a very coarse - grained class level . we suspect that selecting the appropriate level of abstraction could be on between both levels . we use a very simple method for deriving a small set of appropriate meanings using basic structural properties of wordnet . we also empirically demonstrate that this automatically derived set of meanings groups senses into an adequate level of abstraction in order to perform class - based word sense disambiguation , allowing accuracy figures over 80% . 

education using virtual reality for students with learning disabilities
visual education resources ( viser ) is a web based virtual reality ( vr ) platform which is created to target people suffering from dyslexia and to provide them with an educative , interactive and easily accessible learning platform . this paper mainly discusses the implementation of the proposed system and an insight of how natural language processing ( nlp ) has been used for automatic text visualization ( atv ) along with web virtual reality ( webvr ) . the paper further explains how 3d avatars can be incorporated in webvr along with their animations . 

middleware for creating and combining multi - dimensional nlp markup
we present the heart of gold middleware by demonstrating three xml - based integration scenarios where multi - dimensional markup produced online by multilingual natural language processing ( nlp ) components is combined to deliver rich , robust linguistic markup for use in nlp - based applications like information extraction , question answering and semantic web . the scenarios include ( 1 ) robust deep - shallow integration , ( 2 ) shallow processing cascades , and ( 3 ) treebank storage of multi - dimensionally annotated texts . 

classification techniques for sentiment discovery - a review
the sentiment discovery from data sources of social media is a challenging task . since the data is available not only in the structured format but also exists in un structured and semi structured as users are expressing freely their opinions in their desired style . opinion mining is an important research area for many domains . it has most feasible approaches for business intelligance . in this review paper , we tried to summarize all the theoretical and practical aspects for efficient implementation of opinion classification for sentiment discovery . 

a multidimensional partitioning scheme for developing english to bangla dictionary
in this paper we describe a multidimensional implementation scheme for developing english to bangla dictionary using multidimensional array . we have converted the string into an integer key and partitioned the keys based on number of letters a word . multidimensional arrays are good to store dense data . it is hard to use multidimensional array for sparse data . we have compressed the sparse multidimensional array by computing the offset value . we found good results for storage and retrieval costs . our proposed model is explained with sufficient example and performance analysis is described with experimental results . the proposed scheme shows superiority over traditional schemes . 

ecovillages , values , and interactive technology : balancing sustainability with daily life in 21st century america
this project seeks to provide a rich account of the adaptive process that occurs as individuals with explicit value commitments interact with information technology . specifically , ethnographic methods are being used to investigate the information technology adaptive process as it unfolds in the daily life of two ecovillages , communities made up of individuals striving to balance their use of technology with a lifestyle that is environmentally , socially , and economically sustainable . anticipated research outcomes include : ( 1 ) an analytic description of information technology adaptive process ; ( 2 ) a categorization of technological functionalities which support or constrain certain values , ( 3 ) an empirical extension of value sensitive design , and ( 4 ) an analysis of the negotiation around tensions which emerge as a community ' s values influence the use of information technology features and , reciprocally , as information technology features influence a community ' s values . most broadly this work contributes to our larger understanding of how the information technology adaptive process influences the human experience . 

nonlinear programming applied to thermal and fluid design optimization
thermal engineers are now commonly responsible for sizing and selecting active cooling components such as fans and heat sinks , and increasingly single and two - phase coolant loops . . meanwhile , heat transfer and fluid flow design analysis software has matured , growing both in ease of use and in phenomenological modeling prowess . unfortunately , most software retains a focus on point - design simulations and needs to do a better job of helping thermal engineers not only evaluate designs , but also investigate alternatives and even automate the search for optimal designs . this paper shows how readily available nonlinear programming ( nlp ) techniques can be successfully applied to automating design synthesis activities , allowing the thermal engineer to approach the problem from a higher level of automation . this paper briefly introduces nlp concepts , and then demonstrates their application both to a simplified fin ( extended surface ) as well as a more realistic case : a finned heat sink . 

resource - based natural language processing
research on natural language processing ( nlp ) started with so - called rule - based methodology , however , compilation of huge amount of grammar rules and dictionary entries are too difficult to develop practical systems . then , trend of nlp research shifted to corpus - based , or statistical systems . thanks to the rapid improvement of computer power and data storage , nowadays we can utilize huge amount of actual linguistic data . combining such linguistic resources and high quality language analyzer , we can extract useful linguistic information and develop practical systems for specific domain . however , the future direction of nlp is still not obvious . fusion of knowledge and example , or knowledge processing using linguistic resources , is one of the possibilities to develop high - performance nlp systems . as for the research target , machine translation with new paradigm and information retrieval as practical tasks are promising . to realize the fusion of knowledge and example , we try to make a computer system that utilizes linguistic knowledge of different degrees of abstraction as humans do , to make a model of human language function based on the system , and to acquire knowledge on how do humans store and use this kind of knowledge in their minds . 

an analysis of question quality and user performance in crowdsourced exams
automated systems that can measure the difficulty and discrimination power of multiple choice questions ( mcqs ) have value for both educators , who spend large amounts of time creating novel questions , and students , who spend a great deal of effort both practicing for and taking tests . assistance in creating high - quality assessment instruments would be welcomed by educators who do not have direct access to the proprietary data and methods used by educational testing companies . the current approach for measuring question difficulty relies on models of how good pupils will perform and contrasts that with their lower - performing peers . inverting this process and allowing educators to test their questions before students answer them will speed up question development and utility . this paper covers both a method for automatically judging the difficulty and discriminating power of mcqs and how best to build sufficient exams from these good questions . it also presents a wider discussion of this method as extensible in several different domains with data that can be viewed as behaving in a similar manner to mcqs . 

a graph - theoretic framework for semantic distance
many nlp applications entail that texts are classified based on their semantic distance ( how similar or different the texts are ) . for example , comparing the text of a new document to that of documents of known topics can help identify the topic of the new text . typically , a distributional distance is used to capture the implicit semantic distance between two pieces of text . however , such approaches do not take into account the semantic relations between words . in this article , we introduce an alternative method of measuring the semantic distance between texts that integrates distributional information and ontological knowledge within a network flow formalism . we first represent each text as a collection of frequency - weighted concepts within an ontology . we then make use of a network flow method which provides an efficient way of explicitly measuring the frequency - weighted ontological distance between the concepts across two texts . we evaluate our method in a variety of nlp tasks , and find that it performs well on two of three tasks . we develop a new measure of semantic coherence that enables us to account for the performance difference across the three data sets , shedding light on the properties of a data set that lends itself well to our method . 

verification and validation of language processing systems : is it evaluation ? 
if natural language processing ( nlp ) systems are viewed as intelligent systems then we should be able to make use of verification and validation ( v & v ) approaches and methods that have been developed in the intelligent systems community . this paper addresses language engineering infrastructure issues by considering whether standard v & v methods are fundamentally different than the evaluation practices commonly used for nlp systems , and proposes practical approaches for applying v & v in the context of language processing systems . we argue that evaluation , as it is performed in the nl community , can be improved by supplementing it with methods from the v & v community . 

from keywords to social tags : tagging for dialogues
this paper proposes an unsupervised method for generating informative tags for multi - party dialogue in an open domain . our model first extracts keywords from text through a multi - weighting framework , which includes frequency weighting , sentence weighting , speaker weighting and position weighting . then we get their bigrams through frequent pattern matching . in order to generate more flexible and socialized tags , we expand keywords and their bigrams by exploring tag associations mined from a famous bookmarking web del . icio . us . finally we rank the three parts of tag candidates under a uniform metric . unlike previous models used for tag recommendation task , our model needs neither seed tags for source texts nor a complete tag set . it can recommend new tags out of the historical tag set . we evaluate our methods on 10 , 265 chinese dialogues . experimental results show our method outperform previous models like textrank , tfidf rank and knn . 

impacts of homophone normalization on semantic models for amharic
amharic is the second - most spoken semitic language after arabic and serves as the official working language of the government of ethiopia . in amharic writing , there are different characters with the same sound , which are called homophones . the current trend in amharic nlp research is to normalize homophones into a single representation . this means , instead of character 1 1 we have used the ipa notation for amharic character transliteration , , and , the character will be used ; instead of , and , the character will be replaced ; and so on . this was done by the assumption that they are repetitive alphabets as they have the same sound . however , the impact of homophone normalization for amharic nlp applications is not well studied . when one homophone character is substituted by another , there will be a meaning change and it is against the amharic writing regulation . for example , the word is “ poverty ” while means “ salvage ” . these two words are homophones , but they have different meanings . to study the impacts of homophone normalization , we develop different general - purpose pre - trained embedding models for amharic using regular and normalized homophone characters . we fine - tune the pre - trained models and build some amharic nlp applications . for pos tagging , a model that employs a regular flair embedding model performs better , achieving an f1 - score of 77 % . for sentiment analysis , the model from regular roberta embedding outperforms the other models with an f1 - score of 60 % . for ir systems , we achieve an f1 - score of 90 % using the normalized document . the results show that normalization is highly dependent on the nlp applications . for sentiment analysis and pos tagging , normalization has negative impacts while it is essential for ir . our research indicates that normalization should be applied with caution and more effort towards standardization should be given . 

combining a self - organising map with memory - based learning
memory - based learning ( mbl ) has enjoyed considerable success in corpus - based natural language processing ( nlp ) tasks and is thus a reliable method of getting a high - level of performance when building corpus - based nlp systems . however there is a bottleneck in mbl whereby any novel testing item has to be compared against all the training items in memory base . for this reason there has been some interest in various forms of memory editing whereby some method of selecting a subset of the memory base is employed to reduce the number of comparisons . this paper investigates the use of a modified self - organising map ( som ) to select a subset of the memory items for comparison . this method involves reducing the number of comparisons to a value proportional to the square root of the number of training items . the method is tested on the identification of base noun - phrases in the wall street journal corpus , using sections 15 to 18 for training and section 20 for testing . 

autolearn ' s authoring tool : a piece of cake for teachers
this paper presents autolearn ' s authoring tool : autotutor , a software solution that enables teachers ( content creators ) to develop language learning activities including automatic feedback generation without the need of being a programmer . the software has been designed and implemented on the basis of processing pipelines developed in previous work . a group of teachers has been trained to use the technology and the accompanying methodology , and has used materials created by them in their courses in real instruction settings , which served as an initial evaluation . the paper is structured in four sections : section 1 introduces and contextualizes the research work . section 2 describes the solution , its architecture and its components , and specifically the way the nlp resources are created automatically with teacher input . section 3 describes and analyses a case study using the tool to create and test a language learning activity . finally section 4 concludes with remarks on the work done and connections to related work , and with future work . 

increasing maintainability of nlp evaluation modules through declarative implementations
computing precision and recall metrics for named entity tagging and resolution involves classifying text spans as true positives , false positives , or false negatives . there are many factors that make this classification complicated for real world systems . we describe an evaluation system that attempts to control this complexity through a set of rules and a forward chaining inference engine . 

the " whiteboard " architecture : a way to integrate heterogeneous components of nlp systems
we present a new software architecture for nlp systems made of heterogeneous components , and demonstrate an architectural prototype we have built at atr in the context of speech translation . 

question answering based university chatbot using sequence to sequence model
educational chatbots have great potential to help students , teachers and education staff . they provide useful information in educational sectors for inquirers . neural chatbots are more scalable and popular than earlier ruled - based chatbots . recurrent neural network based sequence to sequence ( seq2seq ) model can be used to create chatbots . seq2seq is adapted for good conversational model for sequences especially in question answering systems . in this paper , we explore the ways of communication through neural network chatbot by using the sequence to sequence model with attention mechanism based on rnn encoder decoder model . this chatbot is intended to be used in university education sector for frequently asked questions about the university and its related information . it is the first myanmar language university chatbot using neural network model and gets 0 . 41 bleu score . 

full - w2v : fully exploiting data reuse for w2v on gpu - accelerated systems
word2vec remains one of the highly - impactful innovations in the field of natural language processing ( nlp ) that represents latent grammatical and syntactical information in human text with dense vectors in a low dimension . word2vec has high computational cost due to the algorithm ’ s inherent sequentiality , intensive memory accesses , and the large vocabularies it represents . while prior studies have investigated technologies to explore parallelism and improve memory system performance , they struggle to effectively gain throughput on powerful gpus . we identify memory data access and latency as the primary bottleneck in prior works on gpus , which prevents highly optimized kernels from attaining the architecture ’ s peak performance . we present a novel algorithm , full - w2v , which maximally exploits the opportunities for data reuse in the w2v algorithm and leverages gpu architecture and resources to reduce access to low memory levels and improve temporal locality . full - w2v is capable of reducing accesses to gpu global memory significantly , e . g . , by more than 89% , compared to prior state - of - the - art gpu implementations , resulting in significant performance improvement that scales across successive hardware generations . our prototype implementation achieves 2 . 97x speedup when ported from nvidia pascal p100 to volta v100 cards , and outperforms the state - of - the - art by 5 . 72x on v100 cards with the same embedding quality . in - depth analysis indicates that the reduction of memory accesses through register and shared memory caching and high - throughput shared memory reduction leads to a significantly improved arithmetic intensity . full - w2v can potentially benefit many applications in nlp and other domains . 

midtd : a simple and effective distillation framework for distantly supervised relation extraction
relation extraction ( re ) , an important information extraction task , faced the great challenge brought by limited annotation data . to this end , distant supervision was proposed to automatically label re data , and thus largely increased the number of annotated instances . unfortunately , lots of noise relation annotations brought by automatic labeling become a new obstacle . some recent studies have shown that the teacher - student framework of knowledge distillation can alleviate the interference of noise relation annotations via label softening . nevertheless , we find that they still suffer from two problems : propagation of inaccurate dark knowledge and constraint of a unified distillation temperature . in this article , we propose a simple and effective multi - instance dynamic temperature distillation ( midtd ) framework , which is model - agnostic and mainly involves two modules : multi - instance target fusion ( mitf ) and dynamic temperature regulation ( dtr ) . mitf combines the teacher ’ s predictions for multiple sentences with the same entity pair to amend the inaccurate dark knowledge in each student ’ s target . dtr allocates alterable distillation temperatures to different training instances to enable the softness of most student ’ s targets to be regulated to a moderate range . in experiments , we construct three concrete midtd instantiations with bert , pcnn , and bilstm - based re models , and the distilled students significantly outperform their teachers and the state - of - the - art ( sota ) methods . 

docile agents to process natural language
we present an implementation of docile agents within a natural language processing system . docile agents are very comfortable to interact with . they allow precise adjustments of the whole software ' s behaviour and are able to predict what amount of processing time they need , according to the user ' s or supervising agent ' s wishes . they prepare future massively parallel implementations of nlp systems . < > view less

pretrained neural models for turkish text classification
in the text classification process , which is a sub - task of nlp , the preprocessing and indexing of the text has a direct determining effect on the performance for nlp models . when the studies on pre - trained models are examined , it is seen that the changes made on the models developed for world languages or training the same model with a turkish text dataset . word - embedding is considered to be the most critical point of the text processing problem . the two most popular word embedding methods today are word2vec and glove , which embed words into a corpus using multidimensional vectors . bert , electra and fastext models , which have a contextual word representation method and a deep neural network architecture , have been frequently used in the creation of pre - trained models recently . in this study , the use and performance results of pre - trained models on ttc - 3600 and trt - haber text sets prepared for turkish text classification nlp task are shown . by using pre - trained models obtained with large corpus , a certain time and hardware cost , the text classification process is performed with less effort and high performance . 

comparison of supervised machine learning models for categorizing e - commerce product titles in myanmar text
although the number of online businesses is increasing in myanmar during recent years , the number of e - commerce sites in myanmar language is still very low . the reason is that e - commerce sites are growing rapidly and methods like automatic product categorization are needed for better experience but there is no off - the - shelf system for myanmar text till now . research in this area is limited by the lack of real world datasets . in this paper , an e - commerce product corpus was developed and different supervised machine learning algorithms such as support vector machine ( svm ) , random forest ( rf ) , naive bayes ( nb ) and logistic regression ( lr ) were evaluated for classification of product titles in myanmar language . over 300 , 000 product titles were scraped from a myanmar e - commerce site and classified into 15 predefined categories . the experimental results show that support vector machine gets the best accuracy result compared to other supervised machine learning algorithms in this paper . 

improving the cognitive levels of automatic generated questions using neuro - fuzzy approach in e - assessment
assessment is a fundamental activity to realize the objective of a course and to enhance the teaching learning process . ensuring quality in the question papers for testing the different cognitive skills is important in the test or examination component of any academic or training domain . bloom ' s taxonomy , a popular framework has been used to assess the learning skills of students . this paper describes the methodology to auto - generate questions based on bloom ' s cognitive levels . various natural language processing ( nlp ) techniques are incorporated to construct a textgraph using input statements from the web where native intelligence acquired from conceptnet interrelates the nodes . the proposed work resolves the complexity of categorising autogenerated questions with similar action verbs . it follows a combinatorial formulation of fuzzy logic to generate cognitively fluent questions . a fuzzy restricted boltzman machine coupled with gaussian markov based softmax is used in the proposed architecture . experiments reveal the significance of the proposed system in generating cognitively fluent questions when the same action verb gets interlinked with different cognitive level of auto - generated questions . 

the analysis for quantitative evaluation of palpation skills in maternity nursing
in recent nursing education , it is said that there is a big gap between the practical ability of novice nurses and the level required in the clinical site , and it is required to enhance the practical experience of nursing students and to improve the educational effect . however , it is difficult to increase the facilities for the clinical site and also there are few opportunities for improving practical nursing skills . therefore it is also difficult for nursing students to improve their practical ability , especially in maternity nursing education influenced by declining birth rate problems . in this research , we clarify the differences whether the palpation according to the textbook or not on leopold maneuvers that nurse palpation the position of the fetus in the womb . the palpation according to the textbook tends to examine with less pressure and slowly touching rather than applying excessive pressure . therefore quantitative evaluation of palpation may improve the quality of education in future nursing education . 

a framework for modeling and representing temporal discourse structure
in this paper , we present a framework for modeling temporal reference in chinese discourse . it generates the temporal relations ( i . e . " before " , " after " and " same - as " ) among the sentences by applying inference rules on a set of pre - specified linguistic features , including discourse features , such as theme , rhetorical relation as well as temporal features , such as tense / aspect markers and connectives . they are designed to determine time orientation or estimate the extent of time shift . a variety of experiments are conducted to compare the different impacts of these features on temporal reference determination and show that theme is the most significant one . the overall performance is promising and achieves 91 . 33 % of accuracy on news articles . meanwhile , we obtain the insight that the default forward movement of temporal reference , which is employed in previous researches , is not applicable to all genres of texts . in addition , we develop a graphic representation , namely temporal discourse map , which helps locating temporal references in a discourse . 

fusion of acoustic , linguistic and psycholinguistic features for speaker personality traits recognition
behavioral analytics is an emerging research area that aims at automatic understanding of human behavior . for the advancement of this research area , we are interested in the problem of learning the personality traits from spoken data . in this study , we investigated the contribution of different types of speech features to the automatic recognition of speaker personality trait ( spt ) across diverse speech corpora ( broadcast news and spoken conversation ) . we have extracted acoustic , linguistic , and psycholinguistic features and modeled their combination as input to the classification task . for the classification , we used sequential minimal optimization for support vector machine ( smo ) together with relief feature selection . the present study shows different levels of performance for automatically selected feature sets , and overall improved performance with their combination across diverse corpora . 

evaluating wordnet - based measures of lexical semantic relatedness
the quantification of lexical semantic relatedness has many applications in nlp , and many different measures have been proposed . we evaluate five of these measures , all of which use wordnet as their central resource , by comparing their performance in detecting and correcting real - word spelling errors . an information - content - based measure proposed by jiang and conrath is found superior to those proposed by hirst and st - onge , leacock and chodorow , lin , and resnik . in addition , we explain why distributional similarity is not an adequate proxy for lexical semantic relatedness . 

where should i comment my code ? a dataset and model for predicting locations that need comments
programmers should write code comments , but not on every line of code . we have created a machine learning model that suggests locations where a programmer should write a code comment . we trained it on existing commented code to learn locations that are chosen by developers . once trained , the model can predict locations in new code . our models achieved precision of 74 % and recall of 13 % in identifying comment - worthy locations . this first success opens the door to future work , both in the new where - to - comment problem and in guiding comment generation . our code and data is available at http : / / groups . inf . ed . ac . uk / cup / comment - locator / . 

chinese character relationship extraction method based on bert
with the continuous development of information technology and the advent of the era of big data , today ' s society has entered the era of artificial intelligence . a variety of artificial intelligence application products continue to appear , playing various important roles in many fields , such as ai + agriculture , ai + medical , ai + autonomous driving , ai + education , etc . the combination of artificial intelligence and traditional industries the emergence of artificial intelligence has made these traditional industries shine . in the field of artificial intelligence , there is a very important sub field - - natural language processing ( nlp) . in the development of various industries , the scale of data generated by various industries is increasing , and the problem of information overload is becoming more and more serious , so how to quickly and accurately obtaining key information in the data is of great significance . however , the text data generated by all walks of life is often unstructured and diverse in types . therefore , in recent years , information extraction in the field of natural language has become a research hotspot , and person relationship extraction is an important branch of information extraction . person relationship extraction is the core task of text mining and information extraction , and its task is to identify sentences . the semantic relationship between two person name entities , in recent years , person relationship extraction has been widely promoted and applied in various aspects , such as intelligent semantic search , character knowledge graph construction , question and answer systems , etc . it performs well . therefore , it is of great significance to design and develop a set of chinese character relationship extraction system . in response to this problem , this paper proposes a bert - based chinese character relationship extraction architecture . its fusion model architecture is bert + bi - lstm + multi - head - self - attention + fc . the current popular deep learning for chinese character relationship extraction the model of has a better effect on the relationship extraction of a single character relationship pair . however , when it is extended to single - sentence multi - person relationship pairs and document - level semantic complexity , the evaluation index data of this model is not high . through comparison , it is found that bert has a significant improvement in the pre - training effect of the word2vec model , and the bi - lstm + multi - head - self - attention model has a significant improvement in the effect of a single lstm model . therefore , the proposed bert + bi - lstm + multi - head - self - attention + fc fusion model has certain practical value in chinese character relationship extraction . 

cgpops : a c ++ software for solving multiple - phase optimal control problems using adaptive gaussian quadrature collocation and sparse nonlinear programming
a general - purpose c ++ software program called cgpops is described for solving multiple - phase optimal control problems using adaptive direct orthogonal collocation methods . the software employs a legendre - gauss - radau direct orthogonal collocation method to transcribe the continuous optimal control problem into a large sparse nonlinear programming problem ( nlp) . a class of hp mesh refinement methods are implemented that determine the number of mesh intervals and the degree of the approximating polynomial within each mesh interval to achieve a specified accuracy tolerance . the software is interfaced with the open source newton nlp solver ipopt . all derivatives required by the nlp solver are computed via central finite differencing , bicomplex - step derivative approximations , hyper - dual derivative approximations , or automatic differentiation . the key components of the software are described in detail , and the utility of the software is demonstrated on five optimal control problems of varying complexity . the software described in this article provides researchers a transitional platform to solve a wide variety of complex constrained optimal control problems . 

towards better technical debt detection with nlp and machine learning methods
technical debt ( td ) is an economical term used to depict non - optimal choices made in the software development process . it occurs usually when developers take shortcuts instead of following agreed upon development practices , and unchecked growth of technical debt can start to incur negative effects for software development processes . technical debt detection and management is mainly done manually , and this is both slow and costly way of detecting technical debt . automatic detection would solve this issue , but even state - of - the - art tools of today do not accurately detect the appearance of technical debt . therefore , increasing the accuracy of automatic classification is of high importance , so that we could eliminate significant portion from the costs relating to technical debt detection . this research aims to solve the problem in detection accuracy by bringing in together static code analysis and natural language processing . this combination of techniques will allow more accurate detection of technical debt , when compared to them being used separately from each other . research also aims to discover themes and topics from written developer messages that can be linked to technical debt . these can help us to understand technical debt from developers ' viewpoint . finally , we will build an open - source tool / plugin that can be used to accurately detect technical debt using both static analysis and natural language processing methods . 

augmenting the automated extracted tree adjoining grammars by semantic representation
mica is a fast and accurate dependency parser for english that uses an automatically ltag derived from penn treebank ( ptb ) using the chen ' s approach . however , there is no semantic representation related to its grammar . on the other hand , xtag grammar is a hand crafted ltag that its elementary trees were enriched with the semantic representation by experts . the linguistic knowledge embedded in the xtag grammar caused it to being used in wide variety of natural language applications . however , the current xtag parser is not as fast and accurate as well as the mica parser . generating an xtag derivation tree from a mica dependency structure could make a bridge between these two notions and gets the benefits of both models . also , by having this conversion , the applications that use the xtag parser , may get the helps from mica parser too . in addition , it can enrich the mica ' s grammar by semantic representation of xtag grammar . in this paper , an unsupervised sequence tagger that maps any sequence of mica elementary trees onto an xtag elementary trees sequence is presented . the proposed sequence tagger is based on a hidden markov model ( hmm ) proceeded by an em - based algorithm for setting its initial parameters values . the trained model is tested on a part of ptb and about 82 % accuracy for the detected sequences is achieved . 

quantum of mind : fundamental forces of the universe in natural language semantics
empirical findings about the phenomenology of mind gained from free associative experiment , has opened a new dimension of artificial intelligence and quantum linguistics research , allowing us to create a link between a collective unconscious level and a conscious mind . our research methodology included constructing a database of metaphorical stimuli , randomly presenting these to participants , and tabulating fae responses both by the participant and by the stimulus , analyzed by frequency of semantic type . the project aimed to produce a new model of knowledge representation . a latent semantics network presented here gave us a unique opportunity to discover the probabilistic nature of mind revealing itself in all languages as entangled quantum particles . free association experiment ( fae ) based on cognitive metaphor stimuli . it is the first nlp tool of its kind , providing a fundamentally new source of empirical data examining hidden dimension of the human mind . 

hidden - variable models for discriminative reranking
we describe a new method for the representation of nlp structures within reranking approaches . we make use of a conditional log - linear model , with hidden variables representing the assignment of lexical items to word clusters or word senses . the model learns to automatically make these assignments based on a discriminative training criterion . training and decoding with the model requires summing over an exponential number of hidden - variable assignments : the required summations can be computed efficiently and exactly using dynamic programming . as a case study , we apply the model to parse reranking . the model gives an f - measure improvement of ≈ 1 . 25 % beyond the base parser , and an ≈ 0 . 25 % improvement beyond the collins ( 2000 ) reranker . although our experiments are focused on parsing , the techniques described generalize naturally to nlp structures other than parse trees . 

composing a general - purpose toolbox for swedish
the paper discusses the lessons we have learned from the work on building a reusable toolset for swedish within the framework of gate , the general architecture for text engineering , from the university of sheffield , uk . we describe our toolbox svensk and the reasons behind the choices made in the design , as well as the overall conclusions for language processing toolbox design which can be drawn . 

sentence segmentation from unformatted text using language modeling and sequence labeling approaches
current research devoted to the natural language processing problem of sentence segmentation from raw text . the focus was directed to the task of segmentation of auto - generated transcripts for videos that do not have any punctuation and segmentation . two general approaches to solve the problem of sentence segmentation were proposed and experiments concluded on a comparison of results of pre - trained transformer - based models . research on how different approach of solving problem affects results were carried out . as a result , the sequence labeling approach turned out to be the most suitable . 

annotation intent identification toward enhancement of marketing campaign performance
an annotation is a short free text associating with each financial transaction made on scb easy , one of the leading mobile banking applications in thailand . identifying the intent ( s ) hidden within this text is a challenging task in natural language processing ( nlp ) . on the one hand , the availability of labeled data is limited , and on the other hand , an annotation oftentimes carries multiple intents . this paper describes ai 2 , an nlp framework for identifying multiple intents from short annotations . the framework is based on long short - term memory architectures . we conduct two sets of empirical experiments : one helps identify the best architecture between mibc and smlc ; and the other finds the optimal model parameters . apart from assuring experimental results , ai2 predictions are used in marketing campaigns with promising performance uplifts . 

challenges of sentiment analysis - a survey
sentiment analysis ( sa ) is a field that studies the polarity of text that conveys people ' s feelings or opinions about a topic . people use social media sites to openly express themselves . this paper gives an overview of recent developments in the field . despite the fact that research in this area is still ongoing and rising by the day , it still faces a number of obstacles . researchers also consider finding a research issue to be a challenging challenge . the key contribution of this paper is to discuss some of the most pressing issues in sa and to present a holistic image of these issues based on the contributions of other scholars , which will assist upcoming scientists in this field in identifying a research problem to which they can contribute their own contribution . 

part of speech tagging with na ï ve bayes methods
in this paper we have focused on the problem of automatic prediction of parts of speech in sentences . we present an experimental framework which includes the analysis and the implementation of methods for part of speech ( pos ) labeling ( tagging ) . we have tested three methods that predict the pos without current word ' s context and also three context awareness statistic methods . the main goal of our work was to evaluate the three statistical methods forward , backward and complete method in order to analyze their applicability in the problem of automatically prediction of the pos . these methods are derived from the classic na ï ve bayes classifier . in our research we have used the wordnet database and a set of benchmarks called the brown university standard corpus of present - day american english . the results obtained by the non - context - awareness methods compared to the results obtained by statistical methods are better but not so reliable like the statistical methods . 

end - to - end task - oriented dialog modeling with semi - structured knowledge management
current task - oriented dialog ( tod ) systems mostly manage structured knowledge ( e . g . databases and tables ) to guide the goal - oriented conversations . however , they fall short of handling dialogs which also involve unstructured knowledge ( e . g . reviews and documents ) . in this paper , we formulate a task of modeling tod grounded on a fusion of structured and unstructured knowledge . to address this task , we propose a tod system with semi - structured knowledge management , seknow , which extends the belief state to manage knowledge with both structured and unstructured contents . furthermore , we introduce two implementations of seknow based on a non - pretrained sequence - to - sequence model and a pretrained language model , respectively . both implementations use the end - to - end manner to jointly optimize dialog modeling grounded on structured and unstructured knowledge . we conduct experiments on a modified version of multiwoz 2 . 1 dataset , mod - multiwoz 2 . 1 , where dialogs are processed to involve semi - structured knowledge . experimental results show that seknow has strong performances in both end - to - end dialog and intermediate knowledge management , compared to existing tod systems and their extensions with pipeline knowledge management schemes . 

research on intelligent security protection of privacy data in government cyberspace
based on the analysis of the difficulties and pain points of privacy protection in the opening and sharing of government data , this paper proposes a new method for intelligent discovery and protection of structured and unstructured privacy data . based on the improvement of the existing government data masking process , this method introduces the technologies of nlp and machine learning , studies the intelligent discovery of sensitive data , the automatic recommendation of masking algorithm and the full automatic execution following the improved masking process . in addition , the dynamic masking and static masking prototype with text and database as data source are designed and implemented with agent - based intelligent masking middleware . the results show that the recognition range and protection efficiency of government privacy data , especially government unstructured text have been significantly improved . 

unsupervised and constrained dirichlet process mixture models for verb clustering
in this work , we apply dirichlet process mixture models ( dpmms ) to a learning task in natural language processing ( nlp ) : lexical - semantic verb clustering . we thoroughly evaluate a method of guiding dpmms towards a particular clustering solution using pairwise constraints . the quantitative and qualitative evaluation performed highlights the benefits of both standard and constrained dpmms compared to previously used approaches . in addition , it sheds light on the use of evaluation measures and their practical application . 

sentiment analysis of a document using deep learning approach and decision trees
the given paper describes modern approach to the task of sentiment analysis of movie reviews by using deep learning recurrent neural networks and decision trees . these methods are based on statistical models , which are in a nutshell of machine learning algorithms . the fertile area of research is the application of google ' s algorithm word2vec presented by tomas mikolov , kai chen , greg corrado and jeffrey dean in 2013 . the main idea of word2vec is the representations of words with the help of vectors in such manner that semantic relationships between words preserved as basic linear algebra operations . the extra advantage of the mentioned algorithm above the alternatives is computational efficiency . this paper focuses on using word2vec model for text classification by their sentiment type . 

exploring effective dialogue act sequences in one - on - one computer science tutoring dialogues
we present an empirical study of one - on - one human tutoring dialogues in the domain of computer science data structures . we are interested in discovering effective tutoring strategies , that we frame as discovering which dialogue act ( da ) sequences correlate with learning . we employ multiple linear regression , to discover the strongest models that explain why students learn during one - on - one tutoring . importantly , we define " flexible " da sequence , in which extraneous das can easily be discounted . our experiments reveal several cognitively plausible da sequences which significantly correlate with learning outcomes . 

intelligent platform for the analysis of drug leaflets using nlp techniques
leaflets are one of the most straightforward methods for obtaining details regarding how to administer drugs , their composition , warnings , or precautions in their usage . however , they are many times cluttered with information , making them hard to understand by people . at the same time , the provided indications are frequently either too specific or too broad , thus making their understanding even more difficult . this paper presents the architecture and the prototype of an intelligent platform for drug administration with the following key functionalities : adding specific medications to user ' s profile , searching for potential contraindications or side effects , and defining alerts of drug administration in the provided calendar , based on the physician ' s prescription . the platform aggregates leaflets from two sources ( biofarm and helpnet ) using natural language processing ( nlp ) techniques tailored for romanian language and provides full - text search via elasticsearch . a friendly mobile application , available for ios and android devices , as well as in web browsers , was developed to improve users ' healthcare and ensure self - education . 

making sense of online discussions : can automated reports help ? 
enabling healthier online deliberation around issues of public concerns is an increasingly vital challenge in nowadays society . two fundamental components of a healthier deliberation are : i . the capability of people to make sense of what they read , so that their contribution can be relevant ; and ii . the improvement of the overall quality of the debate , so that noise can be reduced and useful signals can inform collective decision making . platform designers often resort to computational aids to improve these two processes . in this paper , we examine automated reporting as promising mean of improving sensemaking in discussion platforms . we compared three approaches to automated reporting : an abstractive summariser , a template report and an argumentation highlighting system . we then evaluated improvements in sensemaking of participants and the perception on overall quality of the debate . the study suggests that argument mining technologies are particularly promising computational aids to improve sense making and perceived quality of online discussion , thanks to their capability to combine computational models for automated reasoning with users ’ cognitive needs and expectation of automated reporting . 

dialogue tagsets in oncology
dialogue analysis is widely used in oncology for training health professionals in communication skills . parameters and tagsets have been developed independently of work in natural language processing . in relation to emergent standards in nlp , syntactic tagging is minimal , semantics is domain - specific , pragmatics is comparable , and the analysis of cognitive affect is richly developed . we suggest productive directions for convergence . 

extracting clinical information from chest x - ray reports : a case study for russian language
in this paper , we analyze possible approaches for diagnosis identification in russian medical reports . firstly , we introduce the main problems of raw russian medical reports preprocessing . secondly , focusing on the embedding extraction method , we analyzed several publicly available models and discovered that the use of bert model is a promising instrument for this task . performing the first attempt to build the nlp system for the russian medical report classification based on the embeddings extraction method , we formulated the main weaknesses that limit the use of the existing publicly available russian nlp models in the medical - text domain . having no labeled data available , we evaluate each model visually , analyzing embeddings representation in 2d field retrieved by dimensionality reduction using t - sne . we assume that a good model will be able to place reports that describe the same diagnosis close to each other , while moving reports with distinct diagnoses far from each other , forming clusters . finally , we proposed several ways of possible future research that , as we believe , will improve the results achieved in this field so far . 

judging grammaticality with count - induced tree substitution grammars
prior work has shown the utility of syntactic tree fragments as features in judging the grammaticality of text . to date such fragments have been extracted from derivations of bayesian - induced tree substitution grammars ( tsgs) . evaluating on discriminative coarse and fine grammaticality classification tasks , we show that a simple , deterministic , count - based approach to fragment identification performs on par with the more complicated grammars of post ( 2011) . this represents a significant reduction in complexity for those interested in the use of such fragments in the development of systems for the educational domain . 

an emotion information processing model based on a mental state transition network
a machine that lacks of emotion computing ability cannot realize artificial intelligent sufficiently and cannot meet the increasing demanding of human - computer interaction as well . though most current research is focusing on physical components of emotions , rarely are they carried out from the view of psychology . in this paper an emotion information processing model based on the mental state transition network and a corpus of common sense are proposed to detect human emotion . by a series of psychological experiments , we present a new way to predict future human ' s emotions depending on the various current emotional states under various conditions . besides , people in different sexes , characters and ages are taken into consideration in our experiments . from the psychological experiments data that is abstracted from 250 questionnaires , a bayesian network for describing the transitions in distribution among the emotions and relationships between internal mental situations and external reinforcements are concluded . further more , comparing seven relative evaluating experiments we found that the model provided a higher precision average rate of 0 . 843 respectively for the 50 random data examples . 

a parallel adaptive tabu search approach for traveling salesman problems
tsp ( traveling salesman problem ) is one of the typical combinatorial optimization problems , which is np - hard . it is widely believed that there is no efficient polynomial time algorithm that can solve it accurately . on the other hand , this problem is very important since it has many applications in practice . it has been verified that ts ( tabu search ) is one of the meta - heuristic algorithms that can solve this problem satisfied . with the requirement of solving large - scale problems , we presented a new parallel tabu search ( pts ) approach , which was cooperated with genetic crossover operation , for tsps . in addition , a novel adaptive search strategy of intensification and diversification in ts was proposed to improve the solving quality and efficiency . through computational experiment , it is showed that our proposed pts is feasible and effective . 

intelligence is asking the right question : a study on japanese question generation
traditional automatic question generation often requires hand - crafted templates or sophisticated nlp pipelines . such approaches , however , require extensive labor and expertise to morphologically analyze the sentences and create the nlp framework . our works aim to simplify these labors . we conduct a contrastive experiment between two types of sequence learning : statistical - based machine translation and attention - based sequence neural network . these models can be trained end - to - end , and it can capture the pattern between the input sequence and output sequence , thus diminishing the need to prepare a sophisticated nlp pipeline . automatic evaluation results show that our system outperforms the state - of - the - art rule - based system , and also excels in terms of content quality and fluency according to a subjective human test . 

android based educational chatbot for visually impaired people
the purpose of this android application is to provide educational based chatbot for visually impaired people . it will give an answer to the educational based queries asked by the visually impaired people . they can easily launch the application with the help of google voice search . once the application is open , it will give a voice instruction to use an application . output will be provided in voice form as well as in text form . so normal people can also use this application . 

text classification on software requirements specifications using transformer models
text classification in software requirements specifications ( srs ) documents is an essential task for various purposes including automatically extracting requirements and their types as well as identification of duplicate or conflicting information , which all contribute to avoiding potential issues in the later stages of the software development life cycle . while a variety of machine learning approaches have been considered for text classification over srs documents , many of these fail to provide adequate performance as they often ignore the meaning of software artifacts or integrate domain knowledge for the classification task . recent advances in deep learning methodology have significantly contributed to natural language processing ( nlp ) and text classification . one of the main challenges in using deep learning models for various nlp tasks in the software engineering domain is the scarcity of labeled textual data . in addition , even with sufficient data , training from the scratch still requires significant training time and computational resources . transfer learning is a novel approach that proposes a solution to such reservations by providing pre - trained models that enable fine - tuning with the customized data . in this research , we conduct an empirical analysis on multi - class text classification over srs documents using different pre - trained transformer models including bert , distilbert , roberta , albert , and xlnet , and compare their performance . we test the performance of these models using three srs datasets : doors , nfr - promise , and pure . our numerical study shows that the transformer models are able to generate highly accurate results to classify all categories except priority of the requirements . while all models provide a 80 % or higher accuracy for other classification tasks , the accuracy of the models to classify the priority does not exceed 60 % . 

survey of various ai chatbots based on technology used
artificial intelligence is the ability to make machines intelligent . the development in ai made machines capable to initiate the simulation of natural intelligence and are able to take decisions where humans do better . natural language processing , machine learning are some of the major branches of ai . a chatbot is an ai software uses natural language processing techniques to simulate a chat between a system and a user . natural language processing help chatbots to understand natural language more clearly and generate an intelligent response . since the introduction of first chatbot eliza in 1966 , different versions with variety of technology and approach have been developed . in this paper , we describe the generic work flow of chatbots and represents their comparison on the basis of technology / approach used with some significant parameters . to make natural language conversation more efficient , a chatbot must analyze and understand the user input correctly for a relevant response . they are currently being used in various important domains like science , education , health care etc . chatbots have a potential to improve human interaction with machines . 

two level document image classification
classifying documents is an important process for organizations that are responsible for keeping a large number of documents in a digital archive . in this paper , a two - level method was used to classify approximately 253 class of documents . in the first stage , the documents were visually classified . in the second stage , unclassifiable documents were tried to be classified using natural language processing ( nlp) . training set was created by classifying approximately 28 . 000 documents by hand . studies were carried out on single - page documents . performances were measured by making experiments on the full , 1 / 2 and 1 / 3 of document using alexnet for image classification , bag - of - words and lstm algorithm for nlp . the performances of the study were measured according to different options . the success rate was 75 % and 85 % at the end of first and second stages respectivelyview less

sg - net : syntax guided transformer for language representation
understanding human language is one of the key themes of artificial intelligence . for language representation , the capacity of effectively modeling the linguistic knowledge from the detail - riddled and lengthy texts and getting ride of the noises is essential to improve its performance . traditional attentive models attend to all words without explicit constraint , which results in inaccurate concentration on some dispensable words . in this work , we propose using syntax to guide the text modeling by incorporating explicit syntactic constraints into attention mechanisms for better linguistically motivated word representations . in detail , for self - attention network ( san ) sponsored transformer - based encoder , we introduce syntactic dependency of interest ( sdoi ) design into the san to form an sdoi - san with syntax - guided self - attention . syntax - guided network ( sg - net ) is then composed of this extra sdoi - san and the san from the original transformer encoder through a dual contextual architecture for better linguistics inspired representation . the proposed sg - net is applied to typical transformer encoders . extensive experiments on popular benchmark tasks , including machine reading comprehension , natural language inference , and neural machine translation show the effectiveness of the proposed sg - net design . 

web application for screening resume
this paper focuses majorly on the design of the web application which will be used to screen resumes ( curriculum vitae ) for a particular job posting . in the proposed system , a web application will encourage the job applicant candidates as well as the recruiters to use it for job applications and screening of resumes . recruitment is a tedious process wherein the first task for any recruiter is to screen the resumes . the proposed web application is designed in such a way that job applicant as well as recruiters can use it with ease for applying for job openings and screening respectively . the recruiters from various companies can post the details of the job openings available in their respective companies . the interactive web application will allow the job applicants to submit their resume and apply for their job postings they may still be interested in . the resumes submitted by the candidates are then compared with the job profile requirement posted by the company recruiter by using techniques like machine learning and natural language processing ( nlp ) . scores can then be given to the resumes and they can be ranked from highest match to lowest match . this ranking is made visible only to the company recruiter who is interested to select the best candidates from a large pool of candidates . 

qik : a system for large - scale image retrieval on everyday scenes with common objects
in this paper , we propose a system for large - scale image retrieval on everyday scenes with common objects by leveraging advances in deep learning and natural language processing ( nlp ) . unlike recent state - of - the - art approaches that extract image features from a convolutional neural network ( cnn ) , our system exploits the predictions made by deep neural networks for image understanding tasks . our system aims to capture the relationships between objects in an everyday scene rather than just the individual objects in the scene . it works as follows : for each image in the database , it generates most probable captions and detects objects in the image using state - of - the - art deep learning models . the captions are parsed and represented by tree structures using nlp techniques . these are stored and indexed in a database system . when a user poses a query image , its caption is generated using deep learning and parsed into its corresponding tree structures . then an optimized tree - pattern query is constructed and executed on the database to retrieve a set of candidate images . finally , these candidate images are ranked using the tree - edit distance metric computed on the tree structures . a query based on only objects detected in the query image can also be formulated and executed . in this case , the ranking scheme uses the probabilities of the detected objects . we evaluated the performance of our system on the microsoft coco dataset containing everyday scenes ( with common objects ) and observed that our system can outperform state - of - the - art techniques in terms of mean average precision for large - scale image retrieval . 

100 things you always wanted to know about linguistics but were afraid to ask
many nlp tasks have at their core a subtask of extracting the dependencies - - - who did what to whom - - - from natural language sentences . this task can be understood as the inverse of the problem solved in different ways by diverse human languages , namely , how to indicate the relationship between different parts of a sentence . understanding how languages solve the problem can be extremely useful in both feature design and error analysis in the application of machine learning to nlp . likewise , understanding cross - linguistic variation can be important for the design of mt systems and other multilingual applications . the purpose of this tutorial is to present in a succinct and accessible fashion information about the structure of human languages that can be useful in creating more linguistically sophisticated , more language independent , and thus more successful nlp systems . while many kinds of linguistic structure can be relevant to different nlp tasks , the focus of this tutorial will be on morphosyntax . the tutorial will take an explicitly typological perspective as an understanding of cross - linguistic variation can facilitate the design of more portable ( language - independent ) nlp systems . in order to help participants retain the information better , the tutorial will be structured interactively . i will ask participants for examples of tasks and data sets they work with , and then as a group we will brainstorm ways in which each of the linguistic properties discussed can related to feature design and / or error analysis for those tasks . 

chinese sentence similarity research based on semantic string
the computation of chinese sentence similarity has been widely used in many areas . however , due to the complexity of chinese , currently , there have no corresponding algorithms to solve computation of sentence similarity at home and abroad . this paper presents a improved method of the computation of words similarity firstly , and then on this basis , the structure of semantic string has been given and calculating the impact factor of semantic string , lastly , the improved algorithm of computation of sentence similarity has been presented based on it . the experiment of a certain data sets shows that the improved algorithm is effective . 

analyses of hate and non - hate expressions during election using nlp
social media platforms such as twitter were able to connect people around the world closer by enabling them to actively connect and communicate as well as exchange ideas on social issues ; however , it has also become niche for misinformation and hate speech which does not only affects peoples ' perception of truth but can also lead to hate crimes as studies confirmed . recent works in text classification has proved pivotal in detecting hate speech but is continued to be limited by some problems . given this , the study aimed to further analyze the composition of hate expressions using the collection of tweets posted during the 2016 philippine presidential election . word bubbles were generated to identify word similarity and differences between hate and non - hate . principal component analysis with k - means clustering was used to further confirm clustering of hate targets against the grouping from annotation . results showed that both hate , and non - hate tweets have around 55 % different terms while the clustering of hate targets based on annotation does not complement the groupings generated in terms of nearest neighbors . lastly , lexical diversity reveals significant difference between hate classes and targets which suggest that it can be potentially used as a feature to improve hate speech classification . 

recognizing sentiment polarity in chinese reviews based on topic sentiment sentences
we present an approach to recognizing sentiment polarity in chinese reviews based on topic sentiment sentences . considering the features of chinese reviews , we firstly identify the topic of a review using an n - gram matching approach . to extract candidate topic sentiment sentences , we compute the semantic similarity between a given sentence and the ascertained topic and meanwhile determine whether the sentence is subjective . a certain number of these sentences are then selected as representatives according to their semantic similarity value with relation to the topic . the average value of the representative topic sentiment sentences is calculated and taken as the sentiment polarity of a review . experiment results show that the proposed method is feasible and can achieve relatively high precision . 

an error driven method to improve rules for the recognition of chinese modality “ le ” 
we have a “ trinity ” way for the recognition of chinese modality “ le ” , in which dictionary , usage rule base and usage corpora combine as the knowledge base . handcrafted rules can hardly cover all usages in the real texts . so this paper proposes an error driven method for the automatic rules improvement . experimental results show that , after the automatic rules improvement , the recognition precision of the modality “ le ” improves by over 1 . 85% . 

opinion analysis based on a fusion of multiple classifiers approach
with the rapid expansion of network application , more and more customer reviews are available online . in this paper , a method for opinion analysis based on the fusion of multiple classifiers was presented , reliability function was introduced to select the text that is hard to determine by the main classifier , for these texts , multiple classifiers were used to determine which category the unlabeled documents belong to by voting . experiments showed that the performance of text classification was improved by the proposed method . compared with single classifier , this method achieved better performance , only increasing a small amount of time than using single main classifier . 

a 2 - phase frame - based knowledge extraction framework
we present an approach for extracting knowledge from natural language english texts where processing is decoupled in two phases . the first phase comprises several standard nlp tasks whose results are integrated in a single rdf graph of mentions . the second phase processes the mention graph with sparql - like mapping rules to produce a knowledge graph organized around semantic frames ( i . e . , prototypical descriptions of events and situations ) . the decoupling allows : ( i ) choosing different tools for the nlp tasks without affecting the remaining computation ; ( ii ) combining the outputs of different nlp tasks in non - trivial ways , leveraging their integrated and coherent representation in a mention graph ; and ( iii ) relating each piece of extracted knowledge to the mention ( s ) it comes from , leveraging the single rdf representation . we evaluate precision and recall of our approach on a gold standard , showing its competitiveness w . r . t . the state of the art . we also evaluate execution times and ( sampled ) accuracy on a corpus of 110k wikipedia pages , showing the applicability of the approach on large corpora . 

news - comment relevance classification algorithm based on feature extraction
in recent years , with the rapid development of the mobile internet , it has become easier for users to read news and corresponding comments . most people get used to reading news on - line . however , sociologists have shown that up to 50 percent of the comments in a news article are irrelevant to the news content . in this paper , we investigate the news - comments relevance analysis problem . we formalize such problem as a classification problem and proposed a news - comment relevance classification algorithm , based on bert feature extraction . this algorithm uses a model trained in the extensive semantic database to extract the feature vectors of news and its comments and inputs the extracted feature vectors into a full - connection layer network to complete the classification task . several experiments are carried out on five public datasets , and the experimental results show that the model i proposed has good performance . 

civic crowdanalytics : making sense of crowdsourced civic input with big data tools
this paper examines the impact of crowdsourcing on a policymaking process by using a novel data analytics tool called civic crowdanalytics , applying natural language processing ( nlp ) methods such as concept extraction , word association and sentiment analysis . by drawing on data from a crowdsourced urban planning process in the city of palo alto in california , we examine the influence of civic input on the city ' s comprehensive city plan update . the findings show that the impact of citizens ' voices depends on the volume and the tone of their demands . a higher demand with a stronger tone results in more policy changes . we also found an interesting and unexpected result : the city government in palo alto mirrors more or less the online crowd ' s voice while citizen representatives rather filter than mirror the crowd ' s will . while nlp methods show promise in making the analysis of the crowdsourced input more efficient , there are several issues . the accuracy rates should be improved . furthermore , there is still considerable amount of human work in training the algorithm . 

high accuracy citation extraction and named entity recognition for a heterogeneous corpus of academic papers
citation indices are increasingly being used not only as navigational tools for researchers , but also as the basis for measurement of academic performance and research impact . this means that the reliability of tools used to extract citations and construct such indices is becoming more critical ; however , existing approaches to citation extraction still fall short of the high accuracy required if critical assessments are to be based on them . in this paper , we present techniques for high accuracy extraction of citations from academic papers , designed for applicability across a broad range of disciplines and document styles . we integrate citation extraction , reference parsing , and author named entity recognition to significantly improve performance in citation extraction , and demonstrate this performance on a cross - disciplinary heterogeneous corpus . applying our algorithm to previously unseen documents , we demonstrate high f - measure performance of 0 . 98 for author named entity recognition and 0 . 97 for citation extraction . 

seq2seq - vis : a visual debugging tool for sequence - to - sequence models
neural sequence - to - sequence models have proven to be accurate and robust for many sequence prediction tasks , and have become the standard approach for automatic translation of text . the models work with a five - stage blackbox pipeline that begins with encoding a source sequence to a vector space and then decoding out to a new target sequence . this process is now standard , but like many deep learning methods remains quite difficult to understand or debug . in this work , we present a visual analysis tool that allows interaction and “ what if” - style exploration of trained sequence - to - sequence models through each stage of the translation process . the aim is to identify which patterns have been learned , to detect model errors , and to probe the model with counterfactual scenario . we demonstrate the utility of our tool through several real - world sequence - to - sequence use cases on large - scale models . 

sentiment analysis of amazon product reviews based on nlp
this is the age of information explosion . numbers , text , video , and audio are all information . how to extract the instructive information we are interested in from a large amount of data is crucial . in this article , we built an evaluative model based on amazon product reviews . the review content is divided into good , medium , and bad to express the positive , negative and neutral emotions . from the basic information about the data , we find that the number of reviews is increasing exponentially . more and more consumers choose to shop online , among which beauty and baby products are more popular among consumers . to understand consumers ' preferences , we use textblob , a natural language processing library in python , to do word frequency statistics and sentiment analysis on product reviews . it is found from the word cloud that consumers are most concerned about the quality and price of products . the specific quality description of text - based reviews - " love " is closely related to the rating level . 

a hybrid - strategy method combining semantic analysis with rule - based mt for patent machine translation
this paper presents a hybrid method combining semantic analysis with rule - based mt for patent machine translation . based on the theory of hierarchical network of concepts , the semantic analysis used the lv principle to deal with the ambiguity of multiple verbs and the boundary of long np . the determination of main verb can help to select the right syntax tree , and the boundary detection of long np can help to schedule the process of syntax . from the result of the experiments , we can see that this hybrid - strategy method can effectively improve the performance of chinese - english patent machine translation . 

sentiment analysis of covid - 19 vaccine perception using nlp
this paper analyzes the sentiments of the people residing in india with regards to the covid - 19 vaccine . the covid - 19 pandemic has also coincided with the social media companies experiencing increased traffic . this is due to people having limited options / platforms for expressing their opinion . taking advantage of this , the paper uses three different models to perform sentiment analysis on english tweets posted by users in india and chooses a deep learning model after comparing their results . 

boost phase trajectory optimization for hypersonic vehicle based on gpm
in order to minimum the time - to - climb of hypersonic vehicle , the trajectory of hypersonic vehicle boost segment is studied . the optimal control model of boost trajectory is analyzed , and gauss pseudospectral method ( gpm ) is applied to discretize the optimal problem into nonlinear programming problem ( nlp ) . finally , matlab software package snopt is used to solve the nlp problem . numerical simulations show that the trajectory derived by this method is not sensitive to initial conditions and fairly good in real - time property . 

an energy - efficient reconfigurable lstm accelerator for natural language processing
long short - term memory ( lstm ) recurrent neural network ( rnn ) is known for its capability in modeling sequence learning tasks such as language modeling . however , due to the large number of model parameters and compute - intensive operations , existing fpga implementations of lstms are not sufficiently energy - efficient as they require large area and exhibit high power consumption . this work describes a substantially different hardware implementation of an lstm which includes several architectural innovations to achieve high throughput and energy - efficiency . the architectural innovations include ( 1 ) an improved design of an approximate multiplier ( am ) and its integration with the compute - intensive units of the lstm ; ( 2 ) the design of control mechanisms to handle the variable - cycle ( data dependent ) multiply operations ; ( 3 ) incorporation of hierarchical pipelining at multiple levels of the design to maximize the overlap of the variable cycle computations . in addition , this work applies a post - training , range - based , linear quantization to the parameters of the model to further improve the performance and energy - efficiency . a python framework is also developed that allows for analysis and fine tuning of the input parameters before mapping the design to hardware . this paper extensively explores the design trade - offs and demonstrates the advantages for one common application - language modeling . implementation of the design on a xilinx zynq xc7z030 fpga shows maximum improvement as compared to three recent published works in throughput to be 27 . 86x , 7 . 69x and 11 . 06x and in energy - efficiency to be 45 . 26x , 14 . 76x and 16 . 97x , respectively . 

a conversational agent for database query : a use case for thai people map and analytics platform
since 2018 , thai people map and analytics platform ( tpmap ) has been developed with the aims of supporting government officials and policy makers with integrated household and community data to analyze strategic plans , implement policies and decisions to alleviate poverty . however , to acquire complex information from the platform , non - technical users with no database background have to ask a programmer or a data scientist to query data for them . such a process is time - consuming and might result in inaccurate information retrieved due to miscommunication between non - technical and technical users . in this paper , we have developed a thai conversational agent on top of tpmap to support self - service data analytics on complex queries . users can simply use natural language to fetch information from our chatbot and the query results are presented to users in easy - to - use formats such as statistics and charts . the proposed conversational agent retrieves and transforms natural language queries into query representations with relevant entities , query intentions , and output formats of the query . we employ rasa , an open - source conversational ai engine , for agent development . the results show that our system yields fl - score of 0 . 9747 for intent classification and 0 . 7163 for entity extraction . the obtained intents and entities are then used for query target information from a graph database . finally , our system achieves end - to - end performance with accuracies ranging from 57 . 5% - 80 . 0% , depending on query message complexity . the generated answers are then returned to users through a messaging channel . 

bert for stock market sentiment analysis
when breaking news occurs , stock quotes can change abruptly in a matter of seconds . the human analysis of breaking news can take several minutes , and investors in the financial markets need to make quick decisions . such challenging scenarios require faster ways to support investors . in this work , we propose the use of bidirectional encoder representations from transformers bert to perform sentiment analysis of news articles and provide relevant information for decision making in the stock market . this model is pre - trained on a large amount of general - domain documents by means of a self - learning task . to fine - tune this powerful model on sentiment analysis for the stock market , we manually labeled stock news articles as positive , neutral or negative . this dataset is freely available and amounts to 582 documents from several financial news sources . we fine - tune a bert model on this dataset and achieve 72 . 5 % of f - score . then , we perform some experiments highlighting how the output of the obtained model can provide valuable information to predict the subsequent movements of the dow jones industrial ( dji ) index . 

resource development to prevent riots at mass events
currently , the main communication means and management of the participants behavior in mass events , including unauthorized ones , are portable mobile devices , through which , in the social network framework a , text messages are sent to manipulate people ' s behavior . in the article the problem of mass events participants binary classification on representing and not representing a threat is considered . a convolutional neural network is used as a classification tool . the system of loading and processing data from a social network , the neural network architecture a and the results of experiments are described . 

linguistic divergence of sinhala and tamil languages in machine translation
this paper presents a study of the lexical - semantic divergence between sinhala and tamil languages . study of divergence is critical as differences in linguistic and extra - linguistic features in languages play pivotal roles in translation . this research the first study of the divergence between sinhala and tamil languages and is based on dorr ' s classification . we propose a computer - assisted divergence study procedure using statistical machine translation , which is easy and gives good performance compared to traditional approaches . accordingly , this research has the twin aims of revisiting classification of divergence types as outlined by dorr and outlining some of the new divergence patterns specific to sinhala and tamil languages . this study proposes a rule - based algorithm to classify a divergence . 

non - periodic photorefractive photonic lattices in lithium niobate : features of their formation and light propagation
we experimentally demonstrate formation of nonperiodic photorefractive photonic lattices in lithium niobate using the optical modulation and optical induction procedures based on contact and optical projection schemes with amplitude masks . 

re - usable tools for precision machine translation
the logon mt demonstrator assembles independently valuable general - purpose nlp components into a machine translation pipeline that capitalizes on output quality . the demonstrator embodies an interesting combination of hand - built , symbolic resources and stochastic processes . 

scaling nlp algorithms to meet high demand
the growth of digital information and the richness of data shared online make it increasingly valuable to be able to process large amounts of data at a very high throughput rate . at the same time , rising interest in natural language processing ( nlp ) has resulted in the development of a great number of algorithms designed to perform a variety of nlp tasks . there is a need for frameworks that enable multiple users and applications to run individual or a combination of nlp algorithms to derive relevant information from data [ 1] . in this work , we take multiple nlp algorithms that adhere to the adept framework and deploy them on distributed processing architectures to satisfy the dual needs of serving a large user group and meeting high throughput standards , while reducing the time from lab to production environment . the adept framework provides a set of uniform apis for interacting with a diverse set of nlp algorithms by defining a set of data structures for representing nlp concepts [ 2] . it offers multiple access points for interacting with these algorithms ; a rest api , a serialized data api , and processor components that can be used in a larger pipeline . the comprehensive adept architecture can support algorithms that perform sentence - level , document - level , or corpus - level text processing , allowing a wide range of nlp algorithms to make use of the framework . adept interfaces allow parallelization to occur at an optimum level for each algorithm . amazon web services ( aws ) consists of a stack of technologies commonly used in the commercial sphere to host web applications designed to scale rapidly with a growing user base . the amazon elastic compute cloud ( ec2 ) and its auto - scaling feature in particular provide a means of reliably and efficiently scaling a service to meet traffic demands . hadoop and spark are top level apache projects designed to enable massive parallelization of data processing . hadoop employs the mapreduce programming model and uses a distributed file system to store data . it is a widely used processing framework with proven potential for very high throughput . spark is a distributed processing framework that makes use of in - memory primitives , enabling a significant performance advantage over hadoop in certain uses at the cost of higher memory requirements [ 3] . spark processes are able but not required to fit into the mapreduce model , allowing algorithms to be adapted for use in a spark context with minimal effort . to handle high volume of concurrent requests of deft algorithms , we created a mechanism to deploy the algorithms on an aws stack [ 4] . we extended the adept framework to create installers to deploy algorithms on an aws ec2 node . we preserve the ec2 instance along with the algorithm and grow or shrink the number of instances to accommodate the request volume . in summary , we demonstrate that nlp algorithms can be rapidly scaled by leveraging the adept framework , parallelization models , and virtualization technologies to meet the growing demands of high volume and throughput . the adept framework allows managing a diverse set of nlp algorithms . we adapt the nlp algorithms to fit into hadoop and spark architectures in an effort to maximize their throughput . we explore the viability of using amazon ec2 to meet and rapidly scale based on usage demands . 

generation of fractional optical vortices at the edge of the phase wedge
we have made a theoretical modeling of the evolution of a monochromatic gaussian beam diffracted by the angle formed by three sides of the phase wedge different types . we have found that the edges of the phase wedge generate macroscopic chains of identical optical vortices that disappear at the far field zone . at the same time , the π - phase plate can reproduce a very complex wave field whose structure depends on the scale of observation . at large scales there appear two π - cuts resembling broken edge dislocations with perpendicular directions . at small ( some microns ) scales two short vortex chains consisting of alternating - sign optical vortices are nucleated near the corner of the wedge . the analysis shows that the sizes of the chains decrease quickly when approaching the wedge surface . this enables us to assume that the π - phase plate can create so - called optical quarks in the evanescent waves of the edge field . on the basis of theoretical considerations experiment was conducted in which were obtained fractions optical vortices at the edge of the phase wedge . 

anaphora resolution in hindi documents
this paper presents anaphora resolution as a technique of semantic analysis of text documents written in hindi language . the focus is on texts that mainly employ simple sentences , such as children ' s stories , short essays , etc . the technique works by locating sentences in the text that are semantically related through anaphors , analyzing their semantics and exploiting the latter for resolving referents of the respective anaphors . the approach used here is based on matching constraints for the grammatical attributes of different words . the algorithm for anaphora resolution has been tested extensively . the accuracy of anaphora resolution is nearly 96 % for simple sentences and for compound and complex sentences ; the accuracy is of the order of 80 % . the causes of the errors are analyzed and possible techniques for improvements are discussed . 

ontoltcn : a chinese text oriented semi - auto ontology knowledge discovery tool
ontology ( rdf / owl ) plays a foundational role of semantic web for knowledge representation . but nowadays there are few chinese ontology bases available , which hinders the research and development of chinese semantic web applications . this paper introduces an ontology knowledge discovery tool , named ontoltcn , which supports semi - auto domain ontology acquisition from chinese corpus . in brief , ontoltcn is a protege plug - in based on ontolt platform , which integrates chinese nlp and xml pattern mapping technologies for knowledge discovery . a case study in chinese e - government domain is discussed as well , which shows the usability of ontoltcn for ontology construction from digital archives . 

knowledge discovery in hashtags # 
twitter is a breed of social networks that are playing a buoyant role in today ' s world communication . this paper is an attempt to apply knowledge discovery process on twitter dataset comprising hashtags along with the visual analytic techniques whose purpose is to provide information to the people in such a way so that they understand concealed knowledge in the data effortlessly and meritoriously . we further analyze tweet text and metadata associated with each tweet for identification of useful patterns like " who talks to whom " and " how much " . our research reveals the impact of visualization and hierarchical clustering technique in analyzing similar groups of users . further we investigate different social network measures that unveil the influence of users in the particular hashtags . 

an approach towards multilingual translation by semantic - based verb identification and root word analysis
popular and widely available translators like google translator uses statistic based approach to build the multilingual translation system . this approach solely depends on the availability of a large number of samples . which is why , google translator performs interestingly well when it translates among the popular languages like english , french or spanish , however , makes elementary mistakes when it translates the languages that are newly introduced or less known to the system . most of the research found so far on natural language processing ( nlp) , have been performed keeping english as the target language . however , a good number of widely spoken potential languages remain nearly unexplored in the research fields which is quite unexpected in the era of global communication . in this study , we have tried to explore a generalized machine translation system , especially for the languages having insufficient availability in literature . this study basically focuses on bengali language as an example of such low resource languages . in this work , we have proposed different approaches for semantic based verb identification along with its translation , and hence , developed an algorithm for root word detection of a verb in any sentence which reflects significant improvement over google translator . finally , we have shown a comparison among the different approaches in terms of accuracy , time complexity and space complexity . 

algorithms and a tool for automatic decryption of clinical notes
natural language processing ( nlp ) presents a set of techniques that are finding applications in modern healthcare , for the extraction and generation text . clinical notes are classically originated and derived from various sources of narratives such as reports , referral letters , discharge notes and clinical summaries . in this paper , we present algorithms and a tool for enabling each member of a medical team to read and understand each others medical notes , using nlp techniques . we refer to this process as the decryption , or the deciphering , of complex clinical notes into simple readable language . we have presented a tool called the clinical note translator ( cnt ) . based on the replacement of technical terms in clinical notes , cnt translates these notes to plain text . this solution is expected to assist multidisciplinary medical teams in understanding peer - to - peer expert notes . 

comparative study on natural language processing for tourism suggestion system
nowadays , due to a large amount of tourism information such as attractions or activities , it is difficult for users to decide where to go or do specific activities without getting suggestions from experts , books , websites , etc . to support users wisely in choosing tourism activities , a recommendation or suggestion system using natural language processing ( nlp) , which is a subdomain of artificial intelligence ( ai) , is proposed . nlp plays an important part in the pre - processing task of the recommender system to enhance the performance of the suggested output . this paper explores and compares the nlp techniques that are currently applied to the existing recommendation systems . challenges and trends using nlp for suggestion systems in tourism are discussed . 

text normalization on thai twitter messages using ipa similarity algorithm
twitter often contains many noisy short messages . the noisy text are caused by insertion , transformation , transliteration and onomatopoeia . text normalization is used for solving these noisy text . in this paper , we present the algorithm that can normalize insertion and homophonic transformation words by converting to international phonetic alphabet ( ipa ) and find the most similarity ipa of out - of - vocabulary and ipa of in - vocabulary using levenshtein distance . we used twitter corpus that contained 2 , 000 twitter messages for evaluating the proposed algorithm . the experiment result illustrated that the proposed algorithm returned an accuracy of 79 . 03 % when compared to dictionary - based normalization of lextoplus returned an accuracy 24 . 19 % . 

an integral penalty - barrier direct transcription method for optimal control
some direct transcription methods can fail to converge , e . g . when there are singular arcs . we recently introduced a convergent direct transcription method for optimal control problems , called the penalty - barrier finite element method ( pbf) . pbf converges under very weak assumptions on the problem instance . pbf avoids the ringing between collocation points , for example , by avoiding collocation entirely . instead , equality path constraint residuals are forced to zero everywhere by an integral quadratic penalty term . we highlight conceptual differences between collocation - and penalty - type direct transcription methods . theoretical convergence results for both types of methods are reviewed and compared . formulas for implementing pbf are presented , with details on the formulation as a nonlinear program ( nlp) , sparsity and solution . numerical experiments compare pbf against several collocation methods with regard to robustness , accuracy , sparsity and computational cost . we show that the computational cost , sparsity and construction of the nlp functions are roughly the same as for orthogonal collocation methods of the same degree and mesh . as an advantage , pbf converges in cases where collocation methods fail . pbf also allows one to trade off computational cost , optimality and violation of differential and other equality equations against each other . 

aspect - based emotion analysis on speech for predicting performance in collaborative learning
this full research paper focuses on a natural language processing ( nlp ) driven approach to extract emotions from the speech in collaborative learning environments and analyze how they correlate with the learner ' s performance . social competency is one of the base competencies that have been the target of many educational researchers in engineering and computing education during the past several years . studies show that the low level of individual ' s performance is not just due to lack of intellectual or cognitive competencies but also lack of social skills impact performance in both educational and industrial domain . for this reason , engineering education is encapsulating social skills into the curriculum to prepare students for the fourth industrial revolution ( i . e . , industry 4 . 0 ) . towards this goal in earlier work [ 1] , we proposed a model to identify the correlation between the sentiments extracted from students ' speech in teams and their performance . the results of polarity sentiment analysis showed a strong positive correlation between students ' positive feelings in teams and their individual performance in the course . this study takes a further step and conducts multi - class emotion analysis on students ' speech in teams . the process consists of two steps : 1 ) extracting different classes of sentiment such as joy , anger , anxiety , etc . , and identifying their correlation with students ' performance using collaborative speech in an introductory programming course ( cs1 ) , 2 ) aspect - based emotion analysis ( abea ) . the approach we adopt is the supervised machine learning method and rule - based models on speech datasets . after pre - processing the text , we identify multi classes of sentiments . aspect extraction is accomplished through the part of speech ( pos ) tagging , and patterns are extracted from the identified aspects . finally , we use the combination of emotion classes and aspect patterns as feature vectors to train the k - nearest neighbor ( knn ) algorithm to predict students ' performance . 

invited talk # 2 vietnamese neural language model for nlp tasks with limited resources
a statistical language model is a probability distribution over sequences of words . language modeling is used in various computing tasks such as speech recognition , machine translation , optical character and handwriting recognition and information retrieval and other applications . whereas n - gram is considered as a traditional language model , neural language model has been emerging recently as a means to approximate the probability of a sentence using neural networks and word embeddings . an advantage of a neural language model is that it can be further applied to other nlp tasks where the training datasets may be limited . in this talk , we realize this idea by introducing the usage of a vietnamese neural model language trained from a large corpus of social media data . when further applying this neural model language with other nlp tasks including entity recognition , spam detection and topic modeling with relatively small training datasets ; we witness improved performance achieved , as compared to other existing approaches using deep learning with typical word embedding techniques . 

clustering of words using dictionary - learnt word representations
language is the way of communication through words . this will help to get a better insight of the world . natural language processing ( nlp ) mainly concentrate on expanding systems that allow computers to communicate with people using everyday language . one of the challenges inherent in nlp is teaching computers to recognize the way humans learn and use language . word representations give rise to capture syntactic and semantic properties of words . so the main purpose of this work is to find out the set of words which have similar semantics by matching the context in which the words occur . in this work we explore a new method for learning word representations using sparse coding , a technique usually done on signals and images . we present an efficient sparse coding algorithm , orthogonal matching pursuit to generate the sparse code . based on the input given , sparse codes are generated for the input . the input term vectors are classified based on the sparse code by grouping the terms which have same sparse code into one class . k - means algorithm is also used to classify the input terms vectors which have semantic similarities . finally , this paper makes a comparison that gives the best representation from the sparse code and k - means . the result shows an improved set of similar words using sparse code when compared to k - means . this is because svd is used as a part of dictionary learning which captures the latent relationship that exists between the words . 

assisted speech to enable second language
speaking a second language ( l2 ) is a desired capability for billionsof people . currently , the only way to achieve it naturally is througha lengthy and tedious training , which ends up various stages offluency . the process is far away from the natural acquisition of alanguage . in this paper , we propose a system that enables any person withsome basic understanding of l2 speak fluently through " instant as - sistance " provided by digital conversational agents such as googleassistant , microsoft cortana , or apple siri , which monitors thespeaker . it attends to provide assistance to continue to speak whenspeech is interrupted as it is not yet completely mastered . the notyet acquired elements of language can be missing words , unfa - miliarity with expressions , the implicit rules of articles , and thehabits of sayings . we can employ the hardware and software of theassistants to create an immersive , adaptive learning environmentto train the speaker online by a symbiotic interaction for implicit , unnoticeable correction . 

tools to address the interdependence between tokenisation and standoff annotation
in this paper we discuss technical issues arising from the interdependence between tokenisation and xml - based annotation tools , in particular those which use standoff annotation in the form of pointers to word tokens . it is common practice for an xml - based annotation tool to use word tokens as the target units for annotating such things as named entities because it provides appropriate units for stand - off annotation . furthermore , these units can be easily selected , swept out or snapped to by the annotators and certain classes of annotation mistakes can be prevented by building a tool that does not permit selection of a substring which does not entirely span one or more xml elements . there is a downside to this method of annotation , however , in that it assumes that for any given data set , in whatever domain , the optimal tokenisation is known before any annotation is performed . if mistakes are made in the initial tokenisation and the word boundaries conflict with the annotators ' desired actions , then either the annotation is inaccurate or expensive retokenisation and reannotation will be required . here we describe the methods we have developed to address this problem . we also describe experiments which explore the effects of different granularities of tokenisation on ner tagger performance . 

improving performances of log mining for anomaly prediction through nlp - based log parsing
failure prediction of industrial systems is a promising application domain for data mining approaches and should naturally rely on log messages which are a prime source of data as they are generated by many systems . however , before extracting relevant information of such log messages , another critical step is to parse the logs , that is to say to transform a raw unstructured text from the log messages into a suitable input for data mining . these two problems ( log parsing then log mining ) are often studied separately while they are directly related in the context of failure prediction ; moreover , few performance benchmarks are publicly available . in this paper , we focus on the impact of log parsing techniques via natural language processing on the performances of log mining on two datasets . the first one is a log of an industrial aeronautical system comprising over 4 , 500 , 000 messages collected over one year of operation ; the second one is a public benchmark set from an hdfs cluster . on the latter , we show that it is possible to raise the f - score from 96 % to 99 . 2 % while using simpler and more robust log parsing techniques that require less parameter tuning provided that they are correctly combined with log mining techniques . 

combating misinformation dissemination through verification and content driven recommendation
the covid 19 pandemic is a humanitarian emergency that poses an enormous threat to society and has impacted various social media platforms and journalism . news and social media has become an immensely popular platform for consumption of information . however , these platforms are also the bearer of fake news and information which causes negative effects and creates panic . thus , this research work aim to tackle this problem by creating a unique hybrid model using machine learning algorithms with natural language processing ( nlp ) techniques to verify news . in order to make the proposed system foolproof , a superior content based recommendation system is developed which will encourage users to consume authenticated news and content from verified sources . thus , such a system will provide a holistic approach as it not only verifies but also provides genuine and true recommendations for the same . 

generation from lexical conceptual structures
this paper describes a system for generating natural language sentences from an interlingual representation , lexical conceptual structure ( lcs ) . this system has been developed as part of a chinese - english machine translation system , however , it promises to be useful for many other mt language pairs . the generation system has also been used in cross - language information retrieval research ( levow et al . , 2000 ) . 

toxic comment detection using lstm
while online communication media acts as a platform for people to connect , collaborate and discuss , overcoming the barriers for communication , some take it as a medium to direct hateful and abusive comments that may prejudice an individual ' s emotional and mental well being . explosion of online communication makes it virtually impossible for filtering out the hateful tweets manually , and hence there is a need for a method to filter out the hate - speech and make social media cleaner and safer to use . the paper aims to achieve the same by text mining and making use of deep learning models constructed using lstm neural networks that can near accurately identify and classify hate - speech and filter it out for us . the model that we have developed is able to classify given comments as toxic or nontoxic with 94 . 49 % precision , 92 . 79 % recall and 94 . 94 % accuracy score . 

enriching pre - trained language model with entity information for relation classification
relation classification is an important nlp task to extract relations between entities . the state - of - the - art methods for relation classification are primarily based on convolutional or recurrent neural networks . recently , the pre - trained bert model achieves very successful results in many nlp classification / sequence labeling tasks . relation classification differs from those tasks in that it relies on information of both the sentence and the two target entities . in this paper , we propose a model that both leverages the pre - trained bert language model and incorporates information from the target entities to tackle the relation classification task . we locate the target entities and transfer the information through the pre - trained architecture and incorporate the corresponding encoding of the two entities . we achieve significant improvement over the state - of - the - art method on the semeval - 2010 task 8 relational dataset . 

automated assertion generation from natural language specifications
we explore contemporary natural language processing ( nlp ) techniques for converting nl specifications found in design documents directly to an temporal logic - like intermediate representation ( ir ) . generally , attempts to use nlp for assertion generation have relied on restrictive sentence formats and grammars as well as being difficult to handle new sentence formats . we tackle these issues by first implementing a system that uses commonsense mappings to process input sentences into a normalized form . then we use frame semantics to convert the normalized sentences into an ir based on the information and context contained in the frames . through this we are able to handle a large number of sentences from real datasheets allowing for complex formats using temporal conditions , property statements , and compound statements ; all order agnostic . our system can also be easy extended by modifying an external , rather than internal , commonsense knowledge - base to handle new sentence formats without requiring code changes or intimate knowledge of the algorithms used . 

formation of quasi - regular structures in lithium niobate fabry - perot interferometers
this paper presents the results of experimental studies of formation of quasi - regular optical structures in photorefractive fabry - perot interferometers based on y - and z - cut lithium niobate samples . it is shown that the topology of these structures depends on the parameters of the interferometer , and the characteristics of the inducing light fields . 

how to develop universal vocabularies using automatic generation of the meaning of each word
to develop a computer - understandable and international - communication - language , we need not to develop a pivot language but an artificial language which can express every meaning in each natural language . that artificial language needs enough vocabulary to express the meanings contained in each language . those vocabularies can only be developed by native speakers and should be defined by formal ways . considering the situation , at this moment universal networking language ( unl ) is the best solution and universal words ( uws ) are the most promising candidates . but uws itself are formal and not always to be understandable for human . we need to provide the way to express the correct meaning of uws for humans , to ensure every language speakers can create the correct uws . the semantic backgrounds of each uws are provided by unl ontology . this research proposes the way to auto generate the meaning of each uws using unl ontology . 

adapting bert embeddings for text correlation of military domain specific content
this paper addresses the problem of how to achieve similarity search for text sequence correlation with a proper semantic foundation . natural language processing ( nlp ) is fundamental for answering community of interest ( coi ) associated questions and this paper presents and compares three methods for similarity search . the methods are using google introduced transformer models , bert being the most well known . combining techniques for pre - processing data , enhancing bert and post - bert adjustments are tested in an experimental setting and results are presented in this paper . 

intelligent question answering system based on artificial neural network
the ability of the machine to infer knowledge from the user documents can be tested based on its ability to answer the question asked . conventional artificial neural network ( ann ) models for knowledge extraction only answer to the questions which are simple and objective as they don ' t analyze the questions and don ' t try to understand what really the content of document mean . the proposed question answering system ( qas ) uses deep cases along with ann to understand the contents of the documents . we divide the sentences of natural language into knowledge units and assign deep case to each word to improve the quality of knowledge extraction . 

identifying mentions of life stressors in clinical notes
the impact of social determinants on individual health has increasingly been recognized . in this study , we aimed to understand the representation of stressful life events occurring in clinical reports generated outside of mental health specialties . we present a conceptual schema for the representation of stressful events and associated linguistic attributes within free - text clinical notes . we also evaluated existing “ off - the - shelf ” clinical natural language processing ( nlp ) systems for the detection of stress related concepts in clinical notes . we found that mentions of stressful events are prevalent even in non - mental health specialties , and that capturing details of stress mentions is challenging . our results further indicate that existing nlp systems can serve as a reasonable starting point for developing models trained specifically for extracting stress associated information from clinical narratives . 

introduction to special issue on reasoning in natural language information processing
for any applications related to natural language processing ( nlp ) , reasoning has been recognized as a necessary underlying aspect . many of the existing work in nlp deals with specific nlp problems in a highly heuristic manner , yet not from an explicit reasoning perspective . recently , there have been developments on models that allow reasoning in nlp such as language models , logical models , and so on . the goal of this special issue is to present high - quality contributions that integrate reasoning involved in different areas of natural language processing both at theoretical and / or practical levels . in this article , we give a brief overview on some major aspects of explicating reasoning in nlp and summarize the articles included in this special issue . 

emotionexpert : facebook game for crowdsourcing annotations for emotion detection
the current paper explores the use of the social network platform facebook , as a source of emotion annotated textual data as well as a source of annotators . the traditional approach of hiring experts to provide manually labeled ( annotated ) data for nlp research is time - consuming , tedious and expensive . hence , crowdsourcing has emerged as a useful method for obtaining annotated data for natural language processing ( nlp ) research . we have developed a purposeful innovative facebook game called emotionexpert in order to collect human annotated textual data for emotion detection from text . the game provides a means to reach a large number of players , while making the annotation of emotional content of texts an enjoyable and social activity . the findings reported in this paper indicate that emotionexpert is a useful resource for reaching a large number of people to produce reliable annotations . 

feature analysis on english word difficulty by gaussian mixture model
machine learning has significantly improved natural language process ( nlp ) recently . in this paper , we firstly adopt the nlp approaches to extract features which represent the difficulty levels of english words . then , principal component analysis ( pca ) is applied to reduce the feature dimension for visualization of data points , which intuitively justifies the selection of the features . more elaborated analysis is carried out using gaussian mixture model ( gmm ) which clusters the data points in the reduced dimensional space . the analysis verifies that the proposed features appropriately contribute to the prediction of english word difficulty level . finally , we demonstrate that 73 . 5 % of classification accuracy can be achieved with support vector machine ( svm ) with the features we proposed . 

design of the architecture for text recognition and reading in an online assessment applied to visually impaired students
this paper describes the architecture for text recognition and reading in an online assessment applied to visually impaired students . for this purpose , it is intended to implement an online evaluation system exclusively to recognize alphanumeric information , i . e . , letters and numbers , through the use of an application programming interface or also known as speech and text processing api ’ s , where the computer can understand and respond in natural language . usability tests were carried out based on iso 9126 - 4 , taking as variables to measure the effectiveness , efficiency , and satisfaction of the latter through a system usability scale or more commonly known as sus ( system usability scale) . 

detecting and categorizing android malware with graph neural networks
android is the most dominant operating system in the mobile ecosystem . as expected , this trend did not go unnoticed by miscreants , and quickly enough , it became their favorite platform for discovering new victims through malicious apps . these apps have become so sophisticated that they can bypass anti - malware measures implemented to protect the users . therefore , it is safe to admit that traditional anti - malware techniques have become cumbersome , sparking the urge to come up with an efficient way to detect android malware . in this paper , we present a novel natural language processing ( nlp ) inspired android malware detection and categorization technique based on function call graph embedding . we design a graph neural network ( graph embedding ) based approach to convert the whole graph structure of an android app to a vector . we then utilize the graphs ' vectors to detect and categorize the malware families . our results reveal that graph embedding yields better results as we get 99 . 6 % accuracy on average for the malware detection and 98 . 7 % accuracy for the malware categorization . 

probabilistic relational supervised topic modelling using word embeddings
the increasing pace of change in languages affects many applications and algorithms for text processing . researchers in natural language processing ( nlp ) have been striving for more generalized solutions that can cope with continuous change . this is even more challenging when applied on short text emanating from social media . furthermore , increasingly social media have been casting a major influence on both the development and the use of language . our work is motivated by the need to develop nlp techniques that can cope with short informal text as used in social media alongside the massive proliferation of textual data uploaded daily on social media . in this paper , we describe a novel approach for short text topic modelling using word embeddings and taking into account any informality of words in the social media text with the aim of addressing the challenge of reducing noise in messy text . we present a new algorithm derived from the term frequency - inverse document frequency ( tf - idf ) , named term frequency - inverse context term frequency ( tf - ictf ) . tf - ictf relies on a probabilistic relation between words and context with respect to time . our experimental work shows promising results against other state - of - the - art methods . 

automatic acquisition of english topic signatures based on a second language
we present a novel approach for automatically acquiring english topic signatures . given a particular concept , or word sense , a topic signature is a set of words that tend to co - occur with it . topic signatures can be useful in a number of natural language processing ( nlp ) applications , such as word sense disambiguation ( wsd ) and text summarisation . our method takes advantage of the different way in which word senses are lexicalised in english and chinese , and also exploits the large amount of chinese text available in corpora and on the web . we evaluated the topic signatures on a wsd task , where we trained a second - order vector cooccurrence algorithm on standard wsd datasets , with promising results . 

a natural language processing method of chinese instruction for multi - legged manipulating robot
in this paper , a natural language processing ( nlp ) framework is proposed to build the chinese natural language interaction between human and the multi - legged manipulating robot which has both locomotion and manipulation functions . the chinese natural language instruction is transformed into formal representations by this method . firstly , cascaded conditional random fields ( ccrfs ) is employed to analyze the syntax of the instruction . letter - based features are used in each layer of ccrfs to solve problem caused by word segmentation . in order to understand the modification relationship between entity classes in chunk , one judgment method based on support vector machine ( svm ) is proposed . to solve the problem of the large number of core verbs in instructions generated by the multiple robot motion types , a classification of verbs based on naive bayes classifier was presented . and the semantic framework of each type of verbs is established to determine the necessary and unnecessary roles for each kind of verbs . at last , several experiments are carried out and the results of each step of framework are presented to demonstrate the effectiveness of the method . it is instructive to understand chinese natural language instructions for robots with complex motion . 

a practical message - to - speech strategy for dialogue systems
in this paper , we present a message - to - speech system for natural language generation that is to be integrated in a dialogue system . as the system has to function in a very restrictive environment with respect to computational resources , a compromise between concept based and template based generation systems had to be found . still , the approach aims at achieving linguistic flexibility for the utterances and attaining a natural sounding prosody . 

recognizing radicalization indicators in text documents using human - in - the - loop information extraction and nlp techniques
among the operational shortfalls that hinder law enforcement from achieving greater success in preventing terrorist attacks is the difficulty in dynamically assessing individualized violent extremism risk at scale given the enormous amount of primarily text - based records in disparate databases . in this work , we undertake the critical task of employing natural language processing ( nlp ) techniques and supervised machine learning models to classify textual data in analyst and investigator notes and reports for radicalization behavioral indicators . this effort to generate structured knowledge will build towards an operational capability to assist analysts in rapidly mining law enforcement and intelligence databases for cues and risk indicators . in the near - term , this effort also enables more rapid coding of biographical radicalization profiles to augment a research database of violent extremists and their exhibited behavioral indicators . 

evaluation of sentiment databases : a comparison of sentiment databases through social listening statements and azure machine learning studio
nowadays it happens more and more frequently that people get their opinions about products in social media or online reviews . for this purpose , product reviews themselves or channels on social media are searched in order to get an impression of what other users think about products before buying . for companies , these social listening statements ( sls ) can be business critical . a sentiment analysis is part of natural language processing ( nlp ) . the analysis of text tries to identify and extract sentiments in a text . in this paper , two supervised machine learning algorithms for text analysis were combined with four sentiment databases to increase accuracy in recognition . for better comparability , basic sentiment databases without subject - specific terms were used . experiments were conducted with manually and automatically generated data sets , ranging in size from 300 to 16 , 000 entries . the combination of these two methods resulted in an accuracy of up to 94 % in detecting positive or negative sentiment in texts . the combination of sentiment databases ( sd ) and machine learning significantly increased the recognition . 

session summary
the five papers in this session , as well as the ten papers in the other two natural language sessions , can be classified into three broad categories : ( 1 ) statistical approaches to natural language processing and the automatic acquisition of linguistic structure ( 2 out of 5 papers in this session ; 8 out of 15 overall ) ; ( 2 ) robust processing of texts by combining multiple partial analyses ( 2 out of 5 ; 5 out of 15 ) ; and ( 3 ) fundamental issues in linguistic analysis which will become bottlenecks to processing of more complex texts and interactive dialogues ( 1 out of 5 ; 2 out of 15 ) . it is quite remarkable that the largest two of these categories , ( 1 ) and ( 2 ) , would probably not been represented in an nlp session at such a meeting just three or four years ago . from the papers sampled here , it is clear that a revolution in natural language processing is occurring and that the strong surge of work in these two areas is closely linked : both of the text analysis systems discussed in this session bring together statistical and non - statistical techniques . both the rapid change in research direction and the results achieved are surprising and worthy of special note . 

spatial nonlinear effects with higher order modes in linbo3 waveguide arrays
we present theoretical and experimental studies on nonlinear beam propagation in lithium niobate waveguide arrays utilizing higher order second harmonic bands . we find that the implementation of the higher order second harmonic bands leads to a number of new effects . the combined interaction of two second harmonic bands with a propagating fundamental beam can lead to a complete inhibition of nonlinear effects or to the formation of discrete spatial solitons , depending only on the wavelength of the fundamental wave . furthermore we analyze the properties of discrete solitons subjected to linear coupling of the second harmonic . here we predict and demonstrate experimentally a power dependent phase transition of the soliton topology . 

trajectory design for a mars ascent vehicle concept terrestrial demonstration
the trajectory design and propellant sizing of a conceptual mars ascent vehicle ( mav ) can be accomplished via the solution of the associated optimal control problem under given vehicle , mission , path and control constraints . this paper presents the implementation of this optimization using gauss pseudospectral optimization software ( gpops - ii ) in combination with a sparse nonlinear optimizer ( snopt ) as the nonlinear programing ( nlp ) solver . the solution obtained is an input to the propulsion , guidance navigation and control ( gnc ) , mechanical design , aerodynamic and thermal analysis teams , which use it to size systems , tune algorithms and update models . a sensitivity analysis of variables of interest , like dry mass and trajectory altitude is presented . 

automatic classification of documents by formality
this paper addresses the task of classifying documents into formal or informal style . we studied the main characteristics of each style in order to choose features that allowed us to train classifiers that can distinguish between the two styles . we built our data set by collecting documents for both styles , from different sources . we tested several classification algorithms , namely decision trees , na ï ve bayes , and support vector machines , to choose the classifier that leads to the best classification results . we performed attribute selection in order to determine the contribution of each feature to our model . 

lung cancer prediction using machine learning : a comprehensive approach
the prominent cause of cancer - related mortality throughout the globe is “ lung cancer ” . hence beforehand detection , prediction and diagnosis of lung cancer has become essential as it expedites and simplifies the consequent clinical board . to erect the progress and medication of cancerous conditions machine learning techniques have been utilized because of its accurate outcomes . various types of machine learning algorithms ( ml ) like naive bayes , support vector machine ( svm ) , logistic regression , artificial neural network ( ann ) , have been applied in the healthcare sector for analysis and prognosis of lung cancer . in this review , factors that cause lung cancer and application of ml algorithms are discussed up to date and also draws special attention to their relative strengths and weaknesses . this paper will help the researchers to quickly go through the related literature instead of referring to the many papers . □view less

statistically - driven alignment - based multiword expression identification for technical domains
multiword expressions ( mwes ) are one of the stumbling blocks for more precise natural language processing ( nlp ) systems . particularly , the lack of coverage of mwes in resources can impact negatively on the performance of tasks and applications , and can lead to loss of information or communication errors . this is especially problematic in technical domains , where a significant portion of the vocabulary is composed of mwes . this paper investigates the use of a statistically - driven alignment - based approach to the identification of mwes in technical corpora . we look at the use of several sources of data , including parallel corpora , using english and portuguese data from a corpus of pediatrics , and examining how a second language can provide relevant cues for this tasks . we report results obtained by a combination of statistical measures and linguistic information , and compare these to the reported in the literature . such an approach to the ( semi - ) automatic identification of mwes can considerably speed up lexicographic work , providing a more targeted list of mwe candidates . 

tele - synopsis for biblical research : development of nlp based synoptic software for text analysis as a mediator of educational technology and knowledge discovery
the aim of this research is to develop and test a web based software , code name " tele - synopsis " , that allows us to manipulate lexical data of parallel and variant texts . the basic concept design of this software is founded upon the possibilities of natural language processing ( nlp ) for mediating educational technology and knowledge discovery , or thesaurus creation and conceptual mapping , dual problematic fields whose key concept is always cognition of " frame " . our text framing software has been conceived in the humanities where cultural elements as objects of knowledge are so context dependent that supervised data are constantly fluctuating and even remain hidden . we intend to apply this software to examine various problems derived from differences in interpretation , especially the most controversial theme in biblical research , " the synoptic problem " . 

time and domain specific twitter data mining for plastic ban based on public opinion
every product available in our environment has a shelf - life , but plastic is the only material that is non - degradable . the complex polymer presents in the plastic make it durable and non - degradable . as a result , it found in different forms on the earth for a long time . people have become used to of plastic made product in day to day life like carrying bags , disposable cutlery , food packaging and many more . extensive quantities of plastic waste have accumulated in nature and landfills and have posed an alarming hazard to the environment and have achieved the height of crisis level . nowadays , india is amongst the high rated nations that produce a vast amount of plastic wastes and ranked four throughout the world . though there is a law against the use of plastic in india the consumption of plastic made products is at the highest level as the restriction of usage is completely and effectively not implemented . in this paper , a model is proposed for analyzing the opinion of indian population on the plastic ban with the help of sentiment analysis technique on twitter textual data and train and test a machine classifier based on learning approach with several combinations of datasets achieving 77 . 94 % classification accuracy . the result obtained will help to understand how effective and successful polybags ban scheme will be when entirely implemented in india . 

performance of sentimental analysis by studying and mining social media using parsing technique
the social media networks have evolved quickly and many people often use these services to communicate with others and categorize themselves by sharing their opinions , views and concepts . it is usually dispensed by winnowing the corresponding or interlinked events mentioned on social media websites like twitter , facebook , youtube and pinterest etc . a social media analysis has rapt from being a unidirectional to a bidirectional dialogue i . e . between generations to generation . the rudimentary objective behind such analyses is to visualize the extent of criticality with relevancy criticism or appreciation delineate at intervals the comments , tweets or blogs . this analysis shows how individuals assume , assess , talk and opine concerning completely different problems . this paper centers on the pattern location and examine the feelings communicated in a particular sentence , passage or archive . the opinions and sentiments are extracted from the collected data and it is fairly estimated based on the degree of quality . the rule commitment of this paper is to give a blueprint to the individuals who try to use internet based life scratching and examination utilizing distinctive programming apparatuses either in their investigation or business . in addition , the system design to implement the proposed idea has been discussed . 

precision hiv health app , positive peers , powered by data harnessing , ai , and learning
mobile phone applications provide a new and easy - access platform for delivering tailored human immunodeficiency virus ( hiv ) and sexually transmitted disease ( std ) prevention and care . recent researches have shown that mobile interventions have positive effects in adhesive to care program , antiretroviral therapy ( art ) , self - management of disease , and are also critical in decreasing the hiv pandemic , and stigmatization . in this paper , a precision health app , positive peers ( pp ) , has been developed collaboratively while enabled by data harnessing , artificial intelligence ( al ) , and learning . positive peers is an android / ios - based social media app for providing support and information to a young adult subgroup living with hiv who are in strong need of support and motivation . we apply an intervention approach combined with natural language processing ( nlp ) to help the targeted youth to engage more with the app . using nlp facilitates the flow of information that has a critical role in decreasing the uncertainty of patients by being injected to useful related information . it further improves the interaction of users of the app while providing a compact platform for users to better find the answers to their questions and concerns . the nlp system has been evaluated in an alpha test . 

efficient and large scale pre - training techniques for japanese natural language processing
pre - training in natural language processing greatly affects the accuracy of downstream tasks . however , pre - training is a bottleneck in the ai system development process because it takes a long time to train the neural network model by using large - scale input data . our purpose in this paper is to obtain a highly accurate pre - training model in a short time using a large - scale computation environment . since reducing the time per iteration is difficult even if using large number of computation nodes , it is necessary to reduce the number of iterations . therefore , we focus on the learning efficiency per iteration and choice a dense masked language model ( mlm ) task of pretraining in order to utilize a significant power of large - scale cluster . we implemented bert - xlarge using the dense mlm on megatron - lm and evaluated the improvement of learning time and learning efficiency for a japanese language dataset using 768 gpus on ai bridging cloud infrastructure ( abci ) . our bert - xlarge improves the learning efficiency per iteration 10 times and completes pre - training in 4 . 65 hours . this pre - training takes 4 . 9 months if we use a single gpu . we also evaluated two fine - tunings , jsnli and twitter evaluation analysis , to compare the accuracy of downstream tasks with our berts and other berts . as a result , our bert - 3 . 9b achieved 94 . 30 % accuracy of jsnli , and our bert - xlarge achieved 90 . 63 % accuracy of twitter evaluation analysis . 

twitter in mass emergency : what nlp techniques can contribute
we detail methods for entity span identification and entity class annotation of twitter communications that take place during times of mass emergency . we present our motivation , method and preliminary results . 

ontological approach for knowledge extraction from clinical documents
in clinical nlp ( natural language processing ) , knowledge extraction is a very important task to develop a highly accurate information retrieval system . the various approaches used to develop such systems include rule - based approach , statistical approach , shortest path algorithm or hybrid of these approaches . accuracy and coverage are the most important parameters while comparing different approaches . some methodologies have good accuracy but low coverage and vice - versa . in this paper , our focus is to extract domain relationships , for example to extract the relationship between ` disease ' and ` procedure ' or ` symptom ' and ` disease ' etc . from the clinical documents using three different approaches . these three approaches are i ) statistical ii ) shortest path iii ) shortest path using body system . all three approaches use our in - house existing nlp system to extract entities from the un - structured documents . the statistical approach applies a probabilistic algorithm on clinical documents , whereas the shortest path algorithm uses the ontological knowledge base for the hierarchical relationship between entities . this ontological knowledge base is built upon the curated unified medical language system ( umls ) . for the shortest path using body system approach , we have used the domain relationship as well as hierarchical relationship . the output of these approaches is further validated by a domain expert and this validated relationship is used to enrich our ontological knowledge base . we have presented the details of these approaches one - by - one along with the comparative results of these approaches . we finally go through the analysis of the result and conclude on further work . 

clickbait detection in telugu : overcoming nlp challenges in resource - poor languages using benchmarked techniques
clickbait headlines have become a nudge in social media and news websites . the methods to identify clickbaits are largely being developed for english . there is a need for the same in other languages as well with the increase in the usage of social media platforms in different languages . in this work , we present an annotated clickbait dataset of 112 , 657 headlines that can be used for building an automated clickbait detection system for telugu , a resource - poor language . our contribution in this paper includes ( i ) generation of the latest pre - trained language models , including roberta , albert , and electra trained on a large telugu corpora of 8 , 015 , 588 sentences that we had collected , ( ii ) data analysis and benchmarking the performance of different approaches ranging from hand - crafted features to state - of - the - art models . we show that the pre - trained language models trained on telugu outperform the existing pre - trained models viz . bert - mulingual - case [ 1 ] , xlm - mlm [ 2 ] , and xlm - r [ 3 ] on clickbait task . on a large telugu clickbait dataset of 112 , 657 samples , the light gradient boosted machines ( lgbm ) model achieves an f1 - score of 0 . 94 for clickbait headlines . for non - clickbait headlines , f1 - score of 0 . 93 is obtained which is similar to that of clickbait class . we open - source our dataset , pre - trained models , and code ‘ we show that the pre - trained language models trained on telugu outperform the existing pre - trained models viz . bert - mulingual - case [ 1 ] , xlm - mlm [ 2 ] , and xlm - r [ 3 ] on clickbait task . on a large telugu clickbait dataset of 112 , 657 samples , the light gradient boosted machines ( lgbm ) model achieves an f1 - score of 0 . 94 for clickbait headlines . for non - clickbait headlines , f1 - score of 0 . 93 is obtained which is similar to that of clickbait class . we open - source our dataset , pre - trained models , and code 1 1 https : / / github . com / subbareddy248 / clickbait - resourcesview less

leveraging gene ontology annotations to improve a memory - based language understanding system
this work evaluates how detailed knowledge about proteins can be leveraged for language understanding and disambiguation by opendmap . opendmap is a memory - based language understanding system that uses patterns to identify concepts in text . these patterns match not only lexical elements , such as words , but also semantic elements , such as references to proteins . this work started with an existing pattern set used to extract biological activation events from a corpus of generifs ( sentences or phrases that each describe one of many of the functions of a gene) . this is a challenging task because many distinct activation concepts , in addition to being semantically similar , are described using very similar language . we augment the previous approach with additional semantic knowledge about proteins , in the form of associated gene ontology annotations , and a small corresponding modification to the ontology used by opendmap . by incorporating additional background knowledge we demonstrate that performance can be significantly improved without modifying the pattern set being used . specifically precision is improved by 20 % , at a modest 6 % cost to recall . the additional semantic knowledge allows for more specificity in the ontology used by opendmap , which in turn automatically improves the specificity of the patterns being used to extract knowledge from text reducing false positives by 75 % . 

generating varied narrative probability exercises
this paper presents genpex , a system for automatic generation of narrative probability exercises . generation of exercises in genpex is done in two steps . first , the system creates a specification of a solvable probability problem , based on input from the user ( a researcher or test developer ) who selects a specific question type and a narrative context for the problem . then , a text expressing the probability problem is generated . the user can tune the generated text by setting the values of some linguistic variation parameters . by varying the mathematical content of the exercise , its narrative context and the linguistic parameter settings , many different exercises can be produced . here we focus on the natural language generation part of genpex . after describing how the system works , we briefly present our first evaluation results , and discuss some aspects requiring further investigation . 

text classification using word embeddings
growth in the internet made immense impact on the data generation . most of the data in the world is in textual format . there is need to access and use this data efficiently and easily hence , text classification is widely studied problem in research community . the applications of text classification are also in diverse domains such as news filtering , opinion mining , information retrieval and so on . at core of these applications is machine learning algorithms which need input of fixed length as a vector . bag - of - words is popular model used to represent text in numeric vector format . but it has several disadvantages such as word order is ignored , high dimensionality and sparse representation if vocabulary is large . word embeddings are distributed vector representation of words . these representation inherit semantic notion of similarity also they have shown state - of - art results in many core natural language processing tasks . in presented work , we present text classification using these word embeddings and measure the performance . 

an empirical study on analyzing the distortion detection on osn using nlp & sa in banking institution
now days customer expect the company ' s business how to listen , respond and suggest services through social network . based on the customer engagement of social channels is approximately nine times faster than social network . the customer will expect banks to make use of social channels . approximately two billion people using social media around world , and engage the customer on the social channels in banks . in order to improve the product and service development , marketing , business performance and risk management , banks can take out key insights that will enable on the social media . by analyzing the huge volumes of information available on social media is to deliver quicker and more efficient customer service and customized financial advice . bank needs the customer , to build the social media strategies , in order to drive reliability , revenue and success is all about the customer experience . 

multineedle langmuir probe operation and acute probe current susceptibility to spacecraft potential
norsat - 1 was launched on july 14 , 2017 as a satellite carrying , among other instruments , the multineedle langmuir probe ( m - nlp) , an instrument which , on norsat - 1 , is capable of measuring the ionospheric plasma electron density with the high sampling frequency of 1000 hz . the m - nlp instrument operates by analyzing the current - voltage diagram resulting from the measurements from each individual probe . in principle , the m - nlp operation methodology should be insensitive to spacecraft charging . however , this is not always the case . in this paper , we present an overview of the instrument response to passes into and out of eclipse . when the satellite exits eclipse , we observe a collapse in the collected probe currents . this acute drop is unaccounted for by the theoretical operation of the instrument . we present a statistical analysis of the phenomenon based on several months of norsat - 1 data , and we suggest a plausible reason for the observed drop in the current , namely , spacecraft charging , by solar cell arrays upon eclipse exit . we briefly discuss how satellite orientation and plasma wake affect the current drop . with this paper , we address langmuir probe current susceptibility to spacecraft potential . 

automatically answering repeated questions based on semantic question patterns
an automatic method for answering repeated questions based on semantic question patterns is proposed in this paper . the semantic question pattern used is a generalized representation of a group of questions with both similar structure and relevant semantics . specifically , it consists of semantic annotations ( or constraints ) for the variable components in the pattern and hence enhances the semantic representation and greatly reduces the ambiguity of a question instance when asked by a user using such pattern . the proposed method consists of three major steps : structure processing , similar pattern matching and filtering , question similarity evaluation and answer retrieval . preliminary experiments show that the precision of acquisition of the best answer to a new question can be more than 90 % on our testing dataset . 

fuzzy modeling to conceptual design of mechanisms based on axiomatic design
this paper aims to study new effective modeling and reasoning techniques , concept evaluation approaches in conceptual design of mechanisms . first , the axiomatic design ( ad) , set theory and fuzzy logic are introduced and their relationships are analyzed from the viewpoint of application in conceptual design of mechanisms . second , the problem space and solution space are formalized based on set theory and ad , which can be regarded as the underlying representation of design knowledge . then the automated mapping from functional domain to physical domain is proposed . finally , fuzzy logic based problem formulation and evaluation to reflect the imprecise nature of objectives is presented . the modeling and reasoning technique , concept evaluation approach can improve the efficiency of conceptual design of mechanisms and give new insight to realization of automated conceptual design of mechanisms . 

an approach for protein secondary structure prediction using prediction - based language models
prediction based language models are considered as one of the major concepts in natural language processing which gains knowledge from unstructured text data . extracting insights from sequential data such as biological sequences is an important problem in genomics , proteomics and classifying the secondary structures of protein , helps the researchers in aiding to understand protein functions . this is considered as one of the important preliminaries of drug development . traditional techniques such as sequential models , probabilistic techniques and statistical approaches were widely applied in structure prediction which extracts insights from sequence of amino acid . however , handheld feature extraction becomes a tedious task , which eventually leads to less accuracy . our novel approach creates vectors using word embeddings which is assumed to consider contextual information of amino acids thereby improving the accuracy of secondary structure prediction approach . this is considered as an optimistic solution for secondary structure prediction problem . in this approach a variation of word embeddings - continuous bag of words ( cbow ) method is proposed which retains the sequential information of all amino acid in the protein chain . this vector is used as input features of deep neural network classifier and class labels are classified into helix , sheet , coil . we have tested this nlp based approach on genbank dataset . the infrastructure required for this analysis was leveraged from google colab . 

wisdom media “ caiwa channel ” based on natural language interface agent
based on the breakthroughs in natural language interface agent “ caiwa , ” web content distribution platform “ caiwa channel ” has been built for both pc and smatphone clients . the platform is regarded as wisdom media , since it seeks for providing information precisely what users demand , in addition to removing barriers of user interface bottlenecks . not only recommendations of information to the user based on the past behavior , interest and profile , but also the caiwa channel has built - in evolution mechanism by proactively collecting information about the user in a natural manner via conversation with the user . it can accumulate knowledge for better meeting the user ' s personal requirements , while web contents are organized according to the user ' s interest . caiwa channel is not a mere information search system but a knowledge query system and a human - touch system incorporating emotional reactions . 

detection of fake users in twitter using network representation and nlp
social media platforms like facebook , twitter , instagram , etc . have large user base all around the world that generates huge amounts of data every second . this includes a lot of posts by fake and spam users , typically used by many organizations around the globe to gain a competitive edge over others . in this work , we aim at detecting such user accounts on twitter . we show how to distinguish between genuine and spam accounts in twitter using a novel combination of feature engineering , network representation , and natural language processing techniques . 

bottom up : exploring word emotions for chinese sentence chief sentiment classification
in this paper we demonstrate the effectiveness of employing basic sentiment components for analyzing the chief sentiment of chinese sentence among nine categories of sentiments ( including “ no emotion ” ) . compared to traditional lexicon based methods , our research explores emotion intensities of words and phrases in an eight dimensional sentiment space as features . an emotion matrix kernel is designed to evaluate inner product of these sentiment features for svm classification with o ( n ) time complexity . experimental result shows our method significantly improves performance of sentiment classification . 

session details : large - scale nlp algorithms
no abstract available . 

semantic crawling : an approach based on named entity recognition
law enforcement agencies ( leas ) are increasingly more reliant on information and communication technologies and affected by a society shaped by the internet . the richness and quantity of information available from open sources , if properly gathered and processed , can provide valuable intelligence and help in drawing inferences from existing closed source intelligence . today the intelligence cycle is characterized by manual collection and integration of data . named entity recognition ( ner ) plays a fundamental role in open source intelligence ( osint ) solutions when fighting crime . this paper describes the implementation of a ner - based focused web crawler under the eu fp7 security research project caper ( collaborative information , acquisition , processing , exploitation and reporting for the prevention of organized crime ) . the crawler allows 1 . to look for documents starting from a url until a parametric depth of levels - also specifying a keyword that has to be contained in the page and in the related links - and 2 . to look for a parametric number of documents starting from a keyword ( entrusting the keyword search to one of the principal search engines , thus behaving as a meta - search engine ) . in addition , the crawler is able to retrieve only those documents that contain the information semantically relevant to the query ( in other words : the required keyword with the required sense ) . this is achieved through the use of ner technologies . in this paper we present the caper ner - based semantic crawler , which has been proven to be a suitable tool for focused crawling , allowing leas to drastically reduce data collection and integration efforts . 

scaling up a nlu system from text to dialogue understanding
in this paper we will present work carried out to scale up the system for text understanding called getaruns , and port it to be used in dialogue understanding . we will present the adjustments we made in order to cope with transcribed spoken dialogues like those produced in the icsi berkely project . in a final section we present preliminary evaluation of the system on non - referential pronominals individuation . 

a new collocation extraction method combining multiple association measures
as an important linguistic resource , collocation represents a significant relation between words . automatic collocation extraction is very important for many natural language processing applications , such as word sense disambiguation , machine translation and information retrieval etc . while traditional collocation extraction approaches use only one single statistical measure , they may not be optimal in that they can not take advantage of multiple statistical measures . in this paper , we propose a logistic linear regression model ( llrm ) that combines five classical lexical association measures : x 2 - test , t - test , co - occurrence frequency , log - likelihood ratio and mutual information . experiments show that our approach leads to a significant performance improvement in comparison with individual basic methods in both precision and recall . 

using the web corpus to translate the queries in cross - lingual information retrieval
accurate cross - language information retrieval requires that query terms be correctly translated . in this paper , we propose a new method for web corpus based query translation , which contains two steps : ( 1 ) translation candidate extraction and ( 2 ) translation selection . in translation candidate extraction , we use the search engine to find out the corpus data in the target language on the web by submitting the query in source language . the candidate translations are expected to be both in the title and query - biased summary of searched document . then we find the intersection substrings of different title pairs ( or title - summary pairs ) to fix down the possible translation . in translation selection , we determine the possible translation ( s ) from the candidates by combining substring frequency , inverse translation frequency and top result preferred factor to design the ranking function . experimental results indicate that the top 3 inclusion rate of translation is 75 . 57 % and our method is also very effective in clir task . 

distributed representation for neighborhood - based collaborative filtering
collaborative filtering is widely used in recommender systems . when training data are extremely sparse , neighbor selection methods work ineffectively . to address this issue , this paper proposes a distributed representation model that represents users as low - dimensional vectors for neighbor selection by considering the chronological order of users ' ratings . experiments show that the proposed method outperforms the state - of - the - art methods solving the sparsity problem with regard to precision and ranking quality . 

an interactive nlp tool for signout note preparation
we present an interactive approach to ease the construction , review , and revision of natural language processing models for extractive summarization . to demonstrate our approach , we propose an interactive tool to be used by clinicians to identify text for inclusion in brief patient summaries (" signout notes ") from full - text patient notes . lessons learned from the development and evaluation of this tool will provide insight into the generalized design of interactive nlp systems for wider clinical applications . 

measuring the similarity and relatedness of concepts in the medical domain : ihi 2012 tutorial overview
the ability to quantify the degree to which concepts are similar or related to each other is a key component in many natural language processing ( nlp ) and artificial intelligence ( ai ) applications . for example , in a document search application , it can be very useful to identify text snippets that contain terms that are similar to ( but not identical ) to those provided by a user . this tutorial will introduce the theory behind measures of semantic similarity and relatedness , and show how these can be applied in the medical domain by using freely - - available open - - source software ( http : / / umls - similarity . sourceforge . net ) ( umls :: similarity ) . this software takes advantage of the unified medical language system ( http : / / www . nlm . nih . gov / research / umls / ) ( umls ) , which is maintained by the national library of medicine ( usa ) . the tutorial will also show how to evaluate existing measures with manually - - created reference standards . 

forward and backward optical waves in nonlinear metamaterials : parametric processes
propagation of the fundamental and third harmonic solitary waves in medium characterized by negative refraction index at the frequency of fundamental wave and by positive refractive index at the third harmonic frequency and simultons formation are considered . gap soliton in a novel kind of nonlinear coupler with one channel or both channels fabricated from nonlinear medium having negative refraction index is found . 

las : language agnostic system for question answering
in this article we present a deep learning question answering ( qa ) setting that can work for any natural language . we recognize the problem of low - resource languages , i . e . most languages other than english , which lack appropriately sized datasets or cutting - edge nlp tools . to address this problem , we have designed and implemented a qa dataset and system that are independent from language use ; specifically , we test our solution on the polish language which is both low - resource and grammatically complex . both these features make the task of qa significantly harder . to the best of our knowledge , this is the first attempt to train a deep learning qa system in a language - agnostic setting . 

a pre - trained clinical language model for acute kidney injury
pre - trained contextual language models such as bert have dramatically improved performances for many nlp tasks recently . however , few have explored bert on specific medical domain tasks such as early prediction for acute kidney injury ( aki ) . since much of the clinical information is contained in clinical notes that are largely unstructured text , in this paper , we present an aki domain - specific pre - trained language model based on bert ( aki - bert ) that could be used to mine the clinical notes for aki early prediction . our experiments on mimic - iii dataset demonstrate that aki - bert can yield performance improvements for aki early prediction . 

two - phase base noun phrase alignment in chinese - english parallel corpora
a two - phase approach of automatically aligning bilingual base noun phrases from sentence - aligned chinese - english parallel corpus is proposed in this paper . we conduct alignment in two phases : one deals with high - frequency base noun phrases by statistical co - occurrence information between parallel corpus , and the other deals with low - frequency base noun phrases using the bilingual lexical information and dice coefficient similarity metrics . this can be reasonably considered to acquire higher recall without degrading the precision on the whole . furthermore , our approach can escape from complex chinese parsing problems and don ' t need to recognize chinese base noun phrases accurately before the aligning process . also , it can also relieve , to some extent , the serious impacts of error spread which may result from the unstable and impractical chinese base noun phrases extraction tools . another , dealing with high frequency noun phrases with statistical information also can realize the recognition of some non - compositional phrase pairs , which is difficult for pure syntax - based or lexicon - based systems to handle . 

robust continuous speech recognition technology program summary
the major objective of this program is to develop and demonstrate robust , high - performance continuous speech recognition ( csr ) techniques and systems focused on applications in spoken language systems ( sls ) . a key supporting objective is to develop techniques for integration of csr and natural language processing ( nlp ) systems in sls applications . the csr techniques are based on a continuous - observation hidden markov model ( hmm ) approach , using tied gaussian mixtures to model the speech parameters . a stack - decoder control structure is being developed and utilized , both for efficient large - vocabulary recognition , and to facilitate integration of csr and nlp systems . 

combining word sense and usage for modeling frame semantics
models of lexical semantics are core paradigms in most nlp applications , such as dialogue , information extraction and document understanding . unfortunately , the coverage of currently available resources ( e . g . framenet ) is still unsatisfactory . this paper presents a largely applicable approach for extending frame semantic resources , combining word sense information derived from wordnet and corpus - based distributional information . we report a large scale evaluation over the english framenet , and results on extending framenet to the italian language , as the basis of the development of a full framenet for italian . 

sentiment analysis of product reviews : a review
now a day ' s internet is the most valuable source of learning , getting ideas , reviews for a product or a service . everyday millions of reviews are generated in the internet about a product , person or a place . because of their huge number and size it is very difficult to handle and understand such reviews . sentiment analysis is such a research area which understands and extracts the opinion from the given review and the analysis process includes natural language processing ( nlp ) , computational linguistics , text analytics and classifying the polarity of the opinion . in the field of sentiment analysis there are many algorithms exist to tackle nlp problems . each algorithm is used by several applications . in this paper we have shown the taxonomy of various sentiment analysis methods . this paper also shows that support vector machine ( svm ) gives high accuracy compared to na ï ve bayes and maximum entropy methods . 

an extensible framework for efficient document management using rdf and owl
in this paper , we describe an integrated approach towards dealing with various semantic and structural issues associated with document management . we provide motivations for using xml , rdf and owl in building a seamless architecture to serve not only as a document exchange service but also to enable higher level services such as annotations , metadata access and querying . the key idea is to manifest differential treatments for the actual document structure , semantic content of the document and ontological document organization . the deployment of this architecture in the proteus project provides an industrial setting for evaluation and further specification . 

building a chinese dependency graphbank
how to represent the structure of a sentence is a key issue in linguistic and nlp fields . dependency grammar ( dg ) has been widely used as it directly describes the relations between words in a sentence . however , it always follows the tree structure that does not fit the argument sharing phenomenon . on the other hand , the semantic role labeling ( srl ) annotation does not give a full structure for a sentence . in this paper , we combine dg and srl together to build a unified dependency graph for chinese sentences . then we introduce the tagging sets for dependency graph as well as the toolkit designed for manual annotation . annotation problems are discussed at the end . 

semantics of portions and partitive nouns for nlp
this paper describes a system of representation of nouns denoting portions , segments and relative quantities of entities , in order to account for this case of part - whole relationship . the semantics of both constructions denoting portions and nouns used to build them are discussed and eventually formalised in a unification - based formalism ( lkb - lrl ) in terms of pustejovsky ' s theory of qualia and jackendoff ' s conceptual semantics . 

blexisma2 : a distributed agent framework for constructing a semantic lexical database based on conceptual vectors
in the framework of meaning representation in natural language processing ( nlp ) , we aim to develop a system that can be used for heterogeneous applications such as machine translation , information retrieval or lexical access . this system is based on six hypotheses which concern meaning representation and acquisition . in this paper , we discuss the related hypotheses that motivate the construction of a such system and how these hypotheses , together with nlp software engineering concerns , led us to conceive a distributed multi - agent system for our goals . we present blexisma2 , a distributed multi - agent system for nlp , its conceptual properties , and an example of inter - agent collaboration . the system is currently being tested on a grid computing environment . 

answering definitional question by dependency - based knowledge
most current systems apply flat pattern and flat centroid words , which are extracted only by relative position to question target , to identify definition sentences . in contrast to the flat knowledge , we propose dependency - based knowledge , including dependency pattern and dependency centroid word , which are extracted by dependency relation to question target . specifically , we use the improved ultraconservative online algorithm , binary margin infused relaxed algorithm ( mira) , to estimate the weight of each dependency knowledge for the task of candidate sentences ranking . we demonstrate that the dependency - based knowledge is more effective than the flat knowledge . meanwhile , we also show that our definitional question answering system outperforms the state - of - the - art systems on recent trec data . 

sbert - wk : a sentence embedding method by dissecting bert - based word models
sentence embedding is an important research topic in natural language processing ( nlp ) since it can transfer knowledge to downstream tasks . meanwhile , a contextualized word representation , called bert , achieves the state - of - the - art performance in quite a few nlp tasks . yet , it is an open problem to generate a high quality sentence representation from bert - based word models . it was shown in previous study that different layers of bert capture different linguistic properties . this allows us to fuse information across layers to find better sentence representations . in this work , we study the layer - wise pattern of the word representation of deep contextualized models . then , we propose a new sentence embedding method by dissecting bert - based word models through geometric analysis of the space spanned by the word representation . it is called the sbert - wk method . no further training is required in sbert - wk . we evaluate sbert - wk on semantic textual similarity and downstream supervised tasks . furthermore , ten sentence - level probing tasks are presented for detailed linguistic analysis . experiments show that sbert - wk achieves the state - of - the - art performance . our codes are publicly available . 

a web application using rdf / rdfs for metadata navigation
this paper describes using rdf / rdfs / xml to create and navigate a metadata model of relationships among entities in text . the metadata we create is roughly an order of magnitude smaller than the content being modeled , it provides the end - user with context sensitive information about the hyperlinked entities in focus . these entities at the core of the model are originally found and resolved using a combination of information extraction and record linkage techniques . the rdf / rdfs metadata model is then used to " look ahead " and navigate to related information . an rdf aware front - end web application streamlines the presentation of information to the end user . 

picking them up and figuring them out : verb - particle constructions , noise and idiomaticity
this paper investigates , in a first stage , some methods for the automatic acquisition of verb - particle constructions ( vpcs ) taking into account their statistical properties and some regular patterns found in productive combinations of verbs and particles . given the limited coverage provided by lexical resources , such as dictionaries , and the constantly growing number of vpcs , possible ways of automatically identifying them are crucial for any nlp task that requires some degree of semantic interpretation . in a second stage we also study whether the combination of statistical and linguistic properties can provide some indication of the degree of idiomaticity of a given vpc . the results obtained show that such combination can successfully be used to detect vpcs and distinguish idiomatic from compositional cases . 

profit : prolog with features , inheritance and templates
profit is an extension of standard prolog with features , inheritance and templates . profit allows the programmer or grammar developer to declare an inheritance hierarchy , features and templates . sorted feature terms can be used in profit programs together with prolog terms to provide a clearer description language for linguistic structures . profit compiles all sorted feature terms into a prolog term representation , so that the built - in prolog term unification can be used for the unification of sorted feature structures , and no special unification algorithm is needed . profit programs are compiled into prolog programs , so that no meta - interpreter is needed for their execution . profit thus provides a direct step from grammars developed with sorted feature terms to prolog programs usable for practical nlp systems . 

taskgenies : automatically providing action plans helps people complete tasks
people complete tasks more quickly when they have concrete plans . however , they often fail to create such action plans . ( how ) can systems provide these concrete steps automatically ? this article demonstrates that these benefits can also be realized when these plans are created by others or reused from similar tasks . four experiments test these approaches , finding that people indeed complete more tasks when they receive externally - created action plans . to automatically provide plans , we introduce the genies workflow that combines benefits of crowd wisdom , collaborative refinement , and automation . we demonstrate and evaluate this approach through the taskgenies system , and introduce an nlp similarity algorithm for reusing plans . we demonstrate that it is possible for people to create action plans for others , and we show that it can be cost effective . 

bandit multiclass linear classification for the group linear separable case
we consider the online multiclass linear classification under the bandit feedback setting . beygelzimer , pal , szorenyi , thiruvenkatachari , wei , and zhang [ icml ' 19 ] considered two notions of linear separability , weak and strong linear separability . when examples are strongly linearly separable with margin γ , they presented an algorithm based on multiclass perceptron with mistake bound o ( k /γ 2 ) , where k is the number of classes . they employed rational kernel to deal with examples under the weakly linearly separable condition , and obtained the mistake bound of min ( k · 2 õ ( k log2 ( 1 /γ ) ) , k · 2 õ ( √1 /γ log k ) ) . in this paper , we refine the notion of weak linear separability to support the notion of class grouping , called group weak linear separable condition . this situation may arise from the fact that class structures contain inherent grouping . we show that under this condition , we can also use the rational kernel and obtain the mistake bound of k · 2 õ ( √1 /γ log l ) ) , where l ≤ k represents the number of groups . 

distributed asynchronous online learning for natural language processing
recent speed - ups for training large - scale models like those found in statistical nlp exploit distributed computing ( either on multicore or " cloud " architectures ) and rapidly converging online learning algorithms . here we aim to combine the two . we focus on distributed , " mini - batch " learners that make frequent updates asynchronously ( nedic et al . , 2001 ; langford et al . , 2009 ) . we generalize existing asynchronous algorithms and experiment extensively with structured prediction problems from nlp , including discriminative , unsupervised , and non - convex learning scenarios . our results show asynchronous learning can provide substantial speedups compared to distributed and single - processor mini - batch algorithms with no signs of error arising from the approximate nature of the technique . 

predicting economic development using geolocated wikipedia articles
progress on the un sustainable development goals ( sdgs ) is hampered by a persistent lack of data regarding key social , environmental , and economic indicators , particularly in developing countries . for example , data on poverty - the first of seventeen sdgs - is both spatially sparse and infrequently collected in sub - saharan africa due to the high cost of surveys . here we propose a novel method for estimating socioeconomic indicators using open - source , geolocated textual information from wikipedia articles . we demonstrate that modern nlp techniques can be used to predict community - level asset wealth and education outcomes using nearby geolocated wikipedia articles . when paired with nightlights satellite imagery , our method outperforms all previously published benchmarks for this prediction task , indicating the potential of wikipedia to inform both research in the social sciences and future policy decisions . 

lexicographic environments in natural language processing ( nlp ) 
in this paper , a literature review is presented in reference to the most important lexicographical environments that have been developed in the last decades , with the aim of utilizing them in various knowledge management applications , in the wider area of ​​ the semantic web ( sw ) . this paper focuses on the semantic networks of lexical information , in order to highlight their value but also the urgent need to design and implement an electronic conceptual dictionary with ontological structure for the modern greek language , according to international dictionary standards . 

analyzing the impact of natural language processing over feature location in models
feature location ( fl ) is a common task in the software engineering field , specially in maintenance and evolution of software products . the results of fl depend in a great manner in the style in which feature descriptions and software artifacts are written . therefore , natural language processing ( nlp ) techniques are used to process them . through this paper , we analyze the influence of the most common nlp techniques over fl in conceptual models through latent semantic indexing , and the influence of human participation when embedding domain knowledge in the process . we evaluated the techniques in a real - world industrial case study in the rolling stocks domain . 

deep learning for automated sentiment analysis of social media
the spread of information on facebook and twitter is much more efficient than on traditional social media platforms . for word - of - mouth ( wom ) marketing , social media have become a rich information source for companies or scholars to design models to examine this repository and mine useful insights for marketing strategies . however , social media language is relatively short and contains special words and symbols . most natural language processing ( nlp ) methods focus on processing formal sentences and are not well - suited to such short messages . in this study we propose a novel sentiment analysis framework based on deep learning models to extract sentiment from social media . we collect data from which we compile a dataset . after processing these special terms , we seek to establish a semantic dataset for further research . the extracted information will be useful for many future applications . the experimental data have been obtained by crawling several social media platforms . 

a smart dialogue - competent monitoring framework supporting people in rehabilitation
in this paper , we present work in progress on the development of a smart monitoring framework to support people with motor disabilities and their caregivers in clinical and non - clinical rehabilitation and care environments . the innovation of the platforms lies in the combination of smart monitoring solutions , such as activity recognition and lifestyle tracking , with an intelligent virtual agent that aims to empower and motivate people in need through personalized feedback and responses , as well as to assist caregivers and clinicians to easily collect information about the patients . the proposed system exploits and combines state - of - the - art technologies in speech recognition and synthesis , knowledge representation and reasoning , dialogue management and sensor data analysis , infusing clinical knowledge and patient history . aiming for a practical , acceptable solution , the proposed system takes into account aspects of integration , security and privacy . 

experiments with artificially generated noise for cleansing noisy text
recent works show that the problem of noisy text normalization can be treated as a machine translation ( mt ) problem with convincing results . there have been supervised mt approaches which use noisy - regular parallel data for training an mt model , as well as unsupervised models which learn the translation probabilities in alternative ways and try to mimic the mt - based approach . while the supervised approaches suffer from data annotation and domain adaptation difficulties , the unsupervised models lack a holistic approach catering to all types of noise . in this paper , we propose an algorithm to artificially generate noisy text in a controlled way , from any regular english text . we see this approach as an alternative to the unsupervised approaches while getting the advantages of a parallel corpus based mt approach . we generate parallel noisy text from two widely used regular english datasets and test the mt - based approach for text normalization . semi - supervised approaches were also tried to explore different ways of improving the parallel corpus ( manually annotated ) based mt approach by using the generated noisy text . an extensive analysis based on comparison of our approaches with both the supervised as well as unsupervised approaches is presented . 

multilevel classification of pakistani news using machine learning
the availability of innumerable sources of online news has benefitted the masses as they have opportunity to gather news from a diverse set of sources . however , classification of this huge data being generated on regular basis has never been a simple task . this textual information can be invaluable only when it is processed to maximize its usefulness which is possible with automated text classification . natural language processing ( nlp ) and machine learning techniques have been extensively applied in this particular domain to address this challenge . text classification is helpful in several scenarios such as product mining , emotions or sentiment analysis , etc . news classification is one of its applications through which content of news is processed and analyzed to assign predefined label ( s ) . this research is focused on classification of pakistani news obtained from dataset available on open data pakistan . we have applied various machine learning algorithms including logistic regression , random forest , support vector machine , and na ï ve bayes for first - level classification and logistic regression for multilevel classification . comparative analysis of these algorithms is also presented . we achieved a maximum of 97 . 8 % accuracy through support vector machine in single - level classification and 83 % through logistic regression in multilevel text classification . 

deep sentence denoising beyond grammatical error correction
clear and efficient communication requires more than grammatical correctness to ensure fluency and semantic correctness , especially for non - native speakers . thus , we propose a new task – sentence denoising , to go beyond grammatical error correction ( gec) . we define a rich and linguistics - inspired noise taxonomy consisting of 13 types of noise , and categorize them into vagueness , redundancy , and incoherence . we then generate and study 4 types of noise out of the 13 because they serve as building blocks . methods are proposed to inject targeted noise into sentences for building datasets . we publish them and give benchmarks for denoising both individual noise and compound noise . finally , an efficient training approach is designed for denoising combinations of noise . 

rethink e - commerce search
the quality of the search experience on an e - commerce site plays a critical role in customer conversion and the growth of the e - commerce business . in this talk , i will discuss the current status and challenges of product search . in particular , i will highlight the significant amount of effort it takes to create a high - quality product search engine using classical information retrieval methods . then , i will discuss how recent advances in nlp and deep learning , especially the advent of large pre - trained language models , may change the status quo . while embedding - based retrieval has the potential to improve classical information retrieval methods , creating a machine learning - based , end - to - end system for general - purpose , web search is still extremely difficult . nevertheless , i will argue that product search for e - commerce may prove to be an area where deep learning can create the first disruption to classical information retrieval systems . 

analysis of trends in scientific publications by an nlp toolkit : a case study in software development methods for enhanced living environment
as the number of published scientific articles increases , the analysis of trends and state - of - the - art in software engineering is becoming very time - consuming and laborious task . to address the ever - growing demands for systematic literatures review techniques , rapid review and scoping reviews techniques have emerged . we used an nlp powered tool , which employs the prisma surveying methodology , to automate most of the review processes . we used it to automatically review relevant articles indexed in ieee xplore , pubmed and springer digital libraries on the topic “ software development for enhanced living environments and ambient assisted living” . the relevant articles identified by the nlp toolkit contained up to 21 properties clustered into 3 logical groups . we discovered that software development for enhanced and assisted living environments attracted an increased attention from the scientific communities over the last 10 years and showed several trends in the specific research topics that fall into this scope . the research uncovered that iterative software methodology had been the most attractive research topic in the field . despite the enormous empirical evidence on application and success stories of agile development methodologies in many software development engineering , it received a little attention from the scientific community in the software development for enhanced and assisted living environments . the nlp toolkit identified the most relevant articles that contained the defined properties in the search . hence , it significantly reduced the manual work , while also generating informative tables , charts and graphs . 

an intelligent representing system of search results
this paper is mainly about the method of an intelligent representing system of search results with the ability to cluster the search results from famous search engines into several classes . firstly , we combine the n - gram statistical model with syntactic method to conduct word segmentation after preprocessing ; and then some feature words are selected by their part of speech and frequency ; finally , a unique clustering method is proposed in this paper , which is based on a target function by calculating term covers and their overlaps . in addition to the clustering ability , our intelligent system is capable of labelling the main idea of the cluster results with a chinese word , which is superior to many other text categorization systems . 

codebook - based training beam sequence design for millimeter - wave tracking systems
in this paper , we propose a codebook - based beam tracking strategy for mobile millimeter - wave ( mmwave ) systems , where the temporal variation of the angle of departure ( aod ) is considered . a closed - form upper bound of the average tracking error probability ( atep ) is derived and further optimized . we first consider a slow - varying scenario where narrow training beams implemented by single radio - frequency ( rf ) chain are employed . we show that the atep can be reduced by optimizing the power allocation strategy over these training beams , which is formulated and transformed into a second - order cone programming . the fast - varying scenario is further considered where the use of narrow training beams becomes inefficient due to the rapid variations of aod . in order to reduce the training time , multiple rf chains generating wide beams are employed to track the aod ' s variations , and the associated beam pattern design problem is shown to be a 0 - 1 nonlinear optimization problem ( nlp ) . a sequential quadratic programming method is used to solve this binary nlp . to reduce the complexity , a progressive edge - growth algorithm is further introduced by associating the binary nlp with a bipartite graph . numerical results demonstrate significant gains of the proposed beam tracking strategy over existing benchmarks for both scenarios . 

towards an nlp - based topic characterization of social relations
the unstructured text content of online communication artifacts is a salient source of information about social relationships . we investigate the utility of keywords extracted from the message body as a representation of the relationship ' s characteristics , which are reflected by the conversation topics to a certain extent . keyword extraction is performed using standard natural language processing methods . communication data and human assessments of the extracted keywords are obtained from facebook users via a custom application . the overall positive quality assessment provides evidence that the keywords indeed convey relevant information about the relationship . this kind of representation may be of value for various computational tasks from the domain of social computing . 

a foreign vocabulary learning aid for the networked world of tomorrow : the learn project
further increases in society ' s reliance on computing appear inevitable . an important role of computing will be in customizing textual information from the flood of available text to meet the unique needs of individuals . information customization services will likely run on relatively powerful computers dedicated to individual users , as exemplified by work - stations . the learn project is a computer aided language learning ( call ) system that serves ( 1 ) as a demonstration of an information customization service likely to run on the personal workstations of tomorrow ' s electronically networked world , and ( 2 ) as a pragmatically oriented foundation for research on semantically based word sense disambiguation . 

deep learning for nlp ( without magic ) 
machine learning is everywhere in today ' s nlp , but by and large machine learning amounts to numerical optimization of weights for human designed representations and features . the goal of deep learning is to explore how computers can take advantage of data to develop features and representations appropriate for complex interpretation tasks . this tutorial aims to cover the basic motivation , ideas , models and learning algorithms in deep learning for natural language processing . recently , these methods have been shown to perform very well on various nlp tasks such as language modeling , pos tagging , named entity recognition , sentiment analysis and paraphrase detection , among others . the most attractive quality of these techniques is that they can perform well without any external hand - designed resources or time - intensive feature engineering . despite these advantages , many researchers in nlp are not familiar with these methods . our focus is on insight and understanding , using graphical illustrations and simple , intuitive derivations . the goal of the tutorial is to make the inner workings of these techniques transparent , intuitive and their results interpretable , rather than black boxes labeled " magic here " . the first part of the tutorial presents the basics of neural networks , neural word vectors , several simple models based on local windows and the math and algorithms of training via backpropagation . in this section applications include language modeling and pos tagging . in the second section we present recursive neural networks which can learn structured tree outputs as well as vector representations for phrases and sentences . we cover both equations as well as applications . we show how training can be achieved by a modified version of the backpropagation algorithm introduced before . these modifications allow the algorithm to work on tree structures . applications include sentiment analysis and paraphrase detection . we also draw connections to recent work in semantic compositionality in vector spaces . the principle goal , again , is to make these methods appear intuitive and interpretable rather than mathematically confusing . by this point in the tutorial , the audience members should have a clear understanding of how to build a deep learning system for word - , sentence - and document - level tasks . the last part of the tutorial gives a general overview of the different applications of deep learning in nlp , including bag of words models . we will provide a discussion of nlp - oriented issues in modeling , interpretation , representational power , and optimization . 

edu - bot : an ai based smart chatbot for knowledge management system
the pandemic has brought in a lot of changes in various aspects of human life . every sector has been affected by this and so is the educational sector . organizing and accessing of documents was one of the challenges that students faced during the pandemic . to overcome this problem , we propose an ai system that is equipped with a chatbot . in this study , we develop a smart knowledge management system that authenticates the user via face recognition , then a chatbot helps the users in searching and accessing the required document easily . face recognition is implemented using libraries such as opencv , tensorflow and face _ recognition . face _ recognition library provided the highest accuracy so it was used in the smart knowledge management system . a chatbot was implemented to search and access the study materials such as notes , textbooks and question banks from a particular destined folder through the hyperlinks created . to make the system user friendly , gui was designed using tkinter library . this ai based smart knowledge management system provides access to the required document very effectively and effortlessly . 

a comprehensive survey on movie recommendation systems
internet technology has occupied an important part of human lives . users often face the problem of the available excessive information . recommandation system ( rs ) are deployed to help users cope up with the information explosion . rs is mostly used in digital entertainment , such as netflix , prime video , and imdb , and e - commerce portals such as amazon , flipkart , and ebay . the two traditional methods namely , collaborative filtering ( cf ) and content - based approaches consist of few limitations individually . however , any hybrid system , which utilizes the advantage of both the systems to leverage better results . some fundamental issues faced by movie recommendation systems such as scalability , cold start problem , data sparsity and practical usage feedback and verification based on real implementation are still neglected . other issues that require significant research attention are accuracy and time complexity problem , which could make rs , a bad candidate for real - world recommendation systems . this literature survey aims to consolidate and structurally categorize all the major drawbacks present in the most common and popular commercial movie recommendation systems . 

enjoy the paper : lexical semantics via lexicology
current research being undertaken at both cambridge and ibm is aimed at the construction of substantial lexicons containing lexical semantic information capable of use in automated natural language processing ( nlp ) applications . this work extends previous research on the semi - automatic extraction of lexical information from machine - readable versions of conventional dictionaries ( mrds ) ( see e . g . the papers and references in boguraev & briscoe , 1989 ; walker et al . , 1988 ) . the motivation for this and previous research using mrds is that entirely manual development of lexicons for practical nlp applications is infeasible , given the labour - intensive nature of lexicography ( e . g . atkins , 1988 ) and the resources likely to be allocated to nlp in the foreseeable future . in this paper , we motivate a particular approach to lexical semantics , briefly demonstrate its computational tractability , and explore the possibility of extracting the lexical information this approach requires from mrds and , to some extent , textual corpora . 

paradigmatic clustering for nlp
how can we retrieve meaningful information from a large and sparse graph ? . traditional approaches focus on generic clustering techniques and discovering dense cumulus in a network graph , however , they tend to omit interesting patterns such as the paradigmatic relations . in this paper , we propose a novel graph clustering technique modelling the relations of a node using the paradigmatic analysis . we exploit node ' s relations to extract its existing sets of signifiers . the newly found clusters represent a different view of a graph , which provides interesting insights into the structure of a sparse network graph . our proposed algorithm pac ( paradigmatic clustering ) for clustering graphs uses paradigmatic analysis supported by a asymmetric similarity , in contrast to traditional graph clustering methods , our algorithm yields worthy results in tasks of word - sense disambiguation . in addition we propose a novel paradigmatic similarity measure . extensive experiments and empirical analysis are used to evaluate our algorithm on synthetic and real data . 

knowledge extraction and applications utilizing context data in knowledge graphs
context is widely considered for nlp and knowledge discovery since it highly influences the exact meaning of natural language . the scientific challenge is not only to extract such context data , but also to store this data for further nlp approaches . here , we propose a multiple step knowledge graphbased approach to utilize context data for nlp and knowledge expression and extraction . we introduce the graph - theoretic foundation for a general context concept within semantic networks and show a proof - of - concept - based on biomedical literature and text mining . we discuss the impact of this novel approach on text analysis , various forms of text recognition and knowledge extraction and retrieval . 

using entropy and mutual information to extract threat actions from cyber threat intelligence
with the rapid growth of the cyber attacks , cyber threat intelligence ( cti ) sharing becomes essential for providing advance threat notice and enabling timely response to cyber attacks . our goal in this paper is to develop an approach to extract low - level cyber threat actions from publicly available cti sources in an automated manner to enable timely defense decision making . specifically , we innovatively and successfully used the metrics of entropy and mutual information from information theory to analyze the text in the cybersecurity domain . combined with some basic nlp techniques , our framework , called actionminer has achieved higher precision and recall than the state - of - the - art stanford typed dependency parser , which usually works well in general english but not cybersecurity texts . 

identifying non - natural language artifacts in bug reports
bug reports are a popular target for natural language processing ( nlp ) . however , bug reports often contain artifacts such as code snippets , log outputs and stack traces . these artifacts not only inflate the bug reports with noise , but often constitute a real problem for the nlp approach at hand and have to be removed . in this paper , we present a machine learning based approach to classify content into natural language and artifacts at line level implemented in python . we show how data from github issue trackers can be used for automated training set generation , and present a custom preprocessing approach for bug reports . our model scores at 0 . 95 roc - auc and 0 . 93 f1 against our manually annotated validation set , and classifies 10k lines in 0 . 72 seconds . we cross evaluated our model against a foreign dataset and a foreign r model for the same task . the python implementation of our model and our datasets are made publicly available under an open source license . 

covshorts : news summarization application based on deep nlp transformers for sars - cov - 2
amidst the grueling sars - cov - 2 pandemic which has affected the lives of people across the world , the accelerating growth in covid - 19 related news articles is making it difficult for the general public to stay up - to - date with all the information . news articles are a crucial medium to convey coronavirus - related information across the world to the public . short summaries of news articles can assist the public in grasping a gist of an entire article without having to read it fully . with the evolution of deep learning in natural language processing ( nlp ) , we exploited the power of recent advances in pre - trained and transformer - based nlp models to perform text summarization over the covid - 19 public media dataset . for this , we analyzed and compared the results of bert , gpt - 2 , xlnet , bart , and t5 . the first three models are among the most popular extractive summarization models and the last two are abstractive summarization models . we evaluated the results of our experiments using rouge scores ( rouge - 2 and rouge - l ) and found that bert , a transformer autoencoder , outperforms the other models under consideration in sars - cov - 2 news summarization . thus , we leveraged bert in our web application “ covshorts ” to summarize covid - 19 articles . further , we visually analyzed the dataset to depict the most used words in covid - 19 news articles using word cloud to validate the accuracy of the summarization task . covshorts will serve the public by helping them in gaining brief , concise , and to - the - point summaries quickly . 

towards the reuse of lingware systems : a proposed approach with a practical experiment
we are going to present in this document a generic approach for lingware systems reuse . this approach is based on reverse engineering technique in order to wrap up an existing lingware system with web services . this approach permits the reuse of lingware systems regardless of programming languages , development environments and the structures of linguistic resources . in order to preserve the interoperability between the reused lingware systems , the proposed approach performs the unification and the standardization of exchanged linguistic data using the natural language processing ( nlp ) standards and consensus . doing so , we facilitate the integration and the composition of lingware services in order to create a new application that treats several linguistic levels . in order to consolidate the given approach , we developed the lingware reuse environment ( lire ) . a practical experiment was carried out using lire environment on an automatic application summary of arabic texts . 

unsupervised name ambiguity resolution using a generative model
resolving ambiguity associated with names found on the web , wikipedia or medical texts is a very challenging task , which has been of great interest to the research community . we propose a novel approach to disambiguating names using latent dirichlet allocation , where the learned topics represent the underlying senses of the ambiguous name . we conduct a detailed evaluation on multiple data sets containing ambiguous person , location and organization names and for multiple languages such as english , spanish , romanian and bulgarian . we conduct comparative studies with existing approaches and show a substantial improvement of 15 to 35 % in task accuracy . 

fwlsa - score : french and wolof lexicon - based for sentiment analysis
with the advent of internet , people actively express their opinions about products , services , events , political parties and other one in social media , blogs , and website comments . the amount of research work on sentiment analysis is growing explosively . however , the majority of research efforts are devoted to english language data , while a great share of information is available in other languages . it is a challenging task to identify sentiment polarity of reviews written in both wolof and french languages because theirs spelling are usually incorrect or non - uniform . in this paper , we propose a novel framework that contains ( i ) an extended french lexicon [ 1 ] with a new words and expressions currently used in both languages ; and ( ii ) a sentiment scoring algorithm that uses string ( word ) similarity algorithm to address the spelling problem . our algorithm classifies reviews as positive or negative based on the polarity of the words or expressions . our experimental results on a real corpus demonstrated the effectiveness of our proposal . 

an open distributed architecture for reuse and integration of heterogeneous nlp components
the shift from computational linguistics to language engineering is indicative of new trends in nlp . this paper reviews two nlp engineering problems : reuse and integration , while relating these concerns to the larger context of applied nlp . it presents a software architecture which is geared to support the development of a variety of large - scale nlp applications : information retrieval , corpus processing , multilingual mt , and integration of speech components . 

resource - size matters : improving neural named entity recognition with optimized large corpora
this study improves the performance of neural named entity recognition by a margin of up to 11 % in terms of f - score on the example of a low - resource language like german , thereby outperforming existing baselines and establishing a new state - of - the - art on each single open - source dataset ( conll 2003 , germeval 2014 and t ü bingen treebank 2018 ) . rather than designing deeper and wider hybrid neural architectures , we gather all available resources and perform a detailed optimization and grammar - dependent morphological processing consisting of lemmatization and part - of - speech tagging prior to exposing the raw data to any training process . we test our approach in a threefold monolingual experimental setup of a ) single , b ) joint , and c ) optimized training and shed light on the dependency of downstream - tasks on the size of corpora used to compute word embeddings . 

twitter vigilance : a multi - user platform for cross - domain twitter data analytics , nlp and sentiment analysis
the growth and diffusion of online social media have been enormously increased in recent years , as well as the research and commercial interests toward these rising sources of information as a direct public expression of the communities . moreover , the depth and the quality of data that can be harvested by monitoring and analysis tools have evolved significantly . in particular , twitter has revealed to be one of the most widespread microblogging services for instantly publishing and sharing opinions , feedbacks , ratings etc . , contributing in the development of the emerging role of users as sensors . however , due to the huge amount of data to be collected and analyzed and limitations on data access imposed by twitter public apis , more efficient requirements are needed for analytics tools , both in terms of data ingestion and processing , as well as for the computation of analysis metrics , to be provided for deeper statistic insights and further investigations . in this paper , the twitter vigilance platform is presented , realized by the disit lab at university of florence . twitter vigilance has been designed as a cross - domain , multi - user tool for collecting and analyzing twitter data , providing aggregated metrics based on the volume of tweets and retweets , users ' influence network , natural language processing and sentiment analysis of textual content . the proposed architecture has been validated against a dataset of about 270 million tweets showing a high efficiency in recovering twitter data . for this reason it has been adopted by a number of researchers as a study platform for social media analysis , early warning , etc . 

efficient domain module from electronic textbooks using machine learning method
in modern days technologies supported learning system ( tsls ) , intelligent tutoring systems ( its ) , adaptive hypermedia systems ( ahs ) , learning management systems ( lms ) , and system are used . moodle and other educational motivation tools are used in institution . dom - sortze is a technique where many educational nlp are implemented . the electronic book are taken as input and from these book thee searching is done . the keywords are searched in the whole e - book instead of using moodle , and other educational motivation tools , dom - sortze is implemented and being used for the search purpose . 

sun - spot : an rgb - d dataset with spatial referring expressions
we introduce a new dataset , sun - spot , for localizing objects using spatial referring expressions ( res ) . sun - spot is the only re dataset which uses rgb - d images . it also contains a greater average number of spatial prepositions and more cluttered scenes than previous re datasets . using a simple baseline , we show that including a depth channel in re models can improve performance on both generation and comprehension . 

employing natural language processing to analyse grammatical error in a simple japanese sentence
this paper presents our research in building an error analysis system that is used to help the language learning process , especially in writing a japanese sentence with correct grammar . the grammar of the japanese input is evaluated by the system using several natural language processing ( nlp ) tools . type of grammatical errors that can be analysed by the system are writing error , particle usage error , sentence formation error , predicate usage error , and relevance error . the nlp tools used are morphemes analyzer and syntax analyzer , with some other specific modules to process the nlp tools result . the system evaluation is divided into sentence ' s completeness evaluation , tree structure evaluation , and grammatical function evaluation . given a japanese sentence input , the system detects the errors , analyses the cause of the errors , and finally gives instructions to the user about how to correct the sentence . by using 264 input sentences collected for the system evaluation , system achieved an accuracy of 80 . 30 % in giving the fully relevant responses . the user experience ' s evaluation involves 12 respondents and gives the average score of 73 . 09 out of 100 . 

large - scale news classification using bert language model : spark nlp approach
the rise of big data analytics on top of nlp increasing the computational burden for text processing at scale . the problems faced in nlp are very high dimensional text , so it takes a high computation resource . the mapreduce allows parallelization of large computations and can improve the efficiency of text processing . this research aims to study the effect of big data processing on nlp tasks based on a deep learning approach . we classify a big text of news topics with fine - tuning bert used pre - trained models . five pre - trained models with a different number of parameters were used in this study . to measure the efficiency of this method , we compared the performance of the bert with the pipelines from spark nlp . the result shows that bert without spark nlp gives higher accuracy compared to bert with spark nlp . the accuracy average and training time of all model ' s using bert is 0 . 9187 and 35 minutes while using bert with spark nlp pipeline is 0 . 8444 and 9 minutes . the bigger model will take more computation resources and need a longer time to complete the tasks . however , the accuracy of bert with spark nlp only decreased by an average of 5 . 7 % , while the training time was reduced significantly by 62 . 9 % compared to bert without spark nlp . 

twitter sentiment classification using stanford nlp
twitter is a micro blogging site where users review or tweet their approach i . e . , opinion towards the service providers twitter page in words and it is useful to analyze the sentiments from it . analyze means finding approach of users or customers where it is positive , negative , neutral , or in between positive - neutral or in between negative - neutral and represent it . in such a system or tool tweets are fetch from twitter regarding shopping websites , or any other twitter pages like some business , mobile brands , cloth brands , live events like sport match , election etc . get the polarity of it . these results will help the service provider to find out about the customers view toward their products . 

partitioning under timing and area constraints
circuit partitioning is a very extensively studied problem . in this paper we formulate the problem as a nonlinear program ( nlp) . the nlp is solved for the objective of minimum cutset size under the constraints of timing . our proposed methodology easily extends to multiple constraints that are very dominant in the design of large scale vlsi systems . the nlp is solved using the commercial lp / nlp solver minos . we have done extensive testing using large scale rt level benchmarks and have shown that our methods can be used for exploring the design space for obtaining constraint satisfying system designs . we also provide extensions for solving system design problems where a choice between multiple technologies , packaging components , performance , cost , yield , and more can be the constraints for design related decisions . 

rethinking compilers in the rise of machine learning and ai ( keynote ) 
recent years have witnessed some influential progresses in machine learning ( ml ) and artificial intelligence ( ai ) . the progresses may lead to some significant changes to future programming . many programs , for instance , may be not code written in some specially designed programming languages , but high - level user intentions expressed in natural languages . deep learning - based software , despite the difficulties in interpreting their results , may continue its rapid growth in the software market and its influence in people ' s everyday life . this talk will first examine the implications of these changes to compiler research , and then discuss the potential opportunities that ml and ai could bring to possibly transform the field of compiler research . specifically , the talk will focus on the possibilities for ml and ai to help reveal the high - level semantics and attributes of software components that traditional compiler technology cannot do , and hence , open important opportunities for high - level large - scoped code reasoning and optimizations - - - a direction that has some tremendous potential but has been beyond the reach of traditional compiler technology . the talk will discuss how ml and ai may help break the " abstraction wall " - - - barriers formed by layers of abstractions in modern software - - - for program analysis and optimizations , and how ml and ai may transform the way in which high - level user intentions get translated into low - level code implementations . the talk will conclude with a list of grand challenges and possible research directions for future compiler constructions . 

machine translation from japanese and french to vietnamese , the difference among language families
although vietnamese is spoken language of more than 90 million people in the world ( in 2014 ) , vietnamese language is still considered as a low - resourced language . vietnamese nlp still lacks of resources for text and speech processing , especially research on machine translation for vietnamese is very rare . this paper presents our first attempt to collect and construct french - vietnamese and japanese - vietnamese statistical machine translation systems . these two different languages , french and japanese , are less focused in vietnamese - related machine translation research . the differences between these two languages in comparison with vietnamese can bring out interesting observations . 

fuzzy explainable attention - based deep active learning on mental - health data
in this paper , we propose a fuzzy classification deep attention - based model that expands emotional lexicons by using linguistic properties of actual patient authored texts . the active learning methods can expand the trained dataset and fuzzy rules over some time . as a result , the model itself can reduce its labeling efforts for mental health application . thus , the designed model can solve issues related to vocabulary sizes per class , data sources , methods of creation , and create a baseline for human performance levels . this paper also gives fuzzy explainability by visualizing weighted words . our proposed method uses a similarity - based method that includes a subset of unstructured data as the training set . next , using an active learning mechanism cycle , our method updates the training model using new training points . this cycle is repeatedly performed until an optimal solution is reached . the designed model also converts all unlabeled texts into the training set . our in - depth experimental results show that the emotion - based expansion enhances the testing accuracy and helps to build quality rules . 

question answer system for online feedable new born chatbot
designing a new born chatbot and feeding it from the web with specific areas of interest is a new research field . few researchers have investigated empty database chatbots and populating it from web pages or plain text corpora . extracting data from web pages needs considerable processing before the response sentences are ready for the chatbot . feature extraction is also needed in order to filter and quantify the extracted plain text . in addition , ranking and classification are also required . this paper presents a new method that employs multiple feature extraction methods to quantify text responses for a new born ( uneducated ) chatbot . multiple measurement metrics are examined simultaneously in order to find the nearest match to a query . the nearest matches with the highest score have been obtained by re - ranking the scores of extracted features for text responses . the results show that the highest scored sentences have subjectively a good match to the query . the evaluation results indicate that the performance of the system increases significantly by using cosine similarity to find lexical match between the query and the response sentence rather than jaccard ' s coefficient . 

not just dissimilar , but opposite : an algorithm for measuring similarity and oppositeness between words
many natural language processing applications are underpinned by word comparison algorithms . often , such comparison algorithms have been designed to determine how related given words are or how similar they are . in some applications , however , it may be important to distinguish between simply not being similar and oppositeness . in this paper we present work in progress on an algorithm that accounts for the similarity or oppositeness of given words . we evaluate this algorithm against ` gold - standard ' datasets for evaluating similarity and antonymy and discuss some of the issues with these datasets for evaluating algorithms such as ours . we show that our algorithm , using knowledge from wordnet only , is able to achieve precision scores upwards of 84 % on the gre most contrasting word antonymy task , while outperforming state of the art algorithms over the recent simlex - 999 similarity dataset . 

grey relational analysis and natural language processing
this paper investigates validity of using grey relational analysis ( gra ) for natural language processing ( nlp ) . the domain of nlp is one associated with inherent vagueness and abstraction , with many sub - domains all invoking their own associated uncertainties . regardless of the particularisation , the main objective is understanding and making sense of linguistic lexicon . the inferencing and understanding of sentiment from natural language has been investigated thoroughly , however , the use of grey system theory in conjunction with nlp has yet to be explored in any great detail . ergo , an introductory investigation into the effectiveness of using gra on and with regards to nlp . this paper describes the feasibility of using grey system methodologies and tools , specifically the use of grey incidence , to provide a means for analysis of a sequence ' s geometric curve . the use of gra provides one with the ability to inspect and infer sequences of data . using this notion and by having a sequence represented as an input stream , it can be correlated against possible output commands . the use of grey incidence for quantifying and evaluating the correlation between what is inputted , against what output it is most similar to , is novel and should provide an additional facet to grey system theory . 

modeling translation
machine translation ( mt ) has been a challenging application of natural language processing technologies for many years . however , recent major improvements in translation accuracy have led to instances such as web - based services that almost instantly translate any text into different languages or business - to - business services for high - quality translation of domain - specific documents . this article covers the foundation of recent mt systems and introduces translation as a mathematical process . it also focuses on how an mt system automatically learns to translate using samples of translated texts and how it renders output by combining acquired knowledge . 

domain adaptation in nlp based on hybrid generative and discriminative model
this study investigates the domain adaptation problem for nature language processing tasks in the distributional view . a novel method is proposed for domain adaptation based on the hybrid model which combines the discriminative model with the generative model . the advantage of the discriminative model is to have lower asymptotic error , while the advantage of the generative model can easily incorporate the unlabeled data for better generalization performance . the hybrid model can integrate their advantages . for domain transfer , the proposed method exploits the difference of the distributions in different domains to adjust the weights of the instances in the training set so that the source labeled data is more adaptive to the target domain . experimental results on several nlp tasks in different domains indicate that our method outperforms both the traditional supervised learning and the semi - supervised method . 

to minimize the delay of signalized intersection using lp and nlp
this paper develops a new method for minimizing intersection delay based on linear and nonlinear program . on the basis of geometric construction , a new set of formulas are proposed to estimate the steady - and - oversaturation delay . as a dynamic model , it can automatically allocate the green time to obtain the minimal delay . since the objective function of delay is linear or nonlinear , only the combination of linear and nonlinear program can be applied to random situation . the two objective functions , subject to corresponding constraints , can achieve the best split green ratios by matlab . the simulation result shows that green splits got by lp combined with nlp can obtain less delay than using fixed green splits . 

generating entity relationship diagram from requirement specification based on nlp
an entity relationship data model is a high level conceptual model that describes information as entities , attributes relationships and constraints . entity relationship diagrams to design the database of the software . it involves a sequence of tasks including extracting the requirements , identifying the entities , their attributes , the relationship between the entities , constraints and finally drawing the diagram . as such entity relationship diagram design has become a tedious task for novice designer . this research addresses the above issue , proposes a natural language processing based tool which accepts requirement specification written in english language and generates entity relationship diagram . 

the nursing profession : implications for ai and natural language processing
devices which utilise the assistance of artificial intelligence ( ai ) such as robotic technology are included increasingly in nursing and health environments . in accordance with these developments this paper will highlight issues for natural language processing that arise from the contexts of nursing practice with specific emphasis on the way nurses think and talk about holistic care , nursing language and the challenges of communication in highly unpredictable nursing practice environments . it is noted that the practice and language of caring sciences such as nursing must be understood in terms of their emphasis on holism and therefore on the physical , mental , emotional , psychological , social and spiritual dimensions of communicating with people . this emphasis presents important challenges to researchers of natural language processing as interaction and communication in the caring sciences includes a significant amount of interpersonal insight which is informed in part by sensitivity , feelings and emotion . these subjective elements are part of human relations and this paper will close with a discussion on the potential for qualitative research methodologies to join with quantitative methods to contribute to research design and outcomes . 

bringing data science to qualitative analysis
qualitative user research is a human - intensive approach that draws upon ethnographic methods from social sciences to develop insights about work practices to inform software design and development . recent advances in data science , and in particular , natural language processing ( nlp) , enables the derivation of machine - generated insights to augment existing techniques . our work describes our prototype framework based in jupyter , a software tool that supports interactive data science and scientific computing , that leverages nlp techniques to make sense of transcribed texts from user interviews . this work also serves as a starting point for incorporating data science techniques in the qualitative analyses process . 

optimisation of the largest annotated tibetan corpus combining rule - based , memory - based , and deep - learning methods
this article presents a pipeline that converts collections of tibetan documents in plain text or xml into a fully segmented and pos - tagged corpus . we apply the pipeline to the large extent collection of the buddhist digital resource center . the semi - supervised methods presented here not only result in a new and improved version of the largest annotated tibetan corpus to date , the integration of rule - based , memory - based , and neural - network methods also serves as a good example of how to overcome challenges of under - researched languages . the end - to - end accuracy of our entire automatic pipeline of 91 . 99 % is high enough to make the resulting corpus a useful resource for both linguists and scholars of tibetan studies . 

a novel interpolated n - gram language model based on class hierarchy
in this paper , we propose a novel interpolated language model that combines the interpolation and the backing - off along hierarchical classes based on class hierarchy . and the corresponding approach to the estimation of interpolation coefficients is also presented . we use the minimum discriminative information ( mdi ) method to cluster the vocabulary into a word - clustering tree hierarchically . the tree is used to balance the generalization ability of classes ' and word specificity when estimating the likelihood of a n - gram event . experiments are performed on reuter ' s corpus using a vocabulary of 27 , 000 words . results show a reduction on the test perplexity over the standard modified kn n - gram approach by 12% . 

a comparative study of n - gram and skip - gram for clinical concepts extraction
state - of - the - art technologies for clinical knowledge extraction are essential in a clinical decision support system ( cdss ) to make a prediction of a diagnosis . automatic analysis of a patient ' s health data is a requirement in such a process . the unstructured part of the data in electronic health records ( ehr ) is critical , as it may contain hidden risk factors . we present in this paper a comparative study of two well - known techniques n - gram and skip - gram to enhance the extraction of risk factors concepts from the clinical narratives after applying initial natural language processing ( nlp ) techniques . we evaluate the use of both techniques using a case study dataset of patients ' records with venous thromboembolism ( vte ) . results of the techniques ' comparative study yielded an advancement of n - gram precision while skip - gram produced a better performance in terms of the recall measure . 

session details : nlp applications
no abstract available . 

stochastic arabic hybrid diacritizer
this paper introduces a two - layer stochastic system to diacritize raw arabic text automatically . the first layer determines the most likely diacritics by choosing the sequence of full - form arabic word diacritizations with maximum marginal probability via a ∗ lattice search algorithm and m - gram probability estimation . when full - form words are out - of - vocabulary ( oov ) , the system utilizes a second layer , which factorizes each arabic word into its possible morphological constituents ( prefix , root , pattern and suffix ) , then uses m - gram probability estimation and a ∗ lattice search algorithm to select among the possible factorizations to get the most likely diacritization sequence . while the second layer has better coverage of possible arabic forms , the first layer yields better disambiguation results for the same size of training corpora , especially for inferring syntactical ( case - end ) diacritics . the presented hybrid system possesses the advantages of both layers . the paper details the workings of both layers and the architecture of the hybrid system . by comparing our proposed system with the best performing system to our knowledge of habash et al . [ 9 ] using their training and testing corpus ; it is found that the word error rates of 5 . 5 % for the morphological diacritization and 9 . 4 % for the syntactic diacritization by habash et al . , and only 3 . 1 % for the morphological diacritization and 9 . 4 % for the syntactic diacritization by our system . 

medical document classification based on mesh
one of the most challenging projects in information systems is extracting information from unstructured texts , including medical document classification . i am developing a classification algorithm that classifies a medical document by analyzing its content and categorizing it under predefined topics from the medical subject headings ( mesh ) . i collected a corpus of 50 full - text journal articles ( n = 50 ) from medline , which were already indexed by experts based on mesh . using natural language processing ( nlp ) , my algorithm classifies the collected articles under mesh subject headings . i evaluated the algorithm ' s outcome by measuring its precision and recall of resulting subject headings from the algorithm , comparing results to the actual documents ' subject headings . the algorithm classified the articles correctly under 45 % to 60 % of the actual subject headings and got 40 % to 53 % of the total subject headings correct . this holds promising solutions for the global health arena to index and classify medical documents expeditiously . 

text - to - text generative adversarial networks
generative adversarial networks ( gan ) , which are capable of generating realistic synthetic real - valued data , have achieved great progress in machine learning field . however , generator in gan framework requires being differentiable , which means that the generator cannot produce discrete data , and it poses great challenge for gan applied in natural language processing ( nlp ) research . to unlock the potential of gan in nlp , we develop a novel text - to - text generative adversarial networks ( tt - gan ) , through which we can get generated text based on semantic information translated from source text . we demonstrate that our model can generate not only realistic texts , but also the source text ' s paraphrase or its semantic summarization . as our best knowledge , it is the first framework capable of generating natural language on semantic level in real sense , and gives a new perspective to apply gan on nlp research . 

scientific paper classification using convolutional neural networks
the convolutional neural network ( cnn ) , a class of artificial neural networks , has become dominant in various fields , including analysis and text processing . it designed to learn , automatically and adaptively , the spatial hierarchies of backscattered entities using multiple building blocks , such as convolutional layers , grouping layers , and fully connected layers . this article aims to present an approach based on cnn to classify scientific articles by their domains ( 7 different domains ) from their abstracts , the process will base on several features extracted automatically from their summaries . the proposed approach has shown remarkable results for the automatic classification of text compared to other usual automatic learning algorithms . 

multi - language concept normalisation of clinical cohorts
the exploration of multiple cohorts allows researchers to answer new research questions using more substantial clinical data . however , this is only possible if the cohorts are interoperable , which implies the migration of the original cohort into a common data schema . the problem of this procedure is the effort necessary to map the original concepts into their standard definitions . while several automatic mapping solutions can help in this task , its complexity increases when dealing with multi - language cohorts , leading to a significant manual effort in translating and mapping . in this paper we propose a system that combines text mining with language detection techniques , aiming to optimise these migration pipelines . this system was designed to be integrated into already existing migration workflows , without the need of adapting them . the system was validated using alzheimer ' s diseases cohorts , but it is enough general to be applied in other use cases . 

online communication with natural language
nowadays all the university and organization have a dedicated website of their own . the most challenging part in maintaining the website is to effectively handle the user doubts and queries . often , users wish to interact with the authoritative person . answering every customer is tiresome and leads to loss of time and human resource . it is impossible to handle lot of queries simultaneously . in order to make the work of the organization simple , chatbots can be used . a chatbot simulates the conversation with user in mobile applications , web applications and messaging applications . the proposed system will be able to read a piece of text , that is being asked as a question by the user and it will provide the appropriate response by text categorization and keyword matching . the screenshots illustrate the successful implementation of chatbot application . 

what happens when ?: interpreting schedule of activity tables in clinical trial documents
clinical trial protocols are complex documents that must be translated manually for trial execution and management . we have developed a system to automatically transform a schedule of activity ( soa ) table from a pdf document into a machine interpretable form . our system combines semantic , structural , and nlp approaches with a " human in the loop " for verification to determine which cells contain activity or temporal information , and then to understand details of what these cells represent . using a training and test set of 20 protocols , we assess the accuracy of identifying specific types of soa elements . this work is the first stage of a larger effort to use artificial intelligence techniques to extract procedural logic in clinical trial documents and to create a knowledge base of protocols for insights and comparison across studies . 

a deep learning approach for ip hijack detection based on asn embedding
ip hijack detection is an important security challenge . in this paper we introduce a novel approach for bgp hijack detection using deep learning . similar to natural language processing ( nlp ) models , we show that by using bgp route announcements as sentences , we can embed each as number ( asn ) to a vector that represents its latent characteristics . in order to solve this supervised learning problem , we use these vectors as an input to a recurrent neural network and achieve an excellent result : an accuracy of 99 . 99 % for bgp hijack detection with 0 . 00 % false alarm . we test our method on 48 past hijack events between the years 2008 and 2018 and detect 32 of them , and in particular , all the events within two years from our training data . 

informing determiner and preposition error correction with word clusters
we extend our n - gram - based data - driven prediction approach from the helping our own ( hoo ) 2011 shared task ( boyd and meurers , 2011 ) to identify determiner and preposition errors in non - native english essays from the cambridge learner corpus fce dataset ( yannakoudakis et al . , 2011 ) as part of the hoo 2012 shared task . our system focuses on three error categories : missing determiner , incorrect determiner , and incorrect preposition . approximately two - thirds of the errors annotated in hoo 2012 training and test data fall into these three categories . to improve our approach , we developed a missing determiner detector and incorporated word clustering ( brown et al . , 1992 ) into the n - gram prediction approach . 

a hybrid long arabic text summarization system based on integrated approach between abstractive and extractive
inevitably generating a robust summary from a long arabic document is a challenging task owing to the fact that arabic is a complex language and has unique attributes . in this paper , we propose an integrated approach between abstractive and extractive for providing an informative and coherent summary from a long document . the extractive method employs a novel formulation for extracting a set of statistical and semantic features by taking into consideration the semantic , importance , and position of the sentence . the combination of statistical and semantic features is used to learn a soft voting classifier to extract the significant sentences . in the abstractive approach , only significant sentences that classified from the extractive approach will be trained with encoder - decoder bidirectional long short - term memory ( lstm ) for producing a compose novel summary . we show that the mixed proposed architecture between extractive and abstractive outperforms and provides better results comparing to some existing arabic summarizing systems . 

a simple nlp - based approach to support onboarding and retention in open source communities
successful open source communities are constantly looking for new members and helping them become active developers . a common approach for developer onboarding in open source projects is to let newcomers focus on relevant yet easy - to - solve issues to familiarize themselves with the code and the community . the goal of this research is twofold . first , we aim at automatically identifying issues that newcomers can resolve by analyzing the history of resolved issues by simply using the title and description of issues . second , we aim at automatically identifying issues , that can be resolved by newcomers who later become active developers . we mined the issue trackers of three large open source projects and extracted natural language features from the title and description of resolved issues . in a series of experiments , we optimized and compared the accuracy of four supervised classifiers to address our research goals . random forest , achieved up to 91 % precision ( f1 - score 72 % ) towards the first goal while for the second goal , decision tree achieved a precision of 92 % ( f1 - score 91 % ) . a qualitative evaluation gave insights on what information in the issue description is helpful for newcomers . our approach can be used to automatically identify , label , and recommend issues for newcomers in open source software projects based only on the text of the issues . 

mining monitoring concerns implementation in java - based software systems
in this paper we describe a new approach for automatic identification of monitoring concerns implementation in java - based software systems . we also present the results obtained by using our approach on 21 java - based systems , ranging from small to very large systems . 

towards enhanced hierarchical attention networks in icd - 9 tagging of clinical notes
text is an important element in document classification in many natural language applications . natural language processing ( nlp ) is today ' s computational advancement that provides many significant modern uses of text documents such as efficient information retrieval . in this paper , we describe the theoretical framework of predicting icd - 9 codes through tagging of clinical notes using our improved framework in deep learning called enhans . this proposed model improvement covers combination of word and topic embedding , as well as adding character - level representation of a document in a hierarchical attention neural networks . this paper also present the use of sigmoid activation function in the last layer of the enhanced neural network in order to arrive with a multi - label , multi - class prediction of clinical notes with icd - 9 codes . 

evaluation of word and sub - word embeddings for isizulu on semantic relatedness and word sense disambiguation tasks
morphologically rich languages , such as isizulu have a large number of surface words due to their highly productive ( agglutinative ) nature . this results in natural language processing ( nlp ) models learnt from training corpora failing to generalize to the large number of words that would not have be seen in the training data . some researchers believe that the use of morphemes for nlp for morphologically rich languages results in better models . this belief is based on two premises ; ( i ) morphemes are the most basic meaning bearing units of a language ( ii ) the space of morphemes is much smaller than the space of word - forms , hence models based on morphemes are more likely to generalise better to unseen words than word - based models . in this paper we investigate the veracity of these premises by comparing morpheme - level embeddings to word - level embeddings through ( i ) a semantic relatedness task ( ii ) a word sense disambiguation ( wsd ) task . the results obtained showed that morpheme - level embeddings were outperformed by word - level embeddings for the semantic relatedness task , but they fared much better than their word level counterpart on the wsd task . 

expanding the vocabulary : fractal feature space in neural networks
in this paper a method for expanding the limited vocabulary of neural - network based language systems is introduced . the proposed method draws on developmental constraints observed in human language acquisition , to generate increasingly specialist feature maps in linked orthogonal spaces . each space acts as a semantic filter , channelling words to more specialist spaces . the resultant trace through each space corresponds to a full feature list for the word , which can be manipulated symbolically or by another network . this approach allows arbitrary feature accuracy for any word , whilst limiting input dimensionality to the minimum required to uniquely specify the word in the relevant specialist space . consequently crossover between unrelated words is also minimised , so avoiding the n - squared relation between computation and vocabulary size found in fully connected networks . the resultant topology of spaces also suggests that complex inferences are possible , and the use of a perception - based feature set allows a common knowledge base to be shared between languages . 

forging high - quality user stories : towards a discipline for agile requirements
user stories are a widely used notation for formulating requirements in agile development projects . despite their popularity in industry , little to no academic work is available on assessing their quality . the few existing approaches are too generic or employ highly qualitative metrics . we propose the quality user story framework , consisting of 14 quality criteria that user story writers should strive to conform to . additionally , we introduce the conceptual model of a user story , which we rely on to design the aqusa software tool . aqusa aids requirements engineers in turning raw user stories into higher - quality ones by exposing defects and deviations from good practice in user stories . we evaluate our work by applying the framework and a prototype implementation to three user story sets from industry . 

rule - based extraction of family history information from clinical notes
one of the features of electronic health records ( ehr ) is to store the patient clinical data . despite the efforts to structure all this data , clinical reports and notes containing essential information about each patient are still stored in free text . some of this information refers to the family ' s health history and may be highly relevance for diagnosis and prognosis . we proposed two methodologies to unify this knowledge and extract family history information from clinical notes using rule - based techniques in natural language processing ( nlp ) . with these methods , we intend to collect the family members mentioned in the text as well as associations to diseases and living status . the proposed methods were evaluated into two stages , demonstrating f - scores of 0 . 72 and 0 . 74 for the discovery of family members and observations , and 0 . 62 and 0 . 52 for the detection of the family relations with the observations , and their living status . our methodologies raised new strategies to automatically annotate large amounts of ehrs , facilitating the detection of comorbidities within family relations . 

performance analysis of different neural networks for sentiment analysis on imdb movie reviews
with the huge expansion of text data sentiment analysis is playing a crucial role in analyzing the user ’ s perspective about a particular product , company or any other physical or virtual entity . sentiment analysis helps us to analyze user review about an entity and then drawing out a conclusion based on the sentiments it extracted from the reviews . convolution neural network ( cnn ) and long - short - term memory network ( lstm ) are two well - known deep neural networks used for sentiment analysis . in this paper , we have compared between cnn , lstm and lstm - cnn architectures for sentiment classification on the imdb movie reviews in order to find the best - suited architecture for the dataset . experimental results have shown that cnn has achieved an f - score of 91 % which has outperformed lstm , lstm - cnn and other state - of - the - art approaches for sentiment classification on imdb movie reviews . 

dynamic transformer for efficient machine translation on embedded devices
the transformer architecture is widely used for machine translation tasks . however , its resource - intensive nature makes it challenging to implement on constrained embedded devices , particularly where available hardware resources can vary at run - time . we propose a dynamic machine translation model that scales the transformer architecture based on the available resources at any particular time . the proposed approach , ‘ dynamic - hat’ , uses a hat supertransformer as the backbone to search for subtransformers with different accuracy - latency trade - offs at design time . the optimal subtransformers are sampled from the supertransformer at run - time , depending on latency constraints . the dynamic - hat is tested on the jetson nano and the approach uses inherited subtransformers sampled directly from the supertransformer with a switching time of < 1s . using inherited subtransformers results in a bleu score loss of 6 1 . 5 % because the subtransformer configuration is not retrained from scratch after sampling . however , to recover this loss in performance , the dimensions of the design space can be reduced to tailor it to a family of target hardware . the new reduced design space results in a bleu score increase of approximately 1 % for sub - optimal models from the original design space , with a wide range for performance scaling between 0 . 356s - 1 . 526s for the gpu and 2 . 9s - 7 . 31s for the cpu . 

designing a transactional smart assistant in indonesian using rasa framework
the use of chatbots in the industrial world is increasing along with the increasing number of smartphone users in indonesia . many companies take this opportunity to develop their chatbot services with the help of natural language processing ( nlp ) and neural network ( nn ) technologies . in this study , the creation of an indonesian - based chatbot is more emphasized on the questions and answer domain , complaint handling , and transaction support at “ pulsa ” distributor company . chatbots must have the ability to understand natural language , provide accurate predictions of subsequent actions and responses , especially for indonesian chatbots . we compared several pipeline configurations in building the natural language understanding ( nlu ) model , namely the default rasa nlu with dietclassifier , pretrained fasttext with dietclassifier and spacy multi language model using fastai ’ s ulmfit for intent classification . the formation of the intent classification based on the complaint data log , customer service chat history and company faq resulted in 216 intents . the evaluation results on the pipeline configuration show that the intent classification and entity extraction tasks are better using the pretrained fasttext model with an f1 - score intent evaluation reaching 73 . 4 % and entity extraction 83 % . 

malay manuscripts transliteration using statistical machine translation ( smt ) 
natural language processing ( nlp ) is a vital field of artificial intelligence that automates the study of human language . however for malay manuscripts ( mm ) written in old jawi , its exposure on such field is limited . besides , most of the studies related to mm studies and nlp were focused on rule based or rule based machine transliteration ( rbmt ) . hence the objective of this study is to propose a statistical approach for old jawi to modern jawi transliteration of malay manuscript contents using phrase based statistical machine translation ( pbsmt ) as its model . in order to achieve such purpose , quality score of word error rate ( wer ) was computed on the transliteration output . besides , the issues formerly encountered by rule based approach such as vocals limitation and homograph , reduplication , letters error and combination of multiple words were observed in the implementation . moreover , this paper utilized exploratory approach as its research strategy and mixed method as its research method . the data for the analysis were extracted from a mm titled bida ̅ yat al - mubtadi ̅ bifa ḑ lilla ̅ h al - muhdi ̅ . quality score of wer was computed for the evaluation of smt output . afterwards , related issues were identified and assessed . the research found that quality score of pbsmt for old jawi to modern jawi transliteration was high in terms of wer , however the issues of rule based were generally addressed by pbsmt except homograph . the research is however limited to the approach of smt that solely focused on pbsmt as its model . moreover , the corpus size was limited to one manuscript while smt relies on corpus size . nevertheless the research contributes to the wider coverage on malay language as one of the under resource languages in nlp , in form of old and modern jawi . besides , to the best of the researcher ' s knowledge , it is also the first to apply smt ( pbsmt ) approach on old jawi transliteration . most importantly , the study is to contribute on mm ' s . 

human knowledge based efficient interactive data annotation via active weakly supervised learning
machine learning has been contributing significantly to various pervasive computing systems . further diffusion of such systems will require reducing the obstacles of huge data annotation costs and the uninterpretability . weakly supervised learning is gaining attention as a method to solve these problems , especially in natural language processing ( nlp ) . its advantage is due to human - defined labeling functions ( lfs ) based on human knowledge instead of attaching a label to each data point manually . however , this method alone cannot reduce the actual process cost . creating lfs can be a very costly process if there is no insight or support . we propose an interactive data annotation method via weakly supervised learning with an uncertainty - based active learning strategy . the proposed method iteratively presents a few highly - prioritized data points to be annotated considering the outputs of lfs and the uncertainty of the prediction . the humans ' task is only implementing their knowledge that can be applied to the presented data points as an lf . we also verified the effectiveness of the proposed method through two classification tasks ( nlp and non - nlp tasks ) . the experimental results indicate its effectiveness and high potential without limitation of application fields . 

machine understandable contracts with deep learning
this research investigates the automatic translation of contracts to computer understandable rules trough natural language processing . the most challenging aspect , which is studied throughout this paper , is to understand the meaning of the contract and express it into a structured format . this problem can be reduced to the named entity recognition and rule extraction tasks , the latter handles the extraction of terms and conditions . these two problems are difficult , but deep learning models can tackle them . we think that this paper is the first work to approach rule extraction with deep learning . this method is data - hungry , so the research also introduces data sets for these two tasks . additionally , it contributes to the literature by introducing law - bert , a model based on bert which is pre - trained on unlabelled contracts . the results obtained on named entity recognition and rule extraction show that pre - training on contracts has a positive effect on performance for the downstream tasks . 

what will search engines be changed by nlp advancements
i think that the vision of a search engine is " natural search " with which users input his or her search intent in a natural way such as using natural language or an image and immediately obtains the desired accurate information which is concisely and comprehensibly expressed . during this process , nlp is undoubtedly one of the most crucial technologies . in the past , the search engine uses limited and shallow nlp technologies because nlp technology is not as mature as people have expected . in recent years , we have witnessed that nlp has made huge advances in various tasks such as semantic parser , question - answering , machine translation , machine reading comprehension and text generation . i think that now it is the time to consider applying these new technologies to a search engine to further improve the intelligence and naturalness of the search process . it is necessary to understand the new progress of nlp and their potential impact to a search engine . in this talk , i first provide an overview of advancements of methodology and technology in nlp filed in recent years . then i will share my thoughts about the promising change of search engines brought by these new nlp technologies . i will further elaborate my thoughts on changing search engine with a set of intelligent question - answering techniques comprising semantic parser , question - answering and machine reading comprehension . although these new promising nlp have rapidly brought meaningful change to a search engine , there are still many problems unsolved . as a conclusion , a list of the challenging topics will be proposed with my initial thoughts of solutions . 

evolution of communities on twitter and the role of their leaders during emergencies
twitter is presently utilized as a channel of communication and information dissemination . at present , government and non - government emergency management organizations utilize twitter to disseminate emergency relevant information . however , these organizations have limited ability to evaluate the twitter communication in order to discover communication patterns , key players , and messages that are being propagated through twitter regarding the event . more importantly there is a general lack of knowledge of who are the individuals or organizations that disseminate warning information , provide confirmations of an event and associated actions , and urge others to take action . this paper presents a methodology that shows how natural language processing ( nlp ) and social network analysis ( sna ) can aid in addressing these issues . the methodology , in addition to qualitative data collected during on - site interviews and publicly available information , was successfully applied to a twitter data set collected during 2011 japan tsunami . nlp techniques were applied to extract actionable messages . based on the messages extracted by nlp , sna was used to construct a network of actionable messages . while sna discovered communities and extracted the community leaders , nlp was used to determine the behavior of the community members and the role of the community leaders . therefore , the proposed methodology automatically finds communities , evaluates its members ' behaviors , and authenticates cohesive behaviors of the community members during emergencies . moreover , the methodology efficiently finds the leaders of the communities , while also identifying their role in communities . 

improving self - attention networks with sequential relations
recently , self - attention networks show strong advantages of sentence modeling in many nlp tasks . however , self - attention mechanism computes the interactions of every pair of words independently regardless of their positions , which makes it not able to capture the sequential relations between words in different positions in a sentence . in this paper , we improve the self - attention networks by better integrating sequential relations , which is essential for modeling natural languages . specifically , we 1 ) propose a position - based attention to model the interaction between two words regarding positions ; 2 ) perform separated attention for the context before and after the current position , respectively ; and 3 ) merge the above two parts with a position - aware gated fusion mechanism . experiments in natural language inference , machine translation and sentiment analysis tasks show that our sequential relation modeling helps self - attention networks outperform existing approaches . we also provide extensive analyses to shed light on what the models have learned about the sequential relations . 

a survey of intelligent language tutoring systems
intelligent languages tutoring systems ( iltss ) plays a significant role in evaluating students ' answers through interaction with them . iltss implements natural language processing ( nlp ) techniques in order to allow free input of words and sentences . iltss have the capability of identifying the input errors and provide an immediate feedback along with the errors source . it has been observed that iltss were not surveyed intensively ; the reason that motivates us to conduct this research . some nlp recent trends such as latent sematic analysis and entailment were demonstrated . different iltss have been discussed with a dedicated section about the development of arabic iltss . arabic share many of its characteristics with semitic and morphologically rich languages . in our presentation we point out new trends that have been emerged while conducting survey . 

experiments with open - domain textual question answering
this paper describes the integration of several knowledge - based natural language processing techniques into a question answering system , capable of mining textual answers from large collections of texts . surprizing quality is achieved when several lightweight knowledge - based nlp techniques complement mostly shallow , surface - based approaches . 

the generation of referring expressions : where we ' ve been , how we got here , and where we ' re going
summary form only given . the task of referring expression generation is concerned with determining what semantic content should be used in a reference to an intended referent so that the hearer will be able to identify that referent . the task has been a focus of interest within natural language generation at least since the early 1980s , in part because the problem appears relatively well - defined . over the last 25 years , a range of algorithms and approaches have been proposed and explored , making this the most intensely studied problem in natural language generation ; and yet , even a casual analysis of real human - authored texts suggests that we have a long way to go in terms of providing an explanation for the range of real linguistic behaviour that we find . in this talk , i ' ll provide a bird ' s eye view of research in the area to date , to characterise where we are now , and indicate what i believe are the key directions for future research in the area . 

enriching electronic program guides using semantic technologies and external resources
electronic program guides ( epgs ) describe broadcast programming information provided by tv stations . however , users may obtain more information when these guides have been enriched . the main contribution of this work is to present an automation process for epg ' s information enrichment through the use of semantic technologies and external resources . among the several resources involved in the enrichment process , the following can be mentioned : ontologies , web services , semantic repositories and natural language processing techniques . 

the nature of communications and emerging communities on twitter following the 2013 syria sarin gas attacks
social media has become an important communication tool especially following an extreme event . research in social psychology has shown that people engage in gathering and " milling " information , and confirmation seeking during the process of forming intent to take action or voice an opinion . twitter serves as a communications channel where people converge to compile collective intelligence , provide event reporting , and diffuse information . in this paper the investigation of twitter usage seeks to describe human participation on twitter following a controversial extreme event - - 2013 syria sarin gas attack . the methodology employed incorporates natural language processing ( nlp ) and network analysis to trace human response on twitter to this event . nlp techniques include named entity recognition ( ner ) used to extract relevant entities ( e . g . countries ) , event extraction ( ee ) to excerpt relevant events ( e . g . conflict , movement , life , etc . ) , and stanford parser to detect actionable verbs discussed by twitter participants . network analysis constructs a network based on the twitter users ' communications , detects communities , extracts their leaders and identifies their roles based on structural properties of the networks . specifically , the research looked at the twitter data for two days august 22 - 23 , 2013 following the event . the research suggests that ( 1 ) there were no immediate polarization of opinions following the event , ( 2 ) the primary event of twitter communication was the conflict and information about the victims of the event , ( 3 ) twitter communities were too sparse to produce substantial amount of social pressure to force an opinion / opinion shift , ( 4 ) top community leaders were news sources , political activists , and select individuals , ( 5 ) ' individual ' leaders political agendas were not revealed . 

hardware accelerator for multi - head attention and position - wise feed - forward in the transformer
designing hardware accelerators for deep neural networks ( dnns ) has been much desired . nonetheless , most of these existing accelerators are built for either convolutional neural networks ( cnns ) or recurrent neural networks ( rnns ) . recently , the transformer model is replacing the rnn in the natural language processing ( nlp ) area . however , because of intensive matrix computations and complicated data flow being involved , the hardware design for the transformer model has never been reported . in this paper , we propose the first hardware accelerator for two key components , i . e . , the multi - head attention ( mha ) resblock and the position - wise feed - forward network ( ffn ) resblock , which are the two most complex layers in the transformer . firstly , an efficient method is introduced to partition the huge matrices in the transformer , allowing the two resblocks to share most of the hardware resources . secondly , the computation flow is well designed to ensure the high hardware utilization of the systolic array , which is the biggest module in our design . thirdly , complicated nonlinear functions are highly optimized to further reduce the hardware complexity and also the latency of the entire system . our design is coded using hardware description language ( hdl ) and evaluated on a xilinx fpga . compared with the implementation on gpu with the same setting , the proposed design demonstrates a speed - up of 14 . 6 x in the mha resblock , and 3 . 4 x in the ffn resblock , respectively . therefore , this work lays a good foundation for building efficient hardware accelerators for multiple transformer networks . 

secret : semantically enhanced classification of real - world tasks
supervised machine learning ( ml ) algorithms are aimed at maximizing classification performance under available energy and storage constraints . they try to map the training data to the corresponding labels while ensuring generalizability to unseen data . however , they do not integrate meaning - based relationships among labels in the decision process . on the other hand , natural language processing ( nlp ) algorithms emphasize the importance of semantic information . in this article , we synthesize the complementary advantages of supervised ml and nlp algorithms into one method that we refer to as secret ( semantically enhanced classification of real - world tasks ) . secret performs classifications by fusing the semantic information of the labels with the available data : it combines the feature space of the supervised algorithms with the semantic space of the nlp algorithms and predicts labels based on this joint space . experimental results indicate that , compared to traditional supervised learning , secret achieves up to 14 . 0 percent accuracy and 13 . 1 percent f1 score improvements . moreover , compared to ensemble methods , secret achieves up to 12 . 7 percent accuracy and 13 . 3 percent f1 score improvements . this points to a new research direction for supervised classification based on incorporation of semantic information . 

generating human readable transcript for automatic speech recognition with pre - trained language model
modern automatic speech recognition ( asr ) systems can achieve high performance in terms of recognition accuracy . however , a perfectly accurate transcript still can be challenging to read due to disfluency , filter words , and other errata common in spoken communication . many downstream tasks and human readers rely on the output of the asr system ; therefore , errors introduced by the speaker and asr system alike will be propagated to the next task in the pipeline . in this work , we propose an asr post - processing model that aims to transform the incorrect and noisy asr output into a readable text for humans and downstream tasks . we leverage the metadata extraction ( mde ) corpus to construct a task - specific dataset for our study . since the dataset is small , we propose a novel data augmentation method and use a two - stage training strategy to fine - tune the roberta pre - trained model . on the constructed test set , our model outperforms a production two - step pipeline - based post - processing method by a large margin of 13 . 26 on readability - aware wer ( ra - wer ) and 17 . 53 on bleu metrics . human evaluation also demonstrates that our method can generate more human - readable transcripts than the baseline method . 

lexical normalisation of twitter data
twitter with over 500 million users globally , generates over 100 , 000 tweets per minute 1 . the 140 character limit per tweet has , perhaps unintentionally , encourages users to use shorthand notations and to strip spellings to their bare minimum “ syllables ” or elisions e . g . “ srsly ” . the analysis of twitter messages which typically contain misspellings , elisions , and grammatical errors , poses a challenge to established natural language processing ( nlp ) tools which are generally designed with the assumption that the data conforms to the basic grammatical structure commonly used in english language . in order to make sense of twitter messages it is necessary to first transform them into a canonical form , consistent with the dictionary or grammar . this process , performed at the level of individual tokens ( “ words ” ) , is called lexical normalisation . this paper investigates various techniques for lexical normalisation of twitter data and presents the findings as the techniques are applied to process raw data from twitter . 

phrase - based part - of - speech tagging
this paper presents a new approach to part - of - speech ( pos ) tagging in which the basic unit being tagged is a contiguous sequence of words rather than a single word . we run experiments on two different tagsets : the upenn treebank and a treebank annotated with more ambiguous tags that have a semantic component . we show that the phrase - based system alone is a respectable tagger that exceeds the performance of the me tagger on the ambiguous tagset . moreover , when a log - linear model is built using features from both phrase - and word - based techniques , the tagging accuracy improved on both of our data sets yielding the highest reported performance to date on the more ambiguous tagset . 

book review : computation of language : an essay on syntax , semantics and pragmatics in natural man - machine communication by roland hausser ( springer - verlag , new york , 1989 ) 
computation of language is a self - styled essay that addresses issues in natural - language syntax , semantics , and pragmatics from the perspectives of natural - language processing ( nlp) , formal - language theory ( flt) , and philosophy of language . subtitling a 425 - page book an " essay " is somewhat unusual , since an essay usually has the length of a journal article . yet the word does seem to fit in the sense that an essay can be an " attempt , " for the book attempts to provide a philosophical foundation , in terms of a comprehensive theory of natural - language communication , for the practical task of designing and constructing a natural - language - communicating robot ( nlc - robot) . hausser fails , however , to connect satisfactorily his philosophical and practical discussions . the book is not a single essay at all : it ' s really three essays , 1 loosely related by their general concern with language and computation , yet without the sort of sustained unity one would expect for a project of the scope of hausser ' s . though the book as a whole lacks unity , it ' s not without other virtues . hausser proposes an interesting delineation of the proper spheres of syntax , semantics , and pragmatics , and the book ' s parts are insightful and often novel in presentation , thus making them individually worthwhile . 

