An Overview of Named Entity Recognition
Named Entity Recognition ( NER ) is essential for some Natural Language Processing ( NLP ) tasks . Previous researchers gave a survey of NER in statistical machine learning era , however , research on NER has already changed a lot in recent decade . On the one hand , more and more NER systems adopt deep learning , transfer learning , knowledge base and other methods . On the other hand , multilingual and low resource languages NER researches increase rapidly . To reflect these changes , we here give an overview of NER based on 162 papers of NLP related conferences from 1996 to 2017 . In this survey , we discuss two main aspects of NER research - target languages and technical approaches with statistical analysis . Finally , we summarize some conclusions and explore potential future issues in NER research . 

DataCat : Attention - based Open Government Data ( OGD ) Category Recommendation Framework
A data category recommendation framework for Thailand ’ s open government data portal ( ThOGD ) is proposed to assist data providers when publishing and registering a new dataset into the portal ’ s data catalog . However , existing approaches such as a multi - label classification problem , have not adopted the semantic features of data categories sufficiently . Deep learning model for Natural Language Processing has recently demonstrated to achieve high potential in learning the different degrees of semantic feature abstraction because all layers of multi - head attention blocks are provided with different fragments of metadata descriptions and corresponding tags . To obtain a robust recommendation result , this paper proposes DataCat : a Category Recommendation Framework using the attention - based framework through the ThOGD portal . Within this framework , the integrated multi - layers with particular semantic information are directly attached to the output layer of a network to enhance the effectiveness of information retrieval . The results point out that the attention - based framework has a weighted effect on loss of optimization . The performance when looking at the macro average of precision and F1 - score improves by 0 . 664 % and 0 . 557 % , respectively . The micro average of those improves by 0 . 806 % , and 0 . 698 % , respectively . 

Named entity recognition in Assamese using CRFS and rules
Named Entity Recognition ( NER ) is an important task in all Natural Language Processing ( NLP ) applications . It is the process of identifying and classifying the proper noun into classes such as person , location , organization and miscellaneous . Substantial work has been done in English and other European languages , achieving greater accuracy compared to the Indian Languages . Although NER in Indian languages is a difficult and challenging task and suffers from scarcity of resources , such work has started to appear recently . This paper discusses work on NER in Assamese using both Conditional Random Fields and a Rule - Based approach which gives an F - measure of 90 - 95 % accuracy . 

Leveraging natural language analysis of software : Achievements , challenges , and opportunities
Summary form only given . Studies continue to report that more time is spent reading , locating , and comprehending code than actually writing code . The increasing size and complexity of software systems makes it significantly more challenging for humans to perform maintenance tasks on software without automated and semi - automated tools to support them , especially in the error - prone tasks . Thus , software engineers increasingly rely on software engineering tools to automate maintenance tasks as much as possible . The program analyses that drive today ' s software engineering tools have historically focused on analyzing the program ' s data and control flow , dependencies , and other structural information about the program to uncover and prove program properties . Yet , a software system is more than just the source code and its structure . To build effective software tools , the underlying automated analyses need to use all the information available to make the tools as intelligent and useful as possible . By adapting natural language processing ( NLP ) to source code analysis , and integrating information retrieval ( IR ) , NLP , and traditional program analyses , we can expect significant improvement in automated and semi - automated software engineering tools for many different software engineering tasks . In this talk , I will overview research in text analysis of software and discuss our achievements to date , the challenges faced in text analysis , and the opportunities for text analysis of software in the future . 

SiSSA : an infrastructure for NLP application development
Recently there has been a growing interest in infrastructures for sharing NLP tools and resources . This paper presents SiSSA , a project that aims at developing an infrastructure for prototyping , editing and validation of NLP application architectures . The system will provide the user with a graphical environment for ( 1 ) selecting the NLP activities relevant for the particular NLP task and the associated linguistic processors that execute them ; ( 2 ) connecting new linguistic processors to SiSSA ; ( 3 ) checking that the chosen architectural hypothesis corresponds to the functional specifications of the given application . The TRACTOR philosophy is to accept deposits of resources in any format , and to distribute them in the form in which they are received ( with small changes if possible such as additional documentation , and putting a browsable version or sample online . ) In addition , certain standards are recommended and help is offered to providers who wish to make their resources conformant with the standards . This lack of standardisation is not simply a pragmatic measure in the face of problems of heterogeneity , but is based on a profound scepticism towards current resource standardisation practice . In the future , TRACTOR aims to build up particularly parallel corpora and tools for processing and extracting meaning from such resources . 

Supervised learning methods application to sentiment analysis
The field of artificial intelligence ( AI ) is constantly growing and finding new ways to solve real world problems . One of the AI knowledge and research fields is natural language processing ( NLP ) which attempts to categorise and process human language data in an effort to utilise machines to understand humans . Among the most used applications of NLP is Sentiment Analysis . This is because , in addition to other reasons , Sentiment analysis is about understanding how humans are feeling related to an action or event , what could give to the companies with an online presence the power to understand the opinions of their customers online . A commonly used weighting factor to measure the perform of sentiment analysis is the tfidf . In this study we compare several supervised learning methods using the tfidf values in order to identify the most accurate model to analyse sentiment . After we observe which is the best classifier based on metrics and other parameters we will do a real application of sentiment analysis with twitter data . 

Analysis of Intelligent Machines using Deep learning and Natural Language Processing
Intelligent Machines are playing an important role in performing various activities in the industry thereby reducing the human efforts , error rate and increase the efficiency and accuracy . Artificial Intelligence is the backbone for development of intelligent machines which use natural language processing and deep learning as fundamental tools . In this paper , we present the new perspective of intelligent machines in everyday usage , along with the importance of understanding the natural language and generating machine level natural language for intelligent machines . Further , we summarize an overview of deep learning concurrent neural network and recurrent neural network models . Also , we review the significance of sentimental analysis using natural language processing and the ability of machines to take decisions and solve problems . 

Topic segmentation and labeling in asynchronous conversations
Topic segmentation and labeling is often considered a prerequisite for higher - level conversation analysis and has been shown to be useful in many Natural Language Processing ( NLP ) applications . We present two new corpora of email and blog conversations annotated with topics , and evaluate annotator reliability for the segmentation and labeling tasks in these asynchronous conversations . We propose a complete computational framework for topic segmentation and labeling in asynchronous conversations . Our approach extends state - of - the - art methods by considering a fine - grained structure of an asynchronous conversation , along with other conversational features by applying recent graph - based methods for NLP . For topic segmentation , we propose two novel unsupervised models that exploit the fine - grained conversational structure , and a novel graph - theoretic supervised model that combines lexical , conversational and topic features . For topic labeling , we propose two novel ( unsupervised ) random walk models that respectively capture conversation specific clues from two different sources : the leading sentences and the fine - grained conversational structure . Empirical evaluation shows that the segmentation and the labeling performed by our best models beat the state - of - the - art , and are highly correlated with human annotations . 

Optical control with specially - engineered photonic lattices and optical beams
We provide a brief overview of our recent work on spatial control of light propagation in optically induced photonic lattices , and optical control and manipulation of micro - and nano - particles with various specially - designed optical beams . 

Finding a line between trusted and untrusted information on tweets through sequence classification
The Internet has long since established itself as an indispensable source of information for both organizations and individuals . The lack of social responsibility of many digital platforms , however , offers many incentives for various forms of abuse . Disinformation , propaganda and fake news are just a few examples . Among the actors of information campaigns , we find not only individuals but also state actors with a clear agenda . Often , such information campaigns make use of psychological and rhetorical methods to achieve their goals . The manipulation of information is a major challenge for our democracies . It also presents us with major technical problems to identify and assess risks arising from the dissemination of such information . The following system description presents our approach to the detection of misinformation on social media data , which is twofold . Initially , we subjected the given training data to an exploratory analysis to get an overview of the general structure . Then we framed the given task as a simpler classification problem . In order to distinguish between trusted and untrusted information , using BERT ( Bidirectional Encoder Representations from Transformers ) as a neural network architecture for sequence classification , we started with a pre - trained model for language representation . In a supervised training step we fine - tuned this model on the given classification task with the provided annotated data . In this paper we would like to discuss both the quality of the training data and the performance of the trained classifier to derive promising directions for future work . 

Towards a road map on human language technology : natural language processing
This document summarizes contributions and discussions from two workshops that took place in November 2000 and July 2001 . It presents some visions of NLP - related applications that may become reality within ten years from now . It investigates the technological requirements that must be met in order to make these visions realistic and sketches milestones that may help to measure our progress towards these goals . 

Complexity Algorithm Analysis for Edit Distance
Natural Language Processing ( NLP ) is a method which works on any language processing . Some of the algorithms are based on edit distance analysis . It is a process where the statistical calculations between two words or sentences are analyzed . Some of used edit distances for NLP are Levenshtein , Jaro Wrinkler , Soundex , N - grams , and Mahalanobis . The evaluation of edit distance is aimed to analyze the processing time of each edit distance in calculation of two different words or sentences . The objective of this paper is to evaluate the complexity of each distance , based on the time process . 

Inferring Contexts From Facebook Interactions : A Social Publicity Scenario
The great acceptation of the Social Web has converted social networks , blogs and wikis in almost perfect advertising mediums . However , many of the current social publicity strategies do not exploit all the potential of these mediums , since they obviate users ' online life : the social contexts in which they are involved . Our proposal to reverse this situation is a model to infer users ' social contexts by the application of several Natural Language Processing ( NLP ) and data mining techniques over users ' interaction data on Facebook . We take advantage of both Facebook and Groupon APIs to provide a deployment scenario in which knowing users ' social life allows ads to target the most potential customers , which is beneficial for both companies and possible customers . 

Survey on Automatic Text Summarization and Transformer Models Applicability
This survey talks about Automatic Text Summarization . Information explosion , the problem caused by the rapid growth of the internet , increased more and more necessity of powerful summarizers . This article briefly reviews different methods and evaluation metrics . The main attention is on the applications of the latest trends , neural network - based , and pre - trained transformer language models . Pre - trained language models now are ruling the NLP field , as one of the main down - stream tasks , Automatic Text Summarization is quite an interdisciplinary task and requires more advanced techniques . But there is a limitation of input and context length results in that the whole article cannot be encoded completely . Motivated by the application of recurrent mechanism in Transformer - XL , we build an abstractive summarizer for long text and evaluate how well it performs on dataset CNN / Daily Mail . The model is under general sequence to sequence structure with a recurrent encoder and stacked Transformer decoder . The obtained ROUGE scores tell that the performance is good as expected . 

Chatbot User Interface for Customer Relationship Management using NLP models
NLP is the most researched field . Speech - totext conversions , fake - news detection , and text summarization are the hot topics of NLP . ChatBot User Interface ( UI ) using NLP , allows machines to understand customers better . The aim was to use different NLP and machine learning techniques and to add ChatBot UI to guide customers or clients through the CRM software and help them whenever they get stuck . Different approaches , libraries , and algorithms like ' RASA ' , python ' s ' Chatterbot ' , ' Cosine similarity ' , and Google ' s embedder were used to train the model and then later compared to see which gave the best results . After that , during the deployment other 2 approaches were tried , one was fetching questions from the database and then training the model , the other was to maintain a local text document and train the model from that . The advantages and disadvantages of each approach , plus challenges and better methods for deployment is also discussed . 

Solving Structured Electronic Design of Negative Feedback Amplifiers as Nonlinear Programming Problems
This paper searches the best solution for the stages of noise and bandwidth of negative feedback amplifiers by resorting to Structured Electronic Design , through optimization methods . On one side , noise optimization is achieved by establishing the noise - characteristic as a function of bias current . On the other side , bandwidth optimization is obtained by establishing the equation for the open loop gain pole - product ( LP product ) . Both aspects are defined as nonlinear programming ( NLP ) problems , where the design variables are related with the parameters of the device ( bipolar transistors ) used to synthesize the amplifiers . Differential Evolution is used to solve the noise NLP problem and the Hooke - Jeeves method is used to solve the bandwidth NLP problem . The obtained results are presented and some conclusions are established . 

Ontology - based Venous Thromboembolism Risk Factors Mining and Model Developing from Medical Records
Padua linear model is widely used for the risk assessment of venous thromboembolism ( VTE ) , which is a common and preventable complication for inpatients . However , differences of race , genetics and environment between Western and Chinese population limit Padua model ' validity in Chinese patients . Extracting VTE risk factors from unstructured medical records in Chinese hospital can help to understand VTE events and develop efficient risk assessment model . In this study , we proposed an ontology - based method to mine VTE risk factors combining natural language processing ( NLP ) and machine learning ( ML ) methods . Medical records of 3106 inpatients were processed and terms in multiple ontologies from various sections of records enriched in VTE patients were sorted automatically . Then ML methods were used to estimate terms ' importance and terms within admitting diagnosis and progress notes showed better VTE prediction performance than other sections . Finally a novel VTE prediction model was built based on selected terms and showed higher AUC score ( 0 . 815 ) than the Padua model ( 0 . 789 ) . 

A conditional mutual information based selectional association and word sense disambiguation
This paper presents a conditional mutual information based selectional association to measure the selectional preference between two words in the same sentence . This selectional association is integrated conditional mutual information and a syntactic knowledge called link grammar . The selectional association is applied to indicative words selection for target words disambiguation . The experimental results show that this conditional mutual information based selectional association is able to select the appropriate word to indicate the appropriate meaning to the target word in different context . 

Mechanism of intelligence formation and unified theory of AI
Intelligence is regarded as the most precious wealth among human capabilities and it would therefore be of most significance if the secret of intelligence formation could be clearly understood . It is our discovery that intelligence is activated from knowledge and the latter is in turn refined from information . Thus , the transformation of information into knowledge and further into intelligence should be the mechanism of intelligence formation . The discovery would be reported in the article as what could be used for guidelines for building intelligent systems . An interesting by - product , the three existing approaches to AI is then unified , would also be reported in the article . 

Identifying assertions in text and discourse : the presentational relative clause construction
In this paper we investigate the Presentational Relative Clause ( PRC ) construction . In both the linguistic and NLP literature , relative clauses have been considered to contain background information that is not directly relevant or highly useful in semantic analysis . In text summarization in particular , the information contained in the relative clauses is often removed , being viewed as non - central content to the topic or discourse . We discuss the importance of distinguishing the PRC construction from other relative clause types . We show that in the PRC , the relative clause , rather than the main clause , contains the assertion of the utterance . Based on linguistic analysis , we suggest informative features that may be used in automatic extraction of PRC constructions . We believe that identifying this construction will be useful in discriminating central information from peripheral . 

Massive bio - ontology engineering for NLP
We describe an ontology engineering methodology by which conceptual knowledge is extracted from an informal medical thesaurus ( UMLS ) and automatically converted into a formally sound description logics system . Our approach consists of four steps : concept definitions are automatically generated from the UMLS source , integrity checking of taxonomic and partonomic hierarchies is performed by the terminological classifier , cycles and inconsistencies are eliminated , and incremental refinement of the evolving knowledge base is performed by a domain expert . We report on knowledge engineering experiments with a terminological knowledge base composed of 164 , 000 concepts and 76 , 000 relations . 

Algorithm for using NLP with extremely small text datasets
A quick analysis of current technological trends displays an increasing global tendency of turning towards data - centric solutions . The ongoing wave of wide spread implementation of machine learning algorithms and artificial intelligence techniques allows us to harness the power of data to derive solutions and innovations . However , one major constraint that these algorithms face is the requirement of large amounts of data to train the models over . This project attempts to solve that issue by implementing a custom algorithm that is capable of generating NLP models that return medium to high accuracy results despite being trained on a very limited number of text data points . The posited algorithm is tested against a custom dataset consisting of answers to a set of questions pertaining to fears faced in day to day life . 

Microcontroller based Automatic Sun Tracking Solar Panel
To learn a hierarchal representation of data , deep learning techniques can be used that use multiple processing layers , and produce state of art results . Many models and methods are designed in deep learning for classification in natural language processing ( NLP) . Various classification algorithms have been used for Arabic documents classification , but they have two problems High dimensional feature representation and the low accuracy of the classification . In this work , an important experiment is made by using deep related models and methods for classifying Arabic text also compare our model with various models . Also to forward a full understand , present and future of deep learning in Arabic text classification and have obtained encouraging results . 

Cooperation of evolutionary and statistical PoS - tagging
Part - of - Speech tagging which refers to assignment of syntactic categories to words is a fundamental task in Natural Language Processing ( NLP ) . This paper presents a novel algorithm based on Bee Colony Optimization ( BCO ) for POS tagging . Experimental results indicate that the proposed algorithm outperforms the other evolutionary - based and tested classical Part - of - Speech - tagging approaches in terms of average accuracy . 

Comparative Study of the Most Useful Arabic - supporting Natural Language Processing and Deep Learning Libraries
Natural Language Processing ( NLP ) is a key area of Artificial Intelligence ( AI ) that plays a critical role in many intelligent applications . To work on NLP , users can choose different Libraries , depending on their familiarity with a particular programming language . In this paper , we are focusing on Python and Java programming languages because of their libraries ' richness in the Arabic Natural Language Processing ( ANLP ) and deep learning ( DL ) specifically . This paper presents a comparative study of some well - known ANLP and DL libraries considered to be the most valuable Arabic - supporting Python and Java libraries that can suitably deal with the specificities of the Arabic language . We will first focus on some libraries that are most commonly used in NLP tasks , namely NLTK , Gensim , OpenNLP , CoreNLP and GATE . Then , we will present some open - source DL libraries that are considered to be the most powerful DL libraries for ANLP , including TensorFlow , Theano , Keras and DeepLearning4j . These libraries simplify complex jobs and make data integration much easier with fewer codes and in less time . 

Negative expression translation in Japanese and Chinese machine translation
Because there are many differences between Chinese and Japanese about the nature , history , culture , life , manners and customs , it is natural that there are particular words in each language . It is necessary that these particular words meaning are mastered right and translated accurately in translation . Because of the complex corresponding relationship with Chinese in Chinese - Japanese machine translation , it is easy to occur a vagueness , due to the negative in the sentence , many mistranslations are caused by the commercial translation software . In this paper , we analyzed the negative expressionways in Chinese and Japanese languages to translate Chinese negative sentences by using the selection rules of Chinese negative words and position rules . If we translate the basic Japanese negative expression naide1 into Chinese , its meaning is mei2 , bie . Translate nakute into Chinese , according to its negative meaning and grammatical rule , it may be translated into bu . In the current research , we investigated a rough use rules which abstracted from each typical examples including naide and nakute . 

The " casual cashmere diaper bag " : constraining speech recognition using examples
We describe a new technology for using small collections of example sentences to automatically restrict a speech recognition grammar to allow only the more plausible subset of the sentences it would otherwise admit . This technology is unusual because it bridges the gap between hand - built grammars ( used with no training data ) and statistical approaches ( which require significant data ) . 

Application of Natural Language Processing and IoTCloud in Smart Homes
Internet of Things ( IoT ) based systems , most predominantly , the machine to machine communication based systems , have evolved in the recent past which helped to increase the efficiency of services offered without much necessity of human interaction . In general , IoT cloud - assisted solutions could serve several applications , including the Smart Home Automation , due to the availability of high - speed mobile networks coupled with cost effective , accessible and fast embedded hardware . In fact , there exists a few smart home solutions in the market that aim at automating the basic operations of home appliances . However , most of these systems focus on mimicking the basic operations of the electrical switches . This paper attempts to unfold a Smart Home Automation system using Natural Language Processing ( NLP ) and IoT cloud solutions . The proposed system was able to remotely control smart homes in a secure and in a customized manner ; the approach could precisely monitor home devices with the application of GoogleAPI for integrating devices . Experiments were carried out at the IoT Cloud research lab of IIIT Kottayam such that a mini - Smart Home environment was setup to remotely control the sensors such as humidity and temperatures of Smart Homes . The paper described a method to create an end user product using 3D modeling and 3D printing facilities . In addition , the paper has unfolded the state - of - the - art research works carried out in the field of smart home automation using NLPs . 

A Rough Concept Recognition Approach for Information Retrieval Based on Latent Semantic Analysis
This paper presents an information retrieval approach which uses a rough concept clustering in conjunction with Latent Semantic Analysis ( LSA ) to provide better document retrieval results matched to queries . The conceptual context defined in this article can be local , so no domain expert has to be involved in this approach . Our experiment consists of word clustering by similarity and rough concept recognition , associated to a basic LSA retrieval system . Our information retrieval process is illustrated through our experimentation model and results are compared in two different aspects . Experiment results show that retrieval performance benefit can be gained from this approach and further performance benefits can also be obtained according to the further work , which needs researching about parameter settings and algorithm development . 

Cross - Dataset Design Discussion Mining
Being able to identify software discussions that are primarily about design — which we call design mining — can improve documentation and maintenance of software systems . Existing design mining approaches have good classification performance using natural language processing ( NLP ) techniques , but the conclusion stability of these approaches is generally poor . A classifier trained on a given dataset of software projects has so far not worked well on different artifacts or different datasets . In this study , we replicate and synthesize these earlier results in a meta — analysis . We then apply recent work in transfer learning for NLP to the problem of design mining . However , for our datasets , these deep transfer learning classifiers perform no better than less complex classifiers . We conclude by discussing some reasons behind the transfer learning approach to design mining . 

A web application using RDF / RDFS for metadata navigation
This paper describes using RDF / RDFS / XML to create and navigate a metadata model of relationships among entities in text . The metadata we create is roughly an order of magnitude smaller than the content being modeled , it provides the end - user with context sensitive information about the hyperlinked entities in focus . These entities at the core of the model are originally found and resolved using a combination of information extraction and record linkage techniques . The RDF / RDFS metadata model is then used to " look ahead " and navigate to related information . An RDF aware front - end web application streamlines the presentation of information to the end user . 

The Deliberative E - Rulemaking project ( DeER ): improving federal agency rulemaking via natural language processing and citizen dialogue
Many scholars believe that electronic rulemaking has great but largely untapped potential to expand the public ' s democratic input and improve federal agency regulatory rules . The existing federal rulemaking process , however , elicits many redundant and poorly considered comments , and some participants challenge the legitimacy of agency rulemaking decisions . This Deliberative E - Rulemaking ( DeER ) project tests the application of Natural Language Processing ( NLP ) technologies and public deliberation techniques to improve the quality and organization of public comments and the legitimacy of the federal rulemaking process . This paper describes the goals , innovations , research design , technology , and preliminary results of the DeER project . The social science herein advances research on measuring the quality of public deliberation , the effects of deliberation on individuals and communities , and the emergence of distributed intelligence in networked decision making groups . 

Digital Marketing with Social Media : What Twitter Says ! 
Due to the short and simple way of expression on social media platforms such as Facebook and Twitter , millions of people share daily real - time thoughts and opinions about everything . This shared data generates an increasing availability of unstructured , informal and yet valuable information to data science researchers . Traditional approaches are not the wisest path for collecting and studying consumer behavior because they require a large amount of time and resources and therefore lead to considerable losses for companies . In this paper , we develop a system able to identify and classify sentiment represented in an electronic text from Twitter where users post real - time reactions and opinions called tweets ; that are sentences limited to 280 characters about everything to improve the decision - making process for companies . To do so , we used tweepy to access Twitters Streaming API , we combined natural language processing techniques with naive Bayes networks to classify users data , we used GIS ( geographical information system ) and Matplotlib for data visualization and displaying the results . The purpose of this paper is to propose an efficient approach for predicting accurate sentiment from raw unstructured data in order to extract opinions from the Internet and predict online customers preferences , which could be valuable and crucial for economic and marketing researchers . 

Can I find information about rare diseases in some other language ? 
Natural Language Processing ( NLP ) is a field that joins computer science and linguistics in an attempt to mimic , artificially , human language understanding . This paper applied NLP in the medical domain . The trigger that motivated this research was an expert reading an article about a rare disease who was interested in finding related documents . Being aware of the fact that language boundaries often limit , unnecessarily , the amount of information found , the goal of our work is to retrieve information without bounding to translation methods . Semantic similarity approaches offer a framework to represent related words and sentences in a dense space . In this work , we turned to cross - lingual dense spaces to represent bilingual documents in a shared dense space . Our approach helped to retrieve both intraand cross - lingual documents just resting upon a few parallel documents to infer the optimal mapping from . From the experimental results we learned that an important issue is to keep aligned the mapping space and the crosslingual search space . The cosine similarity outperforms both Euclidean and Manhattan distance . The results obtained in our preliminary experiments suggest that , although there is room for improvement , our approach performs satisfactorily achieving a P @ 10 of 71 . 72 searching English documents and returning Spanish related documents and 70 . 80 in the opposite direction . 

Sarcasm Detection Using Recurrent Neural Network
Sarcasm is a sharp and often ironic utterance that is meant to convey contempt or mock . In today ' s world , one of the challenging problems for opinion mining task is sarcasm detection . Many researchers are exploring the properties of sarcasm like semantic properties , syntactical properties , lexical feature etc , to design algorithms for sarcasm detection . We aim at using a recurrent neural network ( RNN ) model for sarcasm detection because it automatically extracts features required for machine learning approaches . Along with the recurrent neural network , this model also uses long short - term memory ( LSTM ) cells on tensorflow to capture syntactic and semantic information over Twitter tweets to detect sarcasm . Finally , we present the result of this model and a statistical overview of the dataset . 

Korea University system in the HOO 2012 shared task
In this paper , we describe the Korea University system that participated in the HOO 2012 Shared Task on the correction of preposition and determiner errors in non - native speaker texts . We focus our work on training the system on a large collection of error - tagged texts provided by the HOO 2012 Shared Task organizers and incrementally applying several methods to achieve better performance . 

Word based emotion conversion in Hindi language
The main system to communicate with computers is the use of natural language in text form . To add naturalness and intelligibility , the speech form is becoming an important method to communicate with computers and other machines . Human - machine and human - robot dialogues in the next generation will be conquered by natural speech , which is fully impulsive and thus obsessed by emotion . Emotion adds expressiveness to the natural language speech . There is a great zeal to research in this field . In this paper we have proposed an algorithm for word based emotion conversion of neutral speech into emotional speech like ` happy ' and ` sad ' for Hindi language . This emotion conversion algorithm is based on the segmentation of the spoken utterance into words and the pitch differences of these words between different emotions . The segmentation of words in the spoken utterance is done using another algorithm of word boundary detection , which gives details of start of the word , end of the word and the no . of words in a sentence . This algorithm of word boundary detection is devised for Hindi language and is based on two main prosodic features ` pitch ' and ` intensity ' . 

Support for Interview Preparation with Deep Learning Based Language Model
Deep - learning based language models ( LMs ) have significantly enhanced services relating to language generation and classification . Our focus in this paper is on the Multiple Mini Interview ( MMI ) which is commonly used internationally by medical schools to screen applicants based on their ability to answer short questions in a considerate , professional manner . In this paper we establish the ability of LMs , specifically GPT - 3 , to generate MMI questions , simulate responses , and rate answers . We compare these simulated questions with their human generated counterparts and , after identifying the optimal hyperparameters , find that 92 % of generated responses are capable of fooling humans . We also find that after identifying the optimal hyperparameters for question - answering , LMs are capable producing high quality simulated MMI responses , with an average human rating of 3 . 5 out of 5 . Finally , GPT - 3 is shown to have some agreement with human ratings , although it tends to overestimate the quality of the response . Conditional text generation by LMs alone seems to be able to significantly support MMI preparation . 

Development and Enhancement of a Stemmer for the Greek Language
Although there are three stemmers published for the Greek language , only the one presented in this paper and called Ntais ' stemmer is freely open and available , together with its enhancements and extensions according to Saroukos ' algorithm . The primary algorithm ( Ntais ' algorithm ) uses only capital letters and works with better performance than other past stemming algorithms for the Greek language , giving 92 . 1 percent correct results . Further extensions of the proposed stemming system ( e . g . from capital to small letters ) and more evaluation methods are presented according to a new and improved algorithm , Saroukos ' algorithm . Stemmer performance metrics are further used for evaluating the existing stemming system and algorithm and show how its accuracy and completeness are enhanced . The improvements were possible by providing an alternative implementation in the programming language PHP , which offers more syntactical rules and exceptions . The two versions of the stemming algorithm are tested and compared . 

Knowledge Enhanced Fact Checking and Verification
As the Internet and social media offer increasing opportunities for organizations and individuals to publicize online contents , it has become essential to develop effective means to identify misinformation like fake news . Recently , fact checking systems have been regarded as a promising tool to automatically deal with large amounts of information . How to effectively take advantage of existing unstructured document knowledge bases and structured knowledge graphs to build robust fact checking systems , however , remains to be a challenge . In this paper , we propose a knowledge enhanced fact checking system , which leverages the Wikidata5M knowledge graph and Wikipedia documents to incorporate external knowledge into the claim to be checked for more robust and accurate fact checking . First , we devise a contextualized knowledge graph selection method to identify the most relevant sub - graph with the checked claim from the large knowledge graph . We then construct a novel claim - evidence - knowledge graph and use a graph attention network to integrate natural language evidence with structured knowledge triplets by allowing them to propagate information among each other . By integrating the claim , retrieved evidence and selected knowledge triplets in a unified claim - evidence - knowledge graph , our method improves the label accuracy of predicted claims by more than 4 % on the FEVER dataset over state - of - the - art fact checking models . 

Predicting Customer Churn in the Telecommunication Industry by Analyzing Phone Call Transcripts with Convolutional Neural Networks
For telecommunication service providers , a principle method for reducing costs and generating revenue is to focus on retaining existing customers rather than acquiring new customers . To support this strategy , it is important to understand customer concerns as early as possible to prevent churn : The customer action of canceling a subscription and moving to a new provider . In this paper , we use actual customer phone call data and develop the convolutional neural network ( CNN ) - based predictive model to detect churn signals from transcript data of phone calls . Experimental results show that when sufficient training data is provided with our text annotation method , our CNN - based predictive model generates state - of - the - art performance in churn prediction . 

Review of Deep Learning Techniques for Improving the Performance of Machine Reading Comprehension Problem
The amazing research of Artificial Intelligence is natural language processing ( NLP ) and the mesmerizing field in NLP is machine reading comprehension ( MRC ) . MRC alleviates the efforts of making machines behave like a human as it helps information accessing in natural language by developing Question answering systems . MRC is summarized as a task to read a piece of text , understand it , and answer the related question of the text . Reading text can be cloze style reading ( fill in the blanks from the text ) as well as open style reading ( separate question ) and understanding the piece of text as well as the query is accomplished by contextual representation and Attention mechanism . In the MRC literature , various methodologies have been used for extracting answers from the given text including primitive methods to the deep learning methods to have a step towards deploying machine intelligence . The introduction of deep learning and large datasets in the recent few years has encouraged the success of MRC . This paper gives a recent review of MRC models based on deep learning , datasets on which they have been evaluated , and also their word representations . 

Using natural language processing for analyzing Arabic poetry rhythm
One from the most difficult tasks for Natural Language Processing ( NLP ) is to analyze poetry , which uses a different genre of language than that considered by computer - based techniques . Therefore , computational analysis is an interesting task when we use NLP in poetry , but it is also challenging . There are a number of researchers that entered this field from NLP and they got promising results using mathematical analysis for poetry , including rhythm analysis . In this paper we focused on providing solution for automating the rhythm detection of the Arabic poems and finding the number of rhythm for each verse in poem and the total percentage for each rhythm in all verses of poem , additional to other characteristics for the Arabic poem like percentage of mobile letter in Arabic “ harf mutaharrik” , and The quiescent letter , in Arabic “ harf sakin” , In spite of the number of studies in the computational analysis of poetry , we think it needs more , not only to make a better understanding of domain but also in developing applications , considering different literary tastes and the psychological effects , to give a recommendation to the readers and in plagiarism detection [ 1] . 

Neural Machine Translation System of Indic Languages - An Attention based Approach
Neural machine translation ( NMT ) is a recent and effective technique which led to remarkable improvements in comparison of conventional machine translation techniques . Proposed neural machine translation model developed for the Gujarati language contains encoder - decoder with attention mechanism . In India , almost all the languages are originated from their ancestral language - Sanskrit . They are having inevitable similarities including lexical and named entity similarity . Translating into Indic languages is always be a challenging task . In this paper , we have presented the neural machine translation system ( NMT ) that can efficiently translate Indic languages like Hindi and Gujarati that together covers more than 58 . 49 percentage of total speakers in the country . We have compared the performance of our NMT model with automatic evaluation matrices such as BLEU , perplexity and TER matrix . The comparison of our network with Google translate is also presented where it outperformed with a margin of 6 BLEU score on English - Gujarati translation . 

Investigating Cybersecurity News Articles by Applying Topic Modeling Method
Machine Learning ( ML ) and specifically Natural Language Processing ( NLP ) are increasingly used as tools in the cybersecurity world . These NLP tools bring new capabilities that support both defenders and attackers in their activities , whether it is risk scenarios such as events and threats or security operations . Ours is a unique case study as we are investigating cybersecurity news on a national and global level . This large study covered six countries and 18 major newspapers and analyzed thousands of cybersecurity articles using the Nonnegative Matrix Factorization ( NMF ) topic modeling method . News making and policymaking complement each other in forming national identities . This research aims to provide the foundation for the field of Cybersecurity in this direction . Our results showed the US dominance and its significance for other countries . This research also highlighted that much of the US media ' s cybersecurity reporting focuses on domestic issues , unlike other nations . 

The Utility of Context When Extracting Entities From Legal Documents
When reviewing documents for legal tasks such as Mergers and Acquisitions , granular information ( such as start dates and exit clauses ) need to be identified and extracted . Inspired by previous work in Named Entity Recognition ( NER ) , we investigate how NER techniques can be leveraged to aid lawyers in this review process . Due to the extremely low prevalence of target information in legal documents , we find that the traditional approach of tagging all sentences in a document is inferior , in both effectiveness and data required to train and predict , to using a first - pass layer to identify sentences that are likely to contain the relevant information and then running the more traditional sentence - level sequence tagging . Moreover , we find that such entity - level models can be improved by training on a balanced sample of relevant and non - relevant sentences . We additionally describe the use of our system in production and how its usage by clients means that deep learning architectures tend to be cost inefficient , especially with respect to the necessary time to train models . 

Graph Learning Based Sentiment Analysis System for Chinese Course Evaluation
Natural language processing ( NLP ) is an important research direction of artificial intelligence . Text sentiment classification in NLP is a compromising method to exploit the constructive feedback to improve teaching quality . This paper captures the course reviews from online learning platform China University MOOC as the dataset , and uses an aspect - level sentiment classification method to analyze the course evaluation , via a graph convolution network ( GCN ) to characterize the syntactic dependency between context words and various aspects of sentences , and decide the emotions described by multiple non - adjacent Chinese words . As for the 1837 comments of online courses , there is obvious aggregation in the aspect of extraction . Most of the comments mainly focus on the two aspects of course and teacher , and a few comments describe other aspects related to the course . The results demonstrate that the accuracy of the model is more than 80% . Additionally , a visual interface is designed to provide the sentiment analysis results no matter what data set of course reviews is given , and make the graph learning based sentiment analysis tool user - friendly . 

Estimation of Spectral Abundance Fractions using Fixed Acceleration Coefficients PSO Approach
The occurrence of mixed pixels is common in hyperspectral data . It is necessary to analyse mixed pixels for classification , detection , discrimination , and quantification . Spectral unmixing is needed for mixed pixel analysis of the hyperspectral data . It includes endmember extraction and abundance estimation of mixed pixels . In this work , fixed acceleration coefficients based PSO approach is applied and analysed for abundance fractions estimation of endmembers in spectral unmixing . Time varying inertia weight strategy and fixed acceleration coefficient values have been used in this approach . For estimation , supervised linear mixing model is considered , following sum - to - one and non - negative constraints , respectively . A proposed approach is tested over real hyperspectral data i . e . , jasper ridge dataset . The performance metrics of the approach are Average Abundance Error ( AAE ) and Root Mean Square Error ( RMSE ) . AAE and RMSE values have been noted over different number of iterations . It is observed that result of fixed acceleration coefficients based PSO approach is promising . 

The Quest for NLP Applications and Tools : The Case of Standard Arabic and the Dialects
The rapid advances of natural language processing ( NLP ) research bring about applications and tools that serve automating Arabic at different linguistic levels . However , there is a focus on the modern standard Arabic ( MSA ) more than the dialects which are used heavily in day - to - day communication and require special handling . This paper introduces research efforts in Arabic NLP with emphasis on tools , applications , and resources related to core NLP areas . Some research and scholarly work come from researchers working on extended projects to compute Arabic . The findings demonstrated that research on Arabic NLP can be characterized by emphasis on morphological analysis and processing of MSA especially tokenization and tagging . Recently there has been a growing interest in processing and identifying Arabic dialects . The proposed solutions were adapted from other solutions used for MSA or designed mainly for certain dialects . Also , advances in developing transformers were applied to Arabic NLP in limited research . It appeared that ambiguity is still a big challenge to overcome and there is a need to innovate applications and tools for Arabic MSA and the dialects in parallel . 

Position - Aware Deep Character - Level CTR Prediction for Sponsored Search
Predicting the click - through rate of an advertisement is a critical component of online advertising platforms . In sponsored search , the click - through rate estimates the probability that a displayed advertisement is clicked by a user after she submits a query to the search engine . Commercial search engines typically rely on machine learning models trained with a large number of features to make such predictions . This inevitably requires a lot of engineering efforts to define , compute , and select the appropriate features . In this paper , we propose two novel approaches ( one working at character level and the other working at word level ) that use deep convolutional neural networks to predict the click - through rate of a query - advertisement pair . Specifically , the proposed architectures consider as input only the textual content appearing in a query - advertisement pair and the page position at which the advertisement appears on the search result page of the query , and produce as output a click - through rate prediction . By comparing the character - level model with the word - level model , we show that language representation can be learnt from scratch at character level when trained on enough data . Through extensive experiments using billions of query - advertisement pairs of a popular commercial search engine , we demonstrate that both approaches significantly outperform a baseline model built on well - selected text features and a state - of - the - art word2vec - based approach . We also show the importance of the position feature in the proposed approaches in improving the prediction accuracy . When combining the predictions of the deep models introduced in this study with the prediction of the model in production of the same commercial search engine , we significantly improve the accuracy and the calibration of the click - through rate prediction of the production system . We also show the potential of leveraging the CTR prediction of the proposed deep learning models for query - ad relevance modeling and query - ad matching tasks in sponsored search . 

A natural language exchange model for enabling human , agent , robot and machine interaction
Models of communications in heterogeneous systems support exchange between agents of different types . A key component is making the heterogeneous agents appear indistinguishable to and from each other in terms of language , to normalize communication . A goal is to act as an open system , where the agents can come and exit as needed by the requirements of the overall goal of the system . The main goal of this research is the creation of a communicative model to support interaction , organization and collective intelligence features between a heterogeneous set of agents as machines , robots , software agents , and humans , all working in a cooperative organization . Communication appears as a natural language interface between all agents to enable clear , indistinguishable communication between all within the organization . 

Topic Modelling Twitterati Sentiments using Latent Dirichlet Allocation during Demonetization
Twitter has surfaced as one of the major social media platforms for sharing political views on pressing issues for the common man . In this paper , we attempt to apply a topic modelling technique , namely Latent Dirichlet Allocation ( LDA ) on tweets to analyse and come up with pertinent topics with the most relevant words that describe the topics most aptly . The tweets contain the demonetization hashtag to help us understand sentiments of people about demonetization . This technique can be used by analysts across any industry to understand the pertinent topics revolving around any social issue to take further actions in their organizations . Leveraging this sort of statistical topic modelling can be quite useful to researchers to correctly identify primary components of huge textual corpora for any kind of further analysis . The model comes up with very meaningful categorization of topics . Further , we measure the inter - topic distances via multidimensional scaling and review words and topics through metrics such as saliency and relevance . 

Semi - Supervised Mix - Hindi Sentiment Analysis using Neural Network
Most of the people in the world now a day would like to share and express their feelings , views , experiences , suggestions and opinions on the web . These opinions are processed by sentiment analysis task and find their polarity . In this paper , we use input text file in Devanagari script stored in UTF - 8 encoding scheme . We propose 3 approaches for doing sentiment analysis for Hindi multidomain review . In approach 1 , classification of data is done using NN Prediction by using pre - classified words . In approach 2 , classification of data is done using IIT - Bombay Hindi SentiWordNet ( HSWN ) . In approach 3 , classification of data is done using NN prediction using pre - classified sentences as labeled data . Finally , we report accuracies in every approach . We have different domain ( Health , Business , Current affairs , Tourism , Movie , Technology and Product ) review dataset manually and randomly collected by us . They contain Mix - Hindi words like ( brave ) , ( careful ) , ( mineral ) , etc . , for which we have created dictionary to deal with them . We achieve overall accuracy of 52 % in approach 1 , 71 . 5 % accuracy in approach 2 and 70 . 27 % accuracy in approach 3 . 

The Intelligent Agent NLP - based Customer Service System
In Nowadays , communication has been more and more important in our lives . It is difficult for us to complete many things without it . With the support of the developing technology , communication is able to become faster and easier . The human - to - machine communication is a creative application used in field of research and industry . To improve the interaction between human and machine , a communication system is specially designed . The technology of natural language processing is implemented in the system to handle the understanding and generation of the chatting language . For higher efficiency , the system is enhanced by designing in the multi - agent system . Which let agents deal with the detailed tasks in the process of NLP by interacting and integrating them together . The system is based on sending and receiving so that it is able to communicate asking question and responding answer . Provide a textbook to the Chatbot , it is capable to understanding the content of book . And then serve as a teaching assistant to help student solve their problems by answering their questions . In this thesis , we will reveal the design and implementation of different parts in the communication system . Which consists of natural language processing system , multi - agent system , user interface ( Model - View - Controller Frame) , and knowledge base . System is implemented by Java programme language . 

Adversarial Examples in Arabic
Several studies have shown that deep neural networks ( DNNs ) are vulnerable to adversarial examples - perturbed inputs that cause DNN - based models to produce incorrect outputs . A variety of adversarial attacks have been proposed in the domains of computer vision and natural language processing ( NLP ) ; however , most attacks in the NLP domain have been applied to DNNs that were trained on English corpora . This paper proposes the first set of black - box adversarial attacks designed to perturb Arabic textual inputs . By intentionally violating the noun - adjective agreement in Arabic , two state - of - the - art DNN architectures are successfully fooled in the task of sentiment analysis , and classification accuracy was reduced by an average of 52 . 97 % for the word - level BiLSTM model and 50 . 44 % for the word - level CNN model . We believe that our findings will encourage other researchers to investigate the robustness of DNNs when applied to natural languages beyond English . 

Exploring grammatical error correction with not - so - crummy machine translation
To date , most work in grammatical error correction has focused on targeting specific error types . We present a probe study into whether we can use round - trip translations obtained from Google Translate via 8 different pivot languages for whole - sentence grammatical error correction . We develop a novel alignment algorithm for combining multiple round - trip translations into a lattice using the TERp machine translation metric . We further implement six different methods for extracting whole - sentence corrections from the lattice . Our preliminary experiments yield fairly satisfactory results but leave significant room for improvement . Most importantly , though , they make it clear the methods we propose have strong potential and require further study . 

DeepQA Jeopardy ! Gamification : A Machine - Learning Perspective
DeepQA is a large - scale natural language processing ( NLP ) question - and - answer system that responds across a breadth of structured and unstructured data , from hundreds of analytics that are combined with over 50 models , trained through machine learning . After the 2011 historic milestone of defeating the two best human players in the Jeopardy ! game show , the technology behind IBM Watson , DeepQA , is undergoing gamification into real - world business problems . Gamifying a business domain for Watson is a composite of functional , content , and training adaptation for nongame play . During domain gamification for medical , financial , government , or any other business , each system change affects the machine - learning process . As opposed to the original Watson Jeopardy ! , whose class distribution of positive - to - negative labels is 1 : 100 , in adaptation the computed training instances , question - and - answer pairs transformed into true - false labels , result in a very low positive - to - negative ratio of 1 : 100 000 . Such initial extreme class imbalance during domain gamification poses a big challenge for the Watson machine - learning pipelines . The combination of ingested corpus sets , question - and - answer pairs , configuration settings , and NLP algorithms contribute toward the challenging data state . We propose several data engineering techniques , such as answer key vetting and expansion , source ingestion , oversampling classes , and question set modifications to increase the computed true labels . In addition , algorithm engineering , such as an implementation of the Newton - Raphson logistic regression with a regularization term , relaxes the constraints of class imbalance during training adaptation . We conclude by empirically demonstrating that data and algorithm engineering are complementary and indispensable to overcome the challenges in this first Watson gamification for real - world business problems . 

Expert and Student User Evaluation of Semantic Protocol Search
Evaluation of intelligent search with support from advanced natural language processing ( NLP ) technologies is labor - intense and related tasks are very trivial . This study introduced user relevance feedback procedures and relevance measures to evaluate our SPRIT - NLP semantic search system . The historical protocol archives of our organization were annotated using UMLS ( Unified Medical Language Systems ) concepts and indexed by Solr to test these evaluation settings . The outcome demonstrated concept - based semantic search is very effective to retrieve many categories of clinical queries . 

The Building of a CBD - Based Domain Ontology in Chinese
This paper describes a method of building a medical ontology prototype in Chinese . During the procedure , we explored the following questions , which are crucial for the task of ontology engineering : ( 1 ) Are there some more computer - understandable knowledge description models ? We proposed a structural and fine - grained knowledge description model called concept - based description model ( CBD ) to describe the rich knowledge in the ontology . That is , to use other concept or the combination of the related concepts to represent the targeted concept , which is supposed to be more computer - understandable ; ( 2 ) During large scale ontology engineering , how to use NLP technologies to reduce domain experts ’ work to the minimal ? In our work , we used some NLP technologies to try to reduce domain experts ’ work to the minimal as possible as it can . The experiments show the significance of our method . 

Bridging the Gap of Dimensions in Distillation : Understanding the knowledge transfer between different - dimensional semantic spaces
In recent years , knowledge distillation has been widely used in the field of deep learning in order to reduce the model size and save time and space . The student - teacher paradigm is a framework for knowledge distillation , and knowledge distillation proposed to minimize the KL divergence between the probabilistic outputs of a teacher and student network . However , apart from the probabilistic outputs , there are much valuable information contained in the middle layers of the teacher network . As for NLP tasks , the hidden vectors from different layers of a model have different semantic information , but the vectors ' dimension of the student network is different from that of the teacher network in many cases , which makes hidden layer distillation hard to be performed directly . We propose to simply use a transition matrix to project the student ' s vector to a space of the same dimension as the teacher ' s vector , and we theoretically prove the effectiveness of this method . Our analysis shows how the transition matrix preserve important semantic information , which is closely related to the vector ' s characteristic in Euclidean space . We provide a geometric method for the interpretability of shared knowledge space for student - teacher architectures . Our experiments show that this method can significantly improve the performance of a small model in different tasks with different models . 

The role of named entities in web people search
The ambiguity of person names in the Web has become a new area of interest for NLP researchers . This challenging problem has been formulated as the task of clustering Web search results ( returned in response to a person name query ) according to the individual they mention . In this paper we compare the coverage , reliability and independence of a number of features that are potential information sources for this clustering task , paying special attention to the role of named entities in the texts to be clustered . Although named entities are used in most approaches , our results show that , independently of the Machine Learning or Clustering algorithm used , named entity recognition and classification per se only make a small contribution to solve the problem . 

Implementation Approach of Indian Language Gujarati Grammar ' s Concept “ sandhi ” using the Concepts of Rule - based NLP
The term ` language ' in NLP has to be understood as natural languages like Gujarati , Hindi , English etc . , which we use in daily life to communicate . Most of the NLP research has been centered on English & other European Languages . NLP research concerning the Indian language like Gujarati is commenced in the last few years . The centre of attention of this paper is to demonstrate the road map of implementation of Gujarati grammar ' s concept “ sandhi ” . In our words sandhi is a word segmentation process & it is present in most of the South Asian language , such as Devnagri , Sanskrit , Hindi , and Gujarati & even in Chinese & Thai languages . ” Sandhi leads to phonetic transformation at word boundaries of a written chunk ( small part ) , and the sounds at the end of word join together to form a single chunk of the character sequence . ” Our main spotlight is on rule - based implementation of “ sandhi” . Similar to every Indian scripting language Gujarati language ( Grammar ) also has its own specified rules of composition for combining the consonants , vowels and modifiers . We have identified certain rules by which we accomplish the practical implementation of “ sandhi ” . There are many sandhi rules available , each denoting a unique combination of phonetic transformations , documented in the grammatical tradition of Gujarati . The Sandhi does not make any syntactic or semantic changes to the words implicated . Sandhi is an elective operation that depends only on the alertness of the writer . 

Document representation combining concepts and words in Chinese text categorization
Word - based representation is widely used in text categorization . However , performance of this approach is affected by the problems derived from language variation . In this paper , we investigate a document representation combining words and concepts to integrate the advantages of two types of representations . The approach takes the part of speech as the concept for the word which is error - prone in word sense disambiguation to reduce the disambiguation mistakes . The approach employs three ways to measure the contributions of different representation forms to classification and selects the most productive one as the feature to drop the concepts not suitable for representation while not losing the lexical semantic information . We conduct experiments to compare the performance of different types of representations on Chinese text categorization corpus of Fudan University . And the results confirm the validity of our combination representation . 

RLV ' s re - entry trajectory optimization based on B - spline theory
A new computational optimization approach is proposed to generate Reusable Launch Vehicle ( RLV ) ' s re - entry trajectory . With the application of the new method , all state variables are approximated by B - spline curves . After the trajectory optimization problem is transformed into a general nonlinear programming ( NLP ) problem , which parameters to be optimized are coefficients of the B - spline curves and control variables at every node and interpolation point , a nonlinear programming solver is selected to determine the parameters . Simulation results show that the optimized re - entry trajectories satisfy the path constraints , and can accomplish the desired terminal conditions . Compared with the Direct Collocation with Nonlinear Programming ( DCNLP ) method , the new approach is better in continuity and has fewer coefficients to be determined ; moreover , the new approach can achieve better performance index . 

NP tree matching for English - Chinese translation of patent titles
This paper proposes a method of NP tree matching to realize the translation of English - Chinese patent titles . Firstly a bilingual example database for patent titles is built . English parse trees are produced by English parser , forming NP tree database . The input patent title to be translated is firstly parsed into a tree . Then NP trees are searched for which match with the input NP tree in NP tree database . If similar NP trees exist , HowNet is used to find the best NP tree by calculating word semantic similarity . Final translations are obtained through calculating cohesion of candidate words . If there are no similar NP trees , subtrees that match the input NP tree are searched for and translations generate by subtree substitutions recursively . Experimental results show that our method outperforms a baseline Pharaoh , by using BLEU evaluation system . 

Synonym Suggestion System using Word Embeddings
Word Embeddings have been used in various Natural Language Processing tasks . Recent methods of learning vector space representations of words have succeeded in capturing semantic and syntactic regularities in vector space . In this project , the main aim has been to use these word embeddings for generating synonyms that could be used to improve the quality of writing . This technique gives us the advantage of taking the context of a particular word into consideration . A regular dictionary framework may give synonyms which may not be used in the same context . 

Canonical representation in NLP system design : a critical evaluation
This paper is a critical evaluation of an approach to control in natural language processing systems which makes use of canonical structures as a way of collapsing multiple analyses in individual components . We give an overview here of how the Lucy natural language interface system currently realizes this control model and then evaluate what we take to be the strengths and weaknesses of such an approach . In particular , we conclude that the use of canonical structures can restrain combinatorial explosion in the search , but at the cost of breaking down the barriers between modules and of letting processing concerns infect the declarative representation of information . 

Intelligent tutoring systems : research status and its development in China
The first application of artificial intelligence ( AI ) to education has been to build intelligent tutoring systems ( ITS ) . With the development of modern information technologies , ITS has been increasingly applied in education more and more widely . The applications of ITS in education have been changing the traditional instructional model and making learning more effective and meaningful . In this paper , the definition , architecture , and characteristics of ITS , some typical ITS and the current status , as well as its research focus of ITS are presented . Then the current research and applications of ITS in China are introduced . In the end , the paper discusses the future development of ITS . 

Performance of Sentimental Analysis by Studying and Mining Social Media using Parsing Technique
The social media networks have evolved quickly and many people often use these services to communicate with others and categorize themselves by sharing their opinions , views and concepts . It is usually dispensed by winnowing the corresponding or interlinked events mentioned on social media websites like Twitter , Facebook , YouTube and Pinterest etc . A social media analysis has rapt from being a unidirectional to a bidirectional dialogue i . e . between generations to generation . The rudimentary objective behind such analyses is to visualize the extent of criticality with relevancy criticism or appreciation delineate at intervals the comments , tweets or blogs . This analysis shows how individuals assume , assess , talk and opine concerning completely different problems . This paper centers on the pattern location and examine the feelings communicated in a particular sentence , passage or archive . The opinions and sentiments are extracted from the collected data and it is fairly estimated based on the degree of quality . The rule commitment of this paper is to give a blueprint to the individuals who try to use internet based life scratching and examination utilizing distinctive programming apparatuses either in their investigation or business . In addition , the system design to implement the proposed idea has been discussed . 

Learning Personal Human Biases and Representations for Subjective Tasks in Natural Language Processing
Many tasks in natural language processing like offensive , toxic , or emotional text classification are subjective by nature . Humans tend to perceive textual content in their own individual way . Existing methods commonly rely on the agreed output values , the same for all consumers . Here , we propose personalized solutions to subjective tasks . Our four new deep learning models take into account not only the content but also the specificity of a given human . The models represent different approaches to learning the representation and processing data about text readers . The experiments were carried out on four datasets : Wikipedia discussion texts labelled with attack , aggression , and toxicity , as well as opinions annotated with ten numerical emotional categories . Emotional data was considered as multivariate regression ( multitask) , whereas Wikipedia data as independent classifications . All our models based on human biases and their representations significantly improve the prediction quality in subjective tasks evaluated from the individual ’ s perspective . 

A Study on a Joint Deep Learning Model for Myanmar Text Classification
Text classification is one of the most critical areas of research in the field of natural language processing ( NLP ) . Recently , most of the NLP tasks achieve remarkable performance by using deep learning models . Generally , deep learning models require a huge amount of data to be utilized . This paper uses pre - trained word vectors to handle the resource - demanding problem and studies the effectiveness of a joint Convolutional Neural Network and Long Short Term Memory ( CNN - LSTM ) for Myanmar text classification . The comparative analysis is performed on the baseline Convolutional Neural Networks ( CNN ) , Recurrent Neural Networks ( RNN ) and their combined model CNN - RNN . 

HMM - based phonemic distance in different speaking styles and its influence on substitutions in Mandarin speech recognition
Statistical confusability between different acoustic models is important to character substitution error rate in large vocabulary continuous speech recognition . In this paper , we take factors of gender and speaking styles into consideration in Mandarin speech recognition . We modeled phonemes in different speaking styles , including read speech of female , male , and spontaneous dialogue . Then minimum gaussian distances between Chinese Initial / Final model pairs are given and average phoneme distances are calculated which denote the pronunciation varieties . The effect of different style to average phonemic distance is studied and relative articulation is given for three databases . Qualitative relationship between phone size and error rate in recognition is analytical researched , showing that for a particular phoneme , pronunciation variety is one of reasons for misidentification in recognizing process , which provides us a novel mind to reduce substitution errors . 

Word Level Language Identification and Back - Transliteration
In this paper , I describe a Rule based and List - Searching system for Word - Level Language Identification and Named Entity recognition in bilingual text . My method uses dictionary search , rules for LI and CRF++ , character n - gram for NER . The model does not use any Language specific rules , therefore can easily be replicated on most languages having mixed pair with English . The model also does back - transliteration of words into native language script and recognizes named entity . The model performance is carried on the test sets provided by the shared task on language Identification for English Hindi ( En - Hi ) Pair , Microsoft Research India . The experimental results show a consistent performance with high precision . 

Automated Resume Evaluation System using NLP
Recruiting candidates to fit a particular job profile is a task crucial to most of the companies . Due to increasing growth in online recruitment , traditional hiring methods are becoming inefficient . The conventional techniques usually include a labor - intensive process of manually searching through the applied candidates , reviewing their resumes , and then producing a shortlist of suitable candidates to be interviewed . In this era of technology , job searching has become smarter and more accessible at the same time . The companies receive enormous numbers of resumes / CVs , which are not always structured . There have been lots of work done for the job searching process . Whereas , the process of selecting a candidate based on their resume has not been entirely automated . This research proposes a model of extracting valuable information from the resume and ranking it according to the preference and requirement of the company . To achieve the desired goal , the entire process has been divided into three segments . The first segment consists of converting the unstructured resumes in structured data using NLP , and the second segment consists of the extraction phase , where the relevant information is extracted from the resume and giving them an identifier value . Finally , based on the values assigned , the resumes are ranked accordingly in the final segment . 

Prediction in speech coding : the modification of the coding of LPC parameters and nonlinear estimation technique by using ANN
Linear and non - linear prediction techniques of speech signal and the different performance of LPC algorithms are discussed . A new coding plan has been suggested to raise the quantification SNR by about 2 dB by coding the difference of the log - area ratio ( LAR ) of adjacent frames instead of the LAR . Then , the stable time delay ANN is used to carry out NLP ( non - linear prediction ) tasks . Experiment results show that only one NLP system is enough to deprive all short - term and long - term correlation from the speech samples and the energy from the output of the NLP system is much smaller than that obtained with the LP system . 

An improved method of keywords extraction based on short technology text
Keywords are the critical resources of information management and retrieval , automatic text classification and clustering . The keywords extraction plays an important role in the process of constructing structured text . Current algorithms of keywords extraction have matured in some ways . However the errors of word segmentation which caused by unknown words have been affected the performance of Chinese keywords extraction , particularly in the field of technological text . In order to solve the problem , this paper proposes an improved method of keywords extraction based on the relationship among words . Experiments show that the proposed method can effectively correct the errors caused by segmentation and improve the performance of keywords extraction , and it can also extend to other areas . 

Preprocessing Methods and Tools in Modelling Japanese for Text Classification
As a subset of Artificial Intelligence , Natural Language Processing ( NLP ) is a breakthrough in surpassing language barrier . Japanese language characteristics bring its own challenge in morphological analysis due to the uniqueness of Japanese grammatical system . By the rapid development of NLP tools , many Japanese NLP tools developed with limited ability yet specialized in running certain preprocessing methods . In this paper , the compilation of various methods and newly discovered tools for preprocess Japanese text are delivered to help people decide which Japanese NLP tools needs to be utilized to run some preprocessing methods . All of the Japanese preprocessing methods and tools are collected through literature review . It is concluded that depending on one NLP tool is not recommended since combination of Japanese NLP tools is required to finish Japanese preprocessing phase . 

Emotions Extracted from Text vs . True Emotions – An Empirical Evaluation in SE Context
Emotion awareness research in SE context has been growing in recent years . Currently , researchers often rely on textual communication records to extract emotion states using natural language processing techniques . However , how well these extracted emotion states reflect people ' s real emotions has not been thoroughly investigated . In this paper , we report a multi - level , longitudinal empirical study with 82 individual members in 27 project teams . We collected their self - reported retrospective emotion states on a weekly basis during their year - long projects and also extracted corresponding emotions from the textual communication records . We then model and compare the dynamics of these two types of emotions using multiple statistical and time series analysis methods . Our analyses yield a rich set of findings . The most important one is that the dynamics of emotions extracted using text - based algorithms often do not well reflect the dynamics of self - reported retrospective emotions . Besides , the extracted emotions match self - reported retrospective emotions better at the team - level . Our results also suggest that individual personalities and the team ' s emotion display norms significantly impact the match / mismatch . Our results should warn the research community about the limitations and challenges of applying text - based emotion recognition tools in SE research . 

Measuring topic homogeneity and its application to dictionary - based word sense disambiguation
The use of topical features is abundant in Natural Language Processing ( NLP ) , a major example being in dictionary - based Word Sense Disambiguation ( WSD ) . Yet previous research does not attempt to measure the level of topic cohesion in documents , despite assertions of its effects . This paper introduces a quantitative measure of Topic Homogeneity using a range of NLP resources and not requiring prior knowledge of correct senses . Evaluation is performed firstly by using the WordNet :: Domains package to create word - sets with varying levels of homogeneity and comparing our results with those expected . Additionally , to evaluate each measure ' s potential value , the homogeneity results are correlated against those of 3 co - occurrence / dictionary - based WSD techniques , tested on 1040 Semcor and SENSEVAL sub - documents . Many low - moderate correlations are found to exist with several in the moderate range ( above . 40 ) . These correlations surpass polysemy and senseentropy , the 2 most cited factors affecting WSD . Finally , a combined homogeneity measure achieves correlations of up to . 52 . 

A Link Prediction Approach for Accurately Mapping a Large - scale Arabic Lexical Resource to English WordNet
Success of Natural Language Processing ( NLP ) models , just like all advanced machine learning models , rely heavily on large - scale lexical resources . For English , English WordNet ( EWN ) is a leading example of a large - scale resource that has enabled advances in Natural Language Understanding ( NLU ) tasks such as word sense disambiguation , question answering , sentiment analysis , and emotion recognition . EWN includes sets of cognitive synonyms called synsets , which are interlinked by means of conceptual - semantic and lexical relations and where each synset expresses a distinct concept . However , other languages are still lagging behind in having large - scale and rich lexical resources similar to EWN . In this article , we focus on enabling the development of such resources for Arabic . While there have been efforts in developing an Arabic WordNet ( AWN ) , the current version of AWN has its limitations in size and in lacking transliteration standards , which are important for compatibility with Arabic NLP tools . Previous efforts for extending AWN resulted in a lexicon , called ArSenL , that overcame the size and the transliteration standard limitation but was limited in accuracy due to the heuristic approach that only considered surface matching between the English definitions from the Standard Arabic Morphological Analyzer ( SAMA ) and EWN synset terms , and that resulted in inaccurate mapping of Arabic lemmas to EWN ’ s synsets . Furthermore , there has been limited exploration of other expansion methods due to expensive manual validation needed . To address these limitations of simultaneously having large - scale size with high accuracy and standard representations , the mapping problem is formulated as a link prediction problem between a large - scale Arabic lexicon and EWN , where a word in one lexicon is linked to a word in another lexicon if the two words are semantically related . We use a semi - supervised approach to create a training dataset by finding common terms in the large - scale Arabic resource and AWN . This set of data becomes implicitly linked to EWN and can be used for training and evaluating prediction models . We propose the use of a two - step Boosting method , where the first step aims at linking English translations of SAMA ’ s terms to EWN ’ s synsets . The second step uses surface similarity between SAMA ’ s glosses and EWN ’ s synsets . The method results in a new large - scale Arabic lexicon that we call ArSenL 2 . 0 as a sequel to the previously developed sentiment lexicon ArSenL . A comprehensive study covering both intrinsic and extrinsic evaluations shows the superiority of the method compared to several baseline and state - of - the - art link prediction methods . Compared to previously developed ArSenL , ArSenL 2 . 0 included a larger set of sentimentally charged adjectives and verbs . It also showed higher linking accuracy on the ground truth data compared to previous ArSenL . For extrinsic evaluation , ArSenL 2 . 0 was used for sentiment analysis and showed , here , too , higher accuracy compared to previous ArSenL . 

Put Deep Learning to Work : Accelerate Deep Learning through Amazon SageMaker and ML Services
Deploying deep learning ( DL ) projects are becoming increasingly more pervasive at enterprises and startups alike . At Amazon , Machine Learning University ( MLU ) - trained engineers are taking DL to every aspect of Amazon ' s businesses , beyond just Amazon Go , Alexa , and Robotics . In this workshop , Wenming Ye ( AWS ) , Rachel Hu ( AWS ) , and Miro Enev ( Nvidia ) offer a practical next step in DL learning with instructions , and hands - on labs using the latest Nvidia GPUs and AWS Inferentia . You will explore the current trends powering AI / DL adoption , powerful new GPU / AWS Inferentia accelerator instances , distributed training and inference optimization in neural networks . 

Recognizing Transliterated Names from Chinese Texts Based on Support Vector Machines and Rules
According to the characteristics of transliterated names in Chinese texts , a method of automatic recognition of Chinese transliterated names combining support vector machines ( SVMs ) with rules is proposed . The attributes of feature vectors based on characters are extracted . A training set is established and the machine learning models of automatic identification of transliterated names are obtained by testing polynomial Kernel functions ; the knowledge cannot be acquired completely if we only use the machine learning model , which will affect the recall . Through careful error analysis , the base of recognition - rules is constructed as post - processing steps to overcome the shortcoming of machine learning model . The results show that the method is efficient for identifying transliterated names from Chinese textsView less

Mixed integer linear programming and nonlinear programming for optimal PMU placement
Phaser measurement units ( PMUs ) provide synchronized measurements of voltage and current phasors and can make state estimation more accurate . The objective of optimal PMU placement ( OPP ) problem is to minimize the number of PMUs required for the system to be completely observable . This paper presents two different formulations of optimal PMU placement ( OPP ) problem : mixed integer linear programming ( MILP ) and nonlinear programming ( NLP ) . For each formulation , modeling of power flow measurements , zero injection , limited communication facility , and single PMU failure is studied . The contribution of our paper is to conduct a comparison between the MILP and NLP formulations and show the advantages and disadvantages of each formulation . 

Automated construction of database interfaces : integrating statistical and relational learning for semantic parsing
The development of natural language interfaces ( NLI ' s ) for databases has been a challenging problem in natural language processing ( NLP ) since the 1970 ' s . The need for NLI ' s has become more pronounced due to the widespread access to complex databases now available through the Internet . A challenging problem for empirical NLP is the automated acquisition of NLI ' s from training examples . We present a method for integrating statistical and relational learning techniques for this task which exploits the strength of both approaches . Experimental results from three different domains suggest that such an approach is more robust than a previous purely logic - based approach . 

A 2 - phase frame - based knowledge extraction framework
We present an approach for extracting knowledge from natural language English texts where processing is decoupled in two phases . The first phase comprises several standard NLP tasks whose results are integrated in a single RDF graph of mentions . The second phase processes the mention graph with SPARQL - like mapping rules to produce a knowledge graph organized around semantic frames ( i . e . , prototypical descriptions of events and situations ) . The decoupling allows : ( i ) choosing different tools for the NLP tasks without affecting the remaining computation ; ( ii ) combining the outputs of different NLP tasks in non - trivial ways , leveraging their integrated and coherent representation in a mention graph ; and ( iii ) relating each piece of extracted knowledge to the mention ( s ) it comes from , leveraging the single RDF representation . We evaluate precision and recall of our approach on a gold standard , showing its competitiveness w . r . t . the state of the art . We also evaluate execution times and ( sampled ) accuracy on a corpus of 110K Wikipedia pages , showing the applicability of the approach on large corpora . 

Short descriptive answer evaluation using word - embedding techniques
The score given for short answers may vary from instructor to instructor . There are many short answer grading and essay grading systems existing that are either automated or semi - automated . Automated grading systems reduce human effort , but no popular tool still exists . In this paper , we are focusing on short - answer grading systems . We use simple and effective methods to evaluate short descriptive student answers . The similarity between each student ' s answer with its model answer is evaluated using word embedding algorithms . The similarity score is used to calculate the score . The accuracy of the scores obtained in the case of each algorithm is calculated and analyzed method - wise . 

Automatic Text Summarization Using Gensim Word2Vec and K - Means Clustering Algorithm
The significance of text summarization in the Natural Language Processing ( NLP ) community has now expanded because of the staggering increase in virtual textual materials . Text summary is the process created from one or multiple texts which convey important insight in a little form of the main text . Multiple text summarization technique assists to pick indispensable points of the original texts reducing time and effort require reading the whole document . The question was approached from a different point of view , in a different domain by using different concepts . Extractive and abstractive are the two main methods of summing up text . Though extractive summary is primarily concerned with what summary content the frequency of words , phrases , and sentences from the original document should be used . This research proposes a sentence based clustering algorithm ( K - Means ) for a single document . For feature extraction , we have used Gensim word2vec which is intended to automatically extract semantic topics from documents in the most efficient way possible . 

Summarising legal texts : sentential tense and argumentative roles
We report on the SUM project which applies automatic summarisation techniques to the legal domain . We pursue a methodology based on Teufel and Moens ( 2002 ) where sentences are classified according to their argumentative role . We describe some experiments with judgments of the House of Lords where we have performed automatic linguistic annotation of a small sample set in order to explore correlations between linguistic features and argumentative roles . We use state - of - the - art NLP techniques to perform the linguistic annotation using XML - based tools and a combination of rule - based and statistical methods . We focus here on the predictive capacity of tense and aspect features for a classifier . 

Integrating high precision rules with statistical sequence classifiers for accuracy and speed
Integrating rules and statistical systems is a challenge often faced by natural language processing system builders . A common subclass is integrating high precision rules with a Markov statistical sequence classifier . In this paper we suggest that using such rules to constrain the sequence classifier decoder results in superior accuracy and efficiency . In a case study of a named entity tagging system , we provide evidence that this method of combination does prove efficient than other methods . The accuracy was the same . 

Web Mining for Improving Risk Assessment in Port State Control Inspection
Port state control ( PSC ) inspection is the most important mechanism to ensure world marine safety . Existing PSC risk assessment systems estimate the risk of each candidate ship on the target factors , which is recorded in the inspection database , to help the port authorities identify ships at high risks . The performance of these systems is difficult to be improved due to the limited available factors . This paper presents an improved risk assessment system , which is strengthened by web mining technique . This system employs profile - based wrapper to extract inspection details from inspection report web pages and adopts a template - matching - based method to extract new target features from deficiency details . By incorporating new target features , the risk assessment system based on Support Vector Machine is improved . Experimental results have shown that the new system improves the risk assessment accuracy effectively . 

KaleCare : Smart Farm for Kale with Pests Detection System using Machine Learning
Kale is a popular ingredient in Thai cuisine and can be grown year - round . However , kale requires particular care , especially pests . Therefore , this study applies the Internet of Things to propose the KaleCare , a smart farm management system for kale with four main functions including automatic watering based on weather forecasting , automatic fertilizing , reporting , and pest detection for cutworms , and aphids . There are three processes to create the pest classification models for pest detection function . Firstly , the raw images were applied to the GrabCut to remove the background . Secondly , data augmentation was applied to generate images due to the small amount of raw data . Finally , the modified GoogLeNet reduced the original GoogLeNet structure is proposed to classify both types of pests . The experimental results show that the proposed model outperforms with 0 . 8903 and 0 . 7959 in average classification rate and 0 . 886 and 0 . 7965 in average F1 - score to classify cutworm and aphid , respectively . 

An NLP tool for decoding the ATC Phraseology from English to Bengali
In recent times , the translation and transliteration capabilities are growing at an unprecedented rate . The increasing research in machine translation [ MT ] has led to the development of more powerful real - time translation systems . Most of the neural machine translation [ NMT ] and statistical machine translation [ SMT ] systems are mainly dependent on the parallel corpus , which has been used to train the model . As the applications of machine translation [ MT ] methods are more focused on agriculture , tourism , health care and other such smart applications . Its implementation lags in certain other domains like aviation , aero - space and so on . Though many SMT and NMT systems exist today , still in lacks to perfectly translate and transliterate the sentences related to aviation . As a result , the aviation related translation and transliteration tools requires significant research attention . Hence , this paper proposes a Natural Language Processing [ NLP ] tool to decode the required Air Traffic Controllers [ ATC ] phraseology from English to a local language Bengali . 

Intelligence Embedded Image Caption Generator using LSTM based RNN Model
Humans tend to extract information from everything they see be it living or non - living . This whole phenomenon motivated us to move in this direction and explore the field of computer vision and how this can be used with recurrent neural networks to generate captions from any image . By witnessing the recent increase in natural language processing - based applications ; various other researchers have also worked on this concept and produced commendable results . Describing an image is not an easy task to implement , the structure and semantics of a sentence hold an important weight age in sentence formation . This paper approaches the problem of caption generation with an LSTM ( Long - Short Term Memory ) based RNN model and builds architecture based on the same to generate efficient and meaningful captions by training the dataset effectively . Flicker8k dataset is used to train our model and worked well . The accuracy of the model is evaluated based on standard evaluation metrics . 

The role of argumentation in online epistemic communities : the anatomy of a conflict in Wikipedia
Motivation - - This research aims to investigate the processes by which knowledge objects - - - in this case Wikipedia pages on astronomy - - - are elaborated , in online communities , focussing on the role of argumentative interactions . Research approach - - We articulate qualitative multidimensional analysis of online discussions , in relation to elaboration of Wikipedia pages , with automatic semantic and syntactic Natural Language Processing ( NLP ) analysis focussed on identifying the roles of dialogical argumentation processes . Findings / Design - - Knowledge objects in online communities are jointly shaped by socio - relational and epistemic processes . Research limitations / Implications - - Our analysis method , based on previous research , is presently restricted to in - depth analysis of a small number of discussions . In ongoing work , our objective is to apply the method to the whole corpus of the Wikipedia astronomy online epistemic community . Originality / Value - - Our qualitative analysis approach distinguishes multiple functions of dialogue applying to diverse contents ( task , interlocutor - related ) , in relation to automatic NLP analysis . Take away message - - The way that online epistemic communities function goes beyond knowledge - based discussion and argumentation , to involve negotiation of competencies of so - called ' experts ' and ' amateurs ' . 

Learning Syllables Using Conv - LSTM Model for Swahili Word Representation and Part - of - speech Tagging
The need to capture intra - word information in natural language processing ( NLP ) tasks has inspired research in learning various word representations at word , character , or morpheme levels , but little attention has been given to syllables from a syllabic alphabet . Motivated by the success of compositional models in morphological languages , we present a Convolutional - long short term memory ( Conv - LSTM ) model for constructing Swahili word representation vectors from syllables . The unified architecture addresses the word agglutination and polysemous nature of Swahili by extracting high - level syllable features using a convolutional neural network ( CNN ) and then composes quality word embeddings with a long short term memory ( LSTM ) . The word embeddings are then validated using a syllable - aware language model ( 31 . 267 ) and a part - of - speech ( POS ) tagging task ( 98 . 78 ) , both yielding very competitive results to the state - of - art models in their respective domains . We further validate the language model using Xhosa and Shona , which are syllabic - based languages . The novelty of the study is in its capability to construct quality word embeddings from syllables using a hybrid model that does not use max - over - pool common in CNN and then the exploitation of these embeddings in POS tagging . Therefore , the study plays a crucial role in the processing of agglutinative and syllabic - based languages by contributing quality word embeddings from syllable embeddings , a robust Conv – LSTM model that learns syllables for not only language modeling and POS tagging , but also for other downstream NLP tasks . 

Predicting learner levels for online exercises of Hebrew
We develop a system for predicting the level of language learners , using only a small amount of targeted language data . In particular , we focus on learners of Hebrew and predict level based on restricted placement exam exercises . As with many language teaching situations , a major problem is data sparsity , which we account for in our feature selection , learning algorithm , and in the setup . Specifically , we define a two - phase classification process , isolating individual errors and linguistic constructions which are then aggregated into a second phase ; such a two - step process allows for easy integration of other exercises and features in the future . The aggregation of information also allows us to smooth over sparse features . 

Does ensemble really work when facing the twitter semantic classification ? 
With the rapid development of Internet social media , twitter has gradually become the most mainstream information release and information sharing platform . A large number of twitter users use the platform to express their views , emotions and opinions . However , it is still a challenge on twitter semantic classification based on the observation that Twitters are short , noisy , arbitrary , etc . Thus , we seek in the mainstream NLP algorithms to find out which algorithm performs best in this problem . After that , we analysis the ensemble methods on the former encode expand to get a better result . However , we find that it dosen ' t work well as we expected . we analysis the reason and give the potential explain . The extensive experiments have shown that the LCF - BERT based model performs best over the mainstream algorithms and the ensemble model on the Twitter dataset . 

Developing a shuffle grammar for parsing Arabic verbs
One of the most important members of natural languages is Arabic . This language spans 14 centuries and 22 countries and has a great traditional and historical background . More than two hundred million people speak in Arabic and this matter also persuades researchers to study in this filed . Due to the mentioned attractiveness , we also studied on a new mechanism of Arabic parsing which based on formal grammars and is underlined by shuffle morphology . This grammar is embedded in body of a morpho - syntactic tagger and works with accuracy of 98 . 5 % on Arabic verbs . 

The Combination of CNN , RNN , and DNN for Relation Extraction
Relation extraction , which is a subtask of NLP ( natural language processing ) field , its target is to identify the entities in texts and extract the relation between entities . Previous works prove that neural networks are feasible for relation extraction . CNN ( convolutional neural networks ) and LSTM ( long short - term memory ) are two majority models used in relation extraction . Further research shows that the combination of CNN and LSTM has a better performance . Inspired by the solution of LVCSR ( Large - Vocabulary - Continuous - Speech - Recognition ) , another task in the NLP field , we propose adding DNN after the combination of CNN and LSTM . This model achieves a better effect on the precision - recall curve than the previous model . 

Predicting Chance of Success on Epiretinal Membrane Surgery using Deep Learning
A preliminary study on predicting chance of success on an epiretinal membrane surgery is studied . Given an optical coherence tomography image , the study shows that the multilayer perceptron neural network can achieve 91 . 0 % accuracy . Due to an unbalance of the images of success and failure classes , under - sampling and over - sampling are applied . For oversampling , the images in the failure class are duplicated to balance the number of images compared to the success class . Utilizing the balance dataset , the prediction performance is improved from 91 . 0 % to 93 . 0 % for over - sampling . With the exploitation of , the salient region for training the model and predicting the outcome . The salient region is manually segmented to express the fovea in the OCT . The experimental results evidence an improvement of 1 . 0 % with achievement of 94 . 0 % accuracy . 

Natural Language Processing for Requirements Engineering : A Systematic Mapping Study
Natural Language Processing for Requirements Engineering ( NLP4RE ) is an area of research and development that seeks to apply natural language processing ( NLP ) techniques , tools , and resources to the requirements engineering ( RE ) process , to support human analysts to carry out various linguistic analysis tasks on textual requirements documents , such as detecting language issues , identifying key domain concepts , and establishing requirements traceability links . This article reports on a mapping study that surveys the landscape of NLP4RE research to provide a holistic understanding of the field . Following the guidance of systematic review , the mapping study is directed by five research questions , cutting across five aspects of NLP4RE research , concerning the state of the literature , the state of empirical research , the research focus , the state of tool development , and the usage of NLP technologies . Our main results are as follows : ( i ) we identify a total of 404 primary studies relevant to NLP4RE , which were published over the past 36 years and from 170 different venues ; ( ii ) most of these studies ( 67 . 08% ) are solution proposals , assessed by a laboratory experiment or an example application , while only a small percentage ( 7% ) are assessed in industrial settings ; ( iii ) a large proportion of the studies ( 42 . 70% ) focus on the requirements analysis phase , with quality defect detection as their central task and requirements specification as their commonly processed document type ; ( iv ) 130 NLP4RE tools ( i . e . , RE specific NLP tools ) are extracted from these studies , but only 17 of them ( 13 . 08% ) are available for download ; ( v ) 231 different NLP technologies are also identified , comprising 140 NLP techniques , 66 NLP tools , and 25 NLP resources , but most of them — particularly those novel NLP techniques and specialized tools — are used infrequently ; by contrast , commonly used NLP technologies are traditional analysis techniques ( e . g . , POS tagging and tokenization ) , general - purpose tools ( e . g . , Stanford CoreNLP and GATE ) and generic language lexicons ( WordNet and British National Corpus ) . The mapping study not only provides a collection of the literature in NLP4RE but also , more importantly , establishes a structure to frame the existing literature   through categorization , synthesis and conceptualization of the main theoretical concepts and relationships that encompass   both RE and NLP aspects . Our work thus produces a conceptual framework of NLP4RE . The framework is used to identify research gaps and directions , highlight technology transfer needs , and encourage more synergies between the RE community , the NLP one , and the software   and systems   practitioners . Our results can be used as a starting point to frame future studies according to a well - defined terminology and can be expanded as new technologies and novel solutions emerge . 

Towards a computational history of the ACL : 1980 - 2008
We develop a people - centered computational history of science that tracks authors over topics and apply it to the history of computational linguistics . We present four findings in this paper . First , we identify the topical subfields authors work on by assigning automatically generated topics to each paper in the ACL Anthology from 1980 to 2008 . Next , we identify four distinct research epochs where the pattern of topical overlaps are stable and different from other eras : an early NLP period from 1980 to 1988 , the period of US government - sponsored MUC and ATIS evaluations from 1989 to 1994 , a transitory period until 2001 , and a modern integration period from 2002 onwards . Third , we analyze the flow of authors across topics to discern how some subfields flow into the next , forming different stages of ACL research . We find that the government - sponsored bakeoffs brought new researchers to the field , and bridged early topics to modern probabilistic approaches . Last , we identify steep increases in author retention during the bakeoff era and the modern era , suggesting two points at which the field became more integrated . 

AI Based Depression and Suicide Prevention System
Suicide is a major issue in the world . The number one reason for suicide is untreated depression . That is why it was decided to focus on depression symptoms more and identify them in order to prevent suicidal attempts . To cure depression , the best way is to talk about their feelings with someone they trusted and release their pain inside of them . Because of that this system has a Chat - bot for the user to interact with . Chat - bot will gather information about the users feelings through text and voice analysis . Also by analyzing their Facebook statuses and recent web history , the application gather more information about their mental state so that the system take more accurate conclusions . After analyzing all the information from each component the back brain will decide on how the chat - bot should act on the user . At the end , the product was able to give more than 75 % accurate results for each component . 

LegalBERT - th : Development of Legal Q & A Dataset and Automatic Question Tagging
Tagging questions according to their topics is useful for internet forum management . In this paper , we use the Bidirectional Encoder Representations from Transformers ( BERT ) model to categorize posts from Thai legal internet forums . First , We construct our new legal Q & A dataset by scraping the internet , cleaning the data , and annotating the data . Second , We perform transfer learning to let our model learn about the legal language model in general and then fine - tune the model for the law topic classification task . As a result , we have developed a legal Q & A dataset of 12 , 695 question / answer pairs and a law topic classification model based on BERT with 92 % accuracy . Finally , we build a prototype legal internet forum which equipped with the automatic tagging function , law topic classification , to provide a concrete example of how to apply the model in the real situation . 

Beyond Hostile Linguistic Cues : The Gravity of Online Milieu for Hate Speech Detection in Arabic
Religious Hate speech poses grave dangers for the cohesion of a democratic society , the protection of human rights and the rule of law . While previous work has shown that linguistic features can be effectively used for text categorization in Arabic , employing information coming from users ` social networks has not yet been explored for such complex user characteristics . Systems relying on language information tend to have low precision because they tend to rely on messages containing particular terms indicating hate speech . In this paper , we study the novel problem of exploiting social context for detection of religious hate speech in Arabic tweets , given information extracted from their online milieu by learning a low - dimensional vector representation of users . 

Improving the Cognitive Levels of Automatic Generated Questions using Neuro - Fuzzy Approach in e - Assessment
Assessment is a fundamental activity to realize the objective of a course and to enhance the teaching learning process . Ensuring quality in the question papers for testing the different cognitive skills is important in the test or examination component of any academic or training domain . Bloom ' s taxonomy , a popular framework has been used to assess the learning skills of students . This paper describes the methodology to auto - generate questions based on bloom ' s cognitive levels . Various Natural Language Processing ( NLP ) techniques are incorporated to construct a textgraph using input statements from the web where native intelligence acquired from ConceptNet interrelates the nodes . The proposed work resolves the complexity of categorising autogenerated questions with similar action verbs . It follows a combinatorial formulation of fuzzy logic to generate cognitively fluent questions . A Fuzzy Restricted Boltzman machine coupled with Gaussian Markov based softmax is used in the proposed architecture . Experiments reveal the significance of the proposed system in generating cognitively fluent questions when the same action verb gets interlinked with different cognitive level of auto - generated questions . 

Recent Advances of Affect Detection from Arabic Text
Emotion Detection ( ED ) from text has been an active research field recently . It has attracted the attention of researchers as it can measure the emotional contexts while humans interact with computers . Humans could express their emotion in various ways ; using typed text , facial expressions , speech , gestures , and physiological measures . ED is considerably different from sentiment analysis SA , where SA goal is to detect polarity from text such as positive , negative or neutral . On the other hand , ED aims to recognize emotions from input text . Emotions can be modeled as discrete categories , e . g . Ekmans six basic emotions ( angry , fear , joy , disgust , surprise and sadness ) . On the other hand there is the dimensional model that express emotions as valence , arousal and dominance values . Social media provides a rich source of emotional text , e . g . Twitter and Facebook . In this paper , we provide a review of recent work on ED from Arabic text . We discuss approaches ( lexicons , machine learning , deep neural networks and ensemble approaches ) , tools for text processing , and We also discuss description of the most popular datasets in this domain . 

NJM - Vis : interpreting neural joint models in NLP
Neural joint models have been shown to outperform non - joint models on several NLP and Vision tasks and constitute a thriving area of research in AI and ML . Although several researchers have worked on enhancing the interpretability of single - task neural models , in this work we present what is , to the best of our knowledge , the first interface to support the interpretation of results produced by joint models , focusing in particular on NLP settings . Our interface is intended to enhance interpretability of these models for both NLP practitioners and domain experts ( e . g . , linguists) . 

With LSA Size DOES Matter
Latent Semantic Analysis ( LSA ) is a technique from the field of Natural Language Processing that enables comparison of semantic similarities between documents using vector operations . This technique has been used in areas from Information Retrieval ( IR ) to the automated assessment of essays . One property used in document comparison is size . The general philosophy is that more text is better although few concrete examples or guidelines exist that demonstrate this . This paper shows , via a novel concrete example taken from real world data , that larger documents do imply more accurate semantic similarity comparisons . 

Experiments with artificially generated noise for cleansing noisy text
Recent works show that the problem of noisy text normalization can be treated as a machine translation ( MT ) problem with convincing results . There have been supervised MT approaches which use noisy - regular parallel data for training an MT model , as well as unsupervised models which learn the translation probabilities in alternative ways and try to mimic the MT - based approach . While the supervised approaches suffer from data annotation and domain adaptation difficulties , the unsupervised models lack a holistic approach catering to all types of noise . In this paper , we propose an algorithm to artificially generate noisy text in a controlled way , from any regular English text . We see this approach as an alternative to the unsupervised approaches while getting the advantages of a parallel corpus based MT approach . We generate parallel noisy text from two widely used regular English datasets and test the MT - based approach for text normalization . Semi - supervised approaches were also tried to explore different ways of improving the parallel corpus ( manually annotated ) based MT approach by using the generated noisy text . An extensive analysis based on comparison of our approaches with both the supervised as well as unsupervised approaches is presented . 

Sentence difficulty evaluation for a learner ' s dictionary
In order to choose the best example sentences for Chinese language learners , we have compared how well different methods of estimating difficulty ( reading time , translation time , direct rating ) could be approximated from a sentence ' s superficial features . We have found that direct rating of difficulty by a user is the most promising , and that character - based features allow for better evaluation than word - based ones , all of which bodes well for user - tailored learner ' s dictionaries . 

NLP Approach to Knowledge Search and Information Credibility Analysis
Judgement criteria used by people in their daily life are starting to heavily depend on text - based information in Web documents . In the GCOE project , we aim to construct a new generation Web search engine that can provide information or knowledge in an organized way , helping users to evaluate information credibility . We strongly believe that the improvement of natural language processing is crucial to realize such a search engine . 

Using cognitive model to automatically analyze Chinese predicate
This paper presents an cognitive approach to semantic role labeling in Chinese based on an extension of Construction - Integration ( CI ) model . The method can implicitly integrate more contextual and general knowledge into the calculating process in contrast with the machine learning methods . First , we define a proposition representation as the basic unit for semantic role labeling using CI model . Then the contextually appropriate propositions will be strengthened and inappropriate ones will be inhibited by simulating the spreading activation of human mind . Finally , experimental results show an encouraging performance on Chinese PropBank ( CPB ) and other two datasets . 

Conditional Random Fields in Speech , Audio , and Language Processing
Conditional random fields ( CRFs ) are probabilistic sequence models that have been applied in the last decade to a number of applications in audio , speech , and language processing . In this paper , we provide a tutorial overview of CRF technologies , pointing to other resources for more in - depth discussion ; in particular , we describe the common linear - chain model as well as a number of common extensions within the CRF family of models . An overview of the mathematical techniques used in training and evaluating these models is also provided , as well as a discussion of the relationships with other probabilistic models . Finally , we survey recent work in speech , audio , and language processing to show how the same CRF technology can be deployed in different scenarios . 

Natural Language Requirements Processing : A 4D Vision
The future evolution of the application of natural language processing technologies in requirements engineering can be viewed from four dimensions : discipline , dynamism , domain knowledge , and datasets . 

Resolving task specification and path inconsistency in taxonomy construction
Taxonomies , such as Library of Congress Subject Headings and Open Directory Project , are widely used to support browsing - style information access in document collections . We call them browsing taxonomies . Most existing browsing taxonomies are manually constructed thus they could not easily adapt to arbitrary document collections . In this paper , we investigate both automatic and interactive techniques to derive taxonomies from scratch for arbitrary document collections . Particular , we focus on encoding user feedback in taxonomy construction process to handle task - specification rising from a given document collection . We also addresses the problem of path inconsistency due to local relation recognition in existing taxonomy construction algorithms . The user studies strongly suggest that the proposed approach successfully resolve task specification and path inconsistency in taxonomy construction . 

LBSociam : Lightbase Social Machine on Criminal Data
Social machine is a rather new approach to deal with relevant problems in society , blending computational and social elements into software . It can be an extension of the Semantic Web , creating processes in which people do the creative work and the machine does the data administration . This article presents a proposal to apply this approach in violence and criminality domain , a relevant matter to Latin America and Caribbean - LAC - - countries . It extends Social Machines by applying two published strategies to obtain semantics over social networks data . The development procedure was documented to provide a systematic procedure and an example application is presented to identify violence and criminality events . The resulting procedure validation was done by testing against developed formal models in the research area . Criminal activity data extraction results were also compared to official data , in order to identify similarities . 

Evaluation of Rough Sets Data Preprocessing on Context - Driven Semantic Analysis with RNN
In the application examples of NLP ( natural language learning ) , the rich semantic information in medical literature can extract characteristic target words through the training of RNN - LSTM ( recurrent neural network - long short - term memory ) . In the process of extracting these target words , we often encounter some wrong target words which cause RNN to reduce the hit rate and extend the training time . In this paper , we take Diabetes in medical research as an example , the data preprocessing of rough sets , and the word vector tagging for target word can improve the hit efficiency of the target words in the RNN - LSTM training process . 

Experiments for various alignment models in Chinese - to - English SMT
In this paper we analyze and compare three basic word alignment models to acquire the useful knowledge and parameters for the statistical machine translation . A series of experiments are made and a very detailed analysis based on the experiments is given . According to the different features of the three models , the merits and demerits of the models are described and analyzed respectively . Finally , the word alignment probabilities and the phrases extracted using HMM Viterbi - alignment are used to develop the Chinese - to - English statistical machine translation system . The experimental results show that the translation quality will be increased with the preprocessing of the training data , joining of the dictionary and enlarging the translation grain appropriately . 

Authorship attribution for textual data on online social networks
Authorship Attribution , ( AA ) is a process of determining a particular document is written by which author among a list of suspected authors . Authorship attribution has been the problem from last six decades ; when there were handwritten documents needed to be identified for the genuine author . Due to the technology advancement and increase in cybercrime and unlawful activities , this problem of AA becomes forth most important to trace out the author behind online messages . Over the past , many years research has been conducted to attribute the authorship of an author on the basis of their writing style as all authors possess different distinctiveness while writing a piece of document . This paper presents a comparative study of various machine learning approaches on different feature sets for authorship attribution on short text . The Twitter dataset has been used for comparison with varying sample size of a dataset of 10 prolific authors with various combinations of feature sets . The significance and impact of combinations of features while inferring different stylometric features has been reflected . The results of different approaches are compared based on their accuracy and precision values . 

Health Care Counselling Via Voicebot Using Multinomial Naive Bayes Algorithm
Voicebot bestow the user , the ability of receiving retort within minutes , no matter their geographic location , or the apparatus , they are using to transmission . Data related to the health issues are gathered from medical experts , and all of them are stored on the server side of the system . The software furnished by the system offers answers and solutions for audit and treating patients after combining descriptions of the illness and the symptoms , diagnostics , predictions and prescription . This software will be greatly useful to people to protect them from complication , in this proposed system the Voicebot is used to solve the difficulties facing by people at the time of illness , which influence the health of the person . With all these factors , the Voicebot updates its knowledge base and allows patients to benefit from using it . This designed bot is commonly used for general Health Care . Here the input is given by the user in format of text . This text will be analyzed by Multinomial Algorithm , by which the data are fetched from the Data Base and generates voice response . When examine all these bots , the oddity of the proposed system can get response by both sensory and textual methods . This graces the untaught customers to use proficiently . It bears the whole system in customer ' s cordial manner . The need of the proposed system is minimum time and energy and can clarify our doubts at any time . 

Hierarchical Three - module Method of Text Classification in Web Big Data
Text analysis is a method for extracting knowledge from text . Memory and time limitations in processing big data is crucial due to data sources distributed in web , search engines and socials network sites . In addition , due to automatizing search process , summarizing and finding the interests of users , immediate classification of various texts in a streaming manner has gained attention in industrial and scientific fields . Hierarchical classification of text is among common issues which is simply possible in traditional methods using bag of words ; however , while talking about big data and when there are a lot of labels of classes , employing traditional methods will not meet the needs of societies . With the improvement of data in internet and social networks , more powerful methods are needed which can classify the data closely and immediately . Through abstraction in textual data , deep learning can deal with these challenges . In this paper a deep learning method will be introduced which is based on hierarchical classification ( HAN ) named HAN - MODI and which can classify texts from social networks and web sites with an accuracy of 98 . 81 % at the real time bilingually in English and Farsi . This paper also shows that this complex network with three modules word , sentence and document can work better at word level and there is no need to know syntactic or semantics structure of language . The novelty of the proposed method is adding a third level to the hierarchical structure for general detection and for more exact detection of the class . In addition , classification using this method will be multi - level classification and finally with a change in HAN , this method can be used with Farsi texts . Model improvement is done by adding a new layer above the architecture HAN . We called it as segmentation of sentences into expressions Bag of Sentences and added a dynamicity window in any stage that applied attention mechanism simultaneously . 

Pulling their weight : exploiting syntactic forms for the automatic identification of idiomatic expressions in context
Much work on idioms has focused on type identification , i . e . , determining whether a sequence of words can form an idiomatic expression . Since an idiom type often has a literal interpretation as well , token classification of potential idioms in context is critical for NLP . We explore the use of informative prior knowledge about the overall syntactic behaviour of a potentially - idiomatic expression ( type - based knowledge ) to determine whether an instance of the expression is used idiomatically or literally ( token - based knowledge ) . We develop unsupervised methods for the task , and show that their performance is comparable to that of state - of - the - art supervised techniques . 

A proposal for screening inconsistencies in ontologies based on query languages using WSD
In this paper , we discuss a method to screen inconsistencies in ontologies by applying a natural language processing ( NLP ) technique , especially , those used for word sense disambiguation ( WSD ) . In the database research field , it is claimed that queries over target ontologies should play a significant role because they represent every aspect of the terms described in each ontology . According to ( Calvanese et al . , 2001 ) , considering the global and the local ontologies , the terms in the global ontology can be viewed as the query over the local ontology , and the mapping between the global and the local ontologies is given by , associating each term in the global ontology with a view . On the other hand , ontology screening systems should be able to take advantage of some popular techniques for WSD , which is supposed to decide the right sense where the target word is used in a specific context . We present several examples regarding inconsistencies in ontologies with the aid of DAML + OIL notation ( DAML + OIL , 2001 ) , and propose that WSD can be one of the promising method to screen such as inconsistencies . 

Tag semantics for the retrieval of XML documents
Word Sense Disambiguation ( WSD ) , in the field of Natural Language Processing ( NLP ) , consists in assigning the correct sense ( semantics ) to a word form ( lexeme ) by means of the context in which the lexeme is found . In this paper we investigate the possibility of applying WSD techniques to the field of Information Retrieval , especially to the retrieval of XML documents . We consider two methods to automatically assign semantic values to XML tags on the grounds of the tagged text contained . Such methods rely on the bayesian supervised approach and on an automatic unsupervised approach and exploit the WordNet ontology . Results show that the applicability of both methods is hampered by the habit of use abbreviation or shortcuts as tags . 

Insurance Data Analysis with COGNITO : An Auto Analysing and Storytelling Python Library
Data pre - processing has taken an enhanced role with the advent of Machine learning . It is a vital element that forms the encore of the data science and business analytics process . Data pre - processing involves generating descriptive statistical summary , data cleaning , and data manipulation based on inputs gained after the initial analysis . Of late , it has been observed that data science practitioners spend 45 % to 50 % of their time cleaning and processing the data . Much time can be saved if the data transformation process can be automated . The COGNITO framework helps in performing the automated feature engineering and data storytelling of the dataset based on end - user discretion . The present work discusses the process and results obtained when automated feature engineering was performed on an insurance dataset using COGNITO . 

Towards Causality Extraction from Requirements
System behavior is often based on causal relations between certain events ( e . g . If event 1 , then event 2 ) . Consequently , those causal relations are also textually embedded in requirements . We want to extract this causal knowledge and utilize it to derive test cases automatically and to reason about dependencies between requirements . Existing NLP approaches fail to extract causality from natural language ( NL ) with reasonable performance . In this paper , we describe first steps towards building a new approach for causality extraction and contribute : ( 1 ) an NLP architecture based on Tree Recursive Neural Networks ( TRNN ) that we will train to identify causal relations in NL requirements and ( 2 ) an annotation scheme and a dataset that is suitable for training TRNNs . Our dataset contains 212 , 186 sentences from 463 publicly available requirement documents and is a first step towards a gold standard corpus for causality extraction . We encourage fellow researchers to contribute to our dataset and help us in finalizing the causality annotation process . Additionally , the dataset can also be annotated further to serve as a benchmark for other RE - relevant NLP tasks such as requirements classification . 

Cost - efficient quality assurance of natural language processing tools through continuous monitoring with continuous integration
More and more modern applications make use of natural language data , e . g . Information Extraction ( IE ) or Question Answering ( QA ) systems . Those application require preprocessing through Natural Language Processing ( NLP ) pipelines , and the output quality of these applications depends on the output quality of NLP pipelines . If NLP pipelines are applied in different domains , the output quality decreases and the application requires domain specific NLP training to improve the output quality . Adapting NLP tools to specific domains is a time - consuming and expensive task , inducing two key questions : a ) how many documents need to be annotated to reach good output quality and b ) what NLP tools build the best performing NLP pipeline ? In this paper we demonstrate a monitoring system based on principles of Continuous Integration which addresses those questions and guides IE or QA application developers to build high quality NLP pipelines in a cost - efficient way . This monitoring system is based on many common tools , used in many software engineering projects . 

Research on Intelligent Security Protection of Privacy Data in Government Cyberspace
Based on the analysis of the difficulties and pain points of privacy protection in the opening and sharing of government data , this paper proposes a new method for intelligent discovery and protection of structured and unstructured privacy data . Based on the improvement of the existing government data masking process , this method introduces the technologies of NLP and machine learning , studies the intelligent discovery of sensitive data , the automatic recommendation of masking algorithm and the full automatic execution following the improved masking process . In addition , the dynamic masking and static masking prototype with text and database as data source are designed and implemented with agent - based intelligent masking middleware . The results show that the recognition range and protection efficiency of government privacy data , especially government unstructured text have been significantly improved . 

Fast methods for kernel - based text analysis
Kernel - based learning ( e . g . , Support Vector Machines ) has been successfully applied to many hard problems in Natural Language Processing ( NLP ) . In NLP , although feature combinations are crucial to improving performance , they are heuristically selected . Kernel methods change this situation . The merit of the kernel methods is that effective feature combination is implicitly expanded without loss of generality and increasing the computational costs . Kernel - based text analysis shows an excellent performance in terms in accuracy ; however , these methods are usually too slow to apply to large - scale text analysis . In this paper , we extend a Basket Mining algorithm to convert a kernel - based classifier into a simple and fast linear classifier . Experimental results on English BaseNP Chunking , Japanese Word Segmentation and Japanese Dependency Parsing show that our new classifiers are about 30 to 300 times faster than the standard kernel - based classifiers . 

A Survey on NLP based Text Summarization for Summarizing Product Reviews
No one can imagine life without a smartphone and internet nowadays . It has become essential for people of all age groups . With an increase in the usage of internet and smartphones , there has been a steady increase in online shopping too . Everyone wishes to get their products delivered at their home without any hassle . How to detect which products are genuine and pick the best among the unlimited options at the same price ? Every user looks at the reviews before ordering anything online . Nevertheless reading those long reviews is not easy for everyone . Therefore , there must be something that can reduce the long reviews to short sentences of limited words depicting the same meaning . Text Summarization can come in hand in this aspect . Many NLP researchers are interested in Text Summarization . This paper is a survey on the various types of text summarization techniques starting from the basic to the advanced techniques . According to this survey , seq2seq model along with the LSTM and attention mechanism is used for increased accuracy . 

Imam : Word Embedding Model for Islamic Arabic NLP
This paper can be considered one of the first works to introduce an efficient distributed word representation model for different NLP tasks in the islamic domain . The Word Embedding Model and the algorithm on top of it is implemented in Imam application where user can ask the application to search for any data related to Isalmic domain and get an answer . The data is gathered from different resources ( Maliks muwataa , Musnad Ahmad Ibn - hanbal , Sahih Muslim ahadith , Sahih Al - bukhari , Sunan Al - darimi , and more ) . The amount of records gathered was more than ninety thousand documents ( Text Blocks ) from 10 different books . After several sequential pipeline processes of Data cleaning , preprocessing and Normalization , Skip - gram technique was used to built the word2vec model and then At last tested with different methods , first by using the K - means clustering and then nonlinear dimensionality reduction technique to represent the data in 2D dimension , secondly by using word similarity to test model ability to understand the Quranic language . The tests clearly show that the model can be used effectively in different NLP Arabic Islamic tasks . 

Exploring Sentence Vector Spaces through Automatic Summarization
Given vector representations for individual words , it is necessary to compute vector representations of sentences for many applications in a compositional manner , often using artificial neural networks . Relatively little work has explored the internal structure and properties of such sentence vectors . In this paper , we explore the properties of sentence vectors in the context of automatic summarization . In particular , we show that cosine similarity between sentence vectors and document vectors is strongly correlated with sentence importance and that vector semantics can identify and correct gaps between the sentences chosen so far and the document . In addition , we identify specific dimensions which are linked to effective summaries . To our knowledge , this is the first time specific dimensions of sentence embeddings have been connected to sentence properties . We also compare the features of different methods of sentence embeddings . Many of these insights have applications in uses of sentence embeddings far beyond summarization . 

Towards understanding code readability and its impact on design quality
Readability of code is commonly believed to impact the overall quality of software . Poor readability not only hinders developers from understanding what the code is doing but also can cause developers to make sub - optimal changes and introduce bugs . Developers also recognize this risk and state readability among their top information needs . Researchers have modeled readability scores . However , thus far , no one has investigated how readability evolves over time and how that impacts design quality of software . We perform a large scale study of 49 open source Java projects , spanning 8296 commits and 1766 files . We find that readability is high in open source projects and does not fluctuate over project ’ s lifetime unlike design quality of a project . Also readability has a non - significant correlation of 0 . 151 ( Kendall ’ s τ ) with code smell count ( indicator of design quality ) . Since current readability measure is unable to capture the increased difficulty in reading code due to the degraded design quality , our results hint towards the need of a better measurement and modeling of code readability . 

Learners ’ non - cognitive skills and behavioral patterns of programming : A sequential analysis
The interest in artificial intelligence ( AI ) education is growing exponentially ; nevertheless , how to learn about AI , particularly Natural Language Processing ( NLP ) , has been a challenging problem for educators and researchers worldwide . This study used a graphical programming platform Snap ! to facilitate learning by allowing learners to explore AI and its NLP techniques in class . Data from 18 , 452 logged events were collected and Lag Sequential Analysis ( LSA ) was used to examine how learners behaved and learned sequentially . Non - cognitive factors were used to group learners as detailed and subtle behavior sequences that did not occur by chance could be uncovered . The results showed that five groups of learners , that is Passive Learners , Performers , Adaptive Learners , Interested Learners , and Dedicated Learners . They presented varied learning behavior patterns , which should be considered further in designing personalized and intelligent learning platforms to support AI education . 

Effect of word complexity on L2 vocabulary learning
Research has shown that a number of factors , such as maturational constraints , previous language background , and attention , can have an effect on L2 acquisition . One related issue that remains to be explored is what factors make an individual word more easily learned . In this study we propose that word complexity , on both the phonetic and semantic levels , affect L2 vocabulary learning . Two studies showed that words with simple grapheme - to - phoneme ratios were easier to learn than more phonetically complex words , and that words with two or fewer word senses were easier to learn that those with three or more . 

Text Classification and Mark Key Fragment
In this paper , a novel architecture is proposed to classify website essays to two classes , " acception " or " need - improvement " , and at the same time mark the key text fragment that influence the final prediction . All essays come from an important popular official website . They are submitted by users around China , and reviewed by human reviewers to decide which one is accepted to be published on website or needing more improvement . In the past eight years human reviewers have accumulated a large labeled dataset , which can be used as the train data and test data for training an artificial intelligence ( AI ) model to aid human reviewers . Because the AI model can ' t be 100 % reliable , the model is designed not only predict the classification , but also mark the key fragment that influence the final prediction . For human perspective , it seems like the AI model gives out the reason why an essay is accepted or where needs improvement . This will definitely help human reviewers to accelerate the examine process . The final trained model can correctly classify 93 . 6 % sample essays in test set and the positive samples ( accepted essay ) recall is 96 . 7 % . The most interesting thing is even human reviewers never tell why an essay is rejected ( where needs improvement ) in train data , the model can give out the reason by marking the key fragment , where need improvement . After checked by human reviewers , it works really well . 

Random Sampling and Inductive Ability Evaluation of Word Embedding in Medical Literature
In the NLP ( natural language learning ) applied to medical literature semantic analysis , we proposed a method to analysis different data base training result in order to show that RNN with rough set training method can learn some relations between medical nouns . In our previous work , we trained a RNN model by rough set data preprocessing in medical literature analysis . So , we improved the word embedding method by using random sampling and inductive ability evaluation . 

Investigating Deep Learning Word2vec Model for Sentiment Analysis in Arabic and English languages for User ’ s reviews
In this paper , we explore natural language processing ( NLP ) methods to perform sentiment analysis or opinion mining . In addition , we show an application on English and Arabic sentiment analysis by implementing sentiment classification for three datasets which are Booking hotel dataset , Food fine Amazon dataset and Arabic movie review dataset . We applied Word2Vec model followed by Random Forest classifier ( RF ) for Arabic movie dataset . The results show that the Word2Vec model shows highly effective performance in sentiment analysis for English language datasets but it does not work for Arabic language as Arabic language need different mechanism . 

Characterizing discussions in the Spanish Wikipedia
Wikipedia , as the largest online encyclopedia , is edited collaboratively by hundreds of users . The content in some articles can have dispute , giving rise to discussions which are registered in the related talk pages . In this paper , we propose an annotation schema for Spanish Wikipedia talk pages in order to determine the type of opinions expressed in them . We apply the annotation schema to a corpus that includes a collection of discussions about 148 topics drawn from 25 Spanish Wikipedia talk pages . We make the resulting dataset publicly available for download on github 1 . Furthermore , we train and evaluate supervised machine learning models to automatically identify the annotation labels . Linear Support Vector classifier ( LinearSVC ) performs better compared to other baseline models , and achieves an accuracy F 1 = 0 . 71 in our experiments . 

DeSoCoRe : detecting source code re - use across programming languages
Source code re - use has become an important problem in academia . The amount of code available makes necessary to develop systems supporting education that could address the problem of detection of source code re - use . We present the DeSoCoRe tool based on techniques of Natural Language Processing ( NLP ) applied to detect source code re - use . DeSoCoRe compares two source codes at the level of methods or functions even when written in different programming languages . The system provides an understandable output to the human reviewer in order to help a teacher to decide whether a source code is re - used . 

Pragmatic analysis based query expansion for Chinese cuisine QA service system
This paper proposes a query expansion method for cooking question answering system based on pragmatic analysis . In our approach , the results of question analysis is used . The original queries are generated by means of the question subject , then the query terms are expanded based on pragmatic function . When submitting the expended queries to Google search engine to retrieve related passages , we get an overall improvement of 36 . 2 % on the mean average precision . 

A Knowledge Graph Based Approach for Automatic Speech and Essay Summarization
Every day , big amounts of unstructured data is generated . This data is of the form of essays , research papers , speeches , patents , scholastic articles , book chapters etc . In today ' s world , it is very important to extract key patterns from huge text passages or verbal speeches . This paper proposes a novel method for summarizing multilingual vocal as well as written paragraphs and speeches , using semantic Knowledge Graphs . Using the proposed model , big text extracts or speeches can be summarized for better understanding and analysis . The method uses speech recognition as well as Named Entity Recognition to identify entities from spoken content to create optimized Knowledge Graphs in the English Language . 

Are they our brothers ?: analysis and detection of religious hate speech in the arabic Twittersphere
Religious hate speech in the Arabic Twittersphere is a notable problem that requires developing automated tools to detect messages that use inflammatory sectarian language to promote hatred and violence against people on the basis of religious affiliation . Distinguishing hate speech from other profane and vulgar language is quite a challenging task that requires deep linguistic analysis . The richness of the Arabic morphology and the limited available resources for the Arabic language make this task even more challenging . To the best of our knowledge , this paper is the first to address the problem of identifying speech promoting religious hatred in the Arabic Twitter . In this work , we describe how we created the first publicly available Arabic dataset annotated for the task of religious hate speech detection and the first Arabic lexicon consisting of terms commonly found in religious discussions along with scores representing their polarity and strength . We then developed various classification models using lexicon - based , n - gram - based , and deep - learning - based approaches . A detailed comparison of the performance of different models on a completely new unseen dataset is then presented . We find that a simple Recurrent Neural Network ( RNN ) architecture with Gated Recurrent Units ( GRU ) and pre - trained word embeddings can adequately detect religious hate speech with 0 . 84 Area Under the Receiver Operating Characteristic curve ( AUROC ) . 

Development of the HRL route navigation dialogue system
In this paper we report on our work on a prototype route navigation dialogue system for use in a vehicle . The system delivers spoken turn - by - turn directions , and has been developed to accept naturally phrased navigation queries , as part of our overall effort to create an in - vehicle information system which delivers information as requested while placing minimal cognitive load on the driver . 

Exploring the use of NLP in the disclosure of electronic patient records
This paper describes a preliminary analysis of issues involved in the production of reports aimed at patients from Electronic Patient Records . We present a system prototype and discuss the problems encountered . 

LS - CAT : A Large - Scale CUDA AutoTuning Dataset
The effectiveness of Machine Learning ( ML ) methods depend on access to large suitable datasets . In this article , we present how we build the LS - CAT ( Large - Scale CUDA AutoTuning ) dataset sourced from GitHub for the purpose of training NLP - based ML models . Our dataset includes 19 683 CUDA kernels focused on linear algebra . In addition to the CUDA codes , our LS - CAT dataset contains 5 028 536 associated runtimes , with different combinations of kernels , block sizes and matrix sizes . The runtime are GPU benchmarks on both Nvidia GTX 980 and Nvidia T4 systems . This information creates a foundation upon which NLP - based models can find correlations between source - code features and optimal choice of thread block sizes . There are several results that can be drawn out of our LS - CAT database . E . g . , our experimental results show that an optimal choice in thread block size can gain an average of 6 % for the average case . We thus also analyze how much performance increase can be achieved in general , finding that in 10 % of the cases more than 20 % performance increase can be achieved by using the optimal block . A description of current and future work is also included . 

Turkish labeled text corpus
A labeled text corpus made up of Turkish papers ' titles , abstracts and keywords is collected . The corpus includes 35 number of different disciplines , and 200 documents per subject . This study presents the text corpus ' collection and content . The classification performance of Term Frequcney — Inverse Document Frequency ( TF - IDF ) and topic probabilities of Latent Dirichlet Allocation ( LDA ) features are compared for the text corpus . The text corpus is shared as open source so that it could be used for natural language processing applications with academic purposes . 

Building test suites for UIMA components
We summarize our experiences building a comprehensive suite of tests for a statistical natural language processing toolkit , ClearTK . We describe some of the challenges we encountered , introduce a software project that emerged from these efforts , summarize our resulting test suite , and discuss some of the lessons learned . 

Design and Analysis of a ChatBot with IPL First Inning Score Prediction
A chat bot aims to make conversations with human and machine both . The use of chat bots has emerged rapidly in many fields , including marketing , support systems , education , health care , cultural heritage , and entertainment . The machine is equipped with the information to identify the sentences and make the decision itself as a response to the question . Response policy compares the input sentence from the user . We have integrated our Chat bot with IPL first inning score prediction . In this paper , details of the last decade IPL seasons from 2008 - 2017 are used for prediction . Six machine learning techniques are used for prediction and their performance is analyzed . The result shows that the prediction accuracy of Random Forest is better than all other models . 

Extracting Precise Link Context Using NLP Parsing Technique
Link context has been exploited extensively ever since the advent of the World Wide Web , but the approach to extracting precise link context has not been fully explored and many state - of - the - art extraction methods are based on simplistic heuristics and require ad - hoc parameters . In this paper , we propose a novel two - step extraction model , which aims to systematically derive link context of quality as high as anchor text . In the macroscopic analysis step , a systematic web page structure analysis is performed to locate the content cohesive text region and potential relevant header or header like tags . In the microscopic extraction step , an English parser is used to extract the relevant sentence fragments in the text region and the nearest heading text is encompassed if the need arises . Preliminary experimental results proved our approach ' s effectiveness . 

VocabChecker : Measuring Language Abilities for Detecting Early Stage Dementia
Recently , dementia patients have been increasing in number worldwide , necessitating the development of techniques to detect dementia as early as possible . Considering that a typical symptom of dementia , especially Alzheimer ' s disease , is language impairment , speech - based dementia detection approaches have drawn much attention . This paper presents a smartphone - based dementia screening application , VocabChecker , which measures language abilities from a speech narrative via automatic speech recognition ( ASR) . It measures four language abilities related to dementia : number of tokens ( token) , number of types ( type) , type token ratio ( TTR) , and potential vocabulary size ( PVS) . We also reported that the use of VocabChecker has distinguished dementia patients from elderly people . 

A Multi - Sentiment Classifier Based on GRU and Attention Mechanism
Previous sentiment analysis studies have focused on monolingual texts , and are basically multi - category tasks ( ie , a sentence belongs to only one category ) . However , in practice , a sentence often expresses multiple sentiments , and the text often contains multiple languages . This paper proposes a multi - label sentiment classifier based on GRU and attention mechanism , which has achieved good results in the data set provided by NLP & CC share task 1 . 

Multilingual DGRC AskCal : querying energy time series in English , Spanish , and Mandarin Chinese
Large collections of heterogeneous data , such as government provided energy time series , can be difficult for users to navigate through in order to find the data of interest . User difficulties result from issues such as lack of familiarity with the terms of art in the domain , lack of expertise with the query mechanisms specific to particular end data sources , the need to combine or relate data from multiple locations in order to satisfy a single query , and the barriers posed when supplementary resources such as metadata , free - text documentation , and query construction tools are not accessible in a language the user understands well . 

Towards a Requirements Engineering Framework based on Semantics
Requirements engineering is one of the most important issues in systems development . Whether it is software or hardware systems or embedded systems , the need for well - defined requirements remains the same . The ultimate success or failure of developing a system stems largely from the initial definition and management of its requirements . However , despite the efforts that have been made , a coherent and easily understood process that leads from the requirements to correct implementations is still an open research issue , which seeks alternative promising approaches . To this end , in this paper , we propose a requirements engineering approach based on Semantics . It provides a novel mechanism that combines semantics , ontologies , and appropriate NLP techniques . The ultimate goal is to propose a framework that will include the minimum consistent set of formalities and languages to determine the requirements and perform the necessary verifications . 

Minority vote : at - least - N voting improves recall for extracting relations
Several NLP tasks are characterized by asymmetric data where one class label NONE , signifying the absence of any structure ( named entity , coreference , relation , etc . ) dominates all other classes . Classifiers built on such data typically have a higher precision and a lower recall and tend to overproduce the NONE class . We present a novel scheme for voting among a committee of classifiers that can significantly boost the recall in such situations . We demonstrate results showing up to a 16 % relative improvement in ACE value for the 2004 ACE relation extraction task for English , Arabic and Chinese . 

An experiment in unifying audio - visual and textual infrastructures for language processing research and development
This paper describes an experimental integration of two infrastructures ( Eudico and GATE ) which were developed independently of each other ; for different media ( video / speech vs . text ) and applications . The integration resulted into gaining an in - depth understanding of the functionality and operation of each of the two systems in isolation , and the benefits of their combined use . It also highlighted some issues ( e . g . , distributed access ) which need to be addressed in future work . The experiment also showed clearly the advantages of modularity and generality adopted in both systems . 

Myanmar to English verb translation disambiguation approach based on Na ï ve Bayesian classifier
Natural Language processing ( NLP ) is a field of computer science and linguistics concerned with the interactions between computers and human ( natural ) languages . Ambiguity is one of these problems which have been a great challenge for computational linguists . This paper concentrates on the problem of target word selection in Myanmar to English machine translation , for which the approach is directly applicable . However this system can only solve the ambiguities of verbs in Myanmar - English translations . This paper presents a corpus - based approach to word sense disambiguation that builds an ensemble of Na ï ve Bayesian classifiers , each of which based on lexical features . Moreover nouns are detail classified in our system . In this paper , we propose a framework to solve ambiguous verb problems . Our system will support to improve the accuracy of the Myanmar to English translation . 

Words to phrase reordering machine translation system in Myanmar - English using English grammar rules
In machine translation ( MT ) , one of the main problems to handle is word reordering . This paper focuses to design and implement an effective machine translation system for Myanmar to English language ‥ The framework of this paper is reordering approach for English sentence . We propose an approach to generate the target sentence by using reordering model that can be incorporated into the Statistical Machine Translation ( SMTS) . Myanmar sentence and English sentence are not semantic . In this paper , we present Myanmar to English translation system that is our ongoing research . Input process , tokenization , segmentation , translation and English sentence generation include in this system . In this paper , we describe about the English sentence generation . The aim of this paper is to reassemble the English word into proper sentence . The resulted raw sentence from translation process is reassembling to form the English sentence . Subject / verb agreement process , article checking process and tense adjustment process will also be performed according to the English grammar rules . The English sentence generation is proper for Myanmar to English translation systemView less

A Psycholinguistic Approach to Career Selection Using NLP with Deep Neural Network Classifiers
Career direction is a crucial matter not to be undermined in the development of a more efficient generation of the corporate workforce . In order to obtain accurate career direction , one would think of different ways of identifying attributes that would lead to an accurate classification of personality . In this paper , the goal is extracting personality from the use of language . The paper covers all aspects of this process in terms of Text Normalization Techniques , Feature Extraction , Feature Selection , Data Pre - Processing , Data Sampling , Training Predictive Models to predict personality types , validating the results on test data , and finally , and finally , compare the findings with other approaches to personality classification . After having a personality type classified , the process is as simple as matching career paths that are most likely suitable for the user . All these processes combined by experimenting with various approaches to each operation would result in personality attribute classifiers yielding an average of 96 % accuracy . 

Fast statistical parsing of noun phrases for document indexing
Information Retrieval ( IR ) is an important application area of Natural Language Processing ( NLP ) where one encounters the genuine challenge of processing large quantities of unrestricted natural language text . While much effort has been made to apply NLP techniques to IR , very few NLP techniques have been evaluated on a document collection larger than several megabytes . Many NLP techniques are simply not efficient enough , and not robust enough , to handle a large amount of text . This paper proposes a new Probabilistic model for noun phrase parsing , and reports on the application of such a parsing technique to enhance document indexing . The effectiveness of using syntactic phrases provided by the parser to supplement single words for indexing is evaluated with a 250 megabytes document collection . The experiment ' s results show that supplementing single words with syntactic phrases for indexing consistently and significantly improves retrieval performance . 

The Multiple Objectives Flexible Jobshop Scheduling Using Reinforcement Learning
Jobshop Scheduling Problem is a classic complex problem in every field , such as education , business , and daily life . This problem has been changed due to the changing of problem space . For this reason , JSP problems are categorized into many different types , which consist of The General Jobshop Scheduling ( GJSP ) , The Flexible Jobshop Scheduling ( FJSP ) and The Multiple - routes Jobshop Scheduling ( MrJSP ) . However , most of the research that tries to solve the JSP problem has focused on the shortest makespan scheduling . Still , sometimes the minimum makespan can be led to very high operating costs , which have a significant impact on operating results . Therefore , the Multiple - objectives Flexible Jobshop Scheduling Problem ( M - FJSP ) become the focused problem in this research . The proposed method is a Reinforcement Learning Model ( RL ) with a Q - Learning algorithm . The experimental dataset uses data from the OR - Library , which is the collection for a variety of Operation Research ( OR ) problems . Our proposed models will be compared between the three different states definition in which we expect the meta - heuristic model will be the best performance model . 

Smart map for smart city
There are some limitations of the current web like high recall , low precision or search result are highly sensitive to vocabulary because of this next generation web i . e . , Semantic web is used . In Semantic Web information is given in well defined and meaningful manner . Proposed system takes the advantages of Semantic web . In proposed approach we used Google Map API to create Map and used as front end to select particular place called entity . Main aim of the system is to change the way we Map the entity . Once a Map entity is identified Semantic Web i . e . , DBpedia is used to retrieve the information about select entity . After that comments , review also called tweets related to that select entity is displayed on Map form Twitter . Sentiment analysis has been applied on this tweet ' s to show the opinions , sentiments , and emotions of other user towards entities . 

Studies on automatic recognition of Chinese adverb CAI ' s usages based on statistics
Studies about the functional words knowledge base began in recent years . It has gotten some achievements . The functional words include adverbs , preposition , conjunction , auxiliary , and modality . The ldquotrinityrdquo knowledge - base of functional words has been initially built which includes function usage dictionary , usage rules - base and usage corpora . This paper bases on the previous work , and further study automatically recognizing Chinese adverb CAI ' s usages using statistical methods . Two statistical models , viz . CRF and ME , are used to tag the adverb CAI ' s usages on the tagged corpus of People ' s Daily ( 1998 . 1) . The precision rate of CRF and ME in opening test is 73 . 8 % and 73 . 9 % respectively . In closing test the precision rate of both are 100 % . The experiments show that statistic - based method is more effective in usage automatic recognition of the adverb CAI than the rule - based method . 

Sentiment Analysis of Movie Reviews Using Heterogeneous Features
Human disposition has always influenced by others suggestion and reviews . People are always eager to know other ' s reviews for their profit but , every website contains a very large amount of review text , the average human reader will have trouble in identifying relevant sites , extracting and abstracting the reviews so they cannot reach to the right decision in less time that is why automated sentiment analysis systems are required . In the proposed approach , heterogeneous features such as machine learning based and Lexicon based features and supervised learning algorithms like Naive Bayes ( NB ) and Linear Support Vector Machine ( LSVM ) used to build the system model . From implementation and observation , conclude that using proposed heterogeneous features and hybrid approach can get an accurate sentiment analysis system compared to other baseline system . In future for big data , we can use these heterogeneous features for bulding advance and more accurate models using Deep Learning ( DL ) algorithms . 

Augean Artificial Neural Network
There is always a big influence of robotics and natural language processing in the field of automation . The current scenario of natural language processing ( NLP ) focuses on machine analysis and machine interpretation . By the term machine analysis , accuracy becomes an important phenomenon . So , it is named Augean that refers to higher accuracy for natural language processing which is difficult to attain involving the use of neural networks . Thus far named as Augean artificial neural network . It is one of the key branches of machine learning which is peculiar from the other branches speculating more towards artificial intelligence . NLP is now one of the key aspects in making machine learn and letting it speak . The current framework of these discusses more of deep learning . One of the biggest challenges is to make the machine learn and speak is known . Thus far processing becomes the most crucial part . Therefore , when it comes to processing there is no better idea than the brain to be involved . Now the current scenario is about making machine learn to the best of the accuracy and if there is something of great accuracy then it involves a real complex algorithm . The machine works possibly best only if the accuracy stays high and stepping on to the precision of work with better learning ability machines are remarkable . So , to frame this , artificial neural network and neuroscience have been used to make the model more robust increasing the accuracy with at least five percent to the previous models involved . 

Deep learning for automated sentiment analysis of social media
The spread of information on Facebook and Twitter is much more efficient than on traditional social media platforms . For word - of - mouth ( WOM ) marketing , social media have become a rich information source for companies or scholars to design models to examine this repository and mine useful insights for marketing strategies . However , social media language is relatively short and contains special words and symbols . Most natural language processing ( NLP ) methods focus on processing formal sentences and are not well - suited to such short messages . In this study we propose a novel sentiment analysis framework based on deep learning models to extract sentiment from social media . We collect data from which we compile a dataset . After processing these special terms , we seek to establish a semantic dataset for further research . The extracted information will be useful for many future applications . The experimental data have been obtained by crawling several social media platforms . 

Aspect of Blame in Tweets : A Deep Recurrent Neural Network Approach
Twitter as an information dissemination tool has proved to be instrumental in generating user curated content in short spans of time . Tweeting usually occurs when reacting to events , speeches , about a service or product . This in some cases comes with its fair share of blame on varied aspects in reference to say an event . Our work in progress details how we plan to collect the informal texts , clean them and extract features for blame detection . We are interested in augmenting Recurrent Neural Networks ( RNN ) with self - developed association rules in getting the most out of the data for training and evaluation . We aim to test the performance of our approach using human - induced terror - related tweets corpus . It is possible tailoring the model to fit natural disaster scenarios . 

Automatic generation of narrative content for digital games
Interactive simulation games used for training usually require a large amount of coherent narrative content . An effective and efficient solution to the narrative content creation problem is to use Natural Language Generation ( NLG ) systems . The use of NLG systems , however , requires sophisticated linguistic and sometimes programming knowledge . For this reason , NLG systems are typically not accessible to the game designers who write narrative content . We have designed and implemented a visual environment for creating and modifying NLG templates that requires no programming knowledge , and can operate with a minimum of linguistic knowledge . It allows specifying templates with any number of variables and dependencies between them . It automatically generates all the sentences that follow the created template . It uses SimpleNLG to provide the linguistic background knowledge . We tested the performance of our system in the context of an interactive simulation game . 

AI - Driven Contextual Virtual Teaching Assistant Using RASA
With traditional classes moving to online platforms and the need for online content growing exponentially , there is a deficit of customized content that helps students understand concepts well . It is the need of the hour to have round the clock support and resources to ensure smooth transition and continuity of academics . Our solution proposes the use of RASA , an open - source conversational Artificial Intelligence ( AI ) framework , to support students with contextual help and provide them with resources like specific slides of a presentation or real - world problems discussed in class in the form of a Virtual Teaching Assistant . It can also ask leading questions to provide tailored answers . Its responses are based on the material the Instructor teaches in class , making it relatable to the students , unlike generic responses . The Contextual Virtual Teaching Assistant will also assist the Instructor in identifying students that need additional help with academics , answering queries related to upcoming tests or assignments , and can even alert students about deadlines

Deep neural networks with pre - train model BERT for aspect - level sentiments classification
With the rapid development of the Internet , the amount of text on the network has shown a rapid growth trend , and the demand for text classification technology is increasing day by day , especially for aspect - level sentiments classification . However , the traditional method mainly relies on time - consuming feature engineering , but due to its context - independent nature , it ignores the rich context information in the text , which greatly reduces the performance in Natural Language Processing ( NLP ) tasks . Bidirectional Encoder Representations from Transformers ( BERT ) , a pre - trained language models , refreshed records on eleven NLP tasks and became a new baseline model for text classification . Although BERT has been widely used for other NLP tasks , it is rarely used on aspect - level sentiments classification . We use the BERT output containing rich contextual information as the input of the optimized DNN network , and use the DNN network for further classification to obtain better performance in aspect - level sentiment classification . We performed comparative experiments on three public datasets . Compared with other latest baseline models , our model has better performance in aspect - level sentiments classification . 

A New Entropy - based Vocabulary Optimization Approach for Chinese Language Modeling
This paper proposed a new entropy - based vocabulary optimization approach for Chinese language modeling . This approach aims to directly optimize the language model by extending the vocabulary , that is , to minimize the character perplexity of the language model . A new criterion for new words selection was developed based on the character perplexity metric . A fast computing method and a simple divide - and - conquer method were proposed to deal with very large corpus . Experiments showed about 3 % character perplexity reduction and 3 % character error rate reduction in a speech recognition task . Comparison experiments were also conducted to compare with other approaches . 

Indoor Positioning Estimation Using BLE Beacons
In this paper , we propose four unsupervised position estimation strategies from very noisy observations . Moreover , we discuss their performance , applying them to data gathered under a sensor system constructed with common devices . An observed RSSI , which denotes radio wave intensity , is distorted by noises because of multipath fading and obstacles to prevent radio wave communication and nobody knows correct RSSI . Hence , a position estimation strategy should be an unsupervised method and we must introduce some assumptions to an observation generation process . The position estimation strategies have the following assumptions ; ( l ) receivers with too low RSSI are not reliable and ( 2 ) human move is enough slow . Using the assumption , we proposed four position estimation strategies with an unsupervised method . We gathered RSSI logs in an international academic conference to discuss the performance of the four strategies . Moreover , we applied the strategies to the logs and estimated positions are discussed from the viewpoint of stability of estimated positions . 

PRFloor : An Automatic Floorplanner for Partially Reconfigurable FPGA Systems
Partial reconfiguration ( PR ) is gaining more attention from the research community because of its flexibility in dynamically changing some parts of the system at runtime . However , the current PR tools need the designer ' s involvement in manually specifying the shapes and locations for the PR regions ( PRRs ) . It requires not only deep knowledge of the FPGA device , the system architecture , but also many trial - and - error attempts to find the best - possible floorplan . Therefore , many research works have been conducted to propose automatic floorplanners for PR systems . However , one of the most significant limitations of those works is that they only consider the PRRs and ignore all other static modules . In this paper , we propose a novel PR floorplanner called PRFloor . It takes into account all components in the system . The main ideas behind PRFloor are the unique recursive pseudo - bipartitioning heuristic using a new , simple , yet effective Nonlinear Integer Programming - based bipartitioner . The PRFloor performs very well in the experiments with various synthetic PR system setups with up to 130 modules , 24 PRRs and 85 % of the FPGA resource . The average maximum clock frequency obtained for the actual PR systems implemented using PRFloor is even 3 % higher than the similar systems without PR capability . 

Auto - generating Textual Data Stories Using Data Science Pipelines
Understanding a dataset directly is challenging but transforming the results of data analysis into data stories could help people build mental models and understand the dataset easily . In this paper , we present a new framework for data - to - text NLG to generate data stories for specific personas . In order to understand the feasibility of this method and if the human generated story is consistent with the story generated by the data science pipelines , we present two experiments : a data story study with 3 financial experts , 4 Ph . D . students , and 20 Amazon Mechanical Turk workers , which offers several data stories generated by humans ; and a validation study involving 39 Amazon Mechanical Turk workers who conducted usability and understandability assessments for 9 high - quality data stories , written by humans and machine . We conduct a qualitative analysis of human - written data stories to determine what people consider when writing data stories and if the human generated story is consistent with the one generated by the data science pipeline . The experimental results show that readers comprehend machine - written data stories as well as they comprehend human - written data stories . 

A robust portable natural language data base interface
This paper describes a NL data base interface which consists of two parts : a Natural Language Processor ( NLP ) and a data base application program ( DBAP ) . The NLP is a general purpose language processor which builds a formal representation of the meaning of the English utterances it is given . The DBAP is an algorithm with builds a query in a augmented relational algebra from the output of the NLP . This approach yields an interface which is both extremely robust and portable . 

Learning dependency translation models as collections of finite - state head transducers
The paper defines weighted head transducers , finite - state machines that perform middle - out string transduction . These transducers are strictly more expressive than the special case of standard left - to - right finite - state transducers . Dependency transduction models are then defined as collections of weighted head transducers that are applied hierarchically . A dynamic programming search algorithm is described for finding the optimal transduction of an input string with respect to a dependency transduction model . A method for automatically training a dependency transduction model from a set of input - output example strings is presented . The method first searches for hierarchical alignments of the training examples guided by correlation statistics , and then constructs the transitions of head transducers that are consistent with these alignments . Experimental results are given for applying the training method to translation from English to Spanish and Japanese . 

GRASP : grammar - and syntax - based pattern - finder in CALL
We introduce a method for learning to describe the attendant contexts of a given query for language learning . In our approach , we display phraseological information in the form of a summary of general patterns as well as lexical bundles anchored at the query . The method involves syntactical analyses and inverted file construction . At run - time , grammatical constructions and their lexical instantiations characterizing the usage of the given query are generated and displayed , aimed at improving learners ' deep vocabulary knowledge . We present a prototype system , GRASP , that applies the proposed method for enhanced collocation learning . Preliminary experiments show that language learners benefit more from GRASP than conventional dictionary lookup . In addition , the information produced by GRASP is potentially useful information for automatic or manual editing process . 

Automated identification of media bias by word choice and labeling in news articles
Media bias can strongly impact the individual and public perception of news events . One difficult - to - detect , yet powerful form of slanted news coverage is bias by word choice and labeling ( WCL) . Bias by WCL can occur when journalists refer to the same concept , yet use different terms , which results in different sentiments being sparked in the readers , such as the terms " economic migrants " vs . " refugees . " We present an automated approach to identify bias by WCL that employs models and manual analysis approaches from the social sciences , a research domain in which media bias has been studied for decades . This paper makes three contributions . First , we present NewsWCL50 , the first open evaluation dataset for the identification of bias by WCL consisting of 8 , 656 manual annotations in 50 news articles . Second , we propose a method capable of extracting instances of bias by WCL while outperforming state - of - the - art methods , such as coreference resolution , which currently cannot resolve very broadly defined or abstract coreferences used by journalists . We evaluate our method on the NewsWCL50 dataset , achieving an F1 = 45 . 7 % compared to F1 = 29 . 8 % achieved by the best performing state - of - the - art technique . Lastly , we present a prototype demonstrating the effectiveness of our approach in finding frames caused by bias by WCL . 

Non - periodic photorefractive photonic lattices in lithium niobate : Features of their formation and light propagation
We experimentally demonstrate formation of nonperiodic photorefractive photonic lattices in lithium niobate using the optical modulation and optical induction procedures based on contact and optical projection schemes with amplitude masks . 

Analyzing Chat Logs in Online Games for Tutorial Improvement
Our current work is dedicated to research and development of methods for tutorial improvement and newcomers adaptation in online games . This report is focused on investigation of help - seeking patterns in online games ' chat logs . Using text mining techniques we detect game - related questions from users , explore their contents , and show the changes across the server lifecycle . 

Context - Aware Deep Learning Approach for Answering Questions
Training neural networks to read and comprehend natural language documents still poses a great challenge . In the recent past , large scale training and test data sets have been made available for testing machine reading systems based on their ability to answer unseen questions , given the context . In Question - Answering , the model generates answers from the given context for the questions ( given as input . Various machine learning methods can be used to build such systems . For understanding natural language , the model should be able to convert the sentences or paragraphs into a representation that is internal to the model ( understandable by it) , to be able to generate valid answers . Valid answers are the ones which answer the question asked correctly . [1 ] This work attempts to design a deep learning model to read and comprehend the context provided and provide answers to the posed questions in natural language accurately with very little knowledge of language structure . 

A Fuzzy , Incremental and Semantic Trending Topic Detection in Social Feeds
Nowadays , a huge number of people participating in social networks is triggering a fast and wide spectrum of topics . Such trending topics are usually derived from the most frequent searches , the published posts and the daily news . The automated analysis for such data requires topics detection and tracking methods . Many challenges are being faced . It is difficult to discover the semantic relatedness when the same event is presented by different titles and to handle merging semantically identical topics from different channels ( aggregation ) . Other hardships are the vagueness regarding the vast web collection , the scalability to analyze them , and the fact that it is a time consuming task . The framework introduced in this paper aims to solve these issues . Because a web document often consists of several topics , the suggested model employs a fuzzy C - Means ( FCM ) clustering based trending topics detection . It applies a semantic document similarity algorithm to resolve such ambiguity issues caused by the usage of synonyms , homonyms or different abstraction levels . This algorithm is also used to summarize the long documents . Furthermore , an incremental clustering technique is utilized to preserve high cohesiveness up - to - date top trending topics . The experimental results finally illustrate the effectiveness and the superiority of this model , compared with other trending topics detection algorithms , in terms of entropy and F - score measures . 

Concept vector extraction from Wikipedia category network
The availability of machine readable taxonomy has been demonstrated by various applications such as document classification and information retrieval . One of the main topics of automated taxonomy extraction research is Web mining based statistical NLP and a significant number of researches have been conducted . However , existing works on automatic dictionary building have accuracy problems due to the technical limitation of statistical NLP ( Natural Language Processing ) and noise data on the WWW . To solve these problems , in this work , we focus on mining Wikipedia , a large scale Web encyclopedia . Wikipedia has high - quality and huge - scale articles and a category system because many users in the world have edited and refined these articles and category system daily . Using Wikipedia , the decrease of accuracy deriving from NLP can be avoided . However , affiliation relations cannot be extracted by simply descending the category system automatically since the category system in Wikipedia is not in a tree structure but a network structure . We propose concept vectorization methods which are applicable to the category network structured in Wikipedia . 

Character gazetteer for Named Entity Recognition with linear matching complexity
A large amount of unstructured data is produced daily through numerous media around us . Despite that computer systems are becoming more powerful , even the commodity hardware , processing of such data and gaining useful information in time efficient manner remains a problem . One of the domains in unstructured data processing is Natural Language Processing ( NLP) . NLP covers areas like information extraction , machine translation , word sense disambiguation , automated question answering , etc . All of these areas require fast and precise Named Entity Recognition ( NER) , which is not a trivial task because of the processed data size and heterogeneity . Our effort in this research area is to provide fast tokenization and precise NER with linear complexity . In this paper , we present a character gazetteer with linear tokenization as well as NER and compare its two tree data structure representations ; i . e . multiway tree implemented by hash maps and first child - next sibling binary tree . Our measurements shows that one outperforms the other in processing time , while the other outperforms it in memory consumption efficiency . 

Mining Implicit Relevance Feedback from User Behavior for Web Question Answering
Training and refreshing a web - scale Question Answering ( QA ) system for a multi - lingual commercial search engine often requires a huge amount of training examples . One principled idea is to mine implicit relevance feedback from user behavior recorded in search engine logs . All previous works on mining implicit relevance feedback target at relevance of web documents rather than passages . Due to several unique characteristics of QA tasks , the existing user behavior models for web documents cannot be applied to infer passage relevance . In this paper , we make the first study to explore the correlation between user behavior and passage relevance , and propose a novel approach for mining training data for Web QA . We conduct extensive experiments on four test datasets and the results show our approach significantly improves the accuracy of passage ranking without extra human labeled data . In practice , this work has proved effective to substantially reduce the human labeling cost for the QA service in a global commercial search engine , especially for languages with low resources . Our techniques have been deployed in multi - language services . 

Autograding " Explain in Plain English " questions using NLP
Previous research suggests that " Explain in Plain English " ( EiPE ) code reading activities could play an important role in the development of novice programmers , but EiPE questions aren ' t heavily used in introductory programming courses because they ( traditionally ) required manual grading . We present what we believe to be the first automatic grader for EiPE questions and its deployment in a large - enrollment introductory programming course . Based on a set of questions deployed on a computer - based exam , we find that our implementation has an accuracy of 87 - 89% , which is similar in performance to course teaching assistants trained to perform this task and compares favorably to automatic short answer grading algorithms developed for other domains . In addition , we briefly characterize the kinds of answers that the current autograder fails to score correctly and the kinds of errors made by students . 

NLP Oriented Japanese Pun Classification
In this paper we describe a phonetic classification of Japanese puns ( dajare ) . Basing on real life examples gathered from available sources ( books , Internet) , we divided Japanese puns into 12 groups with numerous subgroups , according to phonetic changes that occur within them . This classification was prepared for the NLP purpose , i . e . to be used in humor processing . Its usefulness was shown in a research project , aimed at constructing a humor - equipped conversational system for Japanese . 

HOO 2012 error recognition and correction shared task : Cambridge University submission report
Previous work on automated error recognition and correction of texts written by learners of English as a Second Language has demonstrated experimentally that training classifiers on error - annotated ESL text generally outperforms training on native text alone and that adaptation of error correction models to the native language ( L1 ) of the writer improves performance . Nevertheless , most extant models have poor precision , particularly when attempting error correction , and this limits their usefulness in practical applications requiring feedback . We experiment with various feature types , varying quantities of error - corrected data , and generic versus L1 - specific adaptation to typical errors using N ä ive Bayes ( NB ) classifiers and develop one model which maximizes precision . We report and discuss the results for 8 models , 5 trained on the HOO data and 3 ( partly ) on the full error - coded Cambridge Learner Corpus , from which the HOO data is drawn . 

CorpoMate : A framework for building linguistic corpora from the web
A linguistic corpus is a collection of an ample number of text documents serving as a data source for sampling human language , usually in computational linguistics . Conventional methods of building such a corpus involve frequent human intervention and poses difficulties during reproduction . To address the issues , the paper introduces CorpoMate , an extensible framework with a pipeline - inspired and modular architecture for automating the creation of linguistic corpora , from web resources via crawling websites or parsing feeds . It performs the necessary pre - processing as well as related tasks according to programmable queues of standard or customized tasks with easily swappable tools and , can export aggregated data into widely - accepted formats . Results from experiments performed on text processing tools and performance tests on the asynchronous , rule - based web crawling system justify the importance of having swappable tools along with the feasibility of the architecture described in the paper . 

Turkish Sentiment Analysis Using BERT
While sentiment analysis is a popular research area , most of the research has been conducted for English and the number of studies for Turkish are rather limited . Limited resources for Turkish natural language processing ( NLP ) is one of the major challenges for Turkish NLP research . In order to overcome these limitations , we propose two approaches for Turkish sentiment analysis : 1 ) fine tuning multilingual model of BERT 2 ) using main model of BERT after machine translation of Turkish texts into English . We conducted experiments on Turkish movie and hotel review datasets where each review is labeled either positive or negative . Our methods achieve high accuracy scores such that in the movie dataset , our BERT models outperform existing methods . 

Combining classifiers for supertagging Arabic texts
This paper deals with supertagging Arabic texts with ArabTAG formalism , a semi - lexicalised grammar based on TAG and adapted for Arabic . Supertagging is a very useful task because it reduces and speeds the work of parsing . We view this problem as a classification task where elementary structures supertags ( classes ) are affected to words in a given sentence according to their description ( morpho - syntactic and contextual information ) . We propose to combine three classifiers : Na ï ve Bayes , k - Nearest Neighbors ( k - NN ) and Decision tree by a voting procedure . The primary results were satisfactory as we obtained an accuracy rate of 76 % although the small size of our training corpus ( 5 , 000 words ) and the difficulties related to Arabic language specificities . 

Latent topic estimation based on events in a document
Recently , several latent topic model - based methods such as LSI , pLSI , and LDA have been widely used for text analysis . However , those methods basically assign topics to words , and therefore the relationship between words in a document is not considered . Considering this , we propose a latent topic extraction method which assigns topics to events that represent the relation between words in a document . There are several ways to express events , and the accuracy of estimating latent topics differs depending on the definition of an event . Therefore , we propose several event types and examine which event type works well to estimate latent topics in a document with a common document retrieval task . Furthermore , as an application of our proposed method , we also show a multi - document summarization based on latent topics . 

Reengineering a domain - independent framework for spoken dialogue systems
Our work in this area started as a research project but when L2F joined TecnoVoz , a Portuguese national consortium including Academia and Industry partners , our focus shifted to real - time professional solutions . The integration of our domain - independent Spoken Dialogue System ( SDS ) framework into commercial products led to a major reengineering process . This paper describes the changes that the framework went through and that deeply affected its entire architecture . The communication core was enhanced , the modules interfaces were redefined for an easier integration , the SDS deployment process was optimized and the framework robustness was improved . The work was done according to software engineering guidelines and making use of design patterns . 

Coevolution particle filter for mobile robot simultaneous localization and mapping
This paper presents the implementation of particle filter ( PF ) combined with a coevolution mechanism derived from the competition model of ecological species for mobile robot simultaneous localization and mapping ( SLAM ) . The new version of particle filters is termed coevolution particle filter ( CEPF ) . In CEPF particles are clustered into species , each of which represents the posterior estimation of robot ' s pose or landmark locations and is superior to a single particle . Since the coevolution between the species ensures that the multiple distinct hypotheses can be estimated at the same time . And the number of particles can be adjusted adaptively over time according to the population growth model . In addition , by using the crossover and mutation operators in evolutionary computation , intra - species evolution can drive the particles move towards the regions where the desired posterior density is large . So a small number of particles can represent the desired density well enough to make precise posterior estimation . Experimental results show that CEPF is efficient for SLAM and indicate superior performance compared with those of the EKF and PF method . 

Use of dependency tree structures for the microcontext extraction
In several recent years , natural language processing ( NLP ) has brought some very interesting and promising outcomes . In the field of information retrieval ( IR ) , however , these significant advances have not been applied in an optimal way yet . Author argues that traditional IR methods , i . e . methods based on dealing with individual terms without considering their relations , can be overcome using NLP procedures . The reason for this expectation is the fact that NLP methods are able to detect the relations among terms in sentences and that the information obtained can be stored and used for searching . Features of word senses and the significance of word contexts are analysed and possibility of searching based on word senses instead of mere words is examined . The core part of the paper focuses on analysing Czech sentences and extracting the context relations among words from them . In order to make use of lemmatisation and morphological and syntactic tagging of Czech texts , author proposes a method for construction of dependency word microcontexts fully automatically extracted from texts , and several ways how to exploit the microcontexts for the sake of increasing retrieval performance . 

Explainability for Natural Language Processing
This lecture - style tutorial , which mixes in an interactive literature browsing component , is intended for the many researchers and practitioners working with text data and on applications of natural language processing ( NLP ) in data science and knowledge discovery . The focus of the tutorial is on the issues of transparency and interpretability as they relate to building models for text and their applications to knowledge discovery . As black - box models have gained popularity for a broad range of tasks in recent years , both the research and industry communities have begun developing new techniques to render them more transparent and interpretable . Reporting from an interdisciplinary team of social science , human - computer interaction ( HCI ) , and NLP / knowledge management researchers , our tutorial has two components : an introduction to explainable AI ( XAI ) in the NLP domain and a review of the state - of - the - art research ; and findings from a qualitative interview study of individuals working on real - world NLP projects as they are applied to various knowledge extraction and discovery at a large , multinational technology and consulting corporation . The first component will introduce core concepts related to explainability in NLP . Then , we will discuss explainability for NLP tasks and report on a systematic literature review of the state - of - the - art literature in AI , NLP and HCI conferences . The second component reports on our qualitative interview study , which identifies practical challenges and concerns that arise in real - world development projects that require the modeling and understanding of text data . 

Thermal via planning for 3 - D ICs
Heat dissipation is one of the most serious challenges in 3 - D IC designs . One effective way of reducing circuit temperature is to introduce thermal through - the - silicon ( TTS ) vias . In this paper , we extended the TTS - via planning in a multilevel routing framework by Cong and Zhang ( 2005 ) , but use a much enhanced TTS - via planning algorithm . We formulate the TTS - via minimization problem with temperature constraints as a constrained nonlinear programming problem ( NLP ) based on the thermal resistive model and develop an efficient heuristic algorithm , named m - ADVP , which solves a sequence of simplified via planning subproblems in alternating direction in a multilevel framework . The vertical via distribution is formulated as a convex programming problem , and the horizontal via planning is based on two efficient techniques : path counting and heat propagation . Experimental results show that the m - ADVP algorithm is more than 200 / spl times / faster than the direct solution to the NPL formulation for via planning with very similar solution quality ( within 1 % of TS - vias count ) . However , compared to a recent work of multilevel TS - via planning algorithm based on temperature profiling ( Cong and Zhang , 2005 ) , our algorithm can reduce the total TS - via number by over 68 % for the same required temperature with similar runtime . 

Detecting Advance Fee Fraud Using NLP Bag of Word Model
Advance Fee Fraud ( AFF ) is a form of Internet fraud prevalent within the Cybercrimes domain in literature . Evidence shows that huge financial assets are stolen from the global economy as a result of AFF . Consequently , this paper presents a fraudulent email classifier ( FEC ) that detects and classifies an email as fraudulent or non - fraudulent using Natural Language Process ( NLP ) model referred to as Bag - of - Words ( BoW ) . The classifier is designed and trained to detect and classify AFF that originate from known sources using Nigeria as a Case study . Dataset is obtained and used for the training while testing the classifier logs . Experimentally , the classifier was trained using various machine learning algorithms with BoW generated as predictors . By selecting the best algorithms , the classifier was tested and found to perform satisfactorily . 

Open IE - Triples Inference – Corpora Development and DNN Architectures
Natural language inference ( NLI ) is a well established part of natural language understanding ( NLU ) . This task is usually stated as a 3 - way classification of sentence pairs with respect to entailment relation ( entailment , neutral , contradiction ) . In this work , we focus on a derived task of relation inference : we propose a method of transforming a general NLI corpus to an annotated corpus for relation inference that utilizes existing NLI annotations . We subsequently introduce a novel relation inference corpus obtained from a well known SNLI corpus and provide its brief characterization . We investigate several DNN siamese architectures for this task and this particular corresponding corpus . We set several baselines including hypothesis only baseline . Our best architecture achieved 96 . 92 % accuracy . 

Geno : A Developer Tool for Authoring Multimodal Interaction on Existing Web Applications
Supporting voice commands in applications presents significant benefits to users . However , adding such support to existing GUI - based web apps is effort - consuming with a high learning barrier , as shown in our formative study , due to the lack of unified support for creating multi - modal interfaces . We develop Geno - - - a developer tool for adding the voice input modality to existing web apps without requiring significate NLP expertise . Geno provides a unified workflow for developers to specify functionalities to support by voice ( intents ) , create language models for detecting intents and the relevant information ( parameters ) from user utterances , and fulfill the intents by either programmatically invoking the corresponding functions or replaying GUI actions on the web app . Geno further supports references to GUI context in voice commands ( e . g . , " add this to the playlist " ) . In a study , developers with little NLP expertise were able to add the multi - modal support for two existing web apps using Geno . 

A Novel Framework For Sentiment Analysis Using Deep Learning
Large amount of un - structured data is present online in the form of opinions and reviews . The most important task of NLP is to Extract useful information from unstructured data by first converting into structured form . Many customers write down reviews online but do not give rating to them . The main concern of this paper is to perform sentiment analysis by predicting two main types of polarities from reviews available online i - e positive and negative . Neural networks models fail to capture the contextual meaning of words and also fails to save long sequences of words and thus results in reducing performance . To overcome this issue a novel Hybrid model ( RNN - LSTM - BiLSTM - CNN ) using majority voting , word2vec and pre - trained Glove embedding ( 100d ) is proposed to predict sentiment polarity against each review . Loss function used is Binary cross entropy . The proposed model is tested on different state - of - the - art datasets like SST - 1 , SST - 2 and MR Movie review dataset . Results proved that our proposed model results in improved accuracy . 

Bayesian network , a model for NLP ? 
The NLP systems often have low performances because they rely on unreliable and heterogeneous knowledge . We show on the task of non - anaphoric it identification how to overcome these handicaps with the Bayesian Network ( BN ) formalism . The first results are very encouraging compared with the state - of - the - art systems . 

A robust unsupervised speaker clustering of speech utterances
This paper aims at developing and investigating efficient , robust and unsupervised algorithm for speaker clustering . Each utterance is modeled as a single Gaussian model distribution . A novel distance metric is proposed in this paper for the purpose of determining stopping criteria . The advantage of the proposed method is that it achieves comparable performance without requiring an adjusting threshold term . In this paper , we adopt the framework of agglomerative hierarchical clustering ( AHC ) with the merging criterion using Kullback - Leibler ( KL ) distance . The proposed stopping criterion can ensure a right number of speaker clusters . The efficiency of the proposed algorithm is demonstrated with various experiments on data from NIST and HUB5 , respectively . 

Online Communication with Natural Language
Nowadays all the University and Organization have a dedicated website of their own . The most challenging part in maintaining the website is to effectively handle the user doubts and queries . Often , users wish to interact with the authoritative person . Answering every customer is tiresome and leads to loss of time and human resource . It is impossible to handle lot of queries simultaneously . In order to make the work of the organization simple , Chatbots can be used . A Chatbot simulates the conversation with user in mobile applications , web applications and messaging applications . The proposed system will be able to read a piece of text , that is being asked as a question by the user and it will provide the appropriate response by text categorization and keyword matching . The screenshots illustrate the successful implementation of chatbot application . 

Use of Artificial Intelligence in Healthcare Systems : State - of - the - Art Survey
Artificial Intelligence ( AI ) techniques getting at any point present in modern business and regular day to day existence is additionally consistently being applied to Healthcare . In various pieces of regulatory cycles and patient thoughts by making use of Artificial Intelligence in Healthcare services can possibly help Healthcare services suppliers . Most of the AI and medical services innovations have solid importance to the Healthcare field , yet the strategies they backing can shift essentially and keeping in mind that a few review papers on Artificial Intelligence in Healthcare recommend that by making use of Artificial Intelligence in Healthcare can perform similarly too or better than people at specific techniques , like diagnosing disease , it will be a significant number of years before AI in Healthcare services replaces people for an expansive scope of medical tasks . By using Artificial Intelligence it is expecting to mirror human mental capacities and limits . It is acquiring a change point of view to constrain by extending accessibility of Healthcare , services information , and quick progression of investigation methods . Significant illness areas by making use of AI techniques incorporate cardiology , Cancer , neurology , and then we review the applications , Ethical issues in the application of AI to Healthcare and conclude about the different AI frameworks . 

Analysis of Machine Code Using Natural Language Processing
In this paper we have created a Machine Learning Model for Machine Languages , this field of processing machine language and converting / making meaningful iterations out of it comes under Natural Language Processing . Existing approaches on NLP were focused on human - to - human interactions i . e . , English to French , German , Hindi . In this research we try to analyze lexical in machine code , break it down to smaller meaningful bits and make some inferences like the type of languages , number of loops , functionality , and time complexity of code , rewriting of code , converting code from one language to another . We have mainly focused on the recognition of the type of language by testing a small snippet of code . We have also focused on a bigger real - world problem which is conversion of code from one language to another with the same basic meaning and logic , just changing the syntax . We have deployed the machine learning model for public use , and open - sourced Flask API is deployed on the cloud for usage that accepts 3 types of machine languages and returns the type of syntax . We have created a simple Angular App for the demonstration and testing of the model which is hosted and is live online . link ( https: / / nlpapp - 8e2fd . web . app / )View less

Combining graph connectivity and genetic clustering to improve biomedical summarization
Automatic summarization is emerging as a feasible instrument to help biomedical researchers to access online literature and face information overload . The Natural Language Processing community is actively working toward the development of effective summarization applications ; however , automatic summaries are sometimes less informative than the user needs . In this work , our aim is to improve a summarization graph - based process combining genetic clustering with graph connectivity information . In this way , while genetic clustering allows us to identify the different topics that are dealt with in a document , connectivity information ( in particular , degree centrality ) allows us to asses and exploit the relevance of the different topics . Our automatic summaries are compared with others produced by commercial and research applications , to demonstrate the appropriateness of using this combination of techniques for automatic summarization . 

Towards testing the syntax of punctuation
Little work has been done in NLP on the subject of punctuation , owing mainly to a lack of a good theory on which computational treatments could be based . This paper described early work in progress to try to construct such a theory . Two approaches to finding the syntactic function of punctuation marks are discussed , and procedures are described by which the results from these approaches can be tested and evaluated both against each other as well as against other work . Suggestions are made for the use of these results , and for future work . 

First International Workshop on e - Commerce and NLP ( ECNLP ) Chairs ' Welcome
No abstract available . 

Direct trajectory optimization by a Chebyshev pseudospectral method
A Chebyshev pseudospectral method is presented in this paper for directly solving a generic optimal control problem with state and control constraints . This method employs Nth degree Lagrange polynomial approximations for the state and control variables with the values of these variables at the Chebyshev - Gauss - Lobatto ( CGL ) points as the expansion coefficients . This process yields a nonlinear programming problem ( NLP ) with the state and control values at the CGL points as unknown NLP parameters . Numerical examples demonstrate this method yields more accurate results than those obtained from the traditional collocation methods . 

Is it Fake ? News Disinformation Detection on South African News Websites
Disinformation through fake news is an ongoing problem in our society and has become easily spread through social media . The most cost - and time - effective way to filter these large amounts of data is to use a combination of human and technical interventions to identify it . From a technical perspective , Natural Language Processing ( NLP ) is widely used in detecting fake news . Social media companies use NLP techniques to identify the fake news and warn their users , but fake news may still slip through undetected . It is especially a problem in more localised contexts ( outside the United States of America ) . How do we adjust fake news detection systems to work better for local contexts such as in South Africa . In this work we investigate fake news detection on South African websites . We curate a dataset of South African fake news and then train detection models . We contrast this with using widely available fake news datasets ( from mostly USA website ) . We also explore making the datasets more diverse by combining them and observe the differences in behaviour in writing between nations ’ fake news using interpretable machine learning . 

Whats Trending ? An Efficient Trending Research Topics Extractor and Recommender
Finding trending research topics can be of great importance to many domains . However , despite its importance , the works that concentrated on this task are very limited . Furthermore , they did not consider important criteria to detect trending scientific topics . Furthermore , the extraction of trending topics need to be integrated with a recommender system , as it would be difficult for users to browse all the extracted trending scientific topics . In this work , we propose an efficient trending scientific topics extractor and an integrated recommender system . The contribution of this work lies in three points . First , improving the performance of the trending topic extractors by using two important criteria that were not used by the literature . Second , integrating a recommender system with the trending topic extractor to provide intelligent results based on the user profile . Third , insisting on the importance of this field as it deserves more works in this direction . Experimental results showed that the system is efficient in terms of updating time , trending topic extraction time , extraction recall and precision , and recommendation time . 

Passively Mode - Locked Fiber Lasers Based on Nonlinearity at 2 - μm Band
2 - μ m fiber lasers are of wide potential applications in the fields of radar , sensing , and free - space communications . We introduced a simple approach to generate passively mode - locked pulse in thulium - doped fiber lasers based on nonlinear polarization rotation ( NPR ) in our previous work . The high repetition rate of 1 . 78 GHz is obtained by both NPR and semiconductor saturable absorption mirror . By using a polarization - maintaining fiber in the experiment , a birefringence Lyot filter as a fiber comb filter can be constructed , and then the tuning range of 94 nm can be obtained for the mode - locked laser . In this paper , we also demonstrated our recent research work , a square - wave mode - locked fiber laser at 2 - μ m band based on nonlinear amplifying loop mirror . The square - wave noiselike pulse ( NLP ) at longer wavelength of 2 - μ m band can be observed . With the increase of the pump power , pulse width can be increased from 0 . 75 to 1 . 3 ns . The polarization vector characteristics of the square - wave NLP were analyzed experimentally . In addition , with the increase of the pump power , a kind of square - wave mode - locked pulse with low - intensity multipulse bunches can be observed , and these multipulse bunches can be operated at the multipulse mode - locked state . The side - mode suppression ratio decreased from 50 to 30 dB . 

Pre - Trained Language Model Transfer on Chinese Named Entity Recognition
The challenges of natural language processing ( NLP ) lie in the polysemy and insufficiency of human - labeled training data . Bidirectional Encoder Representations from Transformers ( BERT ) facilitates pre - training deep bidirectional representations on large - scale unannotated text on the web and has created state - of - the - art performance on various NLP tasks after simple fine - tuning . The representation and application of semantic knowledge are important steps in the entire process of NLP tasks . In this work , for the encoder of our model , we encode an input sequence into contextual representations using pre - trained language model and design a new model that combines neural network with BERT . Experimental results show that our method achieves new state - of - the - art . 

Unilateral Weighted Jaccard Coefficient for NLP
Similarity measures are essential to solve many pattern recognition problems such as classification , clustering , and retrieval problems . Various similarity measures are categorized in both syntactic and semantic relationships . In this paper we present a novel similarity , Unilateral Weighted Jaccard Coefficient ( uwJaccard) , which takes into consideration not only the space among two points but also the semantics among them in a distributional semantic model , the Unilateral Weighted Jaccard Coefficient provides a measure of uncertainty which will be able to measure the uncertainty among sentences such as " man bites dog " and " dog bites man " . 

Improving accuracy in word class tagging through the combination of machine learning systems
We examine how differences in language models , learned by different data - driven systems performing the same NLP task , can be exploited to yield a higher accuracy than the best individual system . We do this by means of experiments involving the task of morphosyntactic word class tagging , on the basis of three different tagged corpora . Four well - known tagger generators ( hidden Markov model , memory - based , transformation rules , and maximum entropy ) are trained on the same corpus data . After comparison , their outputs are combined using several voting strategies and second - stage classifiers . All combination taggers outperform their best component . The reduction in error rate varies with the material in question , but can be as high as 24 . 3 % with the LOB corpus . 

Prepositional phrase attachment without oracles
Work on prepositional phrase ( PP ) attachment resolution generally assumes that there is an oracle that provides the two hypothesized structures that we want to choose between . The information that there are two possible attachment sites and the information about the lexical heads of those phrases is usually extracted from gold - standard parse trees . We show that the performance of reattachment methods is higher with such an oracle than without . Because oracles are not available in NLP applications , this indicates that the current evaluation methodology for PP attachment does not produce realistic performance numbers . We argue that PP attachment should not be evaluated in isolation , but instead as an integral component of a parsing system , without using information from the gold - standard oracle . 

Recognizing Suicidal Intent in Depressed Population using NLP : A Pilot Study
Depression is a prevalent form of mental disorder that can affect productivity in daily activities and might lead to suicidal thoughts or attempts . Conventional diagnostic techniques performed by mental health professionals can help identify the level of depression present in a person . To facilitate such a diagnostic approach , in this paper , we present an automated conversational platform that was used as a preliminary method of identifying depression associated risks . The platform was developed to understand conversations using Natural Language Processing ( NLP ) via machine learning technique . In the proposed two - phased platform , the initial intent recognition phase would analyze conversation and identify associated sentiments into four categories of ` happy ' , ` neutral ' , ` depressive ' and ` suicidal ' states . In the final emotion nurturing phase , the platform continued with supportive conversations for the first three states while triggering a local call to a suicide prevention helpline for ` suicidal ' state as a preventive measure . This multi - layer platform integrated Google Home mini , Google Dialogflow Machine Learning ( ML ) algorithm and Twilio API . Dialogflow ML obtained classification accuracy of 76 % in recognizing user ' s mental state via NLP and was found efficient over the classic SVM classifier . As a pilot study , current focus of this paper was solely based on the usage of words and intent of the user and was found effective . 

An intelligent searching model based on data integration and its query algorithms
Today , search engine has become a very necessary tool on the Web . But current information search models does not support semantic query . In this paper , we propose a share knowledge system model based on ISMBDI model supported by theories and technologies of semantic Web and ontology . This model takes XML sources as the object of data processing , XML / RDF as the description language of knowledge , and ontology as the design mode of concepts . We then discuss query reconstruction methods and query algorithms in this model . The experiments and analysis show that ISMBDI model can solve the some problems existed in current search method , such as half - baked answer or redundant answer , a great deal of hash , conflicting data pattern and so on . 

Performance of automated scoring for children ' s oral reading
For adult readers , an automated system can produce oral reading fluency ( ORF ) scores ( e . g . , words read correctly per minute ) that are consistent with scores provided by human evaluators ( Balogh et al . , 2005 , and in press ) . Balogh ' s work on NAAL materials used passage - specific data to optimize statistical language models and scoring performance . The current study investigates whether or not an automated system can produce scores for young children ' s reading that are consistent with human scores . A novel aspect of the present study is that text - independent rule - based language models were employed ( Cheng and Townshend , 2009 ) to score reading passages that the system had never seen before . Oral reading performances were collected over cell phones from 1st , 2nd , and 3rd grade children ( n = 95 ) in a classroom environment . Readings were scored 1 ) in situ by teachers in the classroom , 2 ) later by expert scorers , and 3 ) by an automated system . Statistical analyses provide evidence that machine Words Correct scores correlate well with scores provided by teachers and expert scorers , with all ( Pearson ' s correlation coefficient ) r ' s > 0 . 98 at the individual response level , and all r ' s > 0 . 99 at the " test " level ( i . e . , median scores out of 3 ) . 

Automatic NLP - based enrichment of E - learning content for English language learning
The creation of quality content for E - learning resources is a time - consuming task . To simplify the process of content creation for language learning and enable easy adaptability for different requirements and language levels we strive to add as much automation as possible . In order to still obtain high quality , we present in this paper our approaches to enrich E - learning - based English vocabulary tests , which support blended learning and improve direct user feedback . We integrate openly available language resources for selecting and appending usage example sentences for a given vocabulary corpus . Furthermore we discuss our results and suggest to acquire natural language processing ( NLP ) based techniques to improve the generation of language related contents in general and to overcome some of the weaknesses of our current solution . 

Legal Document Classification : An Application to Law Area Prediction of Petitions to Public Prosecution Service
In recent years , there has been an increased interest in the application of Natural Language Processing ( NLP ) to legal documents . The use of convolutional and recurrent neural networks along with word embedding techniques have presented promising results when applied to textual classification problems , such as sentiment analysis and topic segmentation of documents . This paper proposes the use of NLP techniques for textual classification , with the purpose of categorizing the descriptions of the services provided by the Public Prosecutor ' s Office of the State of Paran á to the population in one of the areas of law covered by the institution . Our main goal is to automate the process of assigning petitions to their respective areas of law , with a consequent reduction in costs and time associated with such process while allowing the allocation of human resources to more complex tasks . In this paper , we compare different approaches to word representations in the aforementioned task : including document - term matrices and a few different word embeddings . With regards to the classification models , we evaluated three different families : linear models , boosted trees and neural networks . The best results were obtained with a combination of Word2Vec trained on a domain - specific corpus and a Recurrent Neural Network ( RNN ) architecture ( more specifically , LSTM ) , leading to an accuracy of 90 % and F1 - Score of 85 % in the classification of eighteen categories ( law areas ) . 

Mr . Web : an automated interactive webmaster
This paper describes a system , Mr . Web , designed to interact with users over email to create and update Web pages . Our goal is that users interact with Mr . Web as if it were a human Webmaster . We collected 325 examples of people writing email requests to a Webmaster , and used this to generate the semantics of Mr . Web ' s email parser . The results of the survey indicate that the limited context of a Webmaster gives us a reasonable subset of the natural language processing ( NLP ) problem . This paper explains the system design , user study results , and plans for future work . 

A Cleaning Algorithm for Noiseless Opinion Mining Corpus Construction
This paper presents DyCorC , an extractor and cleaner of web forums contents . Its main points are that the process is entirely automatic , language - independent and adaptable to all kinds of forum architectures . The corpus is built accordingly to user queries using expressions or item keywords as in research engines , and then DyCorC minimizes the boilerplate for further feature - based opinion mining and sentiment analysis , gathering comments and scorings . Such noiseless corpora are usually hand made with the help of crawlers and scrapers , with specific containers devised for each type of forum , entailing lots of work and skills . Our aim is to cut down this preprocessing stage . Our algorithm is compared to state of the art models ( Apache Nutch , BootCat , JusText) , with a gold standard corpus we released . DyCorC offers a better quality of noiseless content extraction . Its algorithm is based on DOM trees with string distances , seven of which have been compared on the reference corpus , and feature - distance has been chosen as the best fit . 

Studying Comments on Russian Patriotic Actions : Sentiment Analysis Using NLP Techniques and ML Approaches
Due to the increasing number of online social media resources and social networks the problem of natural language processing ( NLP ) applying machine learning ( ML ) approaches is becoming an important concept in monitoring non - formalised text data and catching information in real time . Studies in historical memory and appropriate patriotic actions have to be based on detailed people ’ s moods examination . The presented study contains approaches to analyse the polarity of comments on patriotic actions in Russia . Described scheme of NLP text processing and ML classifier were used in the analysis of two regions . There were used standard approaches to convert natural texts into numerical representation . To identify the polarity of the text several models including linear and deep neural networks were examined . Long - short - term - memory network has demonstrated the best result on validation set and was used to the solve the stated problem . 

Achieving magnitude order improvement in Porter stemmer algorithm over multi - core architecture
NLP search takes a long amount of time due to large size of corpus , besides there are too many hits at the server . At present , the strategy to deal with search engines is to have many thousands of servers in order to provide real - time searches . Fast alternatives are therefore sought . In this paper , we present a pioneering work in this direction by taking word stemming , a crucial aspect of search and indexing algorithms and showing how significant performance gain can be accomplished by employing multi - core architectures , which will serve the purpose of home computers in near future . We present our analysis of Porter ' s stemming algorithm on Cell Broadband Engine and describe the manner in which SIMD operations can be utilized to maximize performance . Our results show that cell processors provide performance gains of over 50 times over popular Intel processors and hence possess tremendous potential for NLP - IR applications . 

A RFP System for Generating Response to a Request for Proposal
Responding to Request for Proposal ( RFP ) with comprehensive solutions is central to IT Services business . Typically , an RFP comprises a set of questions spanning across various domain areas . Current industry practices largely rely on Subject Matter Experts to analyze questions and search multiple information sources to come up with the best possible response for a question keeping in view the customer context . With expertise typically in short supply , it becomes increasingly difficult to manage growing RFP volumes . To address this problem we propose a generic solution meta - model and a system that can generate a draft response to an RFP to be augmented manually later . The generation step uses NLP , modelling and search techniques augmented with a knowledge base to generate knowledge search queries to retrieve suitable answers to a set of RFP questions and compose RFP response . In this paper , we share a RFP response generation approach , its implementation , results and lessons learnt from deployment with one of the business units having high volumes of proposal turnover . Initial results show that RFP system is effective in enabling response generation with ~ 76 % mean query precision and 86 % mean query recall . 

Predicting next Word using RNN and LSTM cells : Stastical Language Modeling
Language Modeling is defined as the operation of predicting next word . It is considered as one of the basic tasks of Natural Language Processing ( NLP ) and Language Modeling has several applications . In this research paper , the assorted potentialities for the efficient utilization of language models in structured document retrieval are mentioned . A tree - based generative language model for ranking documents and parts has been used here . Nodes within the tree correspond to different document parts like titles , paragraphs and sections . At every node within the document tree , there ' s a well - defined language model . The language model for a leaf node is predictable directly from the text within the document part related to the node . Inner nodes within the tree are predictable employing a linear interpolation among the various youngster nodes . The paper additionally describes how some common structural queries would be satisfactorily described inside this model . 

Optimal temporal partitioning and synthesis for reconfigurable architectures
We develop a 0 - 1 non - linear programming ( NLP ) model for combined temporal partitioning and high - level synthesis from behavioral specifications destined to be implemented on reconfigurable processors . We present tight linearizations of the NLP model . We present effective variable selection heuristics for a branch and bound solution of the derived linear programming model . We show how tight linearizations combined with good variable selection techniques during branch and bound yield optimal results in relatively short execution times . 

Emotion recognition from blog articles
Suicide of college students has been a universal phenomenon in the world . And the phenomenon has become more and more sever because of the complex and drastic competitions . With the popularization of Internet and the development of information processing technologies , a lot of people have established their own blog Websites to write down their experiences and express their feelings at times . It will be very helpful if the computer can recognize the emotions expressed in blog pages automatically . And then it will be convenient for teachers or psychological consultants to monitor the affective information of college students and take measures for the depression prevention when necessary . Owing to the advances in affective computing and natural language processing , researches have begun to pay more attention to the emotion recognition in NLP all over the world . This paper outlines the approach we have developed to construct a blog emotion - recognizing system . It is based on the lexical contents of words and structural characteristics of blog articles . For the emotion computing of articles , two methods are proposed and the experimental results are compared and analyzed . Finally , the implications of the results are discussed for the future ' s direction of the research . 

Performance comparison of text - based sentiment analysis using recurrent neural network and convolutional neural network
One biggest challenge in sentiment analysis is that it should include Natural Language Processing ( NLP ) , to make the machine understand the human language . With the current development of Artificial Neural Network ( ANN ) , with its implementation , computer can learn to understand human language by such learning mechanism There are many types of ANN and for this research Convolutional Neural Network ( CNN ) and Recurrent Neural Network ( RNN ) were used and compared on their performance . The text data for the sentiment analysis was taken from Stanford publication and transformation from text to vectors were conducted using word2vec . The result shows that RNN is better than CNN . Even the difference of accuracy is not significant with 88 . 35 % ± 0 . 07 for RNN and 87 . 11 % ± 0 . 50 for CNN , the training time for RNN only need 8 . 256 seconds while CNN need 544 . 366 seconds . 

Biomedical Relationship Extraction from Literature Based on Bio - semantic Token Subsequences
Relationship extraction ( RE ) from biomedical literature is an important and challenging problem in both text mining and bioinformatics . Although various approaches have been proposed to extract protein - protein interaction types , their accuracy rates leave a large room for further exploration of more effective methods . In this paper , two supervised learning algorithms based on newly - defined ldquobio - semantic token subsequencerdquo are proposed for multi - class biomedical relationship extraction . The first approach calculates a ldquobio - semantic token subsequence kernelrdquo , while the second one explicitly extracts weighted features from bio - semantic token subsequences . The proposed structure called ldquobio - semantic token subsequencerdquo is able to capture semantic features from natural language sentences for biomedical RE . Two supervised learning algorithms based on the proposed structure outperform the state - of - the - art biomedical RE methods on multi - class protein - protein interaction extraction . 

An Open - Ended Question Self - Explanation Classification Methodology for a Virtual Laboratory Learning System
Scientific experiments are essential for science and technology education . Experiments in laboratory cost materials , require preparations , and sometimes cause hazards . A widely used educational tool with many advantages , e . g . cheap , repeatable , suspendable , and safe , virtual laboratory has gradually become a major experimental tool in most elementary and high schools . In educational science experiments , one major challenge is how to initiate students on scientific inquiry and ensure there are multiple opportunities for their formative self - assessment and revision . The self - explanation strategy has proven effective in deepen students ' understanding of the concepts they are trying to learn . Using self - explanation strategy in educational science experiments might be an effective way to help students think about the observed results of science experiments and build correct scientific concepts . On the other hand , researches point out that using open - ended questions is better than traditional multiple - choice questions for self - explanation strategy . But when using open - ended question self - explanation strategy , without proper prior knowledge and guidance , a student may go wrong in the processes of deduction and result in constructing misconceptions that will become obstacles in further knowledge constructions . Therefore , a learning system that uses open - ended question self - explanation strategy should give proper feedback in order to help students build correct concepts when in self - learning mode . To help students operating in virtual science laboratory and constructing correct concepts from observed results this study constructs an online virtual laboratory learning system with open - ended question self - explanation strategy and proper feedback for natural science course of primary schools . The system uses natural language processing ( NLP ) technology to analyze students ' self - explanation strings , compares the results with coded classification rules , established by an expert from reference explanations , to check the correctness of the strings and possible misconceptions in them , and gives proper learning material , as feedback , for the students to revise possible misconceptions . In the final experiment , the system records and checks all self - explanation strings from 53 students and gives them proper feedback , which reaches an average accuracy of 84 . 45 % after the expert verify the results . 

Enhancing College Chat Bot Assistant with the Help of Richer Human Computer Interaction and Speech Recognition
Chat bots are the expert systems that understands and responds to the query asked by users in their own language . Chat bot responds in conversation just like how a human interact with each other . It works as a virtual assistant and its accuracy is determined by finding correlation between user ' s queries and answers provided by chat bots . Implemented Chat bot provides two modes like text mode and audio mode for better user experience . In audio mode it facilitates an interactive way of answering through voice messages . During Institute ' s Academic Admission procedure there is a huge queue at the enquiry window . Situation is even more difficult for the parents who reside in different cities , states , and countries . The goal of this system is to provide a platform for student and parents to ask queries and clear doubts through simple English language text messages or audio commands . Students and parents will amalgamate with bot instead of making queue at enquiry desk to ask queries related to admission procedure . 

Poster : WallGuard - A Deep Learning Approach for Avoiding Regrettable Posts in Social Media
We develop WallGuard for helping users in online social networks ( OSNs ) avoid regrettable posts and disclosure of sensitive information . Using WallGuard the users can control their posts and can ( i ) detect inappropriate , regrettable messages before they are posted , as well as ( ii ) identify already posted messages that could negatively impact user ' s reputation and life . WallGuard is based on deep learning architectures and NLP based methods . To evaluate the effectiveness of WallGuard , we developed a semi - supervised self - training methodology , which we use to create a new , large - scale corpus for regret detection with 4 , 7 million OSN messages . The corpus is generated by incrementally labelling messages from large OSN platforms relying on human - labelled and machine - labelled messages . Training Facebook ' s FastText word embeddings and Word2vec embeddings on our corpus , we created domain specific word embeddings , we referred to as regret embeddings . Our approach allows us to extract features that are discriminative / intrinsic for regrettable disclosures . Leveraging both regret embeddings and the new corpus , we successfully train and evaluate five new multi - label deep - learning based models for automatically classifying regrettable posts . Our evaluation of the proposed models demonstrate that we can detect messages with regrettable topics , achieving up to 0 , 975 weighted AUC , 82 , 2 % precision and 74 , 6 % recall . WallGuard is free and open - source . 

Compression of Deep Learning Models for NLP
In recent years , the fields of NLP and information retrieval have made tremendous progress thanks to deep learning models like RNNs and LSTMs , and Transformer [ 35 ] based models like BERT [ 9 ] . But these models are humongous in size . Real world applications however demand small model size , low response times and low computational power wattage . We will discuss six different types of methods ( pruning , quantization , knowledge distillation , parameter sharing , matrix decomposition , and other Transformer based methods ) for compression of such models to enable their deployment in real industry NLP projects . Given the critical need of building applications with efficient and small models , and the large amount of recently published work in this area , we believe that this tutorial is very timely . We will organize related work done by the ' deep learning for NLP ' community in the past few years and present it as a coherent story . 

Collocation translation acquisition using monolingual corpora
Collocation translation is important for machine translation and many other NLP tasks . Unlike previous methods using bilingual parallel corpora , this paper presents a new method for acquiring collocation translations by making use of monolingual corpora and linguistic knowledge . First , dependency triples are extracted from Chinese and English corpora with dependency parsers . Then , a dependency triple translation model is estimated using the EM algorithm based on a dependency correspondence assumption . The generated triple translation model is used to extract collocation translations from two monolingual corpora . Experiments show that our approach outperforms the existing monolingual corpus based methods in dependency triple translation and achieves promising results in collocation translation extraction . 

Japanese dependency analysis using fuzzy support vector machines
This paper introduces Fuzzy Support Vector Machines ( FSVMs ) for Japanese dependency analysis . Japanese dependency analysis based on Support Vector Machines ( SVMs ) has been proposed and has achieved high accuracy . While regular SVMs try to find a decision hyperplane from two distinct classes of the input examples , FSVMs apply a fuzzy membership to each input example such that different examples can make different contributions to the decision hyperplane . For nonlinear classification problem , FSVMs can achieve good performance by reducing the effect of outliers . In this paper , a new fuzzy membership function is proposed to Japanese dependency analysis . We train an initial classifier with a small training set . The fuzzy membership is calculated by the distance from each input example to the initial hyperplane . In addition , we employ Nivre ' s algorithm for Japanese dependency analysis since it parses a sentence in linear - time . Experiments using the Kyoto University Corpus show that the parser using Nivre ' s algorithm outperforms the previous systems , and the proposed FSVMs improve the already excellent performance of SVMs for Japanese dependency analysis . 

How well do semantic relatedness measures perform ?: a meta - study
Various semantic relatedness , similarity , and distance measures have been proposed in the past decade and many NLP - applications strongly rely on these semantic measures . Researchers compete for better algorithms and normally only few percentage points seem to suffice in order to prove a new measure outperforms an older one . In this paper we present a meta - study comparing various semantic measures and their correlation with human judgments . We show that the results are rather inconsistent and ask for detailed analyses as well as clarification . We argue that the definition of a shared task might bring us considerably closer to understanding the concept of semantic relatedness . 

Sentiment Analysis on Conversations in Collaborative Active Learning as an Early Predictor of Performance
This full research paper studies affective states in students ' verbal conversations in an introductory Computer Science class ( CS1 ) as they work in teams and discuss course content . Research on the cognitive process suggests that social constructs are an essential part of the learning process . This highlights the importance of teamwork in engineering education . Besides cognitive and social constructs , performance evaluation methods are key components of successful team experience . However , measuring students ' individual performance in low - stake teams is a challenge since the main goal of these teams is social construction of knowledge rather than final artifact production . On the other hand , in low - stake teams the small contribution of teamwork to students ' grade might cause students not to collaborate as expected . We study affective metrics of sentiment and subjectivity in collaborative conversations in low - stake teams to identify the correlation between students ' affective states and their performance in CS1 course . The novelty of this research is its focus on students ' verbal conversations in class and how to identify and operationalize affect as a metric that is related to individual performance . We record students ' conversation during low - stake teamwork in multiple sessions throughout the semester . By applying Natural Language Processing ( NLP ) algorithms , sentiment classes and subjectivity scores are extracted from their speech . The result of this study shows a positive correlation between students ' performance and their positive sentiment as well as the level of subjectivity in speech . The outcome of this research has the potential to serve as a performance predictor in earlier stages of the semester to provide timely feedback to students and enables instructors to make interventions that can lead to student success . 

Abstractive Text Summarizer : A Comparative Study on Dot Product Attention and Cosine Similarity
Text summarization is the process of extracting a subset of the document in such a way that the idea conveyed by the passage is understood while omitting peripheral details which do not have any impact on the passage . The aim of this work is to design an abstractive text summarizer using natural language processing that takes as input a newspaper article and provide a summary on that article in about 100 words . The model is designed using a Sequence to Sequence architecture coupled with an attention mechanism so that the model learns to pay attention to important words rather than trying to remember all of them . The model is trained using a dataset containing newspaper articles and their summaries provided by Kaggle . Pre - trained models such as BERT and T5 are also used to generate summaries and evaluate the performance of the proposed model against the pre - trained models . The three models such as Seq - Seq , BERT and T5 are evaluated on four datasets such as BBC - News - Dataset , Amazon food reviews , News - summary and NewsRoom datasets . Their rouge scores are analysed to select the ideal algorithm for summarization . The attention mechanism is customised to use cosine similarity instead of dot product . Cosine similarity is found to work better in the case of short summaries while dot product is found to work better for long summaries . 

Anomaly Detection in Lexical Definitions via One - Class Classification Techniques
It takes a long time to build vocabularies and their definitions because they must be approved only by the experts in the meeting of building vocabularies and the definitions are also unstructured . To save time , we applied three techniques of classification to the experiments that are one - class SVMs , isolation forests , and local outlier factors , and also observed how well the method can suggest word definition status via the accuracy . As a result , the local outlier factors obtained the highest accuracy when they used vectors that were produced by USE . They can recognize the boundary of the approved class better and there are several approved clusters and outliers are scattered among them . Also , it is found that the detected status of definitions is both identical and opposite to the reference one . For the patterns of definition writing , the approved definitions are always written in the logical order , and start with wide or general information , then is followed by specific details , examples , and references of English terms or examples . In case of the rejected definitions , they are not always written in the logical order , and their definition patterns are also various - only Thai translation , Thai translation with related entries , parts of speech ( POS) , Thai translation , related entries , and English term references followed by definitions , etc . 

SearchEHR : A Family History Search System for Clinical Decision Support
Finding patients with specific clinical conditions , such as having a familial disease history of diabetes , is an important task for clinical decision support . Clinical notes in Electronic Health Records ( EHR ) , which document the patient medical history and familial disease history , are valuable resources for patient cohort selection . However , such information is difficult to discover in clinical text , and full - text search techniques often fail due to the unique characteristics of clinical language . We describe a system - - - SearchEHR - - - that combines Natural Language Processing ( NLP ) and Information Retrieval ( IR ) techniques to facilitate utilising clinical notes to find cohorts of patients , with a special focus on family disease history . 

Swings and Roundabouts : Attention - Structure Interaction Effect in Deep Semantic Matching
In the context of deep learning models for semantic matching problems , we propose a novel Multi - View Progressive Attention ( MV - PA ) mechanism general enough to operate on various linguistic structures of text . More importantly , we study the interaction effect between explicit linguistic structures ( e . g . , linear , constituency , and dependency ) and implicit structures elicited by attention mechanisms . Empirical results on multiple datasets demonstrate salient patterns of substitutability between the two families of structures ( explicit and implicit ) . Our findings not only provide intellectual foundations for the popular use of &# x201C ; linear LSTM + attention &# x201D ; architectures in NLP / QA research , but also have implications in other modalities and domains . 

Extracting phrase - content pairs for Turkish sentences
In this paper we present a framework for extraction of Turkish phrases and their concepts . The objective of the study is meeting the requirement of sources for Turkish Semantic Extractions and represent a Turkish sentence at phrase - concept level . The semantic and grammatical analysis of a sentence is a basic content of Natural Language Processing ( NLP ) which is a branch of Artificial Intelligence ( AI ) . In our study Turkish Phrase - Content Finding system is formed as a source for the other application areas in NLP . This system can be used in Summarization Systems , Information Extraction , Automatic Question Answering System , Semantic Role Labeling , and other semantic application . 

Shallow parsing in Turkish
In this study , shallow parsing is applied on Turkish sentences . These sentences are used to train and test the per - formances of various learning algorithms with various features specified for shallow parsing in Turkish . 

Application of TextRank Algorithm for Credibility Assessment
In this article we examine the use of Text Rank algorithm for identifying web content credibility . Text Rank has come to be a widely applied method for automated text summarization . In our research we apply it to see how well does it fare in recognizing credible statements from a given corpus . So far , research into use of NLP algorithms in credibility assessment was focused more on extracting the most informative statements , or dealing with recognizing the relation between claims within a document . In our paper , we use a collection of 100 websites reviewed by human subjects in regard to their credibility , therefore allowing us to check the algorithm ' s performance in this task . The data collected showed that the Text Rank algorithm can be used for recognizing credibility on the level of aggregated statement credibility . 

ProLiV : a tool for teaching by viewing computational linguistics
ProLiV - Animated Process - modeler of Complex ( Computational ) Linguistic Methods and Theories - is a fully modular , flexible , XML - based stand - alone Java application , used for computer - assisted learning in Natural Language Processing ( NLP ) or Computational Linguistics ( CL ) . Having a flexible and extendible architecture , the system presents the students , by means of text , of visual elements ( such as pictures and animations ) and of interactive parameter set - up , the following topics : Latent Semantics Analysis ( LSA ) , ( computational ) lexicons , question modeling , Hidden - Markov - Models ( HMM ) , and Topic - Focus . These topics are addressed to first - year students in computer science and / or linguistics . 

Annotating attribution in the Penn Discourse TreeBank
An emerging task in text understanding and generation is to categorize information as fact or opinion and to further attribute it to the appropriate source . Corpus annotation schemes aim to encode such distinctions for NLP applications concerned with such tasks , such as information extraction , question answering , summarization , and generation . We describe an annotation scheme for marking the attribution of abstract objects such as propositions , facts and eventualities associated with discourse relations and their arguments annotated in the Penn Discourse TreeBank . The scheme aims to capture the source and degrees of factuality of the abstract objects . Key aspects of the scheme are annotation of the text spans signalling the attribution , and annotation of features recording the source , type , scopal polarity , and determinacy of attribution . 

Identifying emotion topic — An unsupervised hybrid approach with Rhetorical Structure and Heuristic Classifier
This paper describes an unsupervised hybrid approach to identify emotion topic ( s ) from English blog sentences . The baseline system is based on object related dependency relations from parsed constituents . However , the inclusion of the topic related thematic roles present in the verb based syntactic argument structure improves the performance of the baseline system . The argument structures are extracted using VerbNet . The unsupervised hybrid approach consists of two phases ; firstly , the information of Rhetorical Structure ( RS ) is extracted to identify the target span corresponding to the emotional expression from each sentence . Secondly , as an individual target span contains one or more topics corresponding to an emotional expression , a Heuristic Classifier ( HC ) is designed to identify each of the topic spans associated in the target span . The classifier uses the information of Emotion Holder ( EH ) , Named Entities ( NE ) and four types of Similarity features to identify the phrase level components of the topic spans . The system achieves average recall , precision and F - score of 60 . 37 % , 57 . 49 % and 58 . 88 % respectively with respect to all emotion classes on 500 annotated sentences containing single or multiple emotion topics . 

Prediction based Policy setting by finding significance of Attributes from the Ontological Framework in Agricultural domain
In this paper , a methodology has been proposed to tackle the problem of identifying and processing the information conveyed by a huge number of research papers and policy documents , specifically in Agricultural domain . The methodology used in this work includes a series of natural language processing techniques . Pre - processing techniques such as normalizing text to lowercase , stop word removal , tokenization are performed , followed by sentiment analysis for which an algorithm based on the ontological framework has been written . This algorithm works on same concept as the page rank algorithm . Word clouds have been used for analysis . A histogram representing the frequency of occurrence of words in each column is made to identify which domain is being emphasized on by the document . 

Semantic Processing for the Conversion of Unstructured Documents into Structured Information in the Enterprise Context
We present an on - going research project addressing the problem of massive amounts of unstructured data that is generated on a daily basis in most business organisations , regardless of size . Our motivation is to support in particular small and medium seized enterprises to gain a competitive advantage in the market . The goal is to improve their processes for extracting valuable business information from such disorganised data . To achieve this , we introduce a flexible and scalable data analysis framework capable of transforming various types of documents into semantically annotated structures . This includes emails , text files in various formats , slide presentations , blog entries , etc . Additionally , the solution provides a semantic search engine for structured retrieval of the analyzed information and a graphical layer to dynamically visualize the search results as an interactive graph . Throughout the paper , the architecture of two main engines that are responsible for data and text analysis and semantic search are described . We conclude that semantic processing of unstructured sources significantly improves data management and data integration within the enterprises . 

A Framework for Detecting External Plagiarism from Monolingual Documents : Use of Shallow NLP and N - gram Frequency Comparison
The internet has increased the copy - paste scenarios amongst students as well as amongst researchers leading to different levels of plagiarized documents . For this reason , much of research is focused on for detecting plagiarism automatically . In this paper , an initiative is discussed where Natural Language Processing ( NLP ) techniques , as well as supervised machine learning algorithms have been combined to detect plagiarized texts . Here , the major emphasis is on to construct a framework which detects external plagiarism from monolingual texts successfully . For successfully detecting the plagiarism , n - gram frequency comparison approach has been implemented to construct the model framework . The framework is based on 120 characteristics which have been extracted during pre - processing the documents using NLP approach . Afterwards , filter metrics has been applied to select most relevant characteristics and then supervised classification learning algorithm has been used to classify the documents in four levels of plagiarism . Confusion matrix was built to estimate the false positives and false negatives . Our plagiarism framework achieved a very high accuracy score of 89 % with low false positive and false negative rate . 

Proposed Machine Learning Classifier Algorithm for Sentiment Analysis
Text Mining has emerged as an active domain in the field of NLP ( Natural Language Processing ) and due to availability of large data sets of reviews , it has become easy to do sentiment analysis and extract the result from it , but Now - a - days the objectives are expressed in different ways making the data massive and difficult to understand for machines . In this research work , machines are first trained ( Supervised learning ) with the help of the predefined data ( or more clearly reviews ) and then tested with the reviews available . This Research work will show you the working of a system that uses the supervised training which classifies a product review as positive or negative using various classifier algorithms like KNN , Logistic Regression and Support Vector Machines . The model which will give the more accuracy will be considered as the best model . 

Comprehend Medical : A Named Entity Recognition and Relationship Extraction Web Service
Comprehend Medical is a stateless and Health Insurance Portability and Accountability Act ( HIPAA ) eligible Named Entity Recognition ( NER ) and Relationship Extraction ( RE ) service launched under Amazon Web Services ( AWS ) trained using state - of - the - art deep learning models . Contrary to many existing open source tools , Comprehend Medical is scalable and does not require steep learning curve , dependencies , pipeline configurations , or installations . Currently , Comprehend Medical performs NER in five medical categories : Anatomy , Medical Condition , Medications , Protected Health Information ( PHI ) and Treatment , Test and Procedure ( TTP ) . Additionally , the service provides relationship extraction for the detected entities as well as contextual information such as negation and temporality in the form of traits . Comprehend Medical provides two Application Programming Interfaces ( API ) : 1 ) the NERe API which returns all the extracted named entities , their traits and the relationships between them and 2 ) the PHId API which returns just the protected health information contained in the text . Furthermore , Comprehend Medical is accessible through AWS Console , Java and Python Software Development Kit ( SDK ) , making it easier for non - developers and developers to use . 

Chinese Web page classification using noise - tolerant support vector machines
Real - world applications often require the classification of Web documents under the situation of noisy data . Support vector machines ( SVM ) work well for classification applications because of their high generalization ability . But they are very sensitive to noisy training data , which can degrade their classification accuracy . This paper presents a new algorithm to deal with noisy training data , which combines support vector machines and K - nearest neighbor ( KNN ) method . Given a training set , it employs K - nearest neighbor method to remove noisy training examples . Then the remained examples are selected to train SVM classifiers for Web categorization . Empirical results show that this new algorithm has strong tolerance of noise , and it can greatly reduce the influence of noisy data on the SVM classifier . 

Sense - specific lexical information for reading assistance
To support vocabulary acquisition and reading comprehension in a second language , we have developed a system to display sense - appropriate examples to learners for difficult words . We describe the construction of the system , incorporating word sense disambiguation , and an experiment we conducted testing it on a group of 60 learners of English as a second language ( ESL) . We show that sense - specific information in an intelligent reading system helps learners in their vocabulary acquisition , even if the sense information contains some noise from automatic processing . We also show that it helps learners , to some extent , with their reading comprehension . 

Investigating NLP - Based Approaches for Predicting Manual Test Case Failure
System - level manual acceptance testing is one of the most expensive testing activities . In manual testing , typically , a human tester is given an instruction to follow on the software . The results as " passed " or " failed " will be recorded by the tester , according to the instructions . Since this is a labourintensive task , any attempt in reducing the amount of this type of expensive testing is essential , in practice . Unfortunately , most of the existing heuristics for reducing test executions ( e . g . , test selection , prioritization , and reduction ) are either based on source code or specification of the software under test , which are typically not being accessed during manual acceptance testing . In this paper , we propose a test case failure prediction approach for manual testing that can be used as a noncode / specifcation - based heuristic for test selection , prioritization , and reduction . The approach uses basic Information Retrieval ( IR ) methods on the test case descriptions , written in natural language . The IR - based measure is based on the frequency of terms in the manual test scripts . We show that a simple linear regression model using the extracted natural language / IR - based feature together with a typical history - based feature ( previous test execution results ) can accurately predict the test cases ' failure in new releases . We have conducted an extensive empirical study on manual test suites of 41 releases of Mozilla Firefox over three projects ( Mobile , Tablet , Desktop ) . Our comparison of several proposed approaches for predicting failure shows that a ) we can accurately predict the test case failure and b ) the NLP - based feature can improve the prediction models . 

Deep learning for NLP ( without magic ) 
Machine learning is everywhere in today ' s NLP , but by and large machine learning amounts to numerical optimization of weights for human designed representations and features . The goal of deep learning is to explore how computers can take advantage of data to develop features and representations appropriate for complex interpretation tasks . This tutorial aims to cover the basic motivation , ideas , models and learning algorithms in deep learning for natural language processing . Recently , these methods have been shown to perform very well on various NLP tasks such as language modeling , POS tagging , named entity recognition , sentiment analysis and paraphrase detection , among others . The most attractive quality of these techniques is that they can perform well without any external hand - designed resources or time - intensive feature engineering . Despite these advantages , many researchers in NLP are not familiar with these methods . Our focus is on insight and understanding , using graphical illustrations and simple , intuitive derivations . The goal of the tutorial is to make the inner workings of these techniques transparent , intuitive and their results interpretable , rather than black boxes labeled " magic here " . The first part of the tutorial presents the basics of neural networks , neural word vectors , several simple models based on local windows and the math and algorithms of training via backpropagation . In this section applications include language modeling and POS tagging . In the second section we present recursive neural networks which can learn structured tree outputs as well as vector representations for phrases and sentences . We cover both equations as well as applications . We show how training can be achieved by a modified version of the backpropagation algorithm introduced before . These modifications allow the algorithm to work on tree structures . Applications include sentiment analysis and paraphrase detection . We also draw connections to recent work in semantic compositionality in vector spaces . The principle goal , again , is to make these methods appear intuitive and interpretable rather than mathematically confusing . By this point in the tutorial , the audience members should have a clear understanding of how to build a deep learning system for word - , sentence - and document - level tasks . The last part of the tutorial gives a general overview of the different applications of deep learning in NLP , including bag of words models . We will provide a discussion of NLP - oriented issues in modeling , interpretation , representational power , and optimization . 

Racist and Sexist Hate Speech Detection : Literature Review
Hate speech has always existed ; yet , the widespread use of the Internet and social media platforms has led to the exponential rise and spread of hate speech creating a pressing need to make social media platforms a safe place for minority groups , while preserving the freedom of speech . Sexist and racist hate speech are two common forms of hate speech in social media platforms and for which researchers have introduced many detection models . This paper aims to provide a survey of sexist and racist hate speech detection approaches with a focus on three different aspects ; namely , available datasets , features exploited , and machine learning models . 

Multi - agents designed for Web - based cooperative tutoring
Cooperative instruction has always been one of important research focuses in the field of Web - based intelligent tutoring system ( ITS ) . With the rapid advance of distance learning and networking technology , cooperative ITSs have been a rapidly developing area of R & D in AI . Based on multi - agent system ( MAS ) and client - server model , this paper designed multi - agents for Web - based cooperative ITS with autonomous , learning and cooperative abilities , creating a distributed collaborative and interactive e - learning environment . 

Sixth workshop on exploiting semantic annotations in information retrieval ( ESAIR ' 13 ) 
There is an increasing amount of structure on the web as a result of modern web languages , user tagging and annotation , emerging robust NLP tools , and an ever growing volume of linked data . These meaningful , semantic , annotations hold the promise to significantly enhance information access , by enhancing the depth of analysis of today ' s systems . Currently , we have only started exploring the possibilities and only begin to understand how these valuable semantic cues can be put to fruitful use . ESAIR ' 13 focuses on two of the most challenging aspects to address in the coming years . First , there is a need to include the currently emerging knowledge resources ( such as DBpedia , Freebase ) as underlying semantic model giving access to an unprecedented scope and detail of factual information . Second , there is a need to include annotations beyond the topical dimension ( think of sentiment , reading level , prerequisite level , etc ) that contain vital cues for matching the specific needs and profile of the searcher at hand . 

Transformer - based Approaches for Personality Detection using the MBTI Model
Personality Detection is a well - known field in Artificial Intelligence . Similar to Sentiment Analysis , it classifies a text in various labels that denote common patterns according to personality models such as Big - 5 or Myers - Briggs Type Indicator ( MBTI) . Personality detection could be useful for recommendation systems , improvements in health care and counseling , forensics , job screening , to name a few applications . Most of the works on personality detection use traditional machine learning approaches which rely on open dictionaries and tokenizers resulting in low performance and replication issues . In contrast , Deep Learning Transformer models have gained popularity for their high performance . In this research , we propose several Transformer approaches for detecting personality according to the MBTI personality model and compare them to find out the most suitable for this task . In our experiments on the MBTI Kaggle benchmark dataset , we achieved 88 . 63 % in terms of accuracy and 88 . 97 % of F1 - Score which allow us to outperform current state - of - the - art results . 

The FinSim - 2 2021 Shared Task : Learning Semantic Similarities for the Financial Domain
The FinSim - 2 is a second edition of FinSim Shared Task on Learning Semantic Similarities for the Financial Domain , colocated with the FinWeb workshop . FinSim - 2 proposed the challenge to automatically learn effective and precise semantic models for the financial domain . The second edition of the FinSim offered an enriched dataset in terms of volume and quality , and interested in systems which make creative use of relevant resources such as ontologies and lexica , as well as systems which make use of contextual word embeddings such as BERT [ 4] . Going beyond the mere representation of words is a key step to industrial applications that make use of Natural Language Processing ( NLP) . This is typically addressed using either unsupervised corpus - derived representations like word embeddings , which are typically opaque to human understanding but very useful in NLP applications or manually created resources such as taxonomies and ontologies , which typically have low coverage and contain inconsistencies , but provide a deeper understanding of the target domain . Finsim is inspired from previous endeavours in the Semeval community , which organized several competitions on semantic / lexical relation extraction between concepts / words . This year , 18 system runs were submitted by 7 teams and systems were ranked according to 2 metrics , Accuracy and Mean rank . All the systems beat our baseline 1 model by over 15 points and the best systems beat the baseline 2 by over 1 ∼ 3 points in accuracy . 

Context modeling for language and speech generation
It is well known that some of the most important issues in the design of a dialogue system involve the modeling of linguistic context . The present paper highlights a number of these issues , focusing on the language and speech generation components of such systems , and discusses their implications for the way in which context has to be modeled in a spoken dialogue system . We will compare the ' dedicated ' context models that have been proposed in theoretical and computational linguistics with the more general models proposed in artificial intelligence . Our main examples of a ' dedicated ' context model will be the context model of the ' Dial Your Disc ' ( DYD ) music information system ( Collier and Landsbergen , 1995 ) , ( van Deemter and Odijk , 1997 ) and the better - known Discourse Representation Theory ( e . g . ( Kamp and Reyle , 1993 ) ) of which this model is a variant . Our main example of a ' general ' context model is provided by the so - called ' Ist ' formalism ( McCarthy , 1993 ) . 

3DOF ascent phase trajectory optimization for aircraft based on adaptive Gauss Pseudospectral Method
3DOF ascent phase trajectory optimization problem for minimum fuel - to - climb is investigated in the research . The Gauss Pseudospectral Method ( GPM ) and adaptive strategy is proposed to transcribe the trajectory optimization problem into a Nonlinear Program Problem ( NLP ) . Then , the Sequential Quadratic Programming ( SQP ) integrated in a sparse nonlinear program solver named SNOPT is used to solve the resulting NLP problem with a proper initial guess . The optimality of the ascent trajectory is also checked via Bellman ' s principle . The simulation results demonstrate that the method can generate a feasible 3DOF ascent trajectory with all constraints satisfied . 

Concepts extraction in ontology learning using language patterns for better accuracy
The identification of concepts and relations via automatic or semiautomatic are tasks in Ontology Learning . The Ontology Learning is important in minimizing effort of ontology development . It has been used in many disciplines including development of Quran ontology . In the Quran ontology development , there have been efforts to identify concepts and relations for ontology development using various methods . Among the methods employed to discover concepts is a regex pattern . The pattern is based on NLP which use tagging in their rules . This paper proposed a method that used patterns to extract concepts for Hajj Ontology development . It also has been compared against a prominence Ontology Learning system i . e . Text2Onto . The patterns also have been compared with Qterm pattern which is specifically designed for Solah domain in the Quran . Results indicate that the proposed patterns improve the precision with 82 . 4 % and recall with 85 . 7 % as compared to the both approaches . 

Normalization of Numeronyms using NLP Techniques
This paper presents a method to apply Natural Language Processing for normalizing numeronyms to make them understandable by humans . We try to deal with the problem using two approaches , viz . , semi - supervised approach and supervised approach . For the semi - supervised approach , we make use of the state of the art DamerauLevenshtein distance of words . We then apply Cosine Similarity for selection of the normalized text and reach greater accuracy in solving the problem . For the supervised approach , we used a deep learning architecture to solve the problem at hand . Our approach garners accuracy figures of 71 % and 72 % for Bengali and English ( for the semi - supervised approach ) and 89 % for the supervised approach , respectively . 

Nonlinear optimization of Step Duration and Step Location
The modulation of step location and duration plays an important role in realizing robust bipedal walking . This paper formulates it as a nonlinear programming problem ( NLP ) and proposes a novel optimization approach to adjust step location and duration in real time . Based on state feedback , the Linear Inverted Pendulum dynamics is exploited to determine the optimal step parameters . Different from previous works , this work presents three main characteristics : i ) the hyperbolic functions of step duration rather than the step duration itself are chosen to be optimization variables ; ii ) the approach can be switched from baseline two - steps - prediction optimization to one - step - prediction optimization through merely adding several equality constraints in problem formulation ; iii ) the approach can deal with relative step location tracking ( velocity tracking ) or absolute step location tracking ( position tracking ) via changing the reference step parameters . As a result , the first characteristic enables the NLP to be solved in a computational - efficient manner and the latter two endow the approach with versatility under different control modes . The effectiveness has been demonstrated by simulation experiments . 

Contextual Information for Named Entity Recognition in Biomedical Texts
This article presents a study on Named Entities ( NE ) recognition using contextual information present on a Biomedical corpus . Related work indicates that the use of context ( words surrounding a word ) can assist the NE recognition . This work presents experimental results to evaluate the impact of different context settings , using machine learning , for the NE recognition . 

Part of Speech Tagging for Setswana African Language
Part of speech ( POS ) tagging is the technique that assigns appropriate lexical categories to words in a sentence . It is a crucial step in Natural Language Processing ( NLP ) applications such as Machine Translation , Spell and Grammar checking , Word Predictions , Information Retrieval , etc . . A lot of work has been done on POS tagging mainly for European and Asiatic languages , while in Africa , more work is needed mostly due to the lack of the annotated corpus . Some significant works have been done on African languages , such as Arabic , Igbo , Swahili and Yoruba , South African official languages . However , African languages are generally under - resourced , in particular , in terms of lexical semantics annotated corpora , necessary for effective NLP tools and applications . Hence , advances in this direction have been limited . The main aim of the work reported in this paper is the development of a POS tagger model for an under - resourced Setswana African language . A review of some POS taggers for different African languages is conducted , challenges and techniques used in creating the POS taggers are elicited , and a POS tagger model for Setswana language using SVMTool is presented . 

SentiVec : Learning Sentiment - Context Vector via Kernel Optimization Function for Sentiment Analysis
Deep learning - based sentiment analysis ( SA ) methods have drawn more attention in recent years , which calls for more precise word embedding methods . This article proposes SentiVec , a kernel optimization function system for sentiment word embedding , which is based on two phases . The first phase is a supervised learning method , and the second phase consists of two unsupervised updating models , object - word - to - surrounding - words reward model ( O2SR ) and context - to - object - word reward model ( C2OR ) . SentiVec is aimed at : 1 ) integrating the statistical information and sentiment orientation into sentiment word vectors and 2 ) propagating and updating the semantic information to all the word representations in a corpus . Extensive experimental results show that the optimal sentiment vectors successfully extract the features in terms of semantic and sentiment information , which makes it outperform the baseline methods on word similarity , word analogy , and SA tasks . 

Social Robot Interactions for Social Engineering : Opportunities and Open Issues
The field of information security is receiving greater attention in recent years . It is well - known that Social Engineering attacks exploit psychological manipulation techniques to trick people in providing private information without being aware . Robots are entwined in our daily lives and humans tend to trust them more and more . This leads to a kind of " overtrust " in robots that in turn may expose humans to security , fraud and privacy issues . This paper aims to provide insights into how interaction with Social Robots could be exploited for the purpose of Social Engineering . In particular , the paper presents the ability of robots to gather information during an interaction / conversation with humans and how this information could be integrated and enriched with Emotion Recognition techniques . Authors show some preliminary experimental results that are encouraging since the interaction between the robot and the users resembles a humanto - human conversation . In particular , for Emotion Recognition has been obtained two different results : an accuracy of 74 . 38 % fusing two biometric traits ( voice and face ) and a tolerance of 5 bpm to estimate heart - rate . 

Document Summarization and Information Extraction for Generation of Presentation Slides
In this paper , a semi automated technique to generate slide presentations from english text documents is proposed . The technique discussed in this paper is considered to be a pioneering attempt in the field of NLP ( Natural Language Processing ) . The technique involves an information extractor and a slide generator , which combines certain NLP methods such as segmentation , chunking , summarization etc . . , with certain special linguistic features of the text such as the ontology of the words , noun phrases found , semantic links , sentence centrality etc . , In order to aid the language processing task , two tools can be utilized namely , MontyLingua which helps in chunking and Doddle helps in creating an ontology for the input text represented as an OWL ( Ontology Web Language ) file . The process of the technique comprises of extracting text , creating an ontology , identifying important phrases for bullets and generating slides . 

MT on and for the Web
A Systran MT server became available on the minitel network in 1984 , and on Internet in 1994 . Since then we have come to a better understanding of the nature of MT systems by separately analyzing their linguistic , computational , and operational architectures . Also , thanks to the CxAxQ metatheorem , the systems ' inherent limits have been clarified , and design choices can now be made in an informed manner according to the translation situations . MT evaluation has also matured : tools based on reference translations are useful for measuring progress ; those based on subjective judgments for estimating future usage quality ; and task - related objective measures ( such as post - editing distances ) for measuring operational quality . Moreover , the same technological advances that have led to “ Web 2 . 0 ” have brought several futuristic predictions to fruition . Free Web MT services have democratized assimilation MT beyond belief . Speech translation research has given rise to usable systems for restricted tasks running on PDAs or on mobile phones connected to servers . New man - machine interface techniques have made interactive disambiguation usable in large - coverage multimodal MT . Increases in computing power have made statistical methods workable , and have led to the possibility of building low - linguistic - quality but still useful MT systems by machine learning from aligned bilingual corpora ( SMT , EBMT ) . In parallel , progress has been made in developing interlingua - based MT systems , using hybrid methods . Unfortunately , many misconceptions about MT have spread among the public , and even among MT researchers , because of ignorance of the past and present of MT R & D . A compensating factor is the willingness of end users to freely contribute to building essential parts of the linguistic knowledge needed to construct MT systems , whether corpus - related or lexical . Finally , some developments we anticipated fifteen years ago have not yet materialized , such as online writing tools equipped with interactive disambiguation , and as a corollary the possibility of transforming source documents into self - explaining documents ( SEDs ) and of producing corresponding SEDs fully automatically in several target languages . These visions should now be realized , thanks to the evolution of Web programming and multilingual NLP techniques , leading towards a true Semantic Web , “ Web 3 . 0 ” , which will support ubilingual ( ubiquitous multilingual ) computing . 

Embedding Framework for Identifying Ambiguous Words in Code - Mixed Social Media Text
Now a day ' s text on social media contains codeswitched and code - mixed contents . These contents are widely used by people to express their opinions on any topic in the languages known to them . Her code - mixing technique is analyzed to find the words which can be used both in Hindi and in English , having different contexts . This leads to word sense ambiguity problem as one word can have a different meaning when it used in context to other words in a sentence . As Hindi Roman and English language exhibit word sense ambiguity , and resolving this ambiguity is a current research issue using the machine learning model . Here character embedding features are used for the representation of each word written in code - mixed content . The proposed method was developed for identifying context words by classifying the intent for using the ambiguous word in code mixed sentence . A well - known hierarchical LSTM model is used in the paper for context - based sub - word - level ambiguity detection to identify the language of the word . The work on Language Identification in the code - mixed text using character - based embedding for processing ambiguous word is a novel approach and shows promising results . 

Estimation of Oil Content in Oil Palm Fresh Fruit Bunch by Its Surface Color
Oil palm is one of the potential tree crops in Thailand . However , the production of oil palm has been experienced many aspects . Price factor is also one of the problems . Price of oil palm depends on the amount of oil content in the oil palm fruit which are estimated by an expert . The main consideration is the ripeness of the oil palm fresh fruit bunches . An expert determines using its surface color . A different experience of experts leads to a different estimation . The problem may be solved using the chemical analysis methods which more accurate . However , it takes time and uncomfortable . In this research , artificial intelligence ( AI ) will be applied to estimate the oil content in a fresh fruit bunch ( FFB ) . Two popular types of oil palms in Thailand are used in this work . The Nigrescene fruit , color varies from dark purple to red orange depending on its gene and ripeness . The Virescene fruit , color changes from green to orange . The surface color of an oil palm fruit and structure of the bunch were considered as the feature set . An oil palm FFB image from a smartphone camera was fed to the model for predicting the oil content in FFB . Several models such as multi linear regression , artificial neural network and convolution neural network will be observed . The measure of the quality ' s model uses the root mean square error ( RMSE ) . The convolution neural network produces the average of RMSE at 727 for Nigrescene and at 4 . 83 for Virescene . 

Exploring individual differences in student writing with a narrative composition support environment
Novice writers face significant challenges as they learn to master the broad range of skills that contribute to composition . Novice and expert writers differ considerably , and devising effective composition support tools for novice writers requires a clear understanding of the process and products of writing . This paper reports on a study conducted with more than one hundred middle grade students interacting with a narrative composition support environment . The texts are found to pose important challenges for state - of - the - art natural language processing techniques . Furthermore , the study investigates the language usage of middle grade students , the cohesion and coherence of the resulting texts , and the relationship between students ' language arts skills and their writing processes . The findings suggest that composition support environments require robust NLP tools that can account for the variations in students ' writing in order to effectively support each phase of the writing process . 

Natural Language Processing on Diverse Data Layers Through Microservice Architecture
With the rapid growth in Natural Language Processing ( NLP ) , all types of industries find a need for analyzing a massive amount of data . Sentiment analysis is becoming a more exciting area for the businessmen and researchers in Text mining & NLP . This process includes the calculation of various sentiments with the help of text mining . Supplementary to this , the world is connected through Information Technology and , businesses are moving toward the next step of the development to make their system more intelligent . Microservices have fulfilled the need for development platforms which help the developers to use various development tools ( Languages and applications ) efficiently . With the consideration of data analysis for business growth , data security becomes a major concern in front of developers . This paper gives a solution to keep the data secured by providing required access to data scientists without disturbing the base system software . This paper has discussed data storage and exchange policies of microservices through common JavaScript Object Notation ( JSON ) response which performs the sentiment analysis of customer ' s data fetched from various microservices through secured APIs . 

Harnessing the Power of Machine Learning in Dementia Informatics Research : Issues , Opportunities , and Challenges
Dementia is a chronic and degenerative condition affecting millions globally . The care of patients with dementia presents an ever - continuing challenge to healthcare systems in the 21st century . Medical and health sciences have generated unprecedented volumes of data related to health and wellbeing for patients with dementia due to advances in information technology , such as genetics , neuroimaging , cognitive assessment , free texts , routine electronic health records , etc . Making the best use of these diverse and strategic resources will lead to high - quality care of patients with dementia . As such , machine learning becomes a crucial factor in achieving this objective . The aim of this paper is to provide a state - of - the - art review of machine learning methods applied to health informatics for dementia care . We collate and review the existing scientific methodologies and identify the relevant issues and challenges when faced with big health data . Machine learning has demonstrated promising applications to neuroimaging data analysis for dementia care , while relatively less effort has been made to make use of integrated heterogeneous data via advanced machine learning approaches . We further indicate future potential and research directions in applying advanced machine learning , such as deep learning , to dementia informatics . 

Towards an NLP - based log template generation algorithm for system log analysis
System log from network equipment is one of the most important information for network management . Sophisticated log message mining could help in investigating a huge number of log messages for trouble shooting , especially in recent complicated network structure ( e . g . , virtualized networks ) . However , generating log templates ( i . e . , meta format ) from real log messages ( instances ) is still difficult problem in terms of accuracy . In this paper we propose a Natural Language Processing ( NLP ) approach to generate log templates from log messages produced by network equipment in order to overcome this problem . The key idea of the work is to leverage the use of Conditional Random Fields ( CRF ) , a well - studied supervised natural language processing technique . As preliminarily evaluation , with one month network equipment logs in a Japanese academic network , we show that our CRF based algorithm improves the accuracy of generated log templates in reasonable processing time , compared with a traditional method . 

Machine Learning Models for Paraphrase Identification and its Applications on Plagiarism Detection
Paraphrase Identification or Natural Language Sentence Matching ( NLSM ) is one of the important and challenging tasks in Natural Language Processing where the task is to identify if a sentence is a paraphrase of another sentence in a given pair of sentences . Paraphrase of a sentence conveys the same meaning but its structure and the sequence of words varies . It is a challenging task as it is difficult to infer the proper context about a sentence given its short length . Also , coming up with similarity metrics for the inferred context of a pair of sentences is not straightforward as well . Whereas , its applications are numerous . This work explores various machine learning algorithms to model the task and also applies different input encoding scheme . Specifically , we created the models using Logistic Regression , Support Vector Machines , and different architectures of Neural Networks . Among the compared models , as expected , Recurrent Neural Network ( RNN ) is best suited for our paraphrase identification task . Also , we propose that Plagiarism detection is one of the areas where Paraphrase Identification can be effectively implemented . 

LimeSoda : Dataset for Fake News Detection in Healthcare Domain
In this paper , we present our Thai fake news dataset in the healthcare domain , LIMESODA , with the construction guideline . Each document in the dataset is classified as fact , fake , or undefined . Moreover , we also provide token - level annotations for validating classifier decisions . Five high - level annotation tags 1 are 1 ) misleading headline 2 ) imposter 3 ) fabrication 4 ) false connection and 5 ) misleading content . We curate and manually annotated 7 , 191 documents with these tags . We evaluate our dataset with two deep learning approaches ; RNN and Transformer baselines and analyzed token - level contributions to understand model behaviors . For the RNN model , we use the attention weights as token - level contributions . For Transformer models , we use the integrated gradient method at the embedding layers . We finally compared these token - level contributions with human annotations . Although our baseline models yield promising performances , we found that tokens that support model decisions are quite different from human annotation . 

An integrated approach for malicious tweets detection using NLP
Many previous works have focused on detection of malicious user accounts . Detecting spams or spammers on Twitter has become a recent area of research in social network . However , we present a method based on two new aspects : the identification of spam - tweets without knowing previous background of the user ; and the other based on analysis of language for detecting spam on twitter in such topics that are in trending at that time . Trending topics are the topics of discussion that are popular at that time . This growing micro blogging phenomenon therefore benefits spammers . Our work tries to detect spam tweets in based on language tools . We first collected the tweets related to many trending topics , labelling them on the basis of their content which is either malicious or safe . After a labelling process we extracted a many features based on the language models using language as a tool . We also evaluate the performance and classify tweets as spam or not spam . Thus our system can be applied for detecting spam on Twitter , focusing mainly on analysing of tweets instead of the user accounts . 

Concept integration from the caTIES to i2b2 using the UMLS semantic network
A tremendous wealth of valuable information is available in the plain text clinical reports and there are variety types of Natural Language Processing ( NLP ) platforms in place to generate concept codes and mine the reports . The information obtained from the reports has more value if it can be integrated with other clinical and genomics data . The Integrating Biology and the Bedside ( i2b2 ) is being adopted by many institutions . Its open source based scalable framework allows research on genomics and clinical data . In this study , we have shown that any existing information extraction systems can be integrated to i2b2 . In order to address this issue , the UMLS semantic network is adopted to map the concept codes generated by the Cancer Text Information Extraction System ( caTIES ) to i2b2 . With the proposed approach , more than 200 , 000 sample records and 18 , 000 unique concept codes are made accessible and searchable instantly throughout the i2b2 infrastructure . 

Chinese query expansion based on topic - relevant terms
In this paper we present a Chinese query expansion model based on topic - relevant terms which were acquired from the Google search engine automatically . In contrast to earlier methods , our queries are expanded by adding those terms that are most relevant to the concept of the query , rather than selecting terms that are relevant to the query terms . Firstly , we use automatically extracted short terms from document sets to build indexes and use the short terms in both the query and documents to do initial retrieval . Next , we acquire the topic - relevant terms of the short terms from the Internet and the top 30 initial retrieval documents . Finally , we use the topic - relevant terms to do query expansion . The experiments show that our query expansion model is more effective than the standard Rocchio expansion . 

Chatbot Assistant for English as a Second Language Learners
This work demonstrates an experimental implementation of a helper bot using IBM Watson . It is primarily aimed at people who know English as a second language . With the help of IBM Watson Assistant tool , the chatbot uses APIs like Google Translate API , Text to Speech API , SimpleWIki and Musixmatch API , to provide features like rich responses , translation to regional languages , text to speech conversion facilities , useful information in simpler English , and displaying music lyrics for music in regional languages . This is particularly helpful for those who are newly learning English and are more comfortable in their regional language . 

BanglaLM : Data Mining based Bangla Corpus for Language Model Research
Natural language processing ( NLP ) is an area of machine learning that has garnered a lot of attention in recent days due to the revolution in artificial intelligence , robotics , and smart devices . NLP focuses on training machines to understand and analyze various languages , extract meaningful information from those , translate from one language to another , correct grammar , predict the next word , complete a sentence , or even generate a completely new sentence from an existing corpus . A major challenge in NLP lies in training the model for obtaining high prediction accuracy since training needs a vast dataset . For widely used languages like English , there are many datasets available that can be used for NLP tasks like training a model and summarization but for languages like Bengali , which is only spoken primarily in South Asia , there is a dearth of big datasets which can be used to build a robust machine learning model . Therefore , NLP researchers who mainly work with the Bengali language will find an extensive , robust dataset incredibly useful for their NLP tasks involving the Bengali language . With this pressing issue in mind , this research work has prepared a dataset whose content is curated from social media , blogs , newspapers , wiki pages , and other similar resources . The amount of samples in this dataset is 19132010 , and the length varies from 3 to 512 words . This dataset can easily be used to build any unsupervised machine learning model with an aim to performing necessary NLP tasks involving the Bengali language . Also , this research work is releasing two preprocessed version of this dataset that is especially suited for training both core machine learning - based and statistical - based model . As very few attempts have been made in this domain , keeping Bengali language researchers in mind , it is believed that the proposed dataset will significantly contribute to the Bengali machine learning and NLP community . 

Sentic Computing for patient centered applications
Next - generation patients are far from being peripheral to health - care . They are central to understanding the effectiveness and efficiency of services and how they can be improved . Today a lot of patients are used to reviewing local health services on - line but this social information is just stored in natural language text and it is not machine - accessible and machine - processable . To distil knowledge from this extremely unstructured information we use Sentic Computing , a new opinion mining and sentiment analysis paradigm which exploits AI and Semantic Web techniques to better recognize , interpret and process opinions and sentiments in natural language text . In particular , we use a language visualization and analysis system , a novel emotion categorization model , a resource for opinion mining based on a web ontology and novel techniques for finding and defining topic dependent concepts , namely spectral association and CF - IOF weighting respectively . 

TextServer : Cloud - Based Multilingual Natural Language Processing
TextServer is an efficient language analysis platform which offers a variety of robust NLP services for a wide range of languages . Services can be easily accessed via a web interface , where document collections can either be uploaded and sent to batch processing , or processed in real - time via interactive calls to a web - service from any application running on a remote computer or mobile device . The services run on a high - performance - computing cluster ( HPCC) , and a scheduler takes care of having enough instances loaded to satisfy incoming client requests with no delay caused by initializations . TextServer offers a variety of analysis levels , ranging from tokenization or lemmatization to coreference resolution and semantic graph extraction , and aims to become a useful resource for data mining applications working on monolingual or multilingual textual data which require to obtain structured data from unrestricted text . 

Bilingual random walk models for automated grammar correction of ESL author - produced text
We present a novel noisy channel model for correcting text produced by English as a second language ( ESL ) authors . We model the English word choices made by ESL authors as a random walk across an undirected bipartite dictionary graph composed of edges between English words and associated words in an author ' s native language . We present two such models , using cascades of weighted finite - state transducers ( wFSTs ) to model language model priors , random walk - induced noise , and observed sentences , and expectation maximization ( EM ) to learn model parameters after Park and Levy ( 2011 ) . We show that such models can make intelligent word substitutions to improve grammaticality in an unsupervised setting . 

Navigation - orientated natural spoken language understanding for intelligent vehicle dialogue
Voice - based human - machine interfaces are becoming a key feature for next generation intelligent vehicles . For the navigation dialogue systems , it is desired to understand a driver ' s spoken language in a natural way . This study proposes a two - stage framework , which first converts the audio streams into text sentences through Automatic Speech Recognition ( ASR ) , followed by Natural Language Processing ( NLP ) to retrieve the navigation - associated information . The NLP stage is based on a Deep Neural Network ( DNN ) framework , which contains sentence - level sentiment analysis and word / phrase - level context extraction . Experiments are conducted using the CU - Move in - vehicle speech corpus . Results indicate that the DNN architecture is effective for navigation dialog language understanding , whereas the NLP performances are affected by ASR errors . Overall , it is expected that the proposed RNN - based NLP approach , with the corresponding reduced vocabulary designed for navigation - oriented tasks , will benefit the development of advanced intelligent vehicle human - machine interfaces . 

A Review on Social Media Based Profiling Analysis
Social media is one of the many internet services for users of productive age . Major social media such as Facebook , Instagram , and Twitter are having many users in Indonesia . The existence of these applications creates new impacts on social interaction . The behaviour of social media users possible to reflect the character of the user . Some cases related to a person ’ s character often occur starting from what their activities do on social media , for example concerning the relationship between employees and where they work . Human resources play an important role in the success of a company , thus improving the quality of human resources is the main thing . One of the processes in this improvement is by recruiting selective prospective employees . The main goal of the research is doing a literature review to see whether it is possible to use social media activities as one of the factors that can be considered for employee recruitment . This research is focused on surveying the recent journal researches about profiling analysis in social media and then looking further on the methodology , objective and variables regarding personality traits . The result of this study is the social media platforms have big opportunity to be used as one of considering parameters in the employee recruitment process . 

When does pretraining help ?: assessing self - supervised learning for law and the CaseHOLD dataset of 53 , 000 + legal holdings
While self - supervised learning has made rapid advances in natural language processing , it remains unclear when researchers should engage in resource - intensive domain - specific pretraining ( domain pretraining ) . The law , puzzlingly , has yielded few documented instances of substantial gains to domain pretraining in spite of the fact that legal language is widely seen to be unique . We hypothesize that these existing results stem from the fact that existing legal NLP tasks are too easy and fail to meet conditions for when domain pretraining can help . To address this , we first present CaseHOLD ( Case < u > H < /u > oldings < u > O < /u > n < u > L < /u > egal < u > D < /u > ecisions ) , a new dataset comprised of over 53 , 000 + multiple choice questions to identify the relevant holding of a cited case . This dataset presents a fundamental task to lawyers and is both legally meaningful and difficult from an NLP perspective ( F1 of 0 . 4 with a BiLSTM baseline ) . Second , we assess performance gains on CaseHOLD and existing legal NLP datasets . While a Transformer architecture ( BERT ) pretrained on a general corpus ( Google Books and Wikipedia ) improves performance , domain pretraining ( on a corpus of ≈ 3 . 5M decisions across all courts in the U . S . that is larger than BERT ' s ) with a custom legal vocabulary exhibits the most substantial performance gains with CaseHOLD ( gain of 7 . 2 % on F1 , representing a 12 % improvement on BERT ) and consistent performance gains across two other legal tasks . Third , we show that domain pretraining may be warranted when the task exhibits sufficient similarity to the pretraining corpus : the level of performance increase in three legal tasks was directly tied to the domain specificity of the task . Our findings inform when researchers should engage in resource - intensive pretraining and show that Transformer - based architectures , too , learn embeddings suggestive of distinct legal language . 

Nadine - Bot : An Open Domain Migrant Integration Administrative Agent
This paper proposes a conversational agent - chatbot framework that adopts a two - step text similarity approach in order to retrieve relevant answers from large knowledge bases . The proposed " NADINE - bot " is an online question - answer ( QA ) system able to respond to asylum seekers and other vulnerable migrants ’ categories about EU related administrative questions , in their native language . The developed agent combines several state - of - the - art models in order to initially retrieve the most relevant document and then , by splitting it into paragraphs , it seeks for an answer in the most relevant paragraph . The NADINE - bot working knowledge source is a collection of administrative tasks , related frequently asked questions ( FAQs ) concerning EU countries of reception . Nonetheless , for testing purposes the methodology has been applied it to a vast collection of Wikipedia articles , displaying satisfactory results . 

Security Vulnerability Detection Using Deep Learning Natural Language Processing
Detecting security vulnerabilities in software before they are exploited has been a challenging problem for decades . Traditional code analysis methods have been proposed , but are often ineffective and inefficient . In this work , we model software vulnerability detection as a natural language processing ( NLP ) problem with source code treated as texts , and address the auto - mated software venerability detection with recent advanced deep learning NLP models assisted by transfer learning on written English . For training and testing , we have preprocessed the NIST NVD / SARD databases and built a dataset of over 100 , 000 files in C programming language with 123 types of vulnerabilities . The extensive experiments generate the best performance of over 93 % accuracy in detecting security vulnerabilities . 

Signs and Symptoms Tagging for Thai Chief Complaints Based on ICD - 10
This paper presents a natural language processing ( NLP ) approach to construct signs and symptoms corpus in order to identify signs and symptoms recoded in a Thai chief complains ( CCs ) based on the International Statistical Classification of Diseases and Related Health Problems 10th Revision ( ICD - 10 ) form . We define our native language " Thai language " as the natural language in our works thus the challenge is how to apply NLP concept that is originally designed for English language . We start from tokenization to extract Thai token from Thai chief complains , and then the tokens is analyzed in order to assigning a specific tag in terms of ICD - 10 code . 

A Semi - Supervised method for Persian homograph Disambiguation
One of the major challenges in the most natural languages processing ( NLP ) tasks such as machine translation , text to speech and text mining is Word Sense Disambiguation ( WSD ) . Supervised methods are the most common solutions for WSD . However , they need large tagged corpuses which are not available in some languages such as Persian . The Semi - Supervised methods can solve this problem by using small tagged corpus and large untagged corpus . This paper presents a coarse - grained work in WSD that uses tri - training as the semi - supervised method and decision list as supervised classifier for training . The proposed method was evaluated on a corpus . The results show that the proposed method is more precise than the conventional Decision list when the tagged corpus is small . 

Contextual binarization for syntax - based machine translation
In this paper , by relabeling nodes generated during binarization of syntactic trees , contexts can be easily and systematically integrated . This not only helps to restructure syntactic trees to obtain smaller rules , that can be acquired and exploited for translation , also helps to determine which rules are most suitable for translation . By contextual binarization , high - quality translation could be easily generated from the contextual rules , if available ; otherwise the translation just falls back on original syntax - based model without performance loss . Experimental results on the NIST Chinese - to - English corpus show promising improvements , the system applying contextual binarization outperforms over both the original syntax - based system and the original one with right binarization . 

Natural language processing technologies for developing a language learning environment
So far , Computer - Assisted Language Learning ( CALL ) comes in many different flavors . Our research work focuses on developing an integrated e - learning environment that allows improving language skills in specific contexts . Integrated e - learning environment means that it is a Web - based solution that performs language learning tasks using common working environments like , for instance , Web browsers or Email clients . It should be accessible on different platforms , even on mobile devices . Natural Language Processing ( NLP ) forms the technological basis for developing such a learning framework . The paper gives an overview of the state - of - the - art in this area . Therefore , on the one hand , it explains creation processes for NLP resources and gives an overview of corpora . On the other hand , it describes existing NLP standards . Based on our requirements , the paper gives special attention to the evaluation and comparison of toolkits that can suitably support the planned implementation . An outlook at the end points out necessary developments in e - learning to keep in mind . 

Using dependency parsing and probabilistic inference to extract relationships between genes , proteins and malignancies implicit among multiple biomedical research abstracts
We describe BioLiterate , a prototype software system which infers relationships involving relationships between genes , proteins and malignancies from research abstracts , and has initially been tested in the domain of the molecular genetics of oncology . The architecture uses a natural language processing module to extract entities , dependencies and simple semantic relationships from texts , and then feeds these features into a probabilistic reasoning module which combines the semantic relationships extracted by the NLP module to form new semantic relationships . One application of this system is the discovery of relationships that are not contained in any individual abstract but are implicit in the combined knowledge contained in two or more abstracts . 

Cross - Demographic Portability of Deep NLP - Based Depression Models
Deep learning models are rapidly gaining interest for real - world applications in behavioral health . An important gap in current literature is how well such models generalize over different populations . We study Natural Language Processing ( NLP ) based models to explore portability over two different corpora highly mismatched in age . The first and larger corpus contains younger speakers . It is used to train an NLP model to predict depression . When testing on unseen speakers from the same age distribution , this model performs at AUC = 0 . 82 . We then test this model on the second corpus , which comprises seniors from a retirement community . Despite the large demographic differences in the two corpora , we saw only modest degradation in performance for the senior - corpus data , achieving AUC = 0 . 76 . Interestingly , in the senior population , we find AUC = 0 . 81 for the subset of patients whose health state is consistent over time . Implications for demographic portability of speech - based applications are discussed . 

Preprocessing For Crawler Of Short Message Social Media
Social media can be utilized source information in the form of text that is widely exploit as an analytical tool to understand the attitudes , preferences and opinions of society . Companies can use for produce decisions about the needs , attitudes , opinions or trends about customers or potential customers . One of the popular social media now is Twitter . Research aims to design web applications crawl on twitter social media Indonesian language for Natural Language Processing needs . Methodology promote in this study is crawl twitter and preprocessing of data that includes parsing and tokenize , emoticon conversion , cleansing , case folding , normalization , negation handling , stop word and stemming . Research can be further exploited in text processing for classification of analytical sentiments . 

SLAOE - NN : A Deep Network with Structure Learning for Aspect and Opining co - Extraction for NLP
The task of co - extracting aspects and opinion terms is intended to explicitly extract aspect terms that describe entity features and opinion terms that express emotions from user - generated text . An effective way to accomplish this task is to exploit the relationship between aspect terms and opinion terms by parsing the syntax structure of each sentence . However , this method requires a lot of effort to parse and is highly dependent on the quality of the parsing results . In this paper , we present a deep learning model called SLAOE - NN ( Structural Learning and Aspect and Opining Extraction Neural Networks) . The proposed model provides an end - to - end solution and does not require any other language resources for preprocessing . Particularly , we use ON - LSTM to generate hidden layer with language structure information which can generate constituency tree unsupervised and we serve it as an auxiliary task for aspect and opinion terms extraction . For aspect terms and opinion terms extract task , we propose different attention mechanism , which can exploit the indirect relationship between aspect term and corresponding opining term to achieve more accurate information extraction . The experimental results of SemEval ' s three benchmark datasets in 2014 and 2015 show that our model achieves the - state - of - art performance compared to several baselines . 

Knowledge - Based Extraction of Measurement - Entity Relations from German Radiology Reports
A large percentage of relevant radiologic patient information is currently only available in unstructured formats such as free text reports . In particular measurements are important since they are comparable and thus provide insight into the change of the health status over time , for example in response to some treatment . In radiology most of the measurements in reports describe the size of anatomical entities . Even though it is possible to extract measurements and anatomical entities from text using standard information extraction techniques , it is difficult to extract the relation between the measurement and the corresponding anatomical entity . Here we present a knowledge - based approach to extract this relation for size measurements using a model about typical size descriptions of anatomical entities in combination with hierarchical knowledge of existing medical ontologies . We evaluate our approach on two data sets of German radiology reports reaching an F1 - measure of 0 . 85 and 0 . 79 respectively . 

Generating UML Use Case and Activity Diagrams Using NLP Techniques and Heuristics Rules
The process of generating Unified Modeling Language ( UML ) Diagrams from Natural Language ( NL ) requirements is considered a complex and challenging task . Software requirements specification is often written in NL format , which causes potential problems . Requirement ' s analysts analyze and process natural language requirements manually to extract the UML elements . The manual analysis takes a lot of time and effort , which justifies the need for automated support . This paper proposes an approach to facilitate the NL requirements analysis process and UML diagrams extraction from NL textual requirements using natural language processing ( NLP ) techniques and heuristics rules . This approach focuses on generating use - case and activity diagrams . The approach has been applied to a case study and evaluated through an experimental . The evaluation of the approach will be conducted through a comparative study . The experimental results prove that the proposed approach is considerably improved as compared to the other approaches . 

A deep learning approach to machine transliteration
In this paper we present a novel transliteration technique which is based on deep belief networks . Common approaches use finite state machines or other methods similar to conventional machine translation . Instead of using conventional NLP techniques , the approach presented here builds on deep belief networks , a technique which was shown to work well for other machine learning problems . We show that deep belief networks have certain properties which are very interesting for transliteration and possibly also for translation and that a combination with conventional techniques leads to an improvement over both components on an Arabic - English transliteration task . 

A New Concept of Multiple Neural Networks Structure Using Convex Combination
In this article , a new concept of convex - combined multiple neural networks ( NNs ) structure is proposed . This new approach uses the collective information from multiple NNs to train the model . Based on both theoretical and experimental analyses , the new approach is shown to achieve faster training convergence with a similar or even better test accuracy than a conventional NN structure . Two experiments are conducted to demonstrate the performance of our new structure : the first one is a semantic frame parsing task for spoken language understanding ( SLU ) on the Airline Travel Information System ( ATIS ) data set and the other is a handwritten digit recognition task on the Mixed National Institute of Standards and Technology ( MNIST ) data set . We test this new structure using both the recurrent NN and convolutional NNs through these two tasks . The results of both experiments demonstrate a 4 × - 8 × faster training speed with better or similar performance by using this new concept . 

Introduction to the Special Issue on Computational Methods for Biomedical NLP
No abstract available . 

Pragmatic Markers in Russian Spoken Speech : an Experience of Systematization and Annotation for the Improvement of NLP Tasks
Pragmatic markers are an integral part of spontaneous spoken speech , however , they still have no systematic scientific description . These speech elements perform mostly pragmatic functions and are characterized by almost complete absence ( or significant weakening ) of lexical and / or grammatical meaning . The frequency of pragmatic markers in speech exceeds that of almost all content words . Because of that , for the improvement of many current NLP tasks , it is very important to obtain proper systematization of pragmatic markers and to develop effective and reliable schemes for their annotation . In current research , we describe the preliminary set of pragmatic markers categories and present the results of two stages of their pilot annotation made independently by a group of experts . 

The Nursing Profession : Implications for AI and Natural Language Processing
Devices which utilise the assistance of artificial intelligence ( AI ) such as robotic technology are included increasingly in nursing and health environments . In accordance with these developments this paper will highlight issues for natural language processing that arise from the contexts of nursing practice with specific emphasis on the way nurses think and talk about holistic care , nursing language and the challenges of communication in highly unpredictable nursing practice environments . It is noted that the practice and language of caring sciences such as nursing must be understood in terms of their emphasis on holism and therefore on the physical , mental , emotional , psychological , social and spiritual dimensions of communicating with people . This emphasis presents important challenges to researchers of natural language processing as interaction and communication in the caring sciences includes a significant amount of interpersonal insight which is informed in part by sensitivity , feelings and emotion . These subjective elements are part of human relations and this paper will close with a discussion on the potential for qualitative research methodologies to join with quantitative methods to contribute to research design and outcomes . 

COVID - 19 : Data Analysis and the situation Prediction Using Machine Learning Based on Bangladesh perspective
Most of the countries are now affected by COVID19 , COVID - 19 is now the name of the biggest problem in the world . Bangladesh is also affected by COVID - 19 . The whole country is facing this virus as the biggest problem . So try to analyze the data day by day to understand the situation . We also try to use some model , algorithm , logic , analysis to find the solution to this current situation . We are also using some machine learning algorithms to predict the future situation . Machine learning supervised are Linear Regression Model and k - nearest neighbors ( KNN ) Algorithms . There are different types of data sets and algorithms . We have tried to explain these well . 

Leveraging Gene Ontology Annotations to Improve a Memory - Based Language Understanding System
This work evaluates how detailed knowledge about proteins can be leveraged for language understanding and disambiguation by OpenDMAP . OpenDMAP is a memory - based language understanding system that uses patterns to identify concepts in text . These patterns match not only lexical elements , such as words , but also semantic elements , such as references to proteins . This work started with an existing pattern set used to extract biological activation events from a corpus of GeneRIFs ( sentences or phrases that each describe one of many of the functions of a gene) . This is a challenging task because many distinct activation concepts , in addition to being semantically similar , are described using very similar language . We augment the previous approach with additional semantic knowledge about proteins , in the form of associated Gene Ontology annotations , and a small corresponding modification to the ontology used by OpenDMAP . By incorporating additional background knowledge we demonstrate that performance can be significantly improved without modifying the pattern set being used . Specifically precision is improved by 20 % , at a modest 6 % cost to recall . The additional semantic knowledge allows for more specificity in the ontology used by OpenDMAP , which in turn automatically improves the specificity of the patterns being used to extract knowledge from text reducing false positives by 75 % . 

Integrating Linguistic Patterns and Term - Entity Associations in Chinese Person Description Extraction
Person description extraction is an important task in biography generation , question answering and summarization . Most previous extraction approaches select descriptive passages depending on sentence structure and / or word co - occurrence information . In this paper , we focus on Chinese person description extraction verification by measuring the associations between the recognized person entities and the surrounding terms , called Term - Entity associations . The associations are derived from both the semantic knowledge provided in a Chinese well - known thesaurus HowNet and the term distributional information gathered from the news corpus . Relying on Term - Entity associations , the ineligible extracted descriptions could be filtered out so that the higher precision could be achieved in turn . As far as we know , no work on Chinese person description extraction has been reported in the literature . 

Twitter polarity classification with label propagation over lexical links and the follower graph
There is high demand for automated tools that assign polarity to microblog content such as tweets ( Twitter posts ) , but this is challenging due to the terseness and informality of tweets in addition to the wide variety and rapid evolution of language in Twitter . It is thus impractical to use standard supervised machine learning techniques dependent on annotated training examples . We do without such annotations by using label propagation to incorporate labels from a maximum entropy classifier trained on noisy labels and knowledge about word types encoded in a lexicon , in combination with the Twitter follower graph . Results on polarity classification for several datasets show that our label propagation approach rivals a model supervised with in - domain annotated tweets , and it outperforms the noisily supervised classifier it exploits as well as a lexicon - based polarity ratio classifier . 

Extracting domain models from natural - language requirements : approach and industrial evaluation
Domain modeling is an important step in the transition from natural - language requirements to precise specifications . For large systems , building a domain model manually is a laborious task . Several approaches exist to assist engineers with this task , whereby candidate domain model elements are automatically extracted using Natural Language Processing ( NLP ) . Despite the existing work on domain model extraction , important facets remain under - explored : ( 1 ) there is limited empirical evidence about the usefulness of existing extraction rules ( heuristics ) when applied in industrial settings ; ( 2 ) existing extraction rules do not adequately exploit the natural - language dependencies detected by modern NLP technologies ; and ( 3 ) an important class of rules developed by the information retrieval community for information extraction remains unutilized for building domain models . Motivated by addressing the above limitations , we develop a domain model extractor by bringing together existing extraction rules in the software engineering literature , extending these rules with complementary rules from the information retrieval literature , and proposing new rules to better exploit results obtained from modern NLP dependency parsers . We apply our model extractor to four industrial requirements documents , reporting on the frequency of different extraction rules being applied . We conduct an expert study over one of these documents , investigating the accuracy and overall effectiveness of our domain model extractor . 

Joint workshop on bibliometric - enhanced information retrieval and natural language processing for digital libraries ( BIRNDL 2016 ) 
The large scale of scholarly publications poses a challenge for scholars in information - seeking and sensemaking . Bibliometric , information retrieval ( IR) , text mining and NLP techniques could help in these activities , but are not yet widely used in digital libraries . This workshop is intended to stimulate IR researchers and digital library professionals to elaborate on new approaches in natural language processing , information retrieval , scientometric and recommendation techniques which can advance the state - of - the - art in scholarly document understanding , analysis and retrieval at scale . 

Text Imbalance Handling and Classification for Cross - platform Cyber - crime Detection using Deep Learning
Cyberbullying has become a very prevalent issue in recent times . Not just qualifying this as a women issue , starting from politicians to a ten - year - old kid , every person is being bullied on any social platform . It is highly essential to build an artificial intelligence based model that detects the presence of bullying in cross - platform posts . However , textual datasets which are useful for model generation are highly imbalanced in nature . In this paper , we propose two main methods to handle textual data imbalancing which are synonym replacement and artificial data generation using generative adversarial neural networks . We present a systematic analysis of our approaches using a convolutional neural network classifier . Our work shows how removing data imbalance with generative adversarial network techniques before classification improves the overall performance of the model . 

Inheritance in natural language processing
In this introduction to the special issues , we begin by outlining a concrete example that indicates some of the motivations leading to the widespread use of inheritance networks in computational linguistics . This example allows us to illustrate some of the formal choices that have to be made by those who seek network solutions to natural language processing ( NLP ) problems . We provide some pointers into the extensive body of AI knowledge representation publications that have been concerned with the theory of inheritance over the last dozen years or so . We go on to identify the three rather separate traditions that have led to the current work in NLP . We then provide a fairly comprehensive literature survey of the use that computational linguists have made of inheritance networks over the last two decades , organized by reference to levels of linguistic description . In the course of this survey , we draw the reader ' s attention to each of the papers in these issues of Computational Linguistics and set them in the context of related work . 

Automatic Manifesto Comparison using NLP Techniques and The Manifesto Project Domains - Case Study : 2021 Ecuadorian Presidential Elections
Democracies rely on the capability of a country to conduct fair elections . And , fair elections rely on an open participation of candidates and general public . In particular , we explore a way to compare campaign proposals aiding general public to make an informed decision while choosing candidates . This document explores a way to compare campaign proposals through each candidate manifest using natural language processing techniques ( i . e . Doc2Vec algorithm ) . As a linguistic corpus we used all the articles written in Spanish from Wikipedia and we used two models of neural networks , Distributed Bag of Words ( DBOW ) and Distributed Memory Model ( DM ) . We chose the 2021 Ecuadorian Presidential Elections in its second round and tagged each manifesto paragraph ( from the runoff candidates ) into the seven domains according to the Manifesto Project . Finally , we compute manifesto comparisons by topic and also as a whole for different vector configurations . Our results indicate that Doc2Vec produces reasonable results while comparing documents but the DBOW model provides better results while dealing with larger documents and the DM model while dealing with smaller ones . 

Una comparaci ó n de t é cnicas de NLP sem á nticas para analizar casos de uso
The inspection of documents written in natural language with computers has become feasible thanks to the advances in Natural Language Processing ( NLP ) techniques . However , certain applications require a deeper semantic analysis of the text to produce good results . In this article , we present an exploratory study of semantic - aware NLP techniques for discovering latent concerns in use case specifications . For this purpose , we propose two NLP techniques , namely : semantic clustering and semantically - enriched rules . After evaluating these two techniques and comparing them with a technique developed by other researchers , results have showed that semantic NLP techniques hold great potential for detecting candidate concerns . Particularly , if these techniques are properly configured , they can help to reduce the efforts of requirement analysts and promote better quality in software development . 

A Semantic Approach for Traceability Link Recovery in Aerospace Requirements Management System
The efficiency and effectiveness of the recovery of traceability links in requirements management is becoming increasingly important within interdisciplinary industry . Due to the complexity of production development such as automotive , software industry and the aerospace industry , managing requirements is indispensable and challenging . Products in these industries are constantly being updated and modified as the understanding of risks increases with experience and new products are developed in light of such risk . Therefore , the traceability links among the requirements artifacts , which fulfill business objectives , is so critical to reducing the risk and ensuring the success of products . To that end , this paper propose a semantic based traceability link recovery ( STLR ) architecture . According to best of our knowledge this is the first architectural approach that uses DBpedia knowledge - base and Bablenet 2 . 5 multilingual dictionary and semantic network for finding similarity among requirements and the automation of the recovery traceability links using our novel Triple extraction , and Triple disambiguation algorithm . Our preliminary results show the effectiveness in term of precision and recall compared to Vector Space Model and Wu Palmer algorithm . 

Question ranking and selection in tutorial dialogues
A key challenge for dialogue - based intelligent tutoring systems lies in selecting follow - up questions that are not only context relevant but also encourage self - expression and stimulate learning . This paper presents an approach to ranking candidate questions for a given dialogue context and introduces an evaluation framework for this task . We learn to rank using judgments collected from expert human tutors , and we show that adding features derived from a rich , multi - layer dialogue act representation improves system performance over baseline lexical and syntactic features to a level in agreement with the judges . The experimental results highlight the important factors in modeling the questioning process . This work provides a framework for future work in automatic question generation and it represents a step toward the larger goal of directly learning tutorial dialogue policies directly from human examples . 

Anusaaraka : An expert system based machine translation system
Most research in Machine translation is about having the computers completely bear the load of translating one human language into another . This paper looks at the machine translation problem afresh and observes that there is a need to share the load between man and machine , distinguish reliable knowledge from the heuristics , provide a spectrum of outputs to serve different strata of people , and finally make use of existing resources instead of reinventing the wheel . This paper describes a unique approach to develop machine translation system based on the insights of information dynamics from Paninian Grammar Formalism . Anusaaraka is a Language Accessor cum Machine Translation system based on the fundamental premise of sharing the load producing good enough results according to the needs of the reader . The system promises to give faithful representation of the translated text , no loss of information while translating and graceful degradation ( robustness ) in case of failure . The layered output provides an access to all the stages of translation making the whole process transparent . Thus , Anusaaraka differs from the Machine Translation systems in two respects : ( 1 ) its commitment to faithfulness and thereby providing a layer of 100 % faithful output so that a user with some training can “ access the source text ” faithfully . ( 2 ) The system is so designed that a user can contribute to it and participate in improving its quality . Further Anusaaraka provides an eclectic combination of the Apertium architecture with the forward chaining expert system , allowing use of both the deep parser and shallow parser outputs to analyze the SL text . Existing language resources ( parsers , taggers , chunkers ) available under GPL are used instead of rewriting it again . Language data and linguistic rules are independent from the core programme , making it easy for linguists to modify and experiment with different language phenomena to improve the system . Users can become contributors by contributing new word sense disambiguation ( WSD ) rules of the ambiguous words through a web - interface available over internet . The system uses forward chaining of expert system to infer new language facts from the existing language data . It helps to solve the complex behavior of language translation by applying specific knowledge rather than specific technique creating a vast language knowledge base in electronic form . Or in other words , the expert system facilitates the transformation of subject matter expert ' s ( SME ) knowledge available with humans into a computer processable knowledge base . 

Computational Stylometry and Machine Learning for Gender and Age Detection in Cyberbullying Texts
The aim of this paper is to show the importance of Computational Stylometry ( CS ) and Machine Learning ( ML ) support in author ' s gender and age detection in cyberbullying texts . We developed a cyberbullying detection platform and we show the results of performances in terms of Precision , Recall and F - Measure for gender and age detection in cyberbullying texts we collected . 

Fitting document representation to specific datasets by adjusting membership functions
In this work we deal with the problem of web page clustering from the point of view of document representation . Fuzzy ruled - based systems have been successfully used to represent web documents by means of heuristic combinations of criteria . In these systems , rules were established based on the way humans read documents and have been analyzed in previous works . However , membership functions parameters were fixed by default , assuming that any document would follow similar patterns regardless of the rest of documents in the collection . In this work we analyze to what extent collection information could be used to adjust the membership functions in order to improve document representation , and therefore , clustering results . We compare our proposal to the original one in which is based , and to another similar or common approaches . We also perform statistical significance tests to ensure that our modifications have a real effect over the original representation . Results show that adjusting document representation parameters to concrete collections leads to better clustering results . 

Report on the 2nd International Workshop on Recent Trends in News Information Retrieval ( NewsIR ' 18 ) 
The news industry has undergone a revolution in the past decade , with substantial changes continuing to this day . News consumption habits are changing due to the increase in the volume of news and the variety of sources . Readers need new mechanisms to cope with this vast volume of information in order to not only find a signal in the noise , but also to understand what is happening in the world given the multiple points of view describing events . These challenges in journalism relate to Information Retrieval ( IR ) and Natural Language Processing ( NLP ) fields such as : verification of a source ' s reliability ; the integration of news with other sources of information ; real - time processing of both news content and social streams ; de - duplication of stories ; and entity detection and disambiguation . Although IR and NLP have been applied to news for decades , the changing nature of the space requires fresh approaches and a closer collaboration with our colleagues from the journalism environment . Following the success of the previous version of the workshop ( NewsIR ' 16 ) , the goal of this workshop , held in conjunction with ECIR 2018 , is to continue to stimulate such discussion between the communities and to share interesting approaches to solve real user problems . A total number of 19 submissions were received and reviewed , of which 12 were accepted for presentation . In addition to that , we had over 30 registered participants in the workshop who were pleased to attend the two keynote talks given by well - known experts in the field - Edgar Meij ( from industry ) and Peter Tolmie ( from academia ) and oral and poster presentations from the accepted papers . The workshop also included a breakout session to discuss ideas for a future data challenge in news IR and closed with a focused panel discussion to reflect on the day . In summary , several ideas were presented in the workshop on solving complex information needs in the news domain . In addition , the workshop concluded with suggestions of important challenges and shared tasks to work on as a community for News IR . 

A serious game for building a portuguese lexical - semantic network
This paper presents a game with a purpose for the construction of a Portuguese lexical - semantic network . The network creation is implicit , as players collaboratively create links between words while they have fun . We describe the principles and implementation of the platform . As this is an ongoing project , we discuss challenges and long - term goals . We present the current network in terms a quantitative and qualitative analysis , comparing it to other resources . Finally , we describe our target applications . 

A Deep Learning Approach to Outbreak related Tweet Detection
Due to the popularity of social media around the world , people use to report and discuss real - world events , personal health complications , and disaster situations through these platforms . These social media data streams can be used to track and detect different types of outbreaks . A mechanism is needed to identify outbreak - related tweets to predict the outbreak in advance . In this paper , we propose a deep learning model that can detect tweets related to different outbreaks Epidemics , Public Disorders , and Disasters . GloVe ( Global Vectors for Word Representation ) embeddings are used as the feature extraction technique as it can capture the semantic meanings of the tweets . Long Short - term Memory ( LSTM ) which is a specialized Recurrent Neural Network architecture is used as the classification algorithm . In the process , first , outbreak - related tweets were manually collected and curated . Pretrained GloVe word embeddings of 100 dimensions were then used to represent the words of the tweets . As the next step , a Deep Learning Model was trained by using LSTM technique on the curated dataset . Finally , the performance of the model was evaluated using a different dataset . With the results , it can be concluded that the proposed deep learning model is an accurate approach for outbreak - related tweet detection . 

Towards the reuse of lingware systems : a proposed approach with a practical experiment
We are going to present in this document a generic approach for lingware systems reuse . This approach is based on reverse engineering technique in order to wrap up an existing lingware system with web services . This approach permits the reuse of lingware systems regardless of programming languages , development environments and the structures of linguistic resources . In order to preserve the interoperability between the reused lingware systems , the proposed approach performs the unification and the standardization of exchanged linguistic data using the Natural Language Processing ( NLP ) standards and consensus . Doing so , we facilitate the integration and the composition of lingware services in order to create a new application that treats several linguistic levels . In order to consolidate the given approach , we developed the LIngware Reuse Environment ( LIRE ) . A practical experiment was carried out using LIRE environment on an automatic application summary of Arabic texts . 

FedBERT : When Federated Learning Meets Pre - Training
The fast growth of pre - trained models ( PTMs ) has brought natural language processing to a new era , which has become a dominant technique for various natural language processing ( NLP ) applications . Every user can download the weights of PTMs , then fine - tune the weights for a task on the local side . However , the pre - training of a model relies heavily on accessing a large - scale of training data and requires a vast amount of computing resources . These strict requirements make it impossible for any single client to pre - train such a model . In order to grant clients with limited computing capability to participate in pre - training a large model , we propose a new learning approach FedBERT that takes advantage of the federated learning and split learning approaches , resorting to pre - training BERT in a federated way . FedBERT can prevent sharing the raw data information and obtain excellent performance . Extensive experiments on seven GLUE tasks demonstrate that FedBERT can maintain its effectiveness without communicating to the sensitive local data of clients . 

Thesaurus - based efficient example retrieval by generating retrieval queries from similarities
In example - based NLP , the problem of computational cost of example retrieval is severe , since the retrieval time increases in proportion to the number of examples in the database . This paper proposes a novel example retrieval method for avoiding full retrieval of examples . The proposed method has the following three features , 1 ) it generates retrieval queries from similarities , 2 ) efficient example retrieval through the tree structure of a thesaurus , 3 ) binary search along subsumption ordering of retrieval queries . Example retrieval time drastically decreases with the method . 

Dealing with heterogeneous big data when geoparsing historical corpora
It has long been known that ` variety ' is one of the key challenges and opportunities of big data . This is especially true when we consider the variety of content in historical corpora resulting from large - scale digitisation activities . Collections such as Early English Books Online ( EEBO ) and the British Library 19th Century Newspapers are extremely large and heterogeneous data sources containing a variety of content in terms of time , location , topic , style and quality . The range of geographical locations referenced in these corpora poses a difficult challenge for state of the art geoparsing tools . In the context of our work on Spatial Humanities analyses , we present our solution for dealing with the variety and scale of these corpora . 

Automated web development : theme detection and code generation using Mix - NLP
A website helps a business to grow by using different marketing strategies . This paper describes a novel approach to develop a website by just providing the text ( description of the website ) or an image as input . Using Text Input it will suggest template ( screenshots ) after identifying the theme of the site inferred from the input . Those templates are converted into code for further customizations for their personal use . Current problem was that a web developer will take more than 15 days only to just make the basic structure of a website . This issue is resolved by our work which will generate the complete code of the webpage / website in less amount of time . In this paper , it will tokenize each word to find their synonyms and then mapped it with root words for the theme identification and uses deep learning model to convert templates into code . 

NLP @ Desktop : a service oriented architecture for integrating NLP services in desktop clients
Research and development in Natural Language Processing ( NLP ) has made significant progress over the last decade . Many robust NLP systems have been developed for handling machine translation , question - answering , summarization , topic detection , cluster analysis , information extraction , named entity recognition ( NER ) , etc . Despite this advancement in NLP research , the results are still not accessible for common desktop users . In the current scenario , it is difficult to integrate a new NLP tool with the existing text processing applications . To overcome this , we have implemented a service deployer framework based on DBus [ 1 ] for the seamless integration of NLP applications with desktop clients like email clients , word processors , browsers etc . This framework enables NLP researchers to make their products equipped for the desktop clients with minimal efforts . 

Unsupervised Software - Specific Morphological Forms Inference from Informal Discussions
Informal discussions on social platforms ( e . g . , Stack Overflow ) accumulates a large body of programming knowledge in natural language text . Natural language process ( NLP ) techniques can be exploited to harvest this knowledge base for software engineering tasks . To make an effective use of NLP techniques , consistent vocabulary is essential . Unfortunately , the same concepts are often intentionally or accidentally mentioned in many different morphological forms in informal discussions , such as abbreviations , synonyms and misspellings . Existing techniques to deal with such morphological forms are either designed for general English or predominantly rely on domain - specific lexical rules . A thesaurus of software - specific terms and commonly - used morphological forms is desirable for normalizing software engineering text , but very difficult to build manually . In this work , we propose an automatic approach to build such a thesaurus . Our approach identifies software - specific terms by contrasting software - specific and general corpuses , and infers morphological forms of software - specific terms by combining distributed word semantics , domain - specific lexical rules and transformations , and graph analysis of morphological relations . We evaluate the coverage and accuracy of the resulting thesaurus against community - curated lists of software - specific terms , abbreviations and synonyms . We also manually examine the correctness of the identified abbreviations and synonyms in our thesaurus . We demonstrate the usefulness of our thesaurus in a case study of normalizing questions from Stack Overflow and CodeProject . 

Application of Pre - training Models in Named Entity Recognition
Named Entity Recognition ( NER ) is a fundamental Natural Language Processing ( NLP ) task to extract entities from unstructured data . The previous methods for NER were based on machine learning or deep learning . Recently , pre - training models have significantly improved performance on multiple NLP tasks . In this paper , firstly , we introduce the architecture and pre - training tasks of four common pre - training models : BERT , ERNIE , ERNIE2 . 0 - tiny , and RoBERTa . Then , we apply these pre - training models to a NER task by fine - tuning , and compare the effects of the different model architecture and pre - training tasks on the NER task . The experiment results showed that RoBERTa achieved state - of - the - art results on the MSRA - 2006 dataset . 

Braille - 2 automatic interpretation system
In this study the properties of Braille alphabet and 2nd degree Braille rules are touch on . Written texts with the Braille alphabet used to convert written texts with the Latin alphabet are emphasised on image processing steps and language processing steps in the computer environment . For this , Analysis of the Braille characters are done . With 1st degree Braille , syllable abbreviations and a single letter abbreviations , texts reading are carried out for Braille - 2 System . 

Feasibility of Prediction Model for Internal Tumor Target Volume from 4 - D Computed Tomography of Lung cancer
4 - Dimensional computed tomography ( 4DCT ) is the most common technique to determine organ movement due to breathing motion . However , the ability of 4DCT to acquire CT images as a function of the respiratory phase increases higher radiation dose . To reduce the patient ’ s radiation dose , this study created lung motion prediction models used to estimate tumor target movement in ten respiratory phases by detecting only external organ movement during a complete respiration cycle without radiation with Kinect . The average overall amplitude difference between RPM and Kinect signals in the phantom experiment was 0 . 02 ± 0 . 1 mm . F1 score of 100 % for all most all classifications except classification 2 , 3 , 6 , 7 and 8 of 85 % , 83 % , 90 % , 84 % , 85 % where irregular breathing pattern . Essentially , the proposed tumor movement scheme ’ s total accuracy ( average of F1 scores ) is 92 . 7 % . Deep learning model can predict tumor motion range and classification zone by used detection of the external respiratory signalView less

Partnering enhanced - NLP with semantic analysis in support of information extraction
Information extraction using Natural Language Processing ( NLP ) tools focuses on extracting explicitly stated information from textual material . This includes Named Entity Recognition ( NER ) , which produces entities and some of the relationships that may exist among them . Intelligent analysis requires examining the entities in the context of the entire document . While some of the relationships among the recognized entities may be preserved during extraction , the overall context of a document may not be preserved . In order to perform intelligent analysis on the extracted information , we provide an ontology , which describes the domain of the extracted information , in addition to rules that govern the classification and interpretation of added elements . The ontology is at the core of an interactive system that assists analysts with the collection , extraction , organization , analysis and retrieval of information , with the topic of " terrorism financing " as a case study . User interaction provides valuable assistance in assigning meaning to extracted information . The system is designed as a set of tools to provide the user with the flexibility and power to ensure accurate inference . This case study demonstrates the information extraction features as well as the inference power that is supported by the ontology . 

Neu - IR : The SIGIR 2016 Workshop on Neural Information Retrieval
In recent years , deep neural networks have yielded significant performance improvements on speech recognition and computer vision tasks , as well as led to exciting breakthroughs in novel application areas such as automatic voice translation , image captioning , and conversational agents . Despite demonstrating good performance on natural language processing ( NLP ) tasks ( e . g . , language modelling and machine translation , the performance of deep neural networks on information retrieval ( IR ) tasks has had relatively less scrutiny . Recent work in this area has mainly focused on word embeddings and neural models for short text similarity . The lack of many positive results in this area of information retrieval is partially due to the fact that IR tasks such as ranking are fundamentally different from NLP tasks , but also because the IR and neural network communities are only beginning to focus on the application of these techniques to core information retrieval problems . Given that deep learning has made such a big impact , first on speech processing and computer vision and now , increasingly , also on computational linguistics , it seems clear that deep learning will have a major impact on information retrieval and that this is an ideal time for a workshop in this area . Neu - IR ( pronounced " new IR " ) will be a forum for new research relating to deep learning and other neural network based approaches to IR . The purpose is to provide an opportunity for people to present new work and early results , compare notes on neural network toolkits , share best practices , and discuss the main challenges facing this line of research . 

Conglomerate Enhanced Algorithm for Asset Awarding in SDN Enabled Virtual Nexus for Cloud Habitat
Although nexus and server assets are collocated and well maintained by a solitary executive entity , the popular Software - Defined Nexus ( SDN ) hypothesis to sensibly integrate the nexus control plane and automate the layout of creature nexus aspects in Cloud Data Centers employs disjoint control mechanisms for their respective administration ( DCs ) . This paper presents a new hybrid optimization technique for resource allocation ( HOA - RA ) in an SDN enabled essential nexus in an IaaS cloud surroundings to address these problems . The improved pattern search ( IPS ) method which is introduced to the SDN structure in the HOA - RA approach method , which allows the control layer to be transferred from the information broadcast layer to the manage level surface . The ideas of nexus virtualization were then integrated with the Enhanced swallow search ( ESS ) algorithm to split bodily communications in order to accommodate many help providers in finding the nexus . To optimize nexus resources , the biddable retrieve requires an optimal rule - based fusion ( ORBF ) method ; the control plane SDN can be leveraged for systematic virtual network , The HOA - RA concept is employed in the CloudSim habitat . The analysis of the performance shows that the proposed HOA - RA design outshine existing state - of - the - art solution management in terms of organization . 

An improved " fish - search " algorithm for information retrieval
The World - Wide Web ( WWW ) based on Internet services has become a major channel for information delivery . Therefore , a scalable approach to support Internet searching is critical to the success of Internet services . Actually Internet is a directed graph , webpage as node and hyperlink as edge , so the search operation could be abstracted as a process of traversing directed graph . This paper introduces a famous dynamic Web search algorithms , the " fish - search " , and analyzes its merits contrasting to general depth - first algorithm , and points out that the random of search range could lead to repeated search or overlong search time . Based on the analysis of flocking behavior offish school , a parameter " dist " is defined to control the search range and time . This paper presents an improved " fish - search " algorithm , in which the search process could adjust the search range adaptively . Simulation shows some improvements over the original fish - search algorithm . 

An Automatic Approach to Detect Traceability Links Using Fuzzy Logic
Background : The Requirements Trace ability Matrix ( RTM ) is one of the most commonly used ways to represent requirements trace ability . Nevertheless , the difficulty of manually creating such a matrix motivates the investigation into alternatives to generate it automatically . Objective : This article presents one approach to automatically create the RTM based on fuzzy logic , called RTM - Fuzzy , which combines two other approaches , one based on functional requirements ' entry data - called RTM - E - and the other based on natural language processing - called RTM - NLP . Method : To create the RTM based on fuzzy logic , the RTM - E and RTM - NLP approaches were used as entry data for the fuzzy system rules . Aimed at evaluating these approaches , an experimental study was conducted where the RTMs created automatically were compared to the reference RTM ( oracle ) created manually based on stakeholder knowledge . Results : On average the approaches matched the following results in relation to the reference RTM : RTM - E achieved 78 % effectiveness , RTM - NLP 76 % effectiveness and the RTM - Fuzzy 83 % effectiveness . Conclusions : The results show that using fuzzy logic to combine and generate a new RTM offered an enhanced effectiveness for determining the requirement ' s dependencies and consequently the requirement ' s trace ability links . 

A comparative study on Arabic POS tagging using Quran corpus
POS tagging is the process of computationally assigning correct part of speech to each word of a given input text depending on the context . Different POS tagging techniques in the literature have been developed and experimented mostly for English language . Some of the same work has been done for Arabic language . Comparative studies on POS tagging for Arabic language are relatively unexplored . In this paper we compare the performance of some POS tagging techniques for Arabic using Quran corpus . These techniques include N - Gram , Brill , HMM , and TnT taggers . The comparison experiments have been done on diacritized and undiacritized classical Arabic . We tried to see which technique maximizes the performance with our case . 

Towards semi - automatic methods for improving WordNet
WordNet is extensively used as a major lexical resource in NLP . However , its quality is far from perfect , and this alters the results of applications using it . We propose here to complement previous efforts for " cleaning up " the top - level of its taxonomy with semi - automatic methods based on the detection of errors at the lower levels . The methods we propose test the coherence of two sources of knowledge , exploiting ontological principles and semantic constraints . 

Individual vs . Group Violent Threats Classification in Online Discussions
Violent threat is a serious crime affecting the targeted individuals or groups . It is essential for media providers to block the users that post such threats . In this paper , we focused on detection of violent threat language in YouTube comments . We categorized the threatening comments into those targeting an individual or a group . We started from an existing dataset with violent threat language identified , but without any categorization into comments targeting individuals or groups . We adopted a binary classification approach for the prediction of individual - vs . group - targeting threats . We compared two text representations : bag of words ( BOW ) and pre - trained word embedding such as GloVe and fastText . We used deep - learning classifiers such as 1D - CNN , LSTM , and bidirectional LSTM ( BiLSTM ) . GloVe embedding showed the worst results , fastText performed much better , and BiLSTM on BOW with term frequency - inverse document frequency ( TF - IDF ) weighting scheme gave the best results , achieving 0 . 94 % ROC - AUC and Macro - F1 score of 0 . 85 % . 

Mapping Ordinances and Tweets using Smart City Characteristics to Aid Opinion Mining
This research focuses on mining ordinances ( local laws ) and public reactions to them expressed on social media . We place particular emphasis on ordinances and tweets relating to Smart City Characteristics ( SCCs ) , since an important aim of our work is to assess how well a given region heads towards a Smart City . We rely on SCCs as a nexus between a seemingly infinite number of ordinances and tweets to be able to map them , and also to facilitate SCC - based opinion mining later for providing feedback to urban agencies based on public reactions . Common sense knowledge is harnessed in our approach to reflect human judgment in mapping . This paper presents our research in ordinance and tweet mapping with SCCs , including the proposed mapping approach , our initial experiments , related discussion , and future work emerging therein . To the best of our knowledge , ours is among the first works to conduct mining on ordinances and tweets for Smart Cities . This work has a broader impact with a vision to enhance Smart City growth . 

Probabilistic learning models for topic extraction i Thai language
Natural language processing ( NLP ) in Thai language is notoriously complicated . One major problem is the lack of word boundary in a sentence , introducing ambiguity in word tokenization . For topic extraction , semantic ambiguity adds another layer of complexity to the problem . Topic model that disregards word order , such as Latent Dirichlet Allocation ( LDA ) , performs poorly in Thai Language . In this paper , we experimented and tested a probabilistic language model equipped with word location information , the so - called Topic N - grams model ( TNG ) . We deployed several testing tasks to assess TNG ' s capabilities of modeling the generative process of Thai text and established benchmarks that compare the performance of LDA and TNG for various NLP tasks in Thai language . To our knowledge , this paper is the first to explore word - order model in Thai language topic extraction . We concluded that TNG can help boosting performance of Thai language processing in word cutting , semantic checking , word prediction , and document generation task . We also explored how we can measure performance of LDA and TNG on such tasks using perplexity . 

NLP - based Enhancement of Information Security in ITO - A Diffusion of Innovation Theory perspective
Information technology outsourcing ( ITO ) has grown significantly in recent decades and is now over a USD trillion - dollar industry . Service provider organisations are striving to improve the efficiencies of their service deliveries . Natural language processing ( NLP ) provides an opportunity to bring efficiencies through automation in understanding and processing information . Since information security risk management ( ISRM ) in ITO is a growing concern of both , client and service provider organisations , they are adopting to improve ISRM in ITO using NLP . This paper explores those ISRM improvement scenarios . It also investigates the information security risks ( ISRs ) that result from the use of NLP in ITO and proposes strategies to manage those ISRs . To gain insights into the problem , a qualitative research approach is followed using the case study method . Six semi - structured interviews were conducted from participants in three organisations in the ICT industry , engaged in an ITO relationship . To the best of our knowledge , it is the first study to investigate the use of NLP for enhancing ISRM in ITO . 

A Replicable Comparison Study of NER Software : StanfordNLP , NLTK , OpenNLP , SpaCy , Gate
Named Entity Recognition ( NER ) is a key building block of any Natural Language Processing ( NLP ) system , making possible the detection and classification of entities ( e . g . , Person , Location ) in any given text . While a large number of NER software exist today , it remains difficult for NLP and NER practitioners to clearly and objectively identify what software perform ( s ) the best . One of the reasons is the difference in results across the literature and the lack of information needed to be able to fully reproduce the experiment . To overcome this problem , this paper presents a comprehensive and replicable study to assess the performance of NER software , thus laying the groundwork for future benchmarking and meaningful comparison studies . As part of our experiments , the latest version of five well - known NER software were selected , along with two distinct corpora . We observe a discrepancy between the result we get and the result found in the literature being around 50 % in certain cases . We also found that StanfordNLP usually performs the best . 

The Impact of Weighting Schemes and Stemming Process on Topic Modeling of Arabic Long and Short Texts
In this article , first a comprehensive study of the impact of term weighting schemes on the topic modeling performance ( i . e . , LDA and DMM ) on Arabic long and short texts is presented . We investigate six term weighting methods including Word count method ( standard topic models ) , TFIDF , PMI , BDC , CLPB , and CEW . Moreover , we propose a novel combination term weighting scheme , namely , CmTLB . We utilize the mTFIDF that takes into account the missing terms and the number of the documents in which the term appears when calculating the term weight . For further robust term weight , we combine mTFIDF with two weighting methods . We evaluate CmTLB against the studied weighting schemes by the quality of the learned topics ( topic visualization and topic coherence ) , classification , and clustering tasks . We applied weighting schemes to Latent Dirichlet allocation ( LDA ) and Dirichlet multinomial mixture ( DMM ) on eight Arabic long and short document datasets , respectively . The experiment results outline that appropriate weighting schemes can effectively improve topic modeling performance on Arabic texts . More importantly , our proposed CmTLB significantly outperforms the other weighting schemes . Secondly , we investigate whether the Arabic stemming process can improve topic modeling performance . We study the three approaches of Arabic stemming including root - based , stem - based , and statistical approaches . We also train topic models with weighting schemes on documents after applying four stemmers related to different stemming approaches . The results outline that applying the stemming process not only reduces the dimensionality of term - document matrix leading to fast estimation process , but also show enhancement of topic modeling performance both on short and long Arabic documents . Moreover , Farasa stemmer achieves the highest performance in most cases , since it prevents the ambiguity that may happen because of the blind removal of the affixes such as in root - based or stem - based stemmers . 

Linguistic Divergence of Sinhala and Tamil Languages in Machine Translation
This paper presents a study of the lexical - semantic divergence between Sinhala and Tamil languages . Study of divergence is critical as differences in linguistic and extra - linguistic features in languages play pivotal roles in translation . This research the first study of the divergence between Sinhala and Tamil languages and is based on Dorr ' s classification . We propose a computer - assisted divergence study procedure using statistical machine translation , which is easy and gives good performance compared to traditional approaches . Accordingly , this research has the twin aims of revisiting classification of divergence types as outlined by Dorr and outlining some of the new divergence patterns specific to Sinhala and Tamil languages . This study proposes a rule - based algorithm to classify a divergence . 

Optical vortices in nonlinear media : Spirals , forks , and knots
A summary is presented of recent theoretical and experimental results on the dynamics of optical vortices embedded in laser beams propagating in self - focusing nonlinear media . I discuss the spiraling of vortices in nonlocal solitons and elliptic laser beams as well as the pitchfork topological reactions of vortices observed recently in experiments with vortex beams in nematic liquid crystals . Several examples of more exotic structures are presented , such as vortex links and knots , forming spontaneously and under the action of soliton internal modes . 

A multi - agent system using in spatial information sharing on Web - based GIS
Web based geospatial information system ( WebGIS ) has great strengths in the geospatial information community . As the Internet becomes more and more accepted in society as a means to disseminate and gather information , the communication of geographic information over the Web using Web GIS will find its position the evolving medium . The traditional GIS are only used by the special occupation , and now many users want and are able to use the spatial data of GIS on the Web . However , there are two major problems in the Web GIS construct at present , such as the architecture of WebGIS and sharing of spatial information . This paper report related works on the strategy consideration of the multi - agent system using in sharing spatial information and solving architectures successfully based on the WebGIS service . The spatial database adjustment , server composition , XML definition document ( ISO / TC211 and GML ) are easy to materialize via the former methods , but the creation of spatial data converter is needed to devise within a new solution , which might transcend the concept of general GIS or WebGIS engines . Using the peer - to - peer architecture by the multi - agent system , the sharing spatial data is operated directly among the different systems by the requesting from client . The data can be transferred freely in the system . 

Investigation of Viterbi Algorithm Performance on Part - of - Speech Tagger of Natural Language Processing
Many tasks of NLP need to preprocess the words and sentences because it can make the future work convenient . Nowadays , the demand of POS tagging is increasing . POS tagging is an efficient method as preprocessing tagging , even meaningful in text to speech , syntactic analysis and machine translation . When it comes to the POS tagger , they need to know every word POS . It is available for human because it is easy for us to describe it . But when we increase the count of words to the million number , it is not possible to make people do POS tag . In this paper , we introduce Viterbi algorithm to help computer do the better job in tagging lexical categories . Viterbi algorithm is an algorithm that used dynamic programming to solve the POS of sentence . As we know , the word is sensitive about the position of the word . The POS of the word is related about the nearby words . We make simulations about how Viterbi Algorithms work in POS tagger and get the accuracy performance . 

Training conditional random fields using incomplete annotations
We address corpus building situations , where complete annotations to the whole corpus is time consuming and unrealistic . Thus , annotation is done only on crucial part of sentences , or contains unresolved label ambiguities . We propose a parameter estimation method for Conditional Random Fields ( CRFs) , which enables us to use such incomplete annotations . We show promising results of our method as applied to two types of NLP tasks : a domain adaptation task of a Japanese word segmentation using partial annotations , and a part - of - speech tagging task using ambiguous tags in the Penn treebank corpus . 

Discrete light bullet vortices
We discuss the observation of Vortex Light Bullets in Arrays of Waveguides . These are among the most complex spatiotemporal solitary waves observed in an experiment . They are characterized by a complex interplay of angular orbital momentum and nonlinear , spatiotemporal effects . We show that Vortex Light Bullets exhibit a new mode of stability and show that they are robust under realistic experimental conditions . We demonstrate that they can be excited with the help of a discrete phase plate and give proof of their existence using an ultrafast spatiotemporal cross - correlation technique and rigorous numerical analysis . 

Using SVMs with the command relation features to identify negated events in biomedical literature
In this paper we explore the identification of negated molecular events ( e . g . protein binding , gene expressions , regulation , etc . ) in biomedical research abstracts . We construe the problem as a classification task and apply a machine learning ( ML ) approach that uses lexical , syntactic , and semantic features associated with sentences that represent events . Lexical features include negation cues , whereas syntactic features are engineered from constituency parse trees and the command relation between constituents . Semantic features include event type and participants . We also consider a rule - based approach that uses only the command relation . On a test dataset , the ML approach showed significantly better results ( 51 % F - measure ) compared to the command - based rules ( 35 - - 42 % F - measure ) . Training a separate classifier for each event class proved to be useful , as the micro - averaged F - score improved to 63 % ( with 88 % precision ) , demonstrating the potential of task - specific ML approaches to negation detection . 

A Moroccan News Articles Dataset ( MNAD ) For Arabic Text Categorization
In recent years Natural language processing is one of the most active areas of research especially with the emergence of deep learning algorithms . More attention has been given to Latin descendent languages e . g English , French , and Spanish given the availability of high - quality datasets and compute resources . In this paper , we present a moroccan News Articles Corpus collected from four of the major moroccan news websites . The corpus contains more than 418k news articles corresponding to 19 different categories , thus considered to be one of the largest Arabic news articles corpora . A description of the collection and processing steps were presented and exploration analysis was performed . To prove the utility of the dataset . An evaluation step was conducted in the context of text classification using four different Machine Learning baselines : Random Forest ( RF ) , Multinomial Naive Bayes ( MNB ) , Support Vector Machine ( SVC ) , and Gradient Boosting ( GradBoost ) Classifiers . The experimental results are presented in terms of accuracy , F1 - score , and confusion matrix . 

BiLSTM - Autoencoder Architecture for Stance Prediction
The recent surge in the abundance of fake news appearing on social media and news websites poses a potential threat to high - quality journalism . Misinformation hurts people , society , science , and democracy . This reason has led many researchers to develop techniques to identify fake news . In this paper , we discuss a stance prediction technique using the Deep Learning approach , which can be used as a factor to determine the authenticity of news articles . The Fake News Stance Prediction is the process of automatically classifying the stance of a news article towards a target into one of the following classes : Agree , Disagree , Discuss , Unrelated . The stance prediction task ' s input is the news articles containing a pair : a headline as the target and a body as a claim . This paper proposes a deep learning architecture using Bi - directional Long Short Term Memory and Autoencoder for stance prediction . We illustrate , through empirical studies , that the method is reasonably accurate at predicting stance , achieving a classification accuracy as high as 94% . The proposed stance detection method would be useful for assessing the credibility of news articles . 

Identifying multi - word expressions by leveraging morphological and syntactic idiosyncrasy
Multi - word expressions constitute a significant portion of the lexicon of every natural language , and handling them correctly is mandatory for various NLP applications . Yet such entities are notoriously hard to define , and are consequently missing from standard lexicons and dictionaries . Multi - word expressions exhibit idiosyncratic behavior on various levels : orthographic , morphological , syntactic and semantic . In this work we take advantage of the morphological and syntactic idiosyncrasy of Hebrew noun compounds and employ it to extract such expressions from text corpora . We show that relying on linguistic information dramatically improves the accuracy of compound extraction , reducing over one third of the errors compared with the best baseline . 

Optimization of illuminant spectrum for visual detection of foreign substances in jams
Carrying out accurate visual inspection to remove the foreign substances is a crucial issue in food industry . Also in jam factories , the visual inspection by inspectors has been strengthened . However , it is difficult to remove the foreign substances perfectly because they are stained with fruit juice and their color becomes similar to the jam . To overcome the problem , we have proposed the functional illuminant that emphasizes the color difference between jams and foreign substances for the visual inspection . Our previous method for developing the functional illuminant is time consuming to design spectrum . Moreover , this method has low flexibility because it designs an illuminant whose spectrum is produced by a combination of only three types of LEDs , and it has a constraint that the color of the illuminant must be specified before designing . In this study , we propose a new method that formulates the design procedure as an optimization problem solved by NLP ( nonlinear programming) . The proposed method can combine more than three types of LEDs . Furthermore , our method does not require a constraint for the color of the illuminant . As a result , the color difference by the functional illuminant designed by the proposed method is bigger than the previous method . 

Education using Virtual Reality for students with learning disabilities
Visual Education Resources ( VISER ) is a web based Virtual Reality ( VR ) platform which is created to target people suffering from dyslexia and to provide them with an educative , interactive and easily accessible learning platform . This paper mainly discusses the implementation of the proposed system and an insight of how Natural Language Processing ( NLP ) has been used for Automatic Text Visualization ( ATV ) along with Web Virtual Reality ( WebVR ) . The paper further explains how 3D avatars can be incorporated in WebVR along with their animations . 

A vector space analysis of swedish patent claims with different linguistic indices
The purpose of this study was twofold , first to examine if it is possible to use a general automatic retrieval model , the Vector Space Model ( VSM ) , in order to discover similarities between Swedish patent claims ; and second to examine whether an addition morphological decompounding module at the pre - processing level improves the result . In the present study , a comparison between three different topic sets consisting of patent claims was compared against an entire collection of 30 , 117 claims . The VSM was evaluated with and without additional morphological decompounding modules . The results indicate that decompounding will influence the performance of the retrieval model in a positive way . However , the sublanguage of patent claims and the errors made during the Optical Character Recognition ( OCR ) process were harmful towards the overall performance of the Natural Language Processing ( NLP ) applications as well as for the retrieval model . 

An Algorithmic Approach for Generating Behavioral UML Models Using Natural Language Processing
The process of transformation from informal requirements stated in natural language into a formal specification such as Unified Modeling Language ( UML ) is an important challenge . User requirements that are expressed in natural language can be very problematic , which makes the requirements analysis a difficult task . In this paper , we propose a method to analyze the natural language requirements and generate sequence and collaboration diagrams from these requirements , which are commonly used to describe the behavior of software systems . A case study was accomplished to compare the diagrams generated by the proposed approach to the diagrams produced by other approaches . The results showed that the elements of the sequence and collaboration diagrams extracted through our approach are very satisfactory and they would be acceptable as initial analysis models . 

Research on programmed operation of power grid equipment based on Natural Language Processing
With the expanding of State Grid structure , operation instructions logics of power grid are increasingly complex , which makes it even more difficult to compile operation instructions based on the dispatchers ' experiences , consequently may lead to great risks in safe running of the power grid . This paper proposes an artificial intelligence method adopting NLP ( Natural Language Processing ) technology , this method can get the computers understand power grid operation tasks by themselves and automatically generate operation instructions with the deduction method designed by the authors . To reduce the dispatchers ' workload and to improve the accuracy and safety of operations , these instructions need to be checked for safety before and after execution . 

Agent - Based Model Characterization Using Natural Language Processing
This paper reports on Natural Language Processing ( NLP ) as a technique to analyze phenomena towards specifying agent - based models ( ABM ) . The objective of the ABM NLP Analyzer is to facilitate non - simulationists to actively engage in the learning and collaborative designing of ABMs . The NLP model identifies candidate agents , candidate agent attributes , and candidate rules all of which non - simulationists can later evaluate for feasibility . IBM ' s Watson Natural Language Understanding ( NLU ) and Knowledge Studio were used in order to annotate , evaluate , extract agents , agent attributes , and agent rules from unstructured descriptions of phenomena . The software , and related agent - attribute - rule characterization , provides insight into a simple but useful means of conceptualizing and specifying baseline ABMs . Further , it emphasizes on how to approach the design of ABMs without the use of NLP by focusing on the identification of agent , attributes and rules . 

Identification of Food Quality Descriptors in Customer Chat Conversations using Named Entity Recognition
Chatbots are increasingly being used for providing customer support . One of the fundamental challenges for a bot , or for that matter any human agent , is to understand the context of a customer message . Chat conversations are typically associated with agrammatical structure , spelling mistake / variants , informal and slang words , and code - mixing , i . e . , the use of words from more than one language . We focused on a use case related to the conversations between customers and agents regarding issues with the quality of food delivered by an online food delivery company . Accurate identification of words that describe the poor quality of food can immensely benefit prompt resolution of the issue and also provide vital feedback to the company and its partner restaurants . This feature can be used in a chatbot to effectively resolve customers ’ food quality related concerns . This paper presents a named entity recognition ( NER ) approach to identify the food quality descriptors in a given message . On an internal benchmark dataset , we achieved an F1 score of 0 . 93 while outperforming classical baseline approaches in NER . 

Fuzzy information retrieval based on ontology generated by using concept of fuzzy - valued variable
Retrieval of relevant document from a huge set of data is a crucial task particularly in an age when world wide web literally crisscrosses the world . The task becomes all the more difficult if the process involves recovery of fuzzy data . Then , not only does the text comparison between query and document become sufficient , but at the same time concept matching also assumes importance . Our primary aim in this paper is to work out a data extracting system that can handle queries and documents involving fuzzy concepts along with crisp concepts . Here , the keywords from query and document are matched keeping an eye on the meaning of the words . Moreover , the ontology used here is created by treating fuzzy linguistic terms as variables called fuzzy - valued variables , a completely new way of dealing with fuzzy concepts . In our research , we have considered a linguistic variable ( a fuzzy set ) as a fuzzy - valued variable . We have taken a layered approach and given emphasis on modularity . We take the concerned linguistic variable to sit at the root of a tree with the semantically similar terms situated at the end of the branches of the tree and through this process the meaning of the term is reflected . 

Sentiment Analysis of Lockdown in India During COVID - 19 : A Case Study on Twitter
With the rapid increase in the use of the Internet , sentiment analysis has become one of the most popular fields of natural language processing ( NLP ) . Using sentiment analysis , the implied emotion in the text can be mined effectively for different occasions . People are using social media to receive and communicate different types of information on a massive scale during COVID - 19 outburst . Mining such content to evaluate people ' s sentiments can play a critical role in making decisions to keep the situation under control . The objective of this study is to mine the sentiments of Indian citizens regarding the nationwide lockdown enforced by the Indian government to reduce the rate of spreading of Coronavirus . In this work , the sentiment analysis of tweets posted by Indian citizens has been performed using NLP and machine learning classifiers . From April 5 , 2020 to April 17 , 2020 , a total of 12 741 tweets having the keywords “ Indialockdown ” are extracted . Data have been extracted from Twitter using Tweepy API , annotated using TextBlob and VADER lexicons , and preprocessed using the natural language tool kit provided by the Python . Eight different classifiers have been used to classify the data . The experiment achieved the highest accuracy of 84 . 4 % with LinearSVC classifier and unigrams . This study concludes that the majority of Indian citizens are supporting the decision of the lockdown implemented by the Indian government during corona outburst . 

Text Classification Using Word Embeddings
Growth in the internet made immense impact on the data generation . Most of the data in the world is in textual format . There is need to access and use this data efficiently and easily hence , text classification is widely studied problem in research community . The applications of text classification are also in diverse domains such as news filtering , opinion mining , information retrieval and so on . At core of these applications is machine learning algorithms which need input of fixed length as a vector . Bag - of - Words is popular model used to represent text in numeric vector format . But it has several disadvantages such as word order is ignored , high dimensionality and sparse representation if vocabulary is large . Word embeddings are distributed vector representation of words . These representation inherit semantic notion of similarity also they have shown state - of - art results in many core natural language processing tasks . In presented work , we present text classification using these word embeddings and measure the performance . 

Learning User Profile in the Personalization News Service
The model for precise description and measurement of user profile was studied through adopting the technology of knowledge discovery from database . The news browsing manner of users was analyzed , while news similarity was computed based on users ' reading style , then the model for user profile was generated by the mining algorithm . The experiments showed that this model worked very well in describing user profile and promoted efficiency of personalized news service system . 

Combining Word Order and CNN - LSTM for Sentence Sentiment Classification
Neural network models have been demonstrated to be capable of achieving state - of - the - art performance in sentence sentiment classification . Convolutional Neural Networks ( CNNs ) and Recurrent Neural Networks ( RNNs ) are two widely used neural network models for NLP . However , since sentences consist of the same words in different order may represent different meaning in sentiment , it cannot be neglected that the word embedding training model ignores the factor of word order in sentence to quicken the training process . In this work , we mainly consider that word order is important for sentence sentiment classification , designing an encode - decode model called CNN - LSTM combined the strength of CNN and LSTM to demonstrate that word order of sentence plays an important role in sentiment analysis based on the word embedding which is designed as an order _ w2v model taking in word order during word2vec training process . We evaluate the CNN - LSTM and order _ w2v in sentiment classification both on Chinese and English datasets . The experimental results verify that the model considering the word order can achieve better results in sentiment analysis . 

Non - English response detection method for automated proficiency scoring system
This paper presents a method for identifying non - English speech , with the aim of supporting an automated speech proficiency scoring system for non - native speakers . The method uses a popular technique from the language identification domain , a single phone recognizer followed by multiple language - dependent language models . This method determines the language of a speech sample based on the phonotactic differences among languages . The method is intended for use with non - native English speakers . Therefore , the method must be able to distinguish non - English responses from non - native speakers ' English responses . This makes the task more challenging , as the frequent pronunciation errors of non - native speakers may weaken the phonetic and phonotactic distinction between English responses and non - English responses . In order to address this issue , the speaking rate measure was used to complement the language identification based features in the model . The accuracy of the method was 98 % , and there was 45 % relative error reduction over a system based on the conventional language identification technique . The model using both feature sets furthermore demonstrated an improvement in accuracy for speakers at all English proficiency levels . 

Streaming for large scale NLP : language modeling
In this paper , we explore a streaming algorithm paradigm to handle large amounts of data for NLP problems . We present an efficient low - memory method for constructing high - order approximate n - gram frequency counts . The method is based on a deterministic streaming algorithm which efficiently computes approximate frequency counts over a stream of data while employing a small memory footprint . We show that this method easily scales to billion - word monolingual corpora using a conventional ( 8 GB RAM ) desktop machine . Statistical machine translation experimental results corroborate that the resulting high - n approximate small language model is as effective as models obtained from other count pruning methods . 

Language identification at word level in Sinhala - English code - mixed social media text
Automatic analyzing and extracting useful information from the noisy social media content are currently getting attention from the research community . It is common to find people easily mixing their native language along with the English language to express their thoughts in social media , using Unicode characters or the Unicode characters written in Roman Scripts . Thus these types of noisy code - mixed text are characterized by a high percentage of spelling mistakes with phonetic typing , wordplay , creative spelling , abbreviations , Meta tags , and so on . Identification of languages at word level become a necessary part for analyzing the noisy content in social media . It would be used as an intimidate language identifier for chatbot application by using the native languages . For this study we used Sinhala - English code - mixed text from social media . Natural Language Processing ( NLP ) and Machine Learning ( ML ) technologies are used to identify the language tags at the word level . A novel approach proposed for this system implemented is machine learning classifier based on features such as Sinhala Unicode characters written in Roman scripts , dictionaries , and term frequency . Different machine learning classifiers such as Support Vector Machines ( SVM ) , Naive Bayes , Logistic Regression , Random Forest and Decision Trees were used in the evaluation process . Among them , the highest accuracy of 90 . 5 % was obtained when using Random Forest classifier . 

NLP for stress mitigation in employees
Stress management in the workplace is a reality that most of us have to face for one reason or another and coping with it is the key to a long - term career success . Some careers are more stressful than others . The main causes of workplace stress appear to be linked to deficiencies in the management and organization of work . Stress and culture are not only predominantly seen in the industry but are visual even in the service sector . This study directs itself to the study of organization culture and stress in an educational institute . It is normally presumed that the teaching profession is less stressful . With the changing demands in the educational paradigm and expectations from the stakeholders , the study and the findings thereof presented in this paper have significant importance in redefining the premises of the education sector from knowledge management perspective . Having observed that employees do demonstrate stress at different strata of hierarchy in an organization for different Organizational Role Stress ( ORS ) parameters , an effort is made in this work for stress mitigation using Neuro - Linguistic Programming ( NLP ) intervention in counseling employees . NLP describes the fundamental dynamics between mind ( neuro ) and language ( linguistic ) and how their interplay affects our body and behavior ( programming ) . Techniques of NLP such as Rapport , Anchoring , Swish , Reframing Inter - personal Communication and Persuasion are very useful towards stress management . In this paper , two groups of employees are counseled with and with out NLP intervention . Statistical analysis has been carried out to compare the efficacy of NLP intervention and it is observed that NLP intervention do mitigate employee ' s stress . 

An OCR Post - correction Approach using Deep Learning for Processing Medical Reports
According to a recent Deloitte study , the COVID - 19 pandemic continues to place a huge strain on the global health care sector . Covid - 19 has also catalysed digital transformation across the sector for improving operational efficiencies . As a result , the amount of digitally stored patient data such as discharge letters , scan images , test results or free text entries by doctors has grown significantly . In 2020 , 2314 exabytes of medical data was generated globally . This medical data does not conform to a generic structure and is mostly in the form of unstructured digitally generated or scanned paper documents stored as part of a patient ’ s medical reports . This unstructured data is digitised using Optical Character Recognition ( OCR ) process . A key challenge here is that the accuracy of the OCR process varies due to the inability of current OCR engines to correctly transcribe scanned or handwritten documents in which text may be skewed , obscured or illegible . This is compounded by the fact that processed text is comprised of specific medical terminologies that do not necessarily form part of general language lexicons . The proposed work uses a deep neural network based self - supervised pre - training technique : Robustly Optimized Bidirectional Encoder Representations from Transformers ( RoBERTa ) that can learn to predict hidden ( masked ) sections of text to fill in the gaps of non - transcribable parts of the documents being processed . Evaluating the proposed method on domain - specific datasets which include real medical documents , shows a significantly reduced word error rate demonstrating the effectiveness of the approach . 

Use of Social Media and Smartwatch Data Analytics for Mental Health Diagnosis
In today ' s world , mental illness has become a common problem in several individuals life still , the diagnosis of mental disorders relies on traditional methods of testing by interpreting the patient just by psychometric test and by not taking into consideration various other factors . Whilst the number of social media users and smartwatch users have increased rapidly , software applications that interpret psychological data for health - related decisions have not followed a similar trend . The main motive behind this study is to examine the sentiments of the user ' s social media data and detect unusual patterns by smartwatch data analytics to help mental health workers in decision making . The paper proposes the methodology to capture and analyze real - time data of smartwatch and social media . 

Detecting and Correlating Video - Based Event Patterns : An Ontology Driven Approach
With increasing amount of information ( video , text ) being available today , it has become non - trivial to develop techniques to categorize documents into contextually meaningful classes . The information as available in the documents is composed of sequence of events termed as patterns . It is evident to know the important trends as observed from patterns that are emerging over a specific time period and space . For identifying the patterns , we must focus on semantic meaning of documents . Tracing such patterns in videos or texts manually is a time - consuming , cumbersome or an impossible task . So , in this paper we have devised an unsupervised trend discovery approach that detects and correlates event patterns from videos temporally as well as spatially . We begin by building our own document collection on the basis of contextual meaning of documents . This helps in associating an input video with another video or text documents on the basis of their semantic meaning . This approach helps in accumulating variety of information that is scattered over the web thus providing relatively complete information about the video . The highly correlated words are grouped in a topic using Latent Dirichlet Allocation ( LDA ) . To identify topics an E - MOWL based ontology is used . This event ontology helps in discovering associations and relations between the various events . With this kind of representation , the users can infer different concepts as emerged over time . For identifying the various spatial patterns that exist corresponding to an event in a document , we have utilized geographic ontology ( Geoontology ) . We establish validity of our approach using experimental results . 

 “ Can NLP techniques be utilized as a reliable tool for medical science ?” - Building a NLP Framework to Classify Medical Reports
Artificial intelligence persists on being a right - hand tool for many branches of biology . From preliminary advices and treatments , such as understanding if symptoms related to fever or cold , to critical detection of cancerous cell or classification of X - rays , traditional machine learning and deep learning techniques achieved remarkable feats . However , total dependency on machine - based prediction is yet a far fetched concept . In this paper , we provide a framework utilizing several Natural Language Processing ( NLP ) algorithms to construct a comparative analysis . We create an ensemble of top - performing algorithms to accomplish classification task on medical reports . We compare both the traditional machine learning and deep learning techniques and evaluate their probabilities of being reliable on analyzing medical diagnosis . We concluded that an ensemble approach can provide reliable outcomes with accuracy over 92 % and that the current state of the art is unequipped to provide the result with the standard needed for health sectors but an ensemble of these techniques can be a pathway for future research direction . 

splitSVM : fast , space - efficient , non - heuristic , polynomial kernel computation for NLP applications
We present a fast , space efficient and non - heuristic method for calculating the decision function of polynomial kernel classifiers for NLP applications . We apply the method to the MaltParser system , resulting in a Java parser that parses over 50 sentences per second on modest hardware without loss of accuracy ( a 30 time speedup over existing methods) . The method implementation is available as the open - source splitSVM Java library . 

Mevzuat Verisetinde Soru Cevaplama Uygulamasi Question Answering Application on Legalisation Dataset
Question Answering is a widely studied sub - field of Natural Language Processing ( NLP ) . It studies information retrieval techniques that locate the answer in a corpus for a given query . Recently , deep learning techniques are widely employed in this field . This work uses a transfer learning method on Turkish Tax legislation documents . Experts in Tax - Law domain created 355 question - answer pairs in SQuAD 1 . 1 ( Stanford Question Answering Dataset ) format using law documents in UYAP ( National Judiciary Informatics System ) . BERT ( Bidirectional Encoder Representations from Transformers ) contextual word embedding vectors are used to create a representation that can capture different meanings in word representations . Using both these embeddings and the model obtained from SQuAD 1 . 1 dataset , a system was deployed . Also , using the failing answers retrieved from the application of this model , a SQuAD 2 . 0 dataset were created that includes impossible - to - answer questions . New models were obtained by training with this dataset . Our observation is that the most successful model of SQuAD 2 . 0 dataset outperforms that of SQuAD 1 . 1 by 11 % in exact matching measure and by 5 % in F1 . 

Review of Reactive Power Planning : Objectives , Constraints , and Algorithms
The key of reactive power planning ( RPP ) , or Var planning , is the optimal allocation of reactive power sources considering location and size . Traditionally , the locations for placing new Var sources were either simply estimated or directly assumed . Recent research works have presented some rigorous optimization - based methods in RPP . This paper will first review various objectives of RPP . The objectives may consider many cost functions such as variable Var cost , fixed Var cost , real power losses , and fuel cost . Also considered may be the deviation of a given voltage schedule , voltage stability margin , or even a combination of different objectives as a multi - objective model . Secondly , different constraints in RPP are discussed . These different constraints are the key of various optimization models , identified as optimal power flow ( OPF ) model , security - constrained OPF ( SCOPF ) model , and SCOPF with voltage - stability consideration . Thirdly , the optimization - based models will be categorized as conventional algorithms , intelligent searches , and fuzzy set applications . The conventional algorithms include linear programming , nonlinear programming , mixed - integer nonlinear programming , etc . The intelligent searches include simulated annealing , evolutionary algorithms , and tabu search . The fuzzy set applications in RPP address the uncertainties in objectives and constraints . Finally , this paper will conclude the discussion with a summary matrix for different objectives , models , and algorithms . 

Towards building advanced natural language applications : an overview of the existing primary resources and applications in Nepali
The paper gives an overview of some of the major primary resources and applications developed in the field of Natural Language Processing ( NLP ) for the Nepali language and their prospective for building advanced NLP applications . The paper also sheds light on the approaches followed by the current applications and their coverage as well as limitations . 

A fast retrieval algorithm for the earth mover ' s distance using EMD lower bounds and the priority queue
Earth mover ' s distance ( EMD ) is a distance measure between two distributions , and has been widely used in multimedia information retrieval systems , especially content - based image retrieval systems . When the EMD is applied to image problems based on color or texture , the EMD reflects the human perceptual similarities . Its computations , however , is too expensive to use in large - scale databases . In order to achieve the efficient computation of the EMD during query processing , we have developed ldquofastEMDrdquo , a library for high - speed feature - based similarity retrievals in large databases . This paper introduces techniques that are used in the implementation of the fastEMD and demonstrates the efficiency in extensive experiments . 

Survey and Gap Analysis of Word Sense Disambiguation Approaches on Unstructured Texts
Word Sense Disambiguation ( WSD ) is considered as one of the pivotal problems of Semantic classification among polysemous words that can be addressed using Natural Language Processing ( NLP ) for identifying the sense of the ambiguous word in a particular context . The application areas of WSD pertain to machine translation , information extraction and retrieval ( IE - IR ) , dialogue systems , and automatic summarization kind of NLP solutions . This paper presents a survey on WSD approaches in major AI - NLP methods by comparing different approaches for WSD in supervised , unsupervised , and knowledge based algorithms . This paper also aims at providing gap analysis in surveyed WSD systems comparing strengths and weaknesses of various surveyed systems and their accuracy . Based on the findings , a future hybrid approach synergizing rule - based and machine learning based methods are contemplated . The findings of this survey are envisaged through an ongoing research on WSD based Meta - Search algorithm under C - DAC purview for an Intelligent NLP based system to detect the actual sense of search queries and providing semantic classification of news headlines and snippets containing ambiguous words . 

An Intelligent framework for E - Recruitment System Based on Text Categorization and Semantic Analysis
In the field of online job recruiting , accurate job and resume categorization is critical for both the seeker and the recruiter . Using Natural Language Processing ( NLP ) technology we have developed an autonomous text classification system that POS tag , tokenizes , Lemmatize the data . We have utilized Phrase Matcher to calculate the score of resumes based on recruiter ' s information , suggest lacking skills to users , and provide the top resumes to the recruiter . Finally , the proposed system is presented together with its findings and analysis . We divided candidates into groups based on the information in their resumes . We used domain adaptation due to the sensitive nature of the resumes content . A Word Order Similarity between Sentences is used to categorize the resume data on large dataset of job description . The System is evaluated and resulted in improved precision and recall . 

SVM - based Hybrid Pattern for New Word Discovery
New words bring more challenges into Chinese word segmentation . This paper presents a SVM - based hybrid pattern for new word discovery , trying to integrate the advantages of the statistics - based method and the rule - based method to improve the performance of the new word discovery . In the statistics module , new words discovery is defined as a binary classification problem , in which we considered the previous new word features and proposed context information and affix information as new features , as well as constraints , which reveal the relationships among the new word candidates . Finally , some rules are introduced aimed to improve the performance . In the experiment , some new words are simulated by revising the dictionary of a Natural Language Processing ( NLP ) system . The results show that these features and constraints are useful for new word discovery , and the F - measure is 64 . 62 % which is 7 % higher than the baseline . 

The First Wikipedia Questions and Factoid Answers Corpus in the Thai Language
This article introduces a Thai questions - answers corpus for a question - answering task which was extracted from Thai Wikipedia which was downloaded on 17 December 2017 . The answers comprise 5 , 000 annotated factoids . The corresponding questions are exact phrases / sentences that contain the answer , but are replaced by a question word , or synthetic questions acquired from phrases and / or sentences on the wiki page . A question must contain only one of a set of 7 specific question words and a complex question must be avoided . Fifteen annotators used an annotation system specifically designed for this task . Acceptance , rejection , and revision processes were monitored by a language specialist . The final set was divided into 4 , 000 pairs for a training set and 1 , 000 pairs for a validation set . A baseline evaluation was conducted and an F1 score of 27 . 25 was obtained from document readers and 71 . 24 from document retrievals . 

Towards Automatic Narrative Coherence Prediction
Research in Psychology has shown that stories people tell about themselves , and how they recall their experiences , reveal a lot about their individual characteristics and mental well - being . The Narrative Coherence Coding Scheme ( NaCCS ) is a set of guidelines established in psychology research for annotating the “ coherence ” of a narrative along three dimensions : context , chronology and theme . A significant correlation was found between a narrative ’ s coherence score and independently collected mental health markers of the narrator . Currently , all coherence annotations are done manually ; a time consuming task which drains vital resources . In this paper , we propose an Artificial Intelligence based approach involving Natural Language Processing ( NLP ) to predict a narrative ’ s coherence score ( 4 - class classification problem ) . We explore a number of techniques , ranging from traditional machine learning models such as Support Vector Machines ( SVM ) to pre - trained language models such as BERT ( Bidirectional Encoder Representations from Transformers ) . BERT produced the best results for all dimensions in terms of accuracy : 53 . 7 % ( context ) , 71 . 8 % ( chronology ) , and 69 . 6 % ( theme ) . The location of information in the narratives ( beginning , end , throughout ) was helpful in improving predictions . 

Research on the Key Technology Based - NLP Of Chinese Medicine Pulse ' s Mathematical Quantifying
In this paper , Chinese Medicine Pulse is dealt in view of measuration and information extraction . We analyze the research status and problems in quantifying Pulse objectively . A Mathematical model of ancient pulse is constructed by utilizing the technique and method of natural language processing ( NLP) , according to Traditional Chinese Medicine Theory and pulse - symptom knowledge . And the new description of Pulse in modern mathematical quantization level will be established by mining the Modern Biology and Biochemistry correlation index , so as to form the open system of pulse ’ theory , and to realize the development of it . 

Quantum of Mind : Fundamental Forces of the Universe in Natural Language Semantics
Empirical findings about the phenomenology of mind gained from Free Associative Experiment , has opened a new dimension of Artificial intelligence and quantum linguistics research , allowing us to create a link between a collective unconscious level and a conscious Mind . Our research methodology included constructing a database of metaphorical stimuli , randomly presenting these to participants , and tabulating FAE responses both by the participant and by the stimulus , analyzed by frequency of semantic type . The project aimed to produce a new model of knowledge representation . A latent semantics network presented here gave us a unique opportunity to discover the probabilistic nature of mind revealing itself in all languages as entangled quantum particles . Free Association Experiment ( FAE ) based on cognitive metaphor stimuli . It is the first NLP tool of its kind , providing a fundamentally new source of empirical data examining hidden dimension of the human mind . 

Intent Detection for code - mix utterances in task oriented dialogue systems
Intent detection is an essential component of task oriented dialogue systems . Over the years , extensive research has been conducted resulting in many state of the art models directed towards resolving user ' s intents in dialogue . A variety of vector representation for user utterances have been explored for the same . However , these models and vectorization approaches have more so been evaluated in a single language environment . Dialogue systems generally have to deal with queries in different languages and most importantly Code - Mix form of writing . Since Code - Mix texts are not bounded by a formal structure they are difficult to handle . We thus conduct experiments across combinations of models and various vector representations for Code - Mix as well as multi - language utterances and evaluate how these models scale to a multi - language environment . Our aim is to find the best suitable combination of vector representation and models for the process of intent detection for code - mix utterances . We have evaluated the experiments on two different dataset consisting of only Code - Mix utterances and the other dataset consisting of English , Hindi , and Code - Mix ( English - Hindi ) utterances . 

Deep learning approaches to classify the relevance and sentiment of news articles to the economy
We consider a text classification task over an open source dataset involving news snippets and their relevance to the US economy . Text classification and sentiment analysis have been performed using nine different classifiers among which three are the traditional machine learning models , namely , support vector machine , extreme gradient boosting and logistic regression , and six neural network - based methods . The neural net frameworks include long short - term memory ( LSTM ) , bidirectional long short - term memory ( BiLSTM ) and an ensemble of one dimensional convolution network ( 1D CNN ) with LSTM / BiLSTM . Both word - to - vector and term - frequency inverse - document - frequency vectors are used in our analysis with text and sentiment classification tasks . A detailed comparative study is provided to assess the relative performance of different classification approaches . It is observed that the ensemble with 1D CNN performs better in both binary and multiclass classifications . Specifically , in the multinomial sentiment classification , 1D CNN with BiLSTM has the best performance as opposed to 1D CNN with LSTM in the binary text classification . BiLSTM architecture which incorporates the backward dependencies turns out as superior to LSTM by a margin of 30 % in multiclass classification even though the considered dataset is small and inherently challenging . Further analysis to evaluate the impact of successive increases in percentage of augmented data reveals that such augmentation has a limit up to 180 % in this dataset beyond which the performance starts decreasing . 

Email classification using adaptive ontologies Learning
Lot of time is spent on E - Mails for communication in today ' s IT world , peoples prefer to send email for business purpose and information exchange . Email management is necessary because once our inbox is full of mails we avoid to read out one by one in that case some important email may get missed . Always user try to avoid unnecessary email reading for that a better email management system is required . Here author used fuzzy logic techniques for email clustering . Extract concept and feature , same feature keyword goes into one cluster if a new keyword is found and not matched with any existing cluster than a new cluster is defined for that . Based on these clustering techniques authors wants to update that calendar for real time information and hassle free for reading unnecessary emails . 

Analysis of trends in scientific publications by an NLP toolkit : A case study in Software Development Methods for Enhanced Living Environment
As the number of published scientific articles increases , the analysis of trends and state - of - the - art in software engineering is becoming very time - consuming and laborious task . To address the ever - growing demands for systematic literatures review techniques , rapid review and scoping reviews techniques have emerged . We used an NLP powered tool , which employs the PRISMA surveying methodology , to automate most of the review processes . We used it to automatically review relevant articles indexed in IEEE Xplore , PubMed and Springer digital libraries on the topic “ Software Development for Enhanced Living Environments and Ambient Assisted Living” . The relevant articles identified by the NLP toolkit contained up to 21 properties clustered into 3 logical groups . We discovered that Software Development for Enhanced and Assisted living environments attracted an increased attention from the scientific communities over the last 10 years and showed several trends in the specific research topics that fall into this scope . The research uncovered that iterative software methodology had been the most attractive research topic in the field . Despite the enormous empirical evidence on application and success stories of agile development methodologies in many software development engineering , it received a little attention from the scientific community in the software development for Enhanced and Assisted Living Environments . The NLP toolkit identified the most relevant articles that contained the defined properties in the search . Hence , it significantly reduced the manual work , while also generating informative tables , charts and graphs . 

Feature Based Sentimental Analysis for Prediction of Mobile Reviews Using Hybrid Bag - Boost algorithm
Sentiment analysis or opinion mining is one of the major challenge of NLP ( natural language processing ) . Business Analytics plays a key role in the current scenario with a perception that people wants to enhance their enterprise . In particular , these people rely on feedback of their goods to withstand the competition and knowledge mining that can give them an outstanding view into what to expect in the future . Few words or phrases may decide results or outcomes . As a majority of these people seek to boost their company in order to achieve full benefit by providing premium goods . In this aspect , sentiment analysis has gained a lot of interest in the current years . SA is an area of research of NLP that is used to classify a specific feature ' s opinion or perspective within a text . This paper is based on the different methods used to identify a particular text according to the opinions conveyed by the user ' s i . e . whether the overall sentiment of a individual is negative or positive or neutral . We are also looking at the two advance approaches adopted ( feature classification pursued by polarity classification ) along with the experimental results . Finally in this paper we compared 3 ML classification techniques 1 ) Logistic Regression , 2 ) Hybid Bag - Boost algorithm 3 ) SVM in which hybrid algorithm provides more accuracy compared to the other 3 ML algorithms . The Main objective of the proposed method is to predict the user reviews for choosing a best mobile using several classification Algorithms . 

Time adaptive boosting model for topic tracking
The technology of topic tracking can help people find what they are interested from the vast information sea . Since topics develop dynamically , topic excursion problem may appear in the tracking process . To overcome this problem and the shortcomings of current adaptive methods , we propose a new adaptive method for topic tracking . We call it time adaptive boosting ( TAB ) model . This model adopts the idea of boosting and presents new algorithm to the adaptive learning mechanism in the task of topic tracking . This algorithm can solve the problem of topic excursion , and remedy the deficiency of current adaptive methods . Time sequence of topic tracking task is also considered in the algorithm . We use sigmoid function to express it . In experiments we use the Chinese part in TDT4 corpus as test corpus , and use the TDT2004 evaluation metric to evaluate the adaptive Chinese topic tracking system based on TAB . Experimental results show that the adaptive method based on TAB can improve the performance of topic tracking . 

A Collaborative filtering Method based on Forgetting Theory And Neural Item Embedding
The collaborative filtering algorithm ( CF ) is one of the most important algorithms in the recommendation system . Recently , the use of neural word embedding methods to learn the latent representation of words has become a mature method in the field of natural language processing ( NLP ) . Inspired by the SGNS algorithm in the NLP field , we propose f - item2vec as a new method to learn the latent representation in item vector space , and based on this vector to calculate the similarity between items . In addition , according to the forgetting process , we propose a new user preference model to accurately identify the user ' s short - term preferences and long - term preferences . The experimental results show that the proposed method is effective and superior to the traditional algorithm in predicting the score and generating the recommendation list . 

EmotionExpert : Facebook game for crowdsourcing annotations for emotion detection
The current paper explores the use of the social network platform Facebook , as a source of emotion annotated textual data as well as a source of annotators . The traditional approach of hiring experts to provide manually labeled ( annotated ) data for NLP research is time - consuming , tedious and expensive . Hence , crowdsourcing has emerged as a useful method for obtaining annotated data for natural language processing ( NLP ) research . We have developed a purposeful innovative Facebook game called EmotionExpert in order to collect human annotated textual data for emotion detection from text . The game provides a means to reach a large number of players , while making the annotation of emotional content of texts an enjoyable and social activity . The findings reported in this paper indicate that EmotionExpert is a useful resource for reaching a large number of people to produce reliable annotations . 

Making Semantic Annotation on Patient Data of Depression
Patient data , more exactly , electronic medical records ( EMR ) , usually contain a lot of free texts . Those unstructured medical data cannot be easily understood by computers . In addition , EMR data have a strong privacy , which hinders the sharing and use of medical data and makes it impossible to conduct more in - depth medical research . This paper presents a method of the realization of semantic EMR by making semantic annotations on free texts in medical records . We will show how to use Natural Language Processing ( NLP ) tools to create semantic annotation with well - known biomedical terminologies / ontologies such as the Unified Medical Language System ( UMLS ) . Moreover , we will describe how to make the semantic annotations on a set of virtual patient data for depression , which are generated by using the Advanced Patient Data Generator ( APDG ) , a knowledge - based patient data generator . In short , our goal is to use semantic technology to improve the sharing and utilization of medical data and the interoperability among systems . 

Modified LSTM with Memory Layer for Power Grid Signal Classification
In processing of signals coming from power grid system , the major purpose is to filter out the key signals that are highly related to power grid fault or breakdown . Power signal analysis and incident summary are normally done by people . The obvious issue with manpower is when multiple incidents take place and huge number of signals emerge , processing all signals manually in a short time becomes virtually impossible . Attempts to handle similar massive information situation with neural network models have been proven useful . However , ordinary neural network models that learn and process signals usually suffer “ memory loss ” as the sequence gets longer , which leads to inaccurate classification . This paper proposes a modified version of long - short - term - memory ( LSTM ) network that can mitigate the memory dependency issue of recurrent neural network ( RNN ) and basic LSTM structure . The focus of the thesis is how the modified LSTM network can utilize the additional layer of memory slots to store the entire output sequence from the last LSTM cell instead of mere output value in basic LSTM architecture . The modified model can incorporate all previous cell information and prevent memory loss in the modeling process . To substantiate the merit of the LSTM model with stacked memory layer , a signal classification experiment is done with both modified and basic LSTM models and the results are shown . 

How to Provide Developers Only with Relevant Information ? 
After the release of a new software version it isdifficult for individual developers to keep track of all newlysubmitted bug reports complicating their decision making , e . g . , which bug to resolve next ? This problem is emphasized by thepresence of further information sources , such as social media , which offer valuable user feedback to developers regardingthe software . However , due to an abundant amount of infor - mation , developers might never notice this feedback . Hence , we envision a real - time system that provides developers withrelevant information for improving the quality of their systemwhile filtering out irrelevant facts from multiple informationsources . For this system to work , it is necessary to computethe similarity between different types of documents , e . g . , tweetsand bug reports , in order to detect whether they are relevantto a developer or not . In this feasibility study , we focus onanalyzing this core assumption in a simplified scenario inwhich we identify related bugs for a given software fix withthe help of Natural Language Processing methods . In thisexperimental setting , which exhibits the key characteristics ofour envisioned system , we obtain promising results indicatingthat our approach is feasible . 

Text Summarization Application for Indonesian Twitter Document by Using Top - N Feature Selection Algorithm
The rapid development of information technology has changed many things in our lives . One of the most influential technological developments in our lives is the emergence of the internet . One of the phenomenal examples in the world of information technology is the existence of social media services . This service has replaced the habit of people looking for information and writing down its expressions . With a very high data growth rate , it has raised new problems for Twitter users . The large number of tweets sometimes causes difficulties in understanding the information . To solve this problem , one solution that can be applied is to summarize the information circulating on Twitter . Text summarization is a field of study in natural language processing ( NLP) . This algorithm aims to reduce the number of words in a document so that the information is easier to understand . NLP can be used to help us summarize tweet documents so that they are easier to understand . This study aims to develop tweet summarization software using the top - n feature selection algorithm . Based on the experiment that has been done , the application has succeeded in summarizing the tweet document by displaying the terms that have the highest weight . Hence , users can more quickly understand information from Twitter without having to read the entire tweet document . 

Auto - correction of English to Bengali Transliteration System using Levenshtein Distance
The automated transliteration process is a function or software application that checks words against a computerized corpus to ensure that they are correct . A transliteration system is required either during the typing of text or when a user does not know the correct spelling of a word . The main objective of this research is to develop a system that is much better to check the spelling of the transliterated word by calculating Levenshtein distance . To make the mechanism more efficient and accurate unigram method has been implemented . Several techniques have been integrated for data collection to make the system more reliable and flexible . Almost twenty thousand words are included to create a data lexicon for this research work . This system is able to deal with signed or unsigned numeric values and float numbers with 78 . 13 % accuracy . 

Development of a Load Management Algorithm Using Nonlinear Programming ( NLP ) for Optimum Integration of Electric - Mobility Solutions into Rural Off - Grid PV Systems
The electric output and efficiency of the Photovoltaic ( PV ) devices are strongly dependent on metrological variables such as solar irradiation , wind speed and ambient temperature . It is therefore important to optimise the size and electrical loads of an off - grid PV system to meet the required load demands at least cost . However , sizing results dependent on the energy management techniques used for operating the system , especially when considering components with different dynamics . This paper presents an off - grid PV system modelling and simulation approach using MATLAB / Simulink CARNOT 7 . 0 toolbox including a load management methodology using nonlinear programming ( NLP ) . For a better integration of electric mobility into the off - grid PV system with battery storage operation , an optimisation problem was formulated which resulted in a nonlinear programming problem . The optimisation model was developed to solve the NLP problem and to optimise electric loads including electric mobility and battery storages in order to properly utilise the PV generation , thereby reducing energy deficit and cost . Metrological hourly dataset for a complete year was used for successful simulation of the PV system . A sensitivity analysis of the NLP optimisation model was carried out to evaluate the impact of the electric loads ( kWh ) on the objective function . 

Lips : An IDE for model driven engineering based on natural language processing
Combining both , state - of - the art natural language processing ( NLP ) algorithms and semantic information offered by a variety of ontologies and databases , efficient methods have been proposed that assist system designers in automatically translating text - based specifications into formal models . But due to ambiguities in natural language , these approaches usually require user interaction . Following these achievements , we consider natural language as a further input language that is used in the design flow for systems and software . Consequently , concepts from integrated development environments ( IDE ) as they can be found for programming languages such as Java need to be made available for natural language specifications as well . In this paper , we propose lips , an integrated development environment that is seamlessly implemented on top of Eclipse . It contains recent NLP algorithms that extract formal models suited for the Eclipse Modeling Framework and therefore provide a starting point for an ongoing implementation . Whenever user interaction is required , lips makes use of well - known IDE concepts such as markers and quick fixes thereby enabling a holistic user experience . 

Machine reading at the University of Washington
Machine reading is a long - standing goal of AI and NLP . In recent years , tremendous progress has been made in developing machine learning approaches for many of its subtasks such as parsing , information extraction , and question answering . However , existing end - to - end solutions typically require substantial amount of human efforts ( e . g . , labeled data and / or manual engineering) , and are not well poised for Web - scale knowledge acquisition . In this paper , we propose a unifying approach for machine reading by bootstrapping from the easiest extractable knowledge and conquering the long tail via a self - supervised learning process . This self - supervision is powered by joint inference based on Markov logic , and is made scalable by leveraging hierarchical structures and coarse - to - fine inference . Researchers at the University of Washington have taken the first steps in this direction . Our existing work explores the wide spectrum of this vision and shows its promise . 

NLP and text analysis at the University of Massachusetts
Our group is investigating a variety of techniques centered around the use of text corpora to support natural language processing applications . We are interested in information extraction from text , text classification , and knowledge acquisition from text corpora . Our goal is to develop technologies that can be readily ported across domains and scaled up with a minimal amount of manual engineering . In particular , we are experimenting with various kinds of statistical profiles and case based reasoning systems in order to facilitate : • semantically - oriented dictionary construction • the analysis of complex sentence structures • complex domain discriminations • specific aspects of discourse analysis

On measuring the lexical quality of the web
In this paper we propose a measure for estimating the lexical quality of the Web , that is , the representational aspect of the textual web content . Our lexical quality measure is based in a small corpus of spelling errors and we apply it to English and Spanish . We first compute the correlation of our measure with web popularity measures to show that gives independent information and then we apply it to different web segments , including social media . Our results shed a light on the lexical quality of the Web and show that authoritative websites have several orders of magnitude less misspellings than the overall Web . We also present an analysis of the geographical distribution of lexical quality throughout English and Spanish speaking countries as well as how this measure changes in about one year . 

Algorithms and a Tool for Automatic Decryption of Clinical Notes
Natural Language Processing ( NLP ) presents a set of techniques that are finding applications in modern healthcare , for the extraction and generation text . Clinical notes are classically originated and derived from various sources of narratives such as reports , referral letters , discharge notes and clinical summaries . In this paper , we present algorithms and a tool for enabling each member of a medical team to read and understand each others medical notes , using NLP techniques . We refer to this process as the decryption , or the deciphering , of complex clinical notes into simple readable language . We have presented a tool called the Clinical Note Translator ( CNT ) . Based on the replacement of technical terms in clinical notes , CNT translates these notes to plain text . This solution is expected to assist multidisciplinary medical teams in understanding peer - to - peer expert notes . 

Deep Learning to Predict Hospitalization at Triage : Integration of Structured Data and Unstructured Text
Overcrowding in Emergency Departments ( ED ) is considered as an international issue , which could have adverse impacts on multiple care outcomes such as the length of stay for example . Part of the solution could lie in the early prediction of the patient outcome as discharge or hospitalization . This study applies Deep Learning to this end . A large - scale dataset of about 260K ED records was provided by the Amiens - Picardy University Hospital in France . In general , our approach is based on integrating structured data with unstructured textual notes recorded at the triage stage . The key idea is to apply a multi - input of mixed data for training a classification model to predict hospitalization . In a simultaneous manner , the model training utilizes the numeric features along with textual data . On one hand , a standard Multi - Layer Perceptron ( MLP ) model is used with the standard set of features ( i . e . numeric and categorical ) . On the other hand , a Convolutional Neural Network ( CNN ) is used to operate over the textual data . The two components of learning are conducted independently in parallel . The empirical results demonstrated that the classifier could achieve a very good accuracy with ROC - AUC ≈ 0 . 83 . The study is conceived to contribute to the mounting efforts of applying Natural Language Processing in the healthcare domain . 

Clinical Decision Support Systems : A Survey of NLP - Based Approaches from Unstructured Data
Clinical Decision Support on patients health outcomes can be performed from free text with Natural Language Processing techniques . However , it becomes a computational challenge due to the complexity of natural language . In recent years , several NLP - based approaches have been proposed to consider clinical decisions support . This paper presents a survey of Natural Language Processing approaches to support clinical decisions on patient health outcomes . The presented approaches are emphasized on the use of free text as input for diverse languages . An analysis of clinical decision support systems based on natural language processing in terms of their performance results is presented . 

A Research on Length Based Sentence Alignment for Chinese - English Parallel Corpus
Many existing length based Chinese - English sentence alignment methods compute sentence length in terms of the number of bytes . In this paper , we examine the effectiveness of six different ways of sentence length computation , which take , respectively , the number of verbs , nouns , adjectives , content words , bytes and all words in a sentence as its length . Most previous methods are found memory consuming and inefficient . This paper proposes an alignment method to save memory and time via grouping sentence for alignment . Our experimental results show that taking all words into account in the sentence length computation can further enhance alignment performance , giving 99 . 01 % precision and 99 . 5 % recall , respectively . 

How to Tune Parameters in Geographical Ontologies Embedding
Many Natural Language Processing ( NLP ) tasks , like question answering or analyzing verbatim comments , have started to use word embeddings due to their ability to capture semantic relations between words . Recently , embeddings have been also applied in the geospatial context to represent geospatial ontologies , thanks to their ability to capture semantic similarity . In this paper , we present an analysis of a promising embedding technique particularly suitable for representing hierarchical structures . We conduct a deep technical evaluation of many parameters and their impact on the quality of the representation . 

CaPaR : A Career Path Recommendation Framework
In today ' s world , recommendation systems are used to solve the problem of information overload in many areas allowing users to focus on important information based on their interests . One of the areas where such systems can play a major role is in helping students achieve their career goals by generating personalized job and skill recommendations . At present , there are many job posting websites providing a huge amount of information and students need to spend hours to find jobs that match their interests . At the same time , existing job recommendation systems only consider the user ' s field of interest , but do not take into consideration the user ' s profile and skills , which can generate more relevant career recommendations for users . In this work , we propose CaPaR , a Career Path Recommendation framework , which addresses such shortcomings . Using text mining and collaborative filtering techniques the system first scans the user ' s profile and resume , identifies the key skills of the candidate and generates personalized job recommendations . Moreover , the system recommends additional skills to students required for related job openings , as well as learning resources for each skill . In this way , the system not only allows its users to explore large amounts of information , but also expand their portfolio and resume to be able to advance their careers further . We experiment and evaluate the various recommendation algorithms with real - world data collected from the San Jose State University career center web site . 

Nonautoregressive Encoder - Decoder Neural Framework for End - to - End Aspect - Based Sentiment Triplet Extraction
Aspect - based sentiment triplet extraction ( ASTE ) aims at recognizing the joint triplets from texts , i . e . , aspect terms , opinion expressions , and correlated sentiment polarities . As a newly proposed task , ASTE depicts the complete sentiment picture from different perspectives to better facilitate real - world applications . Unfortunately , several major challenges , such as the overlapping issue and long - distance dependency , have not been addressed effectively by the existing ASTE methods , which limits the performance of the task . In this article , we present an innovative encoder - decoder framework for end - to - end ASTE . Specifically , the ASTE task is first modeled as an unordered triplet set prediction problem , which is satisfied with a nonautoregressive decoding paradigm with a pointer network . Second , a novel high - order aggregation mechanism is proposed for fully integrating the underlying interactions between the overlapping structure of aspect and opinion terms . Third , a bipartite matching loss is introduced for facilitating the training of our nonautoregressive system . Experimental results on benchmark datasets show that our proposed framework significantly outperforms the state - of - the - art methods . Further analysis demonstrates the advantages of the proposed framework in handling the overlapping issue , relieving long - distance dependency and decoding efficiency . 

A Digital Forensics Investigation Model for Social Networking Site
Social Networking is fundamentally shifting the way we communicate , sharing idea and form opinions . All people try to use social media for there need , people from every age group are involved in social media site or e - commerce site . Nowadays almost every illegal activity is happened using the social network and instant messages . It means that present system is not capable to found all suspicious words . In this paper , we provided a brief description of problem and review on the different framework developed so far . Propose a better system which can be indentify criminal activity through social networking more efficiently . Use Ontology Based Information Extraction ( OBIE ) technique to identify domain of word and Association Rule mining to generate rules . Heuristic method checks in user database for malicious users according to predefine elements and Na ï ve Bayes method is use to identify the context behind the message or post . The experimental result is used for further action on victim by cyber crime department . 

Harnessing NLP techniques in the processes of multilingual content management
The emergence of the WWW as the main source of distributing content opened the floodgates of information . The sheer volume and diversity of this content necessitate an approach that will reinvent the way it is analysed . The quantitative route to processing information which relies on content management tools provides structural analysis . The challenge we address is to evolve from the process of streamlining data to a level of understanding that assigns value to content . We present an open - source multilingual platform ATALS that incorporates human language technologies in the process of multilingual web content management . It complements a content management software - as - a - service component i - Publisher , used for creating , running and managing dynamic content - driven websites with a linguistic platform . The platform enriches the content of these websites with revealing details and reduces the manual work of classification editors by automatically categorising content . The platform ASSET supports six European languages . We expect ASSET to serve as a basis for future development of deep analysis tools capable of generating abstractive summaries and training models for decision making systems . 

Multi - level topic detection algorithm for Netnews Specials
This paper investigates the topic detection method in Netnews Specials Detection ( NSD ) . We found that when the traditional clustering algorithms are used in NSD , the same topic is usually split into several pieces and the result is not satisfying . So a new algorithm is proposed which uses a multi - level model , better suited for NSD . Firstly , such algorithm elevates the accuracy of single - layer clustering by introducing hot search words , a selective dictionary , and an advanced weight formula . Secondly , the multiple - level model not only avoids the problem of topic over - split but also establishes a structure for Netnews Specials , which lays the foundation for quick viewing , positioning and retrieval . Experimental results show that the algorithm in the real test corpus have high accuracy , doing a better job than the traditional clustering method . 

A lattice - valued model of computing with words
Computing with words , as a methodology , means computing and reasoning by the use of words in place of numbers or symbols . In this paper , we deal with computing with words via lattice - valued finite state automata and lattice - valued regular grammars . Specifically , 1 ) we show that computing with words via lattice - valued finite state automata can be implemented with computing with values via lattice - valued finite state automata ; 2 ) we show that computing with words via lattice - valued regular grammars can be implemented with computing with values via lattice - valued regular grammars ; 3 ) the equivalence between lattice - valued finite state automata and lattice - valued regular grammars are demonstrated . 

Sentiment Analysis on Food Review using Machine Learning Approach
Interpersonal interaction correspondence has obtained a customary standard way to deal with web . Casual correspondence insinuates the use of web - based life destinations and applications . Twitter is one of the mainstream web - based media utilized in the present - day life . Individuals share their inclination with a post in many exercises of our everyday lives . Supposition analysis has become commonly notable . Regardless , stable Twitter thought portrayal execution stays dangerous due to different issues : generous class lopsidedness in a multi - class issue , illustrative extravagance issues for feeling signs , and the usage of different ordinary semantic models . These issues are perilous since various sorts of online life assessment rely upon exact shrouded Twitter thoughts . As necessities seem to be , a book examination structure is proposed for Twitter notion investigation . Estimation investigation by utilizing twitter information is well known in this recorded . Words and articulations bespeak the perspectives of people about the things , organizations , governments and events through electronic systems administration media . Eliminating positive , negative or nonpartisan polarities from electronic life content names task of suspicion assessment in the field of NLP . The outstanding improvement of solicitations for business affiliations and governments , affect experts to accomplish their examination in assumption examination . This exploration utilizes three front line ML classifiers SVM , Logistic Regression , Random Forest , Naive Bayes classifier for development of product review analysis . The tests are performed using Twitter yelp datasets . This data is available online on the web . The discussion conversation , review objections , destinations are a bit of the appraisal of rich resources where the study or posted articles is their inclination or all - around end towards the subject . 

On the design of web crawlers for constructing an efficient Chinese - Portuguese bilingual corpus system
Machine Translation is a very popular and important topic in Natural Language Processing ( NLP ) during the last few decades . This paper focuses on the design of the Web Crawlers for Chinese - Portuguese bilingual corpus construction , and this corpus would be used in corresponding Machine Translation systems . It accomplished a bilingual corpus construction process from bilingual corpus collection with web crawlers based on different sources . By this mean , this system can be considered as an innovative and reasonable attempt in setting up the bilingual corpora with Chinese and Portuguese , and it has solved some practical problems at the initial stage of the corpus construction . 

Integrating extra knowledge into word embedding models for biomedical NLP tasks
Word embedding in the NLP area has attracted increasing attention in recent years . The continuous bag - of - words model ( CBOW ) and the continuous Skip - gram model ( Skip - gram ) have been developed to learn distributed representations of words from a large amount of unlabeled text data . In this paper , we explore the idea of integrating extra knowledge to the CBOW and Skip - gram models and applying the new models to biomedical NLP tasks . The main idea is to construct a weighted graph from knowledge bases ( KBs ) to represent structured relationships among words / concepts . In particular , we propose a GCBOW model and a GSkip - gram model respectively by integrating such a graph into the original CBOW model and Skip - gram model via graph regularization . Our experiments on four general domain standard datasets show encouraging improvements with the new models . Further evaluations on two biomedical NLP tasks ( biomedical similarity / relatedness task and biomedical Information Retrieval ( IR ) task ) show that our methods have better performance than baselines . 

An Exploratory Case Study for Turkish Sentiment Classification Using Graph Convolutional Neural Networks
Graph Convolutional Neural Networks ( GCNs ) are highly popular in recent years . It gives very successful results for various natural language processing ( NLP ) tasks such as sentiment classification . It has recently been shown to be effective and successful models to solve sentiment classification problem of texts . However , there is no research demonstrating the performance of this model on Turkish texts . In this study , we observe performance of the GCN model on the sentiment classification problem of Turkish texts as first research . Since the structure of Turkish language is agglutinative , different preprocessing approaches are presented and performance results on three real - world Turkish sentiment datasets are shown . It is observed that the TripAdv dataset , which was used in this study , yielded a 0 . 76 F - measure value . This can be considered a reasonable success for a sentiment classification with three sentiment classes . On the other hand , this study is presented as an exploratory case study in preparation for more detailed and extensive research in the future . 

Enhancing text clustering model based on Truncated Singular Value Decomposition , fuzzy ART and Cross Validation
Numerical schemes research on clustering model has been quite intensive in the past decade . The difficulties associated with curse of dimensionality and cost functions to reflect the general knowledge about internal structures and distributions of target data . Traditional computational clustering and variables selection schemes are struggling to estimate at high level of accuracy for this type of problem . Hence , in the present study , a novel semantic - based scheme was proposed to enhance the clustering accuracy . The results show that our conceptual model is automatic and optimal . Good comparisons with the experimental studies demonstrate the multidisciplinary applications of our approach . 

Abusive Language Detection in Online User Content
Detection of abusive language in user generated online content has become an issue of increasing importance in recent years . Most current commercial methods make use of blacklists and regular expressions , however these measures fall short when contending with more subtle , less ham - fisted examples of hate speech . In this work , we develop a machine learning based method to detect hate speech on online user comments from two domains which outperforms a state - of - the - art deep learning approach . We also develop a corpus of user comments annotated for abusive language , the first of its kind . Finally , we use our detection tool to analyze abusive language over time and in different settings to further enhance our knowledge of this behavior . 

An XML database for modern standard Arabic ( MSA ) verbs generated from triliteral roots
In this paper , we present an exhaustive database for Modern Standard Arabic ( MSA ) verbs generated from trilateral roots . This database is initially represented as a root - pattern matrix listing rows of all recognized roots and columns of all verb patterns in MSA . The intersection of each row and column contains an index indicating the compatibility of the aforementioned root - pattern pair . This index refers also to a list of morpho - syntactic characteristics of the generated verb . We later converted the database into the more flexible XML format . The aim for our approach is twofold : with the objective of building an exhaustive list , we opted for automatic generation of all possible trilateral roots in the Arabic alphabet and subsequent filtering of roots not recognized in the literature ; secondly , converting the database into XML creates a highly versatile resource for easy integration in Arabic NLP applications . 

Transformation - based learning in the fast lane
Transformation - based learning has been successfully employed to solve many natural language processing problems . It achieves state - of - the - art performance on many natural language processing tasks and does not overtrain easily . However , it does have a serious drawback : the training time is often intorelably long , especially on the large corpora which are often used in NLP . In this paper , we present a novel and realistic method for speeding up the training time of a transformation - based learner without sacrificing performance . The paper compares and contrasts the training time needed and performance achieved by our modified learner with two other systems : a standard transformation - based learner , and the ICA system ( Hepple , 2000) . The results of these experiments show that our system is able to achieve a significant improvement in training time while still achieving the same performance as a standard transformation - based learner . This is a valuable contribution to systems and algorithms which utilize transformation - based learning at any part of the execution . 

Object Distance Estimation with Machine Learning Algorithms for Stereo Vision
This paper presents a novel distance estimation to calculate distances from the stereo camera to the object accurately . This study collected stereo camera images as a dataset , each object determined at two different lighting environments and five different distances between the stereo camera and the object . To estimate the distance , researchers applied supervised learning methods to approach this task . There were performed with two machine learning algorithms : Linear Regression , and Artificial Neuron Network Regression . In the experimental results , the efficiency of the proposed method was examined by using the evaluation metrics to calculate the distance estimation errors . The results showed the model of convolutional neuron networks operated with densely connected neuron networks has the lowest errors rate in comparison with other models . The model eliminates the error rate of distance estimation at 0 . 000531 , 0 . 014490 , and 0 . 000048 meters , measured by mean square error , mean absolute error and mean logarithmic error respectively . 

Time - Optimal Trajectory Generation for Dynamic Vehicles : A Bilevel Optimization Approach
This paper presents a general framework to find time - optimal trajectories for dynamic vehicles like drones and autonomous cars . Hindered by its nonlinear objective and complex constraints , this problem is hard even for state - of the - art nonlinear programming ( NLP ) solvers . The proposed framework addresses the problem by bilevel optimization . Specifically , the original problem is divided into an inner layer , which computes a time - optimal velocity profile along a fixed geometric path , and an outer layer , which refines the geometric path by a Quasi - Newton method . The inner optimization is convex and efficiently solved by interior - point methods . A novel variable reordering method is introduced to accelerate the optimization of the velocity profile . The gradients of the outer layer can be derived from the Lagrange multipliers using sensitivity analysis of parametric optimization problems . The method is guaranteed to return a feasible solution at any time , and numerical experiments on a ground vehicle with friction circle dynamics model show that the proposed method performs more robustly than general NLP solvers . 

Development of a Morph Analyser for Nepali noun token
Morphological Analysis provides grammatical information of a word from its suffix . Morphological ( Morph ) Analyser is a tool for the said purpose . It is an integral part of a successful Natural Language Processing ( NLP ) system . This paper discusses a suffixal Morph Analyser for Nepali Nouns ( MANN ) . This suffixal analyser is based on finite state grammar approach . The suffixal noun morphology of Nepali is well studied , and can be captured in regular grammar through finite state automata . Based on the linguistic rules , MANN identifies noun tokens and tags , their grammatical labels , and generates separate resources like Lexicon . Based on this a tool is developed and is working perfectly with Nepali nouns . 

Plagiarism Detection in Malayalam Language Text using a Composition of Similarity measures
Plagiarism refers to the actof using someone else ' s work without permission or without due acknowledgement . It has become a main concern in educational and research organisations . Though plagiarism existed from the very old times , the fast technological changes due to the internet has increased the problem to a very high level . Numerous plagiarism detection systems have been developed for dealing with this problem , especially for the English language . For a south Indian language like Malayalam which is both inflectional and agglutinative , developing an efficient plagiarism detection system is a challenging task . This paper focuses on the effect of using shallow Natural Language Processing techniques along with a combination of similarity metrics for extrinsic plagiarism detection in Malayalam text documents . 

Session details : Cluster 9 : text mining and NLP applications ( P45 - 47 ) 
No abstract available . 

Individualised NLP - enhanced feedback for distance language learning
The purpose of this paper is to present a strategy for the provision of language courses to learners of Spanish for specific purposes with intelligent feedback . As a byproduct , students can be recommended to proceed through a slightly different learning path in order to overcome their shortcomings . The paper is structured in 5 sections : section 1 is an introduction to this research work ; section 2 describes the didactic assumptions and the learning environment in which the strategy is applied ; section 3 presents the tools for both linguistic analysis and error detection , and describes how the system generates feedback ; and section 4 concludes with some remarks and presents future work . 

Annotating Indirect Anaphora for Hindi : A Corpus Based Study
Natural language processing requires a lot of analysis and information regarding words and segment of sentence . Almost all NLP applications such as machine translation , information extraction , automatic summarization , question answering system , natural language generation , etc . , require successful identification and resolution of anaphora . Information regarding word using POS tagger , parser and other tool can be gathered . Hindi is language of free word order as compare to English . This enforces additional constraints on different NLP task . In this working paper we present an analysis of Hindi genre . We used ten tags from literature . Out of ten tags seven are annotated using Botley ' s annotation scheme manually . We annotated 1540 demonstrative pronoun from twelve files of EMILEE corpus . Input file is EMILEE file and output is fully annotated unicode file . 

Learning word - level dialectal variation as phonological replacement rules using a limited parallel corpus
This paper explores two different methods of learning dialectal morphology from a small parallel corpus of standard and dialect - form text , given that a computational description of the standard morphology is available . The goal is to produce a model that translates individual lexical dialectal items to their standard dialect counterparts in order to facilitate dialectal use of available NLP tools that only assume standard - form input . The results show that a learning method based on inductive logic programming quickly converges to the correct model with respect to many phonological and morphological differences that are regular in nature . 

Tree - Structured Curriculum Learning Based on Semantic Similarity of Text
Inspired by the notion of a curriculum that allows human learners to acquire knowledge from easy to difficult materials , curriculum learning ( CL ) has been applied to many areas including Natural Language Processing ( NLP ) . Most previous CL methods in NLP learn texts according to their lengths . We posit , however , that learning semantically similar texts is more effective than simply relying on superficial easiness such as text lengths . As such , we propose a new CL method that considers semantic dissimilarity as the complexity measure and a tree - structured curriculum as the organization method . The proposed CL method shows better performance than previous CL methods on a sentiment analysis task in an experiment . 

Chinese text orientation analysis based on phrase
Semantic orientation analysis of sentiment word is to determine its polarity and degree , including original orientation , dynamic orientation and modified orientation . In this paper , we correct the orientation in different contexts through dependency relationship and some rules . The result shows that accuracy and recall rate is improved a lot . 

An Ontology for CMMI - ACQ Model
This paper discusses an ontology developed to represent the CMMI - ACQ domain knowledge . CMMI introduces a collection of best practices that helps organizations improve their processes and CMMI - ACQ as one of its constellations is to provide organizations with a resource of effective and proven practices to support acquisition process improvement . This ontology has been developed based on SUMO upper ontology . Through the paper , CMMI and CMMI - ACQ will be illustrated . Also the ontology , its development methodology , and its potential application will be described . 

A new algorithm of fuzzy support vector machine based on niche
A new algorithm of fuzzy support vector machine based on niche is presented in this paper . In this algorithm , through comparing samples niche with class niche , the method of simply using Euclidean distance to measure the relationship of samples and class in the traditional support vector machine is changed by using the minimum radius in class niche , and the disadvantages of traditional support vector machine , which are sensitive to noise and outliers , and poor performance of differentiation of valid samples are overcome . Experimental data show that compared with the traditional support vector machine which only uses the distance between the sample and the center of class , this new algorithm can improve the convergence speed , and thus greatly enhance the discrimination between valid samples and noise samples . 

Extending NLP tools repositories for the interaction with language data resources repositories
This short paper presents some motivations behind the organization of the ACL / EACL01 " Workshop on Sharing Tools and Resources for Research and Education " , concentrating on the possible connection of Tools and Resources repositories . Taking some papers printed in this volume and the ACL Natural Language Software Registry as a basis , we outline some of the steps to be done on the side of NLP tool repositories in order to achieve this goal . 

Consistency Analysis of NLP Approaches for a Conference Reviewer - Manuscript Match - Making System
Peer - review process is an important part of scholarly communication . The quality of a conference also depends on its peer - review process . The selection of a competent reviewer to review a submitted manuscript in a conference , is a crucial facet of a peer - review process . This selection relies on the adopted match - making approach along with the constraint optimization reviewer allocation algorithm . The match - making approach needs to be consistent with its decision of selection and allocation of the reviewers . In this work , we proposed a framework for evaluating the consistency of various standard NLP approaches that are used for match - making process in a conference . The consistency analysis was performed over a real multi - tracked conference organized in 2019 . We showed that the Contextual Neural Topic Modeling ( CNTM ) with word embedding technique was most consistence among all the 13 approaches that we chose to analyze . 

Corrigendum : Algorithm 902 : GPOPS , a MATLAB software for solving multiple - phase optimal control problems using the gauss pseudospectral method
An algorithm is described to solve multiple - phase optimal control problems using a recently developed numerical method called the Gauss pseudospectral method . The algorithm is well suited for use in modern vectorized programming languages such as FORTRAN 95 and MATLAB . The algorithm discretizes the cost functional and the differential - algebraic equations in each phase of the optimal control problem . The phases are then connected using linkage conditions on the state and time . A large - scale nonlinear programming problem ( NLP ) arises from the discretization and the significant features of the NLP are described in detail . A particular reusable MATLAB implementation of the algorithm , called GPOPS , is applied to three classical optimal control problems to demonstrate its utility . The algorithm described in this article will provide researchers and engineers a useful software tool and a reference when it is desired to implement the Gauss pseudospectral method in other programming languages . 

Augmenting Case Based Learning With Dynamic Language Models
This paper describes a novel supporting tool for the case - based learning ( CBL ) . Recent advances in deep - learning based language models ( LMs ) have enabled highly dynamic interactivity in dialog services and story generation . We leverage the progress in modelling language to develop a technologically augmented CBL pedagogy which we analyze with a standardized assessment . Our assessment shows reasonable case interactivity , low rates of factual inaccuracy , and no inappropriate machine - sourced responses . We also compare our assessment results across the case categories of Ethics , Chemistry , Biology , and Medicine , but find no statistically significant differences . In summary , we develop a framework for analyzing the ability of LMs to augment CBL , apply this framework to the GPT - 3 LM , and discuss some of the challenges and potential solutions to ensuring proper usage in the classroom environment . 

Nonlinear optimal power flow by a noninterior - point method based on Chen - Harker - Kanzow NCP - functions
The solution of nonlinear optimal power flow ( OPF ) problems by interior - point ( IP ) and noninterior - point ( NIP ) methods for nonlinear programming ( NLP ) is described . The mathematical development of both algorithms is based on a standard form NLP problem . A distinctive feature of the NIP method is that , in contrast to IP methods , it can start from arbitrary points . Test results are reported . 

Bangla News Classification using Graph Convolutional Networks
Online Bangla news has rapidly increased in the era of the information age . Each news site has its different categorization for grouping their news . The Layout and categorization of the online Bangla news articles cannot perpetually meet the individual ' s needs due to the heterogeneity . So , overcoming this issue and classifying the online Bangla news articles according to the preference of the user is an arduous task . So , it is essential to provide state - of - the - art solutions as well as the best way to solve this problem . The paper aims to build an automated system to classify the Bangla news contents and also find out the state - of - the - art solutions for the small size dataset . It is known that most of the machine learning models need huge amounts of data for the proper training and testing of the models . But due to the scarcity of the dataset , it is not always possible to provide the state - of - the - art solutions . But , in this research work , we have found that Text - GCN performed better than the BiLSTM , GRU - LSTM , LSTM , Char - CNN , and BERT to classify the online Bangla news in spite of the small size of the dataset . The obtained experimental result shows the efficiency of the Text - GCN over the other models in terms of accuracy , precision recall , and F1 - score . 

Question Answering Chatbot using Deep Learning with NLP
In spite of the number of techniques , models and datasets , Question Answering is still an exacting problem because of the issues in understanding the question and extracting the correct answer . It refers to creating platforms that when given a question in a natural language by humans , can automatically answer it . While many information retrieval chatbots achieve the task , recently , deep learning has earned a lot of attention to question answering due to its capability to learn optimal representation for the given task . This paper aims to build a closed domain , factoid Question Answering system . We recruit NLP methods of pattern matching and information retrieval to create an answer candidate pool . Before scoring similarities between the question and answers , we map them into some feature space . Our approach solves this task through distributional representations of the words and sentences wherein encodings store their lexical , semantic , and syntactic aspects . We use a convolutional neural network architecture to rank these candidate answers . Our model learns an optimal representation for the input question and answer sentences and a matching function to relate each such pair in a supervised manner from training data . Our model does not require any manual feature engineering or language sensitive data ; hence can be extended to various domains . Training and testing on TREC QA , a Question Answering dataset , showed very promising metrics for our model . 

Evaluating Machine Learning Algorithms For Bengali Fake News Detection
In this world of modern technologies and media , online news publications and portals are increasing at a high speed . That is why , nowadays , it has become almost impossible to check out the traditional fact of news headlines and examine them due to the increase in the number of content writers , online media portals , and news portals . Mostly , fake headlines are filled with bogus or misleading content . They attract the commoners by putting phony words or misleading fraudulent content in the headlines to increase their views and share . But , these fake and misleading headlines create havoc in the commoner ' s life and misguide them in many ways . That is why we took a step so that the commoners can differentiate between fake and real news . We proposed a model that can successfully detect whether the story is fake or accurate based on the news headlines . We created a novel data set of Bengali language and achieved our aim and reached the target using the Gaussian Naive Bayes algorithm . We have used other algorithms , but the Gaussian Naive Algorithm has performed well in our model . This algorithm used a text feature dependent on TF - IDF and an Extra Tree Classifier to choose the attribute . In our model , using Gaussian Naive Bayes we got 87 % accuracy which is comparatively best than any other algorithm we used in this model . 

Predictive Technique To Improve Classification On Continuous System Deployment
We introduce Last in First Focus ( LIFF ) approach for any system deployment . As per current market need most of the system deployment follow continues integration ( CI ) and continues deployment ( CD ) approach . In CI / CD , regular system release comes for deployment and it requires lot of post system deployment focus on last deployed or updated system . Continuous development , delivery and deployment is one of the essential parts of a product lifecycle . With addition of new features or major redesign with latest technology e . g . containerization , there may be sudden increase in tickets , which need to be handled effectively . When system does not perform expected behavior then system monitory module generates tickets . These tickets need to be analyzed and forward to corresponding department for resolution . With increase in number of tickets , more human resources may be required to perform this task . Here machine learning can provide helping hand in this job via analyzing the ticket and predicting the impacted system component for the resolution of ticket . The dataset collected after deployment of redesigned system component may be imbalanced due to more tickets on one system component only . This impacts the accuracy of classification metrics . This article resolved this problem by using Algorithmic level solution or Data level solution . Data level solution has achieved by oversampling of minority classes or under sampling of majority classes . Synthetic Minority Oversampling Technique , or SMOTE is used to address imbalanced dataset problem . 

A Survey of Automatic Text Summarization Technology Based on Deep Learning
With the rapid development of the Internet , the amount of network text data is increasing day by day . It is increasingly becoming a challenge to quickly mine useful information from massive amounts of text data . The emergence of automatic summarization technology provides new ideas and methods for solving this problem . Compared with extractive summarization model , abstractive summarization model more closely resembles the process of human summarization , giving it important research significance . In recent years , with the development of deep learning methods , text summarization technology based on deep learning has made unprecedented breakthroughs . Based on the current mainstream sequence - to - sequence framework , we summarize the state - of - the - art abstractive summarization models , compare the advantages of different models and applicable scenarios , and provide a clear context for researchers in related fields . Furthermore , we also make statistics on the Chinese and English datasets . Finally , we put forward some thoughts on the common problems in the field of automatic text summarization . 

Algorithmic Framework for QoS and TE in Virtual SDN Services
Network Virtualization ( NV ) and Software Defined Networking ( SDN ) bring new business opportunities for network providers allowing them to offer the network as a service over their network infrastructure . One challenge for network providers is to provision virtual SDN services for multiple tenants while satisfying QoS and traffic engineering ( TE ) objectives of the higher priced SDN services , allocating the network resources cost effectively and managing the virtual services that span over customer premises , wide area and cloud domains . In this paper , we describe an algorithmic framework that includes QoS management and traffic - engineering methods for managing virtual services over large SDN networks . The algorithmic framework is part of an intelligent service management architecture which employs optimization algorithms , machine learning and artificial intelligence techniques into the management of virtual services over SDN networks . 

Re - ranking algorithms for name tagging
Integrating information from different stages of an NLP processing pipeline can yield significant error reduction . We demonstrate how re - ranking can improve name tagging in a Chinese information extraction system by incorporating information from relation extraction , event extraction , and coreference . We evaluate three state - of - the - art re - ranking algorithms ( MaxEnt - Rank , SVMRank , and p - Norm Push Ranking) , and show the benefit of multi - stage re - ranking for cross - sentence and cross - document inference . 

Fall of Giants : How popular text - based MLaaS fall against a simple evasion attack
The increased demand for machine learning applications made companies offer Machine - Learning - as - a - Service ( MLaaS ) . In MLaaS ( a market estimated 8000M USD by 2025 ) , users pay for well - performing ML models without dealing with the complicated training procedure . Among MLaaS , text - based applications are the most popular ones ( e . g . , language translators ) . Given this popularity , MLaaS must provide resiliency to adversarial manipulations . For example , a wrong translation might lead to a misunderstanding between two parties . In the text domain , state - of - the - art attacks mainly focus on strategies that leverage ML models ' weaknesses . Unfortunately , not much attention has been given to the other pipeline ' stages , such as the indexing stage ( i . e . , when a sentence is converted from a textual to a numerical representation ) that , if manipulated , can significantly affect the final performance of the application . In this paper , we propose a novel text evasion technique called “ Zero - Width attack ” ( ZeW ) that leverages the injection of human non - readable characters , affecting indexing stage mechanisms . We demonstrate that our simple yet effective attack deceives MLaaS of “ giants ” such as Amazon , Google , IBM , and Microsoft . Our case study , based on the manipulation of hateful tweets , shows that out of 12 analyzed services , only one is resistant to our injection strategy . We finally introduce and test a simple input validation defense that can prevent our proposed attack . 

Gender Recognition from Facial Images using Local Gradient Feature Descriptors
Local gradient feature descriptors have been proposed to calculate the invariant feature vector . These local gradient methods are very fast to compute the feature vector and achieved very high recognition accuracy when combined with the support vector machine ( SVM ) classifier . Hence , they have been proposed to solve many problems in image recognition , such as the human face , object , plant , and animal recognition . In this paper , we propose the use of the Haar - cascade classifier for the face detection and the local gradient feature descriptors combined with the SVM classifier to solve the gender recognition problem . We detected 4 , 624 face images from the ColorFERET dataset . The face images data used in gender recognition included 2 , 854 male and 1 , 770 female images , respectively . We divided the dataset into train and test set using 2 - fold and 10 - fold cross - validation . First , we experimented on 2 - fold cross - validation , the results showed that the histogram of oriented gradient ( HOG ) descriptor outperforms the scale - invariant feature transform ( SIFT ) descriptor when combined with the support vector machine ( SVM ) algorithm . The accuracy of the HOG + SVM and the SIFT + SVM were 96 . 50 % and 95 . 98 % . Second , we experimented on 10 - fold cross - validation and the SIFT + SVM showed high performance with an accuracy of 99 . 20 % . We discovered that the SIFT + SVM method needed more training data when creating the model . On the other hand , the HOG + SVM method provided better accuracy when the training data was insufficient . 

Speech input - output system in Indian farming sector
In this research paper , we have developed a Speech input - output System in Indian language that can be useful in bridging the gap between lab and land . This voice enabled system will be very helpful to Indian farmers to retrieve the cultivation information like availability of seeds , fertilizers , pesticides and methods of farming . The speech input - output system is very beneficial in two angles for Indian cultivation domain , first it needs the input query of farmers in voice mode which completely avoids the end users to be literate persons and secondly the response of the system also in voice mode which is more comprehensive for every class of end - user . Speech input - output system was developed using CMU Sphinx4 for understanding the enduser query and speech synthesis was achieved by playing the prerecorded sound files for providing the response to the spoken query . We have collected the speech corpus related to Indian cultivation for this research work in Odia language which is one Indian official language . The performance evaluation of the system is measured in terms of percentage of word accuracy and word error rate as well as using mean opinion score ( MOS ) test . The word accuracy for known and new users is found to be 86 . 87 % and 75 . 13 % respectively . The MOS test for speech output is found as 4 . 62 on an average . 

XiSTS : XML in speech technology systems
This paper describes the use of XML in three generic interacting speech technology systems . The first , a phonological syllable recognition system , generates feature - based finite - state automaton representations of phonotactic constraints in XML . It employs axioms of event logic to interpret multilinear representations of speech utterances and outputs candidate syllables to the second system , an XML syllable lexicon . This system enables users to generate their own lexicons and its default lexicon is used to accept or reject the candidate syllables output by the speech recognition system . Furthermore its XML representation facilitates its use by the third system which generates additional lexicons , based on different feature sets , by means of a transduction process . The applicability of these alternative feature sets in the generation of synthetic speech can then be tested using these new lexicons . 

Total recall , language processing , and software engineering
A broad class of software engineering problems can be generalized as the " total recall problem " . This short paper claims that identifying and exploring the total recall problems in software engineering is an important task with wide applicability . To make that case , we show that by applying and adapting the state of the art active learning and natural language processing algorithms for solving the total recall problem , two important software engineering tasks can also be addressed : ( a ) supporting large literature reviews and ( b ) identifying software security vulnerabilities . Furthermore , we conjecture that ( c ) test case prioritization and ( d ) static warning identification can also be generalized as and benefit from the total recall problem . The widespread applicability of " total recall " to software engineering suggests that there exists some underlying framework that encompasses not just natural language processing , but a wide range of important software engineering tasks . 

Efficient text summarization using lexical chains
The rapid growth of the Internet has resulted in enormous amounts of information that has become more difficult to access efficiently . Internet users require tools to help manage this vast quantity of information . The primary goal of this research is to create an efficient and effective tool that is able to summarize large documents quickly . This research presents a linear time algorithm for calculating lexical chains which is a method of capturing the “ aboutness ” of a document . This method is compared to previous , less efficient methods of lexical chain extraction . We also provide alternative methods for extracting and scoring lexical chains . We show that our method provides similar results to previous research , but is substantially more efficient . This efficiency is necessary in Internet search applications where many large documents may need to be summarized at once , and where the response time to the end user is extremely important . 

Adaptation of IDPT System Based on Patient - Authored Text Data using NLP
Background : Internet - Delivered Psychological Treatment ( IDPT ) systems have the potential to provide evidence - based mental health treatments for a far - reaching population at a lower cost . However , most of the current IDPT systems follow a tunnel - based treatment process and do not adapt to the needs of different patients' . In this paper , we explore the possibility of applying Natural Language Processing ( NLP ) for personalizing mental health interventions . Objective : The primary objective of this study is to present an adaptive strategy based on NLP techniques that analyses patient - authored text data and extract depression symptoms based on a clinically established assessment questionnaire , PHQ - 9 . Method : We propose a novel word - embedding ( Depression2Vec ) to extract depression symptoms from patient authored text data and compare it with three state - of - the - art NLP techniques . We also present an adaptive IDPT system that personalizes treatments for mental health patients based on the proposed depression symptoms detection technique . Result : Our results indicate that the performance of proposed embedding Depression2Vec is comparable to WordNet , but in some cases , the former outperforms the latter with respect to extracting depression symptoms from the patient - authored text . Conclusion : Although the extraction of symptoms from text is challenging , our proposed method can effectively extract depression symptoms from text data , which can be used to deliver personalized intervention . 

Part - of - speech tagger based on maximum entropy model
The maximum entropy ( ME ) conditional models don ' t force to adhere to the independence assumption such as in Hidden Markov generative models , and thus the ME - based part - of - speech ( POS ) tagger can depend on arbitrary , non - independent features , which are benefit to the POS tagging , without accounting for the distribution of those dependencies . Since ME models are able to flexibly utilize a wide variety of features , the sparse problem of training data is efficiently solved . Experiments show that the POS tagging error rate is reduced by 54 . 25 % in close test and 40 . 56 % in open test over the hidden - markov - Model - based baseline , and synchronously an accuracy of 98 . 01 % in close test and 95 . 56 % in open test are obtained . 

Realization of a high performance bilingual OCR system for Thai - English printed documents
This paper presents a high performance bilingual OCR system for printed Thai and English text . With the complex nature of both Thai and English languages , the first stage is to identify languages within different zones by using geometric properties for differentiation . The second stage is the process of character recognition , in which the technique developed includes a feature extractor and a classifier . In the feature extraction , the thinned character image is analyzed and categorized into groups . Next , the classifier will take in two steps of recognition : the coarse level , followed by the fine level with a guide of decision trees . As to obtain an even better result , the final stage attempts to make use of dictionary look - up as to check for accuracy improvement in an overall performance . For verification , the system is tested by a series of experiments with printed documents in 141 pages and over 280 , 000 characters , the result shows that the system could obtain an accuracy of 100 % in Thai monolingual , 98 . 18 % in English monolingual , and 99 . 85 % in bilingual documents on the average . In the final stage with a dictionary look - up , the system could yield a better accuracy of improvement up to 99 . 98 % in bilingual documents as expected . 

Generating formal system models from natural language descriptions
The initial starting point of each design process usually is given by means of a textual specification provided in a natural language . However , the process of creating an accurate and complete formal representation has always been a bottleneck in the design . Manually generating such a formal description from the specification is expensive , requires significant time , and a large number of well - trained design and verification engineers . Usually , only humans with expert design knowledge are assumed to have the ability to properly interpret the respective specification documents . 

An Optimized Customers Sentiment Analysis Model Using Pastoralist Optimization Algorithm ( POA ) and Deep Learning
Users usually express their sentiment online which influences purchased products and services . The computational study of people ' s feelings and thoughts on entities is known as sentiment analysis . The Long Short - Term Memory ( LSTM ) model is one of the most common deep learning models for solving sentiment analysis problems . However , they possess some drawbacks such as longer training time , more memory for training , easily over fits , and sensitivity to randomly generated parameters . Hence , there is a need to optimize the LSTM parameters for enhanced sentiment analysis . This paper proposes an optimized LSTM approach using a newly developed novel Pastoralist Optimization Algorithm ( POA ) for enhanced sentiment analysis . The model was used to analyze sentiments of customers retrieved from Amazon product reviews . The performance of the developed POA - LSTM model shows an optimal accuracy , precision , recall and F1 measure of 77 . 36 % , 85 . 06 % , 76 . 29 % , and 80 . 44 % respectively , when compared with LSTM model with 71 . 62 % , 78 . 26 % , 74 . 23 % , and 76 . 19 % respectively . It was also observed that POA with 20 pastoralist population size performs better than other models with 10 , 15 , 25 and 30 population size . 

Learning Context Using Segment - Level LSTM for Neural Sequence Labeling
This article introduces an approach that learns segment - level context for sequence labeling in natural language processing ( NLP ) . Previous approaches limit their basic unit to a word for feature extraction because sequence labeling is a tokenlevel task in which labels are annotated word - by - word . However , the text segment is an ultimate unit for labeling , and we are easily able to obtain segment information from annotated labels in a IOB / IOBES format . Most neural sequence labeling models expand their learning capacity by employing additional layers , such as a character - level layer , or jointly training NLP tasks with common knowledge . The architecture of our model is based on the charLSTM - BiLSTM - CRF model , and we extend the model with an additional segment - level layer called segLSTM . We therefore suggest a sequence labeling algorithm called charLSTM - BiLSTMCRF - segLSTMsLM which employs an additional segment - level long short - term memory ( LSTM ) that trains features by learning adjacent context in a segment . We demonstrate the performance of our model on four sequence labeling datasets , namely , Peen Tree Bank , CoNLL 2000 , CoNLL 2003 , and OntoNotes 5 . 0 . Experimental results show that our model performs better than state - of - theart variants of BiLSTM - CRF . In particular , the proposed model enhances the performance of tasks for finding appropriate labels of multiple token segments . 

An experiment of word sense disambiguation in a machine translation system
In this paper , we demonstrate an experiment of a machine translation ( MT ) system for two different languages , English and Persian . We also describe a model for word sense disambiguation ( WSD ) task inside the MT system , which uses decision trees automatically learned from a training data set , as its disambiguation formalism . Our evaluations can be divided into two different categories : evaluation on the whole MT system and evaluation on the WSD component . The experiments on the whole MT , shows that this system gets 16 % with respect to NIST measure , while the evaluation on WSD using a corpus contains 860 aligned sentences shows that this component disambiguates 81 . 4 % of ambiguous word correctly . 

Model Checking a Rule - based Parser
In the field of natural language processing , one key difficulty for rule - based systems is the debugging and tuning of rules . In this paper we suggest a novel method that use model checking tools to theoretically verify the rule system of a feature - based parser . We discuss the modeling of the system : the establishment of Kripke structures , and most importantly , the method to compress the state space of the model . We use partial Kripke structures as a main tool for space compression . We show that the state space can be reduced considerably while the compressed model still keeps similar behavior as the non - compressed one . Initial examples of specifications are presented and certain restrictions on them are discussed . 

Commonsense - based topic modeling
Topic modeling is a technique used for discovering the abstract ' topics ' that occur in a collection of documents , which is useful for tasks such as text auto - categorization and opinion mining . In this paper , a commonsense knowledge based algorithm for document topic modeling is presented . In contrast to probabilistic models , the proposed approach does not involve training of any kind and does not depend on word co - occurrence or particular word distributions , making the algorithm effective on texts of any length and composition . ' Semantic atoms ' are used to generate feature vectors for document concepts . These features are then clustered using group average agglomerative clustering , providing much improved performance over existing algorithms . 

Knowledge Base Collecting Using Natural Language Processing Algorithms
Natural language processing ( NLP ) is one of the most complicated and fast developing area in Computer Science . There are solutions in this area for special cases , but developing one general solution is impossible due to variety of grammatical , syntactic and semantic forms in different languages . The NLP algorithms and methods are used in speech recognition , text analyzing and understanding , speech generation . This paper is focused on application of NLP approaches to understand quasi - structured or unstructured data with subsequent inclusion in a knowledge base . The article covers the usage of a graph database as a knowledge base , that allows to show and visualize relationships between different pieces of text according to specified data patterns . 

A Data Viz Platform as a Support to Study , Analyze and Understand the Hate Speech Phenomenon
In this paper we present a data visualization platform designed to support the Natural Language Processing ( NLP ) scholar to study and analyze different corpora collected with the purpose to understand the hate speech phenomenon in social media . The project started with the creation of a corpus which collects tweets addressed to specific groups of ethnic minorities considered very controversial in the Italian public debate . Each tweet has been manually tagged with a series of attributes in order to capture the different features used to characterize the hate speech phenomenon . This corpus is mainly built to be used for training an automatic classifier and helping us in its testing and validation , before being it adopted to detect tweets targeted as hate speech on larger scale datasets . As opposed as many other traditional machine learning tasks , to build a good classifier achieving high scores in terms of accuracy is very challenging in such scenario , because of the intrinsic ambiguity of the language , the lack of a proper and explicable context in social media , and the attitude of on line users of being sarcastic and ironical . Therefore , in order to properly validate an effective feature selection process , correlations between selected attributes must be studied and analyzed . This motivated us to build an interactive platform to explore data in our corpora across the dimensions that have been used to characterize collected tweets . In our paper , after a brief introduction of the hate speech dataset , we will show how the dashboard can fit into the NLP pipeline , and how its architecture can be structured . Finally , we will present some of the challenges we have faced to visualize data with spatial , temporal and numerical attributes . 

Detailed Study of Deep Learning Models for Natural Language Processing
Natural Language Processing involves computational processing , and understanding of human languages . With the increase in computation power , deep learning models are being used for various NLP tasks . Further availability of large datasets of various languages enables the training of deep learning models . Multiple processing layers are used by the deep learning methods for learning representations of data which are hierarchical in nature and which gives excellent results for different NLP tasks . This paper reviews the important models and methods in deep learning which are applied to natural language problems . In particular , Convolutional Neural Network , Recurrent Neural Network , Long Short - Term Memory , Gated Recurrent Unit , Recursive Neural Network have been described . Also , their advantages and suitability to various natural language processing applications such as text classification , sentiment analysis , etc . have been reviewed . 

A survey on abstractive summarization techniques
Text Summarization solves climacteric problems in furnishing information to the necessities of user . Due to explosive growth of digital data on internet , information floods are the results to the user queries . This makes user impractical to read entire documents and select the desirables . To this problem summarization is a novel approach which surrogates the original document by not deviating from the theme helps the user to find documents easily . Summarization area was broadly spread over different research fields , Natural Language Processing ( NLP) , Machine Learning and Semantics etc … Summarization is classified mainly into two techniques Abstract and Extract . This article gives a deep review of Abstract summarization techniques . 

Optimal target aiming maneuver for an advanced aircraft via timescale pseudospectral method
A two - timescale Chebyshev pseudospectral optimal control method , which can reduce the converted nonlinear programming ( NLP ) significantly , is proposed . The optimal aiming maneuver problem of an advanced aircraft , which is a six - degree - of - freedom trajectory problem , is solved by this two - timescale method . It is the first time that the six - degree - of - freedom equations are used to solve optimization of the flight trajectory problems . 

Adaptive multilingual sentence boundary disambiguation
The sentence is a standard textual unit in natual language processing applications . In many language the punctuation mark that indicates the end - of - sentence boundary is ambiguous ; thus the tokenizers of most NLP systems must be equipped with special sentence boundary recognition rules for every new text collection . As an alternative , this article presents an efficient , trainable system for sentence boundary disambiguation . The system , called Satz , makes simple estimates of the parts of speech of the tokens immediately preceding and following each punctuation mark , and uses these estimates as input to a machine learning algorithm that then classifies the punctuation mark . Satz is very fast both in training and sentence analysis , and its combined robustness and accuracy surpass existing techniques . The system needs only a small lexicon and training corpus , and has been shown to transfer quickly and easily from English to other languages , as demonstrated on Franch and German . 

Automatic assessment of spoken modern standard Arabic
Proficiency testing is an important ingredient in successful language teaching . However , repeated testing for course placement , over the course of instruction or for certification can be time - consuming and costly . We present the design and validation of the Versant Arabic Test , a fully automated test of spoken Modern Standard Arabic , that evaluates test - takers ' facility in listening and speaking . Experimental data shows the test to be highly reliable ( test - retest r = 0 . 97 ) and to strongly predict performance on the ILR OPI ( r = 0 . 87 ) , a standard interview test that assesses oral proficiency . 

On the role of lexical features in sequence labeling
We use the technique of SVM anchoring to demonstrate that lexical features extracted from a training corpus are not necessary to obtain state of the art results on tasks such as Named Entity Recognition and Chunking . While standard models require as many as 100K distinct features , we derive models with as little as 1K features that perform as well or better on different domains . These robust reduced models indicate that the way rare lexical features contribute to classification in NLP is not fully understood . Contrastive error analysis ( with and without lexical features ) indicates that lexical features do contribute to resolving some semantic and complex syntactic ambiguities -- but we find this contribution does not generalize outside the training corpus . As a general strategy , we believe lexical features should not be directly derived from a training corpus but instead , carefully inferred and selected from other sources . 

Design and implementation of a Luganda text normalization module for a speech synthesis software program
This paper describes a Luganda text normalization module , a crucial component needed for a Luganda Text to Speech system . We describe the use of a rule - based approach for detection , classification and verbalization of Luganda text . At the core of this module are the Luganda grammar rules that were hand - built to normalize Non - Standard Words ( NSWs ) from different semiotic and noun classes . Input text is first analyzed , matched against handcrafted patterns developed using regular expressions to detect any NSWs . Upon detection , NSWs are tokenized and classified into one of the semiotic classes and then if necessary , into one of the Luganda noun classes . These are subsequently verbalized , each according to its semiotic as well as noun class , and a new text file is produced . We tested the module with 7 datasets and achieved average detection and normalization rates of 82 % and 77 . 7 % respectively . 

Annotated Corpus of Comments and Basic Semantic Analysis
This article presents an annotated corpus of Turkish comment texts gathered from employees . Special attention is given to neutrality of paragraphs in the corpus and quality of the annotation . We employ the majority voting of the annotators . We describe the details of the dataset , the annotation methodology and the experiments with basic methods to investigate the corpus . The corpus has three classes , positive , negative and neutral . 

Using canonical correlation analysis for generalized sentiment analysis , product recommendation and search
Standard Sentiment Analysis applies Natural Language Processing methods to assess an " approval " value of a given text , categorizing it into " negative " , " neutral " , or " positive " or on a linear scale . Sentiment Analysis can be used to infer ratings values for users based on textual reviews of items such as books , films , or products . We propose an approach to generalizing the concept to multiple dimensions to estimate user ratings along multiple axes such as " service " , " price " and " value " . We use Canonical Correlation Analysis ( CCA ) and derive a mathematical model that can be used as a multivariate regression tool . This model has a number of valuable properties : it can be trained offline and used efficiently on live stream of texts like blogs and tweets , can be used for visualization and data clustering and labeling , and finally it can potentially be incorporated into natural language product search algorithms . At the end we propose an evaluation procedure that can be used on live data when a ground truth is not available . Based on this model we present our preliminary results from empirical data that we have collected from our system Opinion Space1 . We show that for this dataset the CCA model outperforms the PCA that was originally used in Opinion Space . 

Corpus Development for Dzongkha Automatic Speech Recognition
Speech recognition technology has been very popular and reached an advanced stage in computational linguistics that enables the recognition and transcription of spoken language into text by computers . However , the research on automatic speech recognition ( ASR ) for Dzongkha is the first of its kind and none has carried out any research and development on Dzongkha language ; whereby making the data resource preparation information very limited . More than 11000 Dzongkha utterances of raw data are collected from Dzongkha Development Commission . The data wrangling is carried out on the limited data and the Dzongkha ASR corpus has been developed . In corpus , a total of 10566 usable utterances is prepared which includes Dzongkha text , audio recordings , and transcriptions . The pronunciation dictionary ( lexicon ) is generated which contains a total of 12605 unique Dzongkha words ; and also , a phone list is recorded using Dzongkha phonemic inventory table for the representation of the smallest unit of the sound at the phoneme level . Such speech corpus is the primary requirement for developing the new speech recognition system for Dzongkha language . 

Obtaining chinese semantic knowledge from online encyclopedia
This paper proposes a method to obtain the semantic knowledge from an online encyclopedia called Hudong encyclopedia 2 ( hudong baike ) . We obtain concepts and then their semantic related concepts and compute the semantic relatedness by utilizing inner hyperlinks and the open category information in Hudong encyclopedia . By comparing our results with human judgments , we show that our relatedness computing method is quite effective . 

Two perspectives on software documentation quality in stack overflow
This paper studies the software documentation quality in Stack Overflow from two perspectives : the questioners ’ who are accepting answers and the community ’ s who is voting for answers . We show what developers can do to increase the chance that their questions or answers get accepted by the community or by the questioners . We found different expectations of what information such as code or images should be included in a question or an answer . We evaluated six different quality indicators ( such as Flesch Reading Ease or images ) which a developer should consider before posting a question and an answer . In addition , we found different quality indicators for different types of questions , in particular error , discrepancy , and how - to questions . Finally we use a supervised machine - learning algorithm to predict when an answer will be accepted or voted . 

Multi - label Classification for Marine Science and Technology Literature
We propose a deep - learning based multi - label text classification model F - CNN for marine science and technology documents . We have set up a data set that covers about 300 thousand marine related fields for multi - label text classification . In order to solve the problem of disequilibrium of class distribution of data sets , we propose two ways of data enhancement which improves the efficiency of the model . Comparing with traditional text classification methods , the new F - CNN model optimizes the structure of traditional Text - CNN models according to the characteristics of the paper data , and has good classification effect and high execution efficiency in multi - label classification of document data . 

An open distance learning web - course for NLP in IR
A computer - based course addressing the topic of applying Natural Language resources and techniques to Information Retrieval is presented . The course provides several Internet on - line resources to support a learning by doing approach in a real world context . Rationale for the design of the course is presented and a detailed description of the course structure and content is given . 

Hybrid Method of Selective Nonlinear Precoding and Interference Alignment
Interference alignment ( IA ) is a well known technique to boost the system capacity of MIMO interfering broadcast channels ( IBCs ) by aligning interferences into spatial dimensions , which are almost orthogonal to the dimensions occupied by the desired signals . However , the conventional IA algorithms with iterations suffer from large convergence times . We propose a novel hybrid method of nonlinear precoding ( NLP ) and IA for more efficient interference cancellation . Firstly , we modify the minimum mean squared error IA scheme by ignoring some interference . Then , we selectively apply the Tomlinson - Harashima Precoding ( THP ) NLP on top of selected IA beams . It is shown through the simulations that the bit error rate performances for all MIMO links with or without NLP are improved dramatically . Meanwhile , the computational complexity in terms of number of iterations is reduced significantly . In addition , to maximize the capacity and overcome the unfairness inherent in THP , that is the latter coded stream has smaller diversity gain , we propose to repeat the low - cost modified IA with random initializations . The optimal precoders and decoders are selected among the repetitions with the largest determinant factor related to the effective channel over the selected IA beams . 

Using an ontology for improved automated content scoring of spontaneous non - native speech
This paper presents an exploration into automated content scoring of non - native spontaneous speech using ontology - based information to enhance a vector space approach . We use content vector analysis as a baseline and evaluate the correlations between human rater proficiency scores and two cosine - similarity - based features , previously used in the context of automated essay scoring . We use two ontology - facilitated approaches to improve feature correlations by exploiting the semantic knowledge encoded in WordNet : ( 1 ) extending word vectors with semantic concepts from the WordNet ontology ( synsets ) ; and ( 2 ) using a reasoning approach for estimating the concept weights of concepts not present in the set of training responses by exploiting the hierarchical structure of WordNet . Furthermore , we compare features computed from human transcriptions of spoken responses with features based on output from an automatic speech recognizer . We find that ( 1 ) for one of the two features , both ontologically based approaches improve average feature correlations with human scores , and that ( 2 ) the correlations for both features decrease only marginally when moving from human speech transcriptions to speech recognizer output . 

Stock Price Prediction Based on LSTM Neural Network : the Effectiveness of News Sentiment Analysis
This paper retrieves news articles from the New York Times and conducts sentiment analysis for news headline and text body , then combine quantitative sentiment score with stock historical stock basic features together , using LSTM neural network to predict both future stock close price and stock return . The main purpose is to compare the prediction result of model which includes sentiment factors with that only considers historical stock basic features . LSTM neural network shows a good ability in long - term prediction , the experiment is based on LSTM model for three representative large companies in the US . The final results confirm that the prediction accuracy is higher for model that consider sentiment influence from website news article . 

LAS : Language Agnostic System for Question Answering
In this article we present a deep learning question answering ( QA ) setting that can work for any natural language . We recognize the problem of low - resource languages , i . e . most languages other than English , which lack appropriately sized datasets or cutting - edge NLP tools . To address this problem , we have designed and implemented a QA dataset and system that are independent from language use ; specifically , we test our solution on the Polish language which is both low - resource and grammatically complex . Both these features make the task of QA significantly harder . To the best of our knowledge , this is the first attempt to train a deep learning QA system in a language - agnostic setting . 

Character - level Adversarial Examples in Arabic
Several adversarial attacks have been pro - posed in the domains of computer vision and natural language processing ( NLP ) . However , most attacks in the NLP domain have been applied to evaluate deep neural networks ( DNNs ) that were trained on English corpora . This paper proposes the first set of character - level adversarial attacks designed for models trained on Arabic . We present an efficient method to generate character - level adversarial examples against neural classifiers . Our method relies on flip operations that were designed based on the most common spelling mistakes that non - native Arabic learners make . We find that only a few manipulations are needed to mislead powerful and popular DNN - based classifiers trained on Arabic corpora . 

Prediction Model for Amphetamine Behaviors Based on Bayes Network Classifier
This paper focus to present a prediction model for drug addiction of the accused in type 1 drug abuse case as amphetamine . Case studies in the Suan Phueng police station area Ratchaburi province . The data set that used for modeling is obtained from the collection from Suan Phueng police station from 2016 - 2018 . The data set 1 , 598 items consist of gender , age , number of offenses , education status , nationality , occupation , and non - drug abuse . For our contribute a prediction model into two classes as “ take ” and “ untake ” by using data mining techniques namely Bayes Network classifier . In our experimental , a group of bayes are used to comparison such as Bayes Network , Naive Bayes and Naive Bayes Updateable . The results displayed the Bayes Network classifier shown the highest accuracy rate , Naive Bayes and Naive Bayes Updateable with 81 . 53 , 80 . 85 and 80 . 85 respectively . 

Automatic summarisation of legal documents
We report on the SUM project which applies automatic summarisation techniques to the legal domain . We describe our methodology whereby sentences from the text are classified according to their rhetorical role in order that particular types of sentence can be extracted to form a summary . We describe some experiments with judgments of the House of Lords : we have performed automatic linguistic annotation of a small sample set and then hand - annotated the sentences in the set in order to explore the relationship between linguistic features and argumentative roles . We use state - of - the - art NLP techniques to perform the linguistic annotation using XML - based tools and a combination of rule - based and statistical methods . We focus here on the predictive capacity of tense and aspect features for a classifier . 

Information credibility analysis of web content
General users write daily news about themselves and post information they consider interesting as digital documents for blogs and SNS . Such digital content includes both valuable information as well as worthless , false , and demagogic information . Ordinary web search engines can display web pages in a particular order . The ranking method evaluates the score of web content and generates a ranked list . The top - ranked web content on search engines is often relevant to the user ' s query , though , in some cases , the content may not be credible or valuable . Nevertheless , readers often trust the authenticity of the displayed information . Even if users believe that the content is useful , the search engine cannot evaluate the retrieved digital content , and users have to retrieve a variety of content using different keywords . The need for an information analysis technology that helps find credible and valuable information from large amounts of Web content is progressively growing . In Japan , the NICT ( National Institute of Information and Communications Technology ) initiated the ' ' Information Credibility Criteria Project ' ' in 2oo6 , and the MIC ( Ministry of Internal Affairs and Communications ) , too , initiated the ' ' Research and Development of Information Credibility Verification Technology for Telecommunication Service ' ' in 2007 . The NICT ' s project addresses the issue of information credibility by analyzing credibility based on the following criteria : ( 1 ) content , ( 2 ) sender , ( 3 ) appearance , and ( 4 ) authenticity of content . We believe that the understanding of texts by a machine is important and that an NLP ( Natural Language Processing ) approach is very effective in evaluating the credibility criteria . The MIC ' s project aims to develop methods to analyze not only text information but also multimedia content using NLP , information retrieval and data mining approaches . By using different methods for analyzing the information credibility criteria , credible information can be acquired , which eventually becomes valuable knowledge . This talk will throw light on the activities of both projects in Japan . 

Age Inference on Twitter using SAGE and TF - IGM
Social media is increasingly influential in day - to - day life . People are more than ever sharing , posting , liking , and following different activities on disparate social media . Deriving specific attributes of users based on their online behavior is a growing research field . In this study , a novel methodology is proposed for determining the age of Twitter users . We classify three separate age groups , namely , 18 - - 24 , 25 - - 54 , 55 > . We compute numerous linguistic features from the tweets of users , obtain significant terms extracted by the SAGE algorithms , and retrieve relevant meta - data of users by extracting information on their followed interests on Twitter using TF - IGM . The final logistic regression model obtains a macro F1 - score of 78% . This way , effectively combining NLP and IR techniques for attribute inference on social media . 

NLP based sentiment analysis on Twitter data using ensemble classifiers
Most sentiment analysis systems use bag - of - words approach for mining sentiments from the online reviews and social media data . Rather considering the whole sentence / paragraph for analysis , the bag - of - words approach considers only individual words and their count as the feature vectors . This may mislead the classification algorithm especially when used for problems like sentiment classification . Traditional machine learning algorithms like Naive Bayes , Maximum Entropy , SVM etc . are widely used to solve the classification problems . These machine learning algorithms often suffer from biasness towards a particular class . In this paper , we propose Natural Language ( NLP ) based approach to enhance the sentiment classification by adding semantics in feature vectors and thereby using ensemble methods for classification . Adding semantically similar words and context - sense identities to the feature vectors will increase the accuracy of prediction . Experiments conducted demonstrate that the semantics based feature vector with ensemble classifier outperforms the traditional bag - of - words approach with single machine learning classifier by 3 – 5% . 

Mining bilingual linguistic patterns with aligned and parsed bilingual corpus
Classical grammar for natural languages , which is defined by the linguistics , is widely used in many natural languages processing ( NLP ) tasks , such as information extraction , machine translation and parsing . The classical grammar is well defined but is context free and does not include the complex patterns which contain multiple linguistic units . On the other hand , there are also many simple patterns which are not included in the classical grammar but are useful in the NLP tasks . Therefore , the recognition of special linguistic patterns from natural language is an important step in various NLP systems . We propose an unsupervised method to automatically discover the complex monolingual linguistic patterns from a classically parsed and aligned bilingual corpus . And all the patterns in one language are qualified by the other parallel language . A specialized and efficient algorithm is applied to mine the frequent bilingual subtrees in the forest and the found subtrees are formalized as the linguistic patterns . 

Design and implementation of Chinese words clustering based on atomic - concepts
Cluster analysis is an important technique in statistics ; it is an effective way to find hidden knowledge behind huge amounts of data . Especially now , in one hand , the amount of all kinds of papers is too huge to read ; in the other hand , the extensive use of search engine technology also provides a huge number of words to find all kinds of information , and thus how to find the information from these words using words cluster become a meaningful issue . The method of words cluster in this paper is based on a certain improvement to the previous methods , which is the idea of putting forward atomic concepts , by calculating the similarity between the source words and the atomic concepts and weighted to obtain the clustering space . Then we use the FCM cluster algorithm of Matlab to calculate and achieve relatively good results . 

Detecting structural events for assessing non - native speech
Structural events , ( i . e . , the structure of clauses and disfluencies ) in spontaneous speech , are important components of human speaking and have been used to measure language development . However , they have not been actively used in automated speech assessment research . Given the recent substantial progress on automated structural event detection on spontaneous speech , we investigated the detection of clause boundaries and interruption points of edit disfluencies on transcriptions of non - native speech data and extracted features from the detected events for speech assessment . Compared to features computed on human - annotated events , the features computed on machine - generated events show promising correlations to holistic scores that reflect speaking proficiency levels . 

Improving classification accuracy using automatically extracted training data
Classification is a core task in knowledge discovery and data mining , and there has been substantial research effort in developing sophisticated classification models . In a parallel thread , recent work from the NLP community suggests that for tasks such as natural language disambiguation even a simple algorithm can outperform a sophisticated one , if it is provided with large quantities of high quality training data . In those applications , training data occurs naturally in text corpora , and high quality training data sets running into billions of words have been reportedly used . We explore how we can apply the lessons from the NLP community to KDD tasks . Specifically , we investigate how to identify data sources that can yield training data at low cost and study whether the quantity of the automatically extracted training data can compensate for its lower quality . We carry out this investigation for the specific task of inferring whether a search query has commercial intent . We mine toolbar and click logs to extract queries from sites that are predominantly commercial ( e . g . , Amazon ) and non - commercial ( e . g . , Wikipedia ) . We compare the accuracy obtained using such training data against manually labeled training data . Our results show that we can have large accuracy gains using automatically extracted training data at much lower cost . 

NLP based Machine Learning Approaches for Text Summarization
Due to the plethora of data available today , text summarization has become very essential to gain just the right amount of information from huge texts . We see long articles in news websites , blogs , customers ' review websites , and so on . This review paper presents various approaches to generate summary of huge texts . Various papers have been studied for different methods that have been used so far for text summarization . Mostly , the methods described in this paper produce Abstractive ( ABS ) or Extractive ( EXT ) summaries of text documents . Query - based summarization techniques are also discussed . The paper mostly discusses about the structured based and semantic based approaches for summarization of the text documents . Various datasets were used to test the summaries produced by these models , such as the CNN corpus , DUC2000 , single and multiple text documents etc . We have studied these methods and also the tendencies , achievements , past work and future scope of them in text summarization as well as other fields . 

Multimodular Text Normalization of Dutch User - Generated Content
As social media constitutes a valuable source for data analysis for a wide range of applications , the need for handling such data arises . However , the nonstandard language used on social media poses problems for natural language processing ( NLP ) tools , as these are typically trained on standard language material . We propose a text normalization approach to tackle this problem . More specifically , we investigate the usefulness of a multimodular approach to account for the diversity of normalization issues encountered in user - generated content ( UGC ) . We consider three different types of UGC written in Dutch ( SNS , SMS , and tweets ) and provide a detailed analysis of the performance of the different modules and the overall system . We also apply an extrinsic evaluation by evaluating the performance of a part - of - speech tagger , lemmatizer , and named - entity recognizer before and after normalization . 

A Supporting Tool for Learning to Improve Thinking Skill through Reading Activities
Thinking skill is an important skill required in most of person ' s activities , especially in academic level . This paper proposes a tool to support in learning thinking skills from learning by example of good published articles . The focused concept of the tool is to analyze content expression and relations of the contents as a representation of thinking process . The tool is designed to assist on assigning pre - defined content type - related tags and relation among the chosen types on each clause from the selected articles . From experiments , the results showed that the tool helps to increase learning performance . The average precision and recall scores from tagging of the participants using the tool were higher than the participant not using the tool for 0 . 15 and 0 . 22 , respectively . Moreover , the participants showed significant growth in thinking skills in terms of more correct analysis and critical thinking after using the tool . 

Chinese person name recognition with semantic radical
Radical is the primary component of Chinese character , but its function in Chinese information processing has not yet been well studied . With both theoretical analysis and corpus statistics , this paper presents the correlation between semantic radicals and Chinese characters used in person names . A person name recognition method that uses semantic radicals as computational resources in maximum entropy framework is proposed . This method was contrasted with a baseline method that doesn ' t take radicals into consideration . The experiments results show that semantic radicals can be quite useful in Chinese text processing . 

A Study in the Automation of Service Ticket Recognition using Natural Language Processing
Natural language processing ( NLP ) is a branch of computer science concerned with the understanding of human language and communication , and translating these into a computer - comprehensible embedding . Our goal in this paper is to capture meaning from human natural language through NLP and provide an automated solution for aiding the process of service ticket solving , through the intelligent classification of tickets , pattern recognition and similarities between texts . The difficulty of this task lies in translating the human language into a mathematical format : transforming a non - formal language , into a formal one , without losing any details . Also what raises even more complication is the context in which this language appears : service tickets , that come from a technical and specialized jargon of computer science and IT industry , and the brief manner in which the tickets are written . This paper aims to tackle this challenge through multiple methods of text classification and recognition , and data analysis , followed by comparison and interpretation of the results . In completion , we find that our methods yield plausible results to be implemented in helping the service process . 

Knowledge element analogy relation recognition using text and graph structure
Knowledge element analogy relation is a corresponding relationship in content , function or other aspects between two knowledge elements . This paper proposes a framework of relation Gaussian processes - based learning for knowledge element analogy relation recognition , which can integrate information from text and relation graph structure . Based on terms or core terms co - occurrence and type compatibility , two rules are first developed to construct candidate analogy relation instances from knowledge element set . Next , three kernels are devised to capture information of terms , semantic types and relative positions of two knowledge elements , and graph Laplacian and expectation propagation algorithm are employed to approximate the relation graph structure . Then , these two types of information are integrated to predict analogy relation . Experimental evaluation on four data sets related to ldquocomputerrdquo discipline demonstrates that the rules are effective and integrating three text kernels with relation graph structure can achieve better performance than only text kernels . 

Hidden - variable models for discriminative reranking
We describe a new method for the representation of NLP structures within reranking approaches . We make use of a conditional log - linear model , with hidden variables representing the assignment of lexical items to word clusters or word senses . The model learns to automatically make these assignments based on a discriminative training criterion . Training and decoding with the model requires summing over an exponential number of hidden - variable assignments : the required summations can be computed efficiently and exactly using dynamic programming . As a case study , we apply the model to parse reranking . The model gives an F - measure improvement of ≈ 1 . 25 % beyond the base parser , and an ≈ 0 . 25 % improvement beyond the Collins ( 2000 ) reranker . Although our experiments are focused on parsing , the techniques described generalize naturally to NLP structures other than parse trees . 

NLP - assisted Web Element Identification Toward Script - free Testing
End - to - end test automation is important in modern web application development . However , existing test automation techniques have challenges in implementing and maintaining test scripts . It is difficult to keep correct locators , which test scripts require to identify web elements on web pages . The reason is that locators depend on the metadata in web elements or the structure of each web page . One efficient way to solve the problem of locators is to make test cases written in natural language executable without test scripts . As the first step of script - free testing , we propose a technique to identify web elements to be operated and to determine test procedures by interpreting test cases . The test cases are written in a domain - specific language without relying on the metadata of web elements or the structural information of web pages . We leverage natural language processing techniques to understand the semantics of web elements . We also create heuristic search algorithms to find promising test procedures . To evaluate our proposed technique , we applied it to two open - source web applications . The experimental results show that our technique successfully identified 94 % of web elements to be operated in the test cases . 

Measuring the impact of sense similarity on word sense induction
Word Sense Induction ( WSI ) is an unsupervised learning approach to discovering the different senses of a word from its contextual uses . A core challenge to WSI approaches is distinguishing between related and possibly similar senses of a word . Current WSI evaluation techniques have yet to analyze the specific impact of similarity on accuracy . Therefore , we present a new WSI evaluation that quantifies the relationship between the relatedness of a word ' s senses and the ability of a WSI algorithm to distinguish between them . Furthermore , we perform an analysis on sense confusions in SemEval - 2 WSI task according to sense similarity . Both analyses for a representative selection of clustering - based WSI approaches reveals that performance is most sensitive to the clustering algorithm and not the lexical features used . 

Separable verbs in a reusable morphological dictionary for German
Separable verbs are verbs with prefixes which , depending on the syntactic context , can occur as one word written together or discontinuously . They occur in languages such as German and Dutch and constitute a problem for NLP because they are lexemes whose forms cannot always be recognized by dictionary lookup on the basis of a text word . Conventional solutions take a mixed lexical and syntactic approach . In this paper , we propose the solution offered by Word Manager , consisting of string - based recognition by means of rules of types also required for periphrastic inflection and clitics . In this way , separable verbs are dealth with as part of the domain of reusable lexical resources . We show how this solution compares favourably with conventional approaches . 

Matching needs and resources : how NLP can help theoretical linguistics
While some linguistic questions pose challenges that could be met by developing and applying NLP techniques , other problems can best be approached with a blend of old - fashioned linguistic investigation and the use of simple , well - established NLP tools . Unfortunately , this means that the NLP component is too simple to be of interest to the computationally - minded , while existing tools are often difficult for the programming novice to use . For NLP to come to the aid of research in theoretical linguistics , a continuing investment of effort is required to bridge the gap . This investment can be made from both sides . 

The Power of Regular Expressions in Recognizing Dates and Epochs
The digitization of cultural heritage objects discovered over time is a process that allows librarians , historians and researchers to identify various common elements such as the lifestyle of the inhabitants , their traditions , the distribution of the population by areas , and many other such important points of interest in the study of communities and human behavior throughout history . In order to perform such analyses , one of the most important characteristics to be analyzed is the identification in time of the events in which the actors were involved . Calendar data and epochs play an important role in this direction , but their use is not always easy because they can be expressed in various forms , or they can even be altered from the very data collection process . Therefore , before starting any analysis , the standardization of temporal characteristics is a key step to follow . Unfortunately , neither the use of Machine Translation techniques nor NLP libraries provide adequate support , because by using Machine Translation techniques the context is lost , and NLP libraries often provide support only for texts written in English that comply with the spelling rules of the language . Instead , regular expressions can be adapted to any language . By applying a series of regular expressions through which both calendar da - ta and epochs expressed in centuries and millennia can be identified , it has been possible to standardize approximately 95 % of the 6 , 630 distinct records related to cultural assets published in digital format by the National Heritage Institute of Romania . 

Automatic Extraction of Major Osteoporotic Fractures from Radiology Reports using Natural Language Processing
In this study , we developed a rule - based natural language processing ( NLP ) algorithm for automatic extraction of six major osteoporotic fractures from radiology reports . We validated the NLP algorithm using a dataset of radiology reports from Mayo Clinic with the gold standard constructed by medical experts . The micro - averaged sensitivity , specificity , positive predictive value ( PPV ) , negative predictive value ( NPV ) , and F1 - score of the proposed NLP algorithm are 0 . 796 , 0 . 978 , 0 . 972 , 0 . 831 , 0 . 874 , respectively . The highest F1 - score was achieved at 0 . 958 for the extraction of proximal femur fracture while the lowest was 0 . 821 for the hand and finger / wrists fracture . The experimental results verified the effectiveness of the proposed rule - based NLP algorithm in the automatic extraction of major osteoporotic fractures from radiology reports . 

Measuring the similarity and relatedness of concepts in the medical domain : IHI 2012 tutorial overview
The ability to quantify the degree to which concepts are similar or related to each other is a key component in many Natural Language Processing ( NLP ) and Artificial Intelligence ( AI ) applications . For example , in a document search application , it can be very useful to identify text snippets that contain terms that are similar to ( but not identical ) to those provided by a user . This tutorial will introduce the theory behind measures of semantic similarity and relatedness , and show how these can be applied in the medical domain by using freely - - available open - - source software ( http : / / umls - similarity . sourceforge . net ) ( UMLS :: Similarity ) . This software takes advantage of the Unified Medical Language System ( http : / / www . nlm . nih . gov / research / umls / ) ( UMLS ) , which is maintained by the National Library of Medicine ( USA ) . The tutorial will also show how to evaluate existing measures with manually - - created reference standards . 

An N - gram based Chinese syllable evaluation approach for speech recognition error detection
In order to find errors and correct words after Chinese speech recognition to improve its accuracy rate , an N - gram based phonetic syllable evaluation approach is proposed according to the conjunction rules in Chinese syllables . In this paper , Bigram and Trigram model are used with two kinds of smoothing methods for comparison . Evaluation results show that it can achieve a precision rate up to 72 . 44 % for error detection , which can provide more reliable target for error correction . 

College Enquiry Chatbot using Rasa Framework
The growth of technologies like Artificial Intelligence ( AI ) , Big Data & Internet of Things ( IoT ) , etc . has marked many advancements in the technological world since the last decade . These technologies have a wide range of applications . One such application is “ Chatterbot or “ Chatbot” . Chatbots are conversational AIs , which mimics the human while conversing & eliminates the need of human by automating mundane tasks . In the study undertaken , we have created a chatbot in education domain & it is named as “ College Enquiry Chatbot” , This chatbot is a web - based application that analyses and understands user ' s queries and provides an instant and accurate response . Rasa technology is used to construct this chatbot . It ' s an open - source technology , which uses its two main packages i . e . , Rasa Core & Rasa Natural Language Understanding ( NLU ) in order to build a Contextual AI Chatbot . NLU is used to infer the intent and to extract the necessary entities from user input & the Rasa Core provides the output by building a probabilistic model with the help of Recurrent Neural Network ( RNN ) . Evaluation of the model is done by getting a confusion matrix and performance measures like Precision , Accuracy & F1 Score which come out to be 0 . 628 , 0 . 725 and 0 . 669 respectively on average basis . This chatbot ' s accuracy , lack of dependability on human resources , 24 x 7 accessibility and low maintenance creates various opportunities for its implementation . This conversational agent can not only be used in educational institutions but also in places where enquiry becomes a tedious task . 

BioSentVec : creating sentence embeddings for biomedical texts
Sentence embeddings have become an essential part of today ' s natural language processing ( NLP ) systems , especially together advanced deep learning methods . Although pre - trained sentence encoders are available in the general domain , none exists for biomedical texts to date . In this work , we introduce BioSentVec : the first open set of sentence embeddings trained with over 30 million documents from both scholarly articles in PubMed and clinical notes in the MIMICIII Clinical Database . We evaluate BioSentVec embeddings in two sentence pair similarity tasks in different biomedical text genres . Our benchmarking results demonstrate that the BioSentVec embeddings can better capture sentence semantics compared to the other competitive alternatives and achieve state - of - the - art performance in both tasks . We expect BioSentVec to facilitate the research and development in biomedical text mining and to complement the existing resources in biomedical word embeddings . The embeddings are publicly available at https : / / github . com / ncbi - nlp / BioSentVec . 

Elastic Search in Cache Based Service Management For Healthcare Automation
Healthcare Automation is an area , which involves various services , such as the housekeeping & patient management system . There is a incredible quantity of consideration and focusing to improve health . Big healthcare survey organizations illustrate that sickness and death rate , to a great extent rely on accessing of suitable healthcare systems , which is not accessible to an enormous preponderance of the worldwide inhabitants . This research paper introduce automating healthcare functions such as monitoring , accessing of information of patients by doctors using effective grid search technology , interactive user interface assistance for the doctors and junior doctors for any queries & cache management technology based web application for efficient performance . We elucidate our thoughts on how technology can aid in this vital and crucial life sustaining goings - on . We provide insight hooked on our effort on automating medical sector by means of text mining techniques , Natural Language Processing , Elastic Search , PHP - MVC , Web Services and Cache and include some initial results . 

A morphological analysis of Arabic language based on multicriteria decision making : TAGHIT system
In this paper , we present our work on Arabic morphology and especially the mechanisms for resolving the morphological ambiguity in Arabic text . These researches , which have given birth to TAGHIT system which is a morphosyntactic tagger for Arabic , where the originality of our work lies in the implementation of our internal system of a new approach to disambiguation different from those that currently exist , which is based on the principles and techniques issued from multicriteria decision making . 

Sentiment analysis for Arabic e - commerce websites
Sentiment analysis is a research area where studies focus on the analysis and the processing of opinions found on the web content . It is about finding the polarity of sentiment expressed by internet user in their interaction about a giving subject in order to classify them on positive or negative . In this paper , we propose the implementation of a tool for sentiment analysis able to find the polarity of opinions in reviews extracted from e - commerce magazines and blogs in Arabic language . To do this , we conducted various experiments like testing different techniques of stemming and the performance of some classification algorithms to have the combination that gives us satisfactory results . In spite of the huge difficulties that we found in this research area like the lack of resources in data collection and the complexity of processing the Arabic language dialects , the results were hopeful and satisfying . 

On UNL as the future " html of the linguistic content " & the reuse of existing NLP components in UNL - related applications with the example of a UNL - French deconverter
After 3 years of specifying the UNL ( Universal Networking Language ) language and prototyping deconverters from more than 12 languages and enconverters for about 4 , the UNL project has opened to the community by publishing the specifications ( v2 . 0 ) of the UNL language , intended to encode the meaning of NL utterances as semantic hypergraphs and to be used as a " pivot " representation in multilingual information and communication systems . A UNL document is an html document with special tags to delimit the utterances and their rendering in UNL and in all natural languages currently handled . UNL can be viewed as the future " html of the linguistic content " . It is only an interface format , leading as well to the reuse of existing NLP components as to the development of original tools in a variety of possible applications , from automatic rough enconversion for information retrieval and information gathering translation to partially interactive enconversion or deconversion for higher quality . We illustrate these points by describing an UNL - French deconverter organized as a specific " localizer " followed by a classical MT transfer and an existing generator . 

VTKEL : a resource for visual - textual - knowledge entity linking
To understand the content of a document containing both text and pictures , an artificial agent needs to jointly recognize the entities shown in the pictures and mentioned in the text , and to link them to its background knowledge . This is a complex task , that we call Visual - Textual - Knowledge Entity Linking ( VTKEL ) , which aims at linking visual and textual entity mentions to the corresponding entity ( or a newly created one ) of the agent knowledge base . Solving the VTKEL task opens a wide range of opportunities for improving semantic visual interpretation . For instance , given the effectiveness and robustness of state - of - the - art NLP technologies in entity linking , by automatically linking visual and textual mentions of the same entities with the ontology , we can obtain a huge amount of automatically annotated images with detailed categories . In this paper , we propose the VTKEL dataset , consisting of images and corresponding captions , in which the image and textual mentions are both annotated with the corresponding entities typed according to the YAGO ontology . The VTKEL dataset can be used for training and evaluating algorithms for visual - textual - knowledge entity linking . 

Precision HIV Health App , Positive Peers , Powered by Data Harnessing , AI , and Learning
Mobile phone applications provide a new and easy - access platform for delivering tailored human immunodeficiency virus ( HIV ) and sexually transmitted disease ( STD ) prevention and care . Recent researches have shown that mobile interventions have positive effects in adhesive to care program , antiretroviral therapy ( ART ) , self - management of disease , and are also critical in decreasing the HIV pandemic , and stigmatization . In this paper , a precision health app , Positive Peers ( PP ) , has been developed collaboratively while enabled by data harnessing , Artificial Intelligence ( Al ) , and learning . Positive Peers is an Android / iOS - based social media app for providing support and information to a young adult subgroup living with HIV who are in strong need of support and motivation . We apply an intervention approach combined with Natural Language Processing ( NLP ) to help the targeted youth to engage more with the app . Using NLP facilitates the flow of information that has a critical role in decreasing the uncertainty of patients by being injected to useful related information . It further improves the interaction of users of the app while providing a compact platform for users to better find the answers to their questions and concerns . The NLP system has been evaluated in an alpha test . 

Multiple - source and multiple - destination charge migration in hybrid electrical energy storage systems
Hybrid electrical energy storage ( HEES ) systems consist of multiple banks of heterogeneous electrical energy storage ( EES ) elements that are connected to each other through the Charge Transfer Interconnect . A HEES system is capable of providing an electrical energy storage means with very high performance by taking advantage of the strengths ( while hiding the weaknesses ) of individual EES elements used in the system . Charge migration is an operation by which electrical energy is transferred from a group of source EES elements to a group of destination EES elements . It is a necessary process to improve the HEES system ' s storage efficiency and its responsiveness to load demand changes . This paper is the first to formally describe a more general charge migration problem , involving multiple sources and multiple destinations . The multiple - source , multiple - destination charge migration optimization problem is formulated as a nonlinear programming ( NLP ) problem where the goal is to deliver a fixed amount of energy to the destination banks while maximizing the overall charge migration efficiency and not depleting the available energy resource of the source banks by more than a given percentage . The constraints for the optimization problem are the energy conservation relation and charging current constraints to ensure that charge migration will meet a given deadline . The formulation correctly accounts for the efficiency of chargers , the rate capacity effect of batteries , self - discharge currents and internal resistances of EES elements , as well as the terminal voltage variation of EES elements as a function of their state of charges ( SoC ' s ) . An efficient algorithm to find a near - optimal migration control policy by effectively solving the above NLP optimization problem as a series of quasi - convex programming problems is presented . Experimental results show significant gain in migration efficiency up to 35% . 

An information extraction system for protein function prediction
We present a Natural Language Processing extraction system called IESforPFP , which can retrieve useful information from biomedical abstracts . IESforPFP aims at enhancing the state of the art of biological text mining by applying novel linguistic computational technique . By retrieving significant patterns of associations between proteins and molecules from biomedical abstracts , IESforPFP can determine the functions of un - annotated proteins . The system determines the semantic relationship between each protein - molecule pair in sentences using novel semantic rules . It applies a semantic relationship extraction model that retrieves information from different structural forms of constituents in sentences . In the framework of IESforPFP , each protein p is represented by a vector of weights . Each weight reflects the significance of a molecule m in the biomedical abstracts associated with p . That is , each weight quantifies the likelihood of the association between m and p . IESforPFP determines the set of annotated proteins that is semantically similar to p by comparing their vectors . It then annotates p with the functions of these annotated proteins . We evaluated the quality of IESforPFP by comparing it experimentally with two other systems . Results showed marked improvement . 

e - assessment using latent semantic analysis in the computer science domain : a pilot study
Latent Semantic Analysis ( LSA ) is a statistical Natural Language Processing ( NLP ) technique for inferring meaning from a text . Existing LSA - based applications focus on formative assessment in general domains . The suitability of LSA for summative assessment in the domain of computer science is not well known . The results from the pilot study reported in this paper encourage us to pursue further research in the use of LSA in the narrow , technical domain of computer science . This paper explains the theory behind LSA , describes some existing LSA applications , and presents some results using LSA for automatic marking of short essays for a graduate class in architectures of computing systems . 

Automatic Generation Method of Airborne Display and Control System Requirement Domain Model Based on NLP
Domain modeling is a crucial step from natural language requirements to precise specifications , and an essential support for the development of automation system design tools . The existing domain model extraction methods are not accurate enough to be applied into specific fields . In this paper , we present a method for extracting the requirement domain model of airborne display and control system based on Natural Language Processing ( NLP) . Firstly , the domain model template is defined on the basis of the detailed study of the existing rules . Then in the requirement statement , the parse tree generated from Stanford Parser is utilized to preprocess the requirements for special symbols and conjunctions . Finally , we conduct the comparative experiment and the results indicate that the precision of domain model extraction is 20 . 01 % higher than the existing approaches without preprocessing . 

A Method for Generating Synthetic Electronic Medical Record Text
Machine learning ( ML ) and Natural Language Processing ( NLP ) have achieved remarkable success in many fields and have brought new opportunities and high expectation in the analyses of medical data , of which the most common type is the massive free - text electronic medical records ( EMR ) . However , the free EMR texts are lacking consistent standards , rich of private information , and limited in availability . Also , it is often hard to have a balanced number of samples for the types of diseases under study . These problems hinder the development of ML and NLP methods for EMR data analysis . To tackle these problems , we developed a model called Medical Text Generative Adversarial Network or mtGAN , to generate synthetic EMR text . It is based on the GAN framework and is trained by the REINFORCE algorithm . It takes disease tags as inputs and generates synthetic texts as EMRs for the corresponding diseases . We evaluate the model from micro - level , macro - level and application - level on a Chinese EMR text dataset . The results show that the method has a good capacity to fit real data and can generate realistic and diverse EMR samples . This provides a novel way to avoid potential leakage of patient privacy while still supply sufficient well - controlled cohort data for developing downstream ML and NLP methods . 

Ikhtasir — A user selected compression ratio Arabic text summarization system
Automatic text summarization is an active research field . The rapid growth of the Web , and the associated information overloading , has injected new life into this research area . In certain languages there has been plenty of research in automatic text summarization . Arabic is not one of them . In this paper we present an automatic extractive Arabic text summarization system where the user can cap the size of the summary . The system does not require learning and employs rhetorical structure theory ( RST ) along with a sentence scoring scheme , where individual sentences are scored . For output , sentences are selected with an objective of maximizing the overall score of the summary whose size is within the user selected compression ratio . For evaluation , system generated summaries of various lengths were compared against those performed by a professional human . Experiments on sample texts show our system outperforms some of the other existing systems including those that require learning . 

Clinical Trial Information Extraction with BERT
Natural language processing ( NLP ) of clinical trial documents can be useful in new trial design . Here we identify entity types relevant to clinical trial design and propose a framework called CT - BERT for information extraction from clinical trial text . We trained named entity recognition ( NER ) models to extract eligibility criteria entities by fine - tuning a set of pre - trained BERT models . We then compared the performance of CT - BERT with recent baseline methods including attention - based BiLSTM and Criteria2Query . The results demonstrate the superiority of CT - BERT in clinical trial NLP . 

