a tool for a high-carat gold-standard word alignment
in this paper , we describe a tool designed to produce a gold-standard word alignment between a text and its translation with a novel visualization . in addition , the tool is designed to aid the aligners in producing an alignment at a high level of quality and consistency . this tool is presently being used to align the hebrew bible with an english translation of it .

solving the problem of cascading errors : approximate bayesian inference for linguistic annotation pipelines
the end-to-end performance of natural language processing systems for compound tasks , such as question answering and textual entailment , is often hampered by use of a greedy 1-best pipeline architecture , which causes errors to propagate and compound at each stage . we present a novel architecture , which models these pipelines as bayesian networks , with each low level task corresponding to a variable in the network , and then we perform approximate inference to find the best labeling . our approach is extremely simple to apply but gains the benefits of sampling the entire distribution over labels at each stage in the pipeline . we apply our method to two tasks semantic role labeling and recognizing textual entailment and achieve useful performance gains from the superior pipeline architecture .

hierarchical topical segmentation with affinity propagation anna kazantseva & stan szpakowicz
we present a hierarchical topical segmenter for free text . hierarchical affinity propagation for segmentation ( haps ) is derived from a clustering algorithm affinity propagation . given a document , haps builds a topical tree . the nodes at the top level correspond to the most prominent shifts of topic in the document . nodes at lower levels correspond to finer topical fluctuations . for each segment in the tree , haps identifies a segment centre a sentence or a paragraph which best describes its contents . we evaluate the segmenter on a subset of a novel manually segmented by several annotators , and on a dataset of wikipedia articles . the results suggest that hierarchical segmentations produced by haps are better than those obtained by iteratively running several one-level segmenters . an additional advantage of haps is that it does not require the gold standard number of segments in advance .

benchmarking noun compound interpretation su nam kim and timothy baldwin
in this paper we provide benchmark results for two classes of methods used in interpreting noun compounds ( ncs ) : semantic similarity-based methods and their hybrids . we evaluate the methods using 7-way and binary class data from the nominal pair interpretation task of semeval-2007.1 we summarize and analyse our results , with the intention of providing a framework for benchmarking future research in this area .

exploiting comparable corpora with ter and terp
in this paper we present an extension of a successful simple and effective method for extracting parallel sentences from comparable corpora and we apply it to an arabic/english nist system . we experiment with a new terp filter , along with wer and ter filters . we also report a comparison of our approach with that of ( munteanu and marcu , 2005 ) using exactly the same corpora and show performance gain by using much lesser data . our approach employs an smt system built from small amounts of parallel texts to translate the source side of the nonparallel corpus . the target side texts are used , along with other corpora , in the language model of this smt system . we then use information retrieval techniques and simple filters to create parallel data from a comparable news corpora . we evaluate the quality of the extracted data by showing that it significantly improves the performance of an smt systems .

shallow discourse structure for action item detection
we investigated automatic action item detection from transcripts of multi-party meetings . unlike previous work ( gruenstein et al , 2005 ) , we use a new hierarchical annotation scheme based on the roles utterances play in the action item assignment process , and propose an approach to automatic detection that promises improved classification accuracy while enabling the extraction of useful information for summarization and reporting .

making biographical data in wikipedia readable : a pattern-based
in this paper we present biografix , a pattern based tool that simplifies parenthetical structures with biographical information , whose aim is to create simple , readable and accessible sentences . to that end , we analysed the parenthetical structures that appear in the first paragraph of the basque wikipedia , and concentrated on biographies . although it has been designed and developed for basque we adapted it and evaluated with other five languages . we also perform an extrinsic evaluation with a question generation system to see if biografix improve its results .

hedge detection using the relhunter approach
relhunter is a machine learning based method for the extraction of structured information from text . here , we apply relhunter to the hedge detection task , proposed as the conll-2010 shared task1 . relhunters key design idea is to model the target structures as a relation over entities . the method decomposes the original task into three subtasks : ( i ) entity identification ; ( ii ) candidate relation generation ; and ( iii ) relation recognition . in the hedge detection task , we define three types of entities : cue chunk , start scope token and end scope token . hence , the entity identification subtask is further decomposed into three token classification subtasks , one for each entity type . in the candidate relation generation subtask , we apply a simple procedure to generate a ternary candidate relation . each instance in this relation represents a hedge candidate composed by a cue chunk , a start scope token and an end scope token . for the relation recognition subtask , we use a binary classifier to discriminate between true and false candidates . the four classifiers are trained with the entropy guided transformation learning algorithm .

efficient , feature-based , conditional random field parsing
discriminative feature-based methods are widely used in natural language processing , but sentence parsing is still dominated by generative methods . while prior feature-based dynamic programming parsers have restricted training and evaluation to artificially short sentences , we present the first general , featurerich discriminative parser , based on a conditional random field model , which has been successfully scaled to the full wsj parsing data . our efficiency is primarily due to the use of stochastic optimization techniques , as well as parallelization and chart prefiltering . on wsj15 , we attain a state-of-the-art f-score of 90.9 % , a 14 % relative reduction in error over previous models , while being two orders of magnitude faster . on sentences of length 40 , our system achieves an f-score of 89.0 % , a 36 % relative reduction in error over a generative baseline .

constraining robust constructions for broad-coverage parsing with precision grammars
this paper addresses two problems that commonly arise in parsing with precisionoriented , rule-based models of grammar : lack of speed and lack of robustness . first , we show how we can reduce parsing times by restricting the number of tasks the parser will carry out , based on a generative model of rule applications . second , we show that a combination of search space restriction and radically overgenerating robustness rules lead to a more robust parser , with only a small penalty in precision . applying both the robustness rules and a fragment fallback strategy showed better recall than just giving fragment analyses , with equal precision . results are reported on a medium-sized hpsg grammar for german .

symptom recognition issue
this work focuses on signs and symptoms recognition in biomedical texts abstracts . first , this specific task is described from a linguistic point of view . then a methodology combining pattern mining and language processing is proposed . in the absence of an authoritative annotated corpus , our approach has the advantage of being weakly-supervised . preliminary experimental results are discussed and reveal promising avenues .

constructing a practical constituent parser from a japanese treebank
we present an empirical study on constructing a japanese constituent parser , which can output function labels to deal with more detailed syntactic information . japanese syntactic parse trees are usually represented as unlabeled dependency structure between bunsetsu chunks , however , such expression is insufficient to uncover the syntactic information about distinction between complements and adjuncts and coordination structure , which is required for practical applications such as syntactic reordering of machine translation . we describe a preliminary effort on constructing a japanese constituent parser by a penn treebank style treebank semi-automatically made from a dependency-based corpus . the evaluations show the parser trained on the treebank has comparable bracketing accuracy as conventional bunsetsu-based parsers , and can output such function labels as the grammatical role of the argument and the type of adnominal phrases .

adaptive recursive neural network for target-dependent twitter sentiment classification
we propose adaptive recursive neural network ( adarnn ) for target-dependent twitter sentiment classification . adarnn adaptively propagates the sentiments of words to target depending on the context and syntactic relationships between them . it consists of more than one composition functions , and we model the adaptive sentiment propagations as distributions over these composition functions . the experimental studies illustrate that adarnn improves the baseline methods . furthermore , we introduce a manually annotated dataset for target-dependent twitter sentiment analysis .

a hybrid model for urdu hindi transliteration abbas malik laurent besacier christian boitet
we report in this paper a novel hybrid approach for urdu to hindi transliteration that combines finite-state machine ( fsm ) based techniques with statistical word language model based approach . the output from the fsm is filtered with the word language model to produce the correct hindi output . the main problem handled is the case of omission of diacritical marks from the input urdu text . our system produces the correct hindi output even when the crucial information in the form of diacritic marks is absent . the approach improves the accuracy of the transducer-only approach from 50.7 % to 79.1 % . the results reported show that performance can be improved using a word language model to disambiguate the output produced by the transducer-only approach , especially when diacritic marks are not present in the urdu input .

optimization-based content selection for opinion summarization jackie chi kit cheung
we introduce a content selection method for opinion summarization based on a well-studied , formal mathematical model , the p-median clustering problem from facility location theory . our method replaces a series of local , myopic steps to content selection with a global solution , and is designed to allow content and realization decisions to be naturally integrated . we evaluate and compare our method against an existing heuristic-based method on content selection , using human selections as a gold standard . we find that the algorithms perform similarly , suggesting that our content selection method is robust enough to support integration with other aspects of summarization .

limsiiles : basic english substitution for student answer assessment at limsi-cnrs & ens limsi-cnrs & ensiie
in this paper , we describe a method for assessing student answers , modeled as a paraphrase identification problem , based on substitution by basic english variants . basic english paraphrases are acquired from the simple english wiktionary . substitutions are applied both on reference answers and student answers in order to reduce the diversity of their vocabulary and map them to a common vocabulary . the evaluation of our approach on the semeval 2013 joint student response analysis and 8th recognizing textual entailment challenge data shows promising results , and this work is a first step toward an opendomain system able to exhibit deep text understanding capabilities .

thoughtland : natural language descriptions for machine learning n-dimensional error functions
this demo showcases thoughtland , an end-to-end system that takes training data and a selected machine learning model , produces a cloud of points via crossvalidation to approximate its error function , then uses model-based clustering to identify interesting components of the error function and natural language generation to produce an english text summarizing the error function .

scaling textual inference to the web
most web-based q/a systems work by finding pages that contain an explicit answer to a question . these systems are helpless if the answer has to be inferred from multiple sentences , possibly on different pages . to solve this problem , we introduce the holmes system , which utilizes textual inference ( ti ) over tuples extracted from text . whereas previous work on ti ( e.g. , the literature on textual entailment ) has been applied to paragraph-sized texts , holmes utilizes knowledge-based model construction to scale ti to a corpus of 117 million web pages . given only a few minutes , holmes doubles recall for example queries in three disparate domains ( geography , business , and nutrition ) . importantly , holmess runtime is linear in the size of its input corpus due to a surprising property of many textual relations in the web corpusthey are approximately functional in a well-defined sense .

multilingual mobile-phone translation services for world travelers
this demonstration introduces two new multilingual translation services for mobile phones . the first translation service provides state-of-the-art text-to-text translations of japanese as well as english conversational spoken language in the travel domain into 17 languages using statistical machine translation technologies trained automatically from a large-scale multilingual corpus . the second demonstration is a speech translation service between japanese and english for real environments . it is based on distributed speech recognition with noise suppression . flexible interfaces between internal and external speech translation resources ease the portability of the system to other languages and enable real-time location-free communication world-wide .

representing predicative terms in the field of the environment marie-claude lhomme benot robichaud
terminological resources have traditionally focused on terms referring to entities , thereby ignoring other important concepts ( processes , events and properties ) in specialized fields of knowledge . consequently , large parts of the conceptual structure of these fields are not taken into consideration nor represented . in this article , we show how terms that refer to processes and events ( and , to a lesser extent , properties ) can be characterized using frame semantics ( fillmore , 1982 ) and the methodology developed within the framenet project ( ruppenhofer et al. , 2010 ) . more specifically , we applied the framework to a subset of terms in the field of the environment . frames are unveiled first by comparing similarities between the argument structures of terms already recorded in a terminological database and the relationships they share with other terms . a comparison is also carried out with the lexical units recorded in framenet . then , relations between frames are defined that allow us to build small conceptual scenarios that are specific to the field of the environment . these relations are determined on the basis of the set of relations listed in the framenet project . this article reports on the methodology , the frames defined up to now and two specific conceptual scenarios ( risk_scenario and managing_waste ) .

a unified morpho-syntactic scheme of stanford dependencies
stanford dependencies ( sd ) provide a functional characterization of the grammatical relations in syntactic parse-trees . the sd representation is useful for parser evaluation , for downstream applications , and , ultimately , for natural language understanding , however , the design of sd focuses on structurally-marked relations and under-represents morphosyntactic realization patterns observed in morphologically rich languages ( mrls ) . we present a novel extension of sd , called unified-sd ( u-sd ) , which unifies the annotation of structurally- and morphologically-marked relations via an inheritance hierarchy . we create a new resource composed of u-sdannotated constituency and dependency treebanks for the mrl modern hebrew , and present two systems that can automatically predict u-sd annotations , for gold segmented input as well as raw texts , with high baseline accuracy .

computational linguistics in costa rica : an overview
this paper aims to bring a general overview on the situation of computational linguistics in costa rica , particularly in the academic world .

off-topic detection in conversational telephone speech
in a context where information retrieval is extended to spoken documents including conversations , it will be important to provide users with the ability to seek informational content , rather than socially motivated small talk that appears in many conversational sources . in this paper we present a preliminary study aimed at automatically identifying irrelevance in the domain of telephone conversations . we apply a standard machine learning algorithm to build a classifier that detects offtopic sections with better-than-chance accuracy and that begins to provide insight into the relative importance of features for identifying utterances as on topic or not .

finding more bilingual webpages with high credibility via
this paper presents an efficient approach to finding more bilingual webpage pairs with high credibility via link analysis , using little prior knowledge or heuristics . it extends from a previous algorithm that takes the number of bilingual url pairs that a key ( i.e. , a url pairing pattern ) can match as the objective function to search for the best set of keys yielding the greatest number of webpage pairs within targeted bilingual websites . enhanced algorithms are proposed to match more bilingual webpages following the credibility based on statistical analysis of the link relationship of the seed websites available . with about 12,800 seed websites as test set , the enhanced algorithms improve precision over baseline by more than 5 % , from 94.06 % to 99.40 % , and hence find above 20 % more true bilingual url pairs , illustrating that significantly more bilingual webpages with high credibility can be mined with the help of the link analysis .

improved language modeling for english-persian statistical machine
as interaction between speakers of different languages continues to increase , the everpresent problem of language barriers must be overcome . for the same reason , automatic language translation ( machine translation ) has become an attractive area of research and development . statistical machine translation ( smt ) has been used for translation between many language pairs , the results of which have shown considerable success . the focus of this research is on the english/persian language pair . this paper investigates the development and evaluation of the performance of a statistical machine translation system by building a baseline system using subtitles from persian films . we present an overview of previous related work in english/persian machine translation , and examine the available corpora for this language pair . we finally show the results of the experiments of our system using an in-house corpus and compare the results we obtained when building a language model with different sized monolingual corpora . different automatic evaluation metrics like bleu , nist and ibm-bleu were used to evaluate the performance of the system on half of the corpus built . finally , we look at future work by outlining ways of getting highly accurate translations as fast as possible .

markovian discriminative modeling for dialog state tracking
discriminative dialog state tracking has become a hot topic in dialog research community recently . compared to generative approach , it has the advantage of being able to handle arbitrary dependent features , which is very appealing . in this paper , we present our approach to the dstc2 challenge . we propose to use discriminative markovian models as a natural enhancement to the stationary discriminative models . the markovian structure allows the incorporation of transitional features , which can lead to more efficiency and flexibility in tracking user goal changes . results on the dstc2 dataset show considerable improvements over the baseline , and the effects of the markovian dependency is tested empirically .

parallel distributed grammar engineering for practical applications
based on a detailed case study of parallel grammar development distributed across two sites , we review some of the requirements for regression testing in grammar engineering , summarize our approach to systematic competence and performance profiling , and discuss our experience with grammar development for a commercial application . if possible , the workshop presentation will be organized around a software demonstration .

extraction of translation unit from chinese-english parallel corpora
more and more researchers have recognized the potential value of the parallel corpus in the research on machine translation and machine aided translation . this paper examines how chinese english translation units could be extracted from parallel corpus . an iterative algorithm based on degree of word association is proposed to identify the multiword units for chinese and english . then the chinese-english translation equivalent pairs.are extracted from the parallel corpus . we also made comparison between different statistical association measurement in this paper .

machine transliteration using target-language grapheme and phoneme : multi-engine transliteration approach
this paper describes our approach to news 2009 machine transliteration shared task . we built multiple transliteration engines based on different combinations of two transliteration models and three machine learning algorithms . then , the outputs from these transliteration engines were combined using re-ranking functions . our method was applied to all language pairs in news 2009 machine transliteration shared task . the official results of our standard runs were ranked the best for four language pairs and the second best for three language pairs .

eliciting subjectivity and polarity judgements on word senses
there has been extensive work on eliciting human judgements on the sentiment of words and the resulting annotated word lists have frequently been used for opinion mining applications in natural language processing ( nlp ) . however , this word-based approach does not take different senses of a word into account , which might differ in whether and what kind of sentiment they evoke . in this paper , we therefore introduce a human annotation scheme for judging both the subjectivity and polarity of word senses . we show that the scheme is overall reliable , making this a well-defined task for automatic processing . we also discuss three issues that surfaced during annotation : the role of annotation bias , hierarchical annotation ( or underspecification ) and bias in the sense inventory used .

integrating joint n-gram features into a discriminative training framework
phonetic string transduction problems , such as letter-to-phoneme conversion and name transliteration , have recently received much attention in the nlp community . in the past few years , two methods have come to dominate as solutions to supervised string transduction : generative joint n-gram models , and discriminative sequence models . both approaches benefit from their ability to consider large , flexible spans of source context when making transduction decisions . however , they encode this context in different ways , providing their respective models with different information . to combine the strengths of these two systems , we include joint n-gram features inside a state-of-the-art discriminative sequence model . we evaluate our approach on several letter-to-phoneme and transliteration data sets . our results indicate an improvement in overall performance with respect to both the joint n-gram approach and traditional feature sets for discriminative models .

a computational approach to yoru`ba morphology
we demonstrate the use of default default inheritance hierarchies to represent the morphology of yoru`ba verbs in the katr formalism , treating inflectional exponences as markings associated with the application of rules by which complex word forms are deduced from simpler roots or stems . in particular , we suggest a scheme of slots that together make up a verb and show how each slot represents a subset of the morphosyntactic properties associated with the verb . we also show how we can account for the tonal aspects of yoru`ba , in particular , the tone associated with the emphatic ending . our approach allows linguists to gain an appreciation for the structure of verbs , gives teachers a foundation for organizing lessons in morphology , and provides students a technique for generating forms of any verb .

ranking human and machine summarization systems
the text analysis conference ( tac ) ranks summarization systems by their average score over a collection of document sets . we investigate the statistical appropriateness of this score and propose an alternative that better distinguishes between human and machine evaluation systems .

active learning with multiple annotations for comparable data
supervised learning algorithms for identifying comparable sentence pairs from a dominantly non-parallel corpora require resources for computing feature functions as well as training the classifier . in this paper we propose active learning techniques for addressing the problem of building comparable data for low-resource languages . in particular we propose strategies to elicit two kinds of annotations from comparable sentence pairs : class label assignment and parallel segment extraction . we also propose an active learning strategy for these two annotations that performs significantly better than when sampling for either of the annotations independently .

ner systems that suit users preferences : adjusting the recall-precision trade-off for entity extraction
we describe a method based on tweaking an existing learned sequential classifier to change the recall-precision tradeoff , guided by a user-provided performance criterion . this method is evaluated on the task of recognizing personal names in email and newswire text , and proves to be both simple and effective .

machine translation based on nlg from xmldb for advanced studies
the purpose of this study is to propose a new method for machine translation . we have proceeded through with two projects for report generation kittredge and polguere weather forecast and monthly economic report to be produced in four languages : english japanese french and german . their input data is stored in xmldb . we applied a threestage pipelined architecture reiter and dale and each stage was implemented as xml transformation processes . we regard xml stored data as languageneutral intermediate form and employ the so called sublanguage approach somers . the machine translation process is implemented via xmldb as a kind of interlinguage approach instead of the conventional structure transfer approach .

multilingual semantic parsing : parsing multiple languages into semantic representations & technology of china information systems technology and design
we consider multilingual semantic parsing the task of simultaneously parsing semantically equivalent sentences from multiple different languages into their corresponding formal semantic representations . our model is built on top of the hybrid tree semantic parsing framework , where natural language sentences and their corresponding semantics are assumed to be generated jointly from an underlying generative process . we first introduce a variant of the joint generative process , which essentially gives us a new semantic parsing model within the framework . based on the different models that can be developed within the framework , we then investigate several approaches for performing the multilingual semantic parsing task . we present our evaluations on a standard dataset annotated with sentences in multiple languages coming from different language families .

the phenogrammar of coordination
linear categorial grammar ( lincg ) is a sign-based , curryesque , relational , logical categorial grammar ( cg ) whose central architecture is based on linear logic . curryesque grammars separate the abstract combinatorics ( tectogrammar ) of linguistic expressions from their concrete , audible representations ( phenogrammar ) . most of these grammars encode linear order in string-based lambda terms , in which there is no obvious way to distinguish right from left . without some notion of directionality , grammars are unable to differentiate , say , subject and object for purposes of building functorial coordinate structures . we introduce the notion of a phenominator as a way to encode the term structure of a functor separately from its string support . this technology is then employed to analyze a range of coordination phenomena typically left unaddressed by linear logic-based curryesque frameworks .

exploring syntactic features for native language identification : a variationist perspective on feature encoding and seminar fur sprachwissenschaft seminar fur sprachwissenschaft
in this paper , we systematically explore lexicalized and non-lexicalized local syntactic features for the task of native language identification ( nli ) . we investigate different types of feature representations in single- and cross-corpus settings , including two representations inspired by a variationist perspective on the choices made in the linguistic system . to combine the different models , we use a probabilities-based ensemble classifier and propose a technique to optimize and tune it . combining the best performing syntactic features with four types of n-grams outperforms the best approach of the nli shared task 2013 .

using sketches to estimate associations
we should not have to look at the entire corpus ( e.g. , the web ) to know if two words are associated or not.1 a powerful sampling technique called sketches was originally introduced to remove duplicate web pages . we generalize sketches to estimate contingency tables and associations , using a maximum likelihood estimator to find the most likely contingency table given the sample , the margins ( document frequencies ) and the size of the collection . not unsurprisingly , computational work and statistical accuracy ( variance or errors ) depend on sampling rate , as will be shown both theoretically and empirically . sampling methods become more and more important with larger and larger collections . at web scale , sampling rates as low as 104 may suffice .

graeme blackwood jamie brunning william byrne
this paper describes the cambridge university engineering department submission to the fifth workshop on statistical machine translation . we report results for the french-english and spanish-english shared translation tasks in both directions . the cued system is based on hifst , a hierarchical phrase-based decoder implemented using weighted finite-state transducers . in the french-english task , we investigate the use of context-dependent alignment models . we also show that lattice minimum bayes-risk decoding is an effective framework for multi-source translation , leading to large gains in bleu score .

summarisation of time-series data
we present a novel approach for automatic report generation from time-series data , in the context of student feedback generation . our proposed methodology treats content selection as a multi-label ( ml ) classification problem , which takes as input time-series data and outputs a set of templates , while capturing the dependencies between selected templates . we show that this method generates output closer to the feedback that lecturers actually generated , achieving 3.5 % higher accuracy and 15 % higher f-score than multiple simple classifiers that keep a history of selected templates . furthermore , we compare a ml classifier with a reinforcement learning ( rl ) approach in simulation and using ratings from real student users . we show that the different methods have different benefits , with ml being more accurate for predicting what was seen in the training data , whereas rl is more exploratory and slightly preferred by the students .

a framework for ( under ) specifying dependency syntax without overloading annotators
we introduce a framework for lightweight dependency syntax annotation . our formalism builds upon the typical representation for unlabeled dependencies , permitting a simple notation and annotation workflow . moreover , the formalism encourages annotators to underspecify parts of the syntax if doing so would streamline the annotation process . we demonstrate the efficacy of this annotation on three languages and develop algorithms to evaluate and compare underspecified annotations .

towards a discourse relation-aware approach for chinese-english machine translation
translation of discourse relations is one of the recent efforts of incorporating discourse information to statistical machine translation ( smt ) . while existing works focus on disambiguation of ambiguous discourse connectives , or transformation of discourse trees , only explicit discourse relations are tackled . a greater challenge exists in machine translation of chinese , since implicit discourse relations are abundant and occur both inside and outside a sentence . this thesis proposal describes ongoing work on bilingual discourse annotation and plans towards incorporating discourse relation knowledge to a chineseenglish smt system with consideration of implicit discourse relations . the final goal is a discourse-unit-based translation model unbounded by the traditional assumption of sentence-to-sentence translation .

a system for summarizing scientific topics starting from keywords
in this paper , we investigate the problem of automatic generation of scientific surveys starting from keywords provided by a user . we present a system that can take a topic query as input and generate a survey of the topic by first selecting a set of relevant documents , and then selecting relevant sentences from those documents . we discuss the issues of robust evaluation of such systems and describe an evaluation corpus we generated by manually extracting factoids , or information units , from 47 gold standard documents ( surveys and tutorials ) on seven topics in natural language processing . we have manually annotated 2,625 sentences with these factoids ( around 375 sentences per topic ) to build an evaluation corpus for this task . we present evaluation results for the performance of our system using this annotated data .

hierarchical bayesian domain adaptation
multi-task learning is the problem of maximizing the performance of a system across a number of related tasks . when applied to multiple domains for the same task , it is similar to domain adaptation , but symmetric , rather than limited to improving performance on a target domain . we present a more principled , better performing model for this problem , based on the use of a hierarchical bayesian prior . each domain has its own domain-specific parameter for each feature but , rather than a constant prior over these parameters , the model instead links them via a hierarchical bayesian global prior . this prior encourages the features to have similar weights across domains , unless there is good evidence to the contrary . we show that the method of ( daume iii , 2007 ) , which was presented as a simple preprocessing step , is actually equivalent , except our representation explicitly separates hyperparameters which were tied in his work . we demonstrate that allowing different values for these hyperparameters significantly improves performance over both a strong baseline and ( daume iii , 2007 ) within both a conditional random field sequence model for named entity recognition and a discriminatively trained dependency parser .

classifying temporal relations with simple features
approaching temporal link labelling as a classification task has already been explored in several works . however , choosing the right feature vectors to build the classification model is still an open issue , especially for event-event classification , whose accuracy is still under 50 % . we find that using a simple feature set results in a better performance than using more sophisticated features based on semantic role labelling and deep semantic parsing . we also investigate the impact of extracting new training instances using inverse relations and transitive closure , and gain insight into the impact of this bootstrapping methodology on classifying the full set of tempeval-3 relations .

ensemble-based active learning for parse selection
supervised estimation methods are widely seen as being superior to semi and fully unsupervised methods . however , supervised methods crucially rely upon training sets that need to be manually annotated . this can be very expensive , especially when skilled annotators are required . active learning ( al ) promises to help reduce this annotation cost . within the complex domain of hpsg parse selection , we show that ideas from ensemble learning can help further reduce the cost of annotation . our main results show that at times , an ensemble model trained with randomly sampled examples can outperform a single model trained using al . however , converting the single-model al method into an ensemble-based al method shows that even this much stronger baseline model can be improved upon . our best results show a reduction in annotation cost compared with single-model random sampling .

a transcription scheme for languages employing the arabic script motivated by speech processing application
this paper offers a transcription system for persian , the target language in the transonics project , a speech-to-speech translation system developed as a part of the darpa babylon program ( the darpa babylon program ; narayanan , 2003 ) . in this paper , we discuss transcription systems needed for automated spoken language processing applications in persian that uses the arabic script for writing . this system can easily be modified for arabic , dari , urdu and any other language that uses the arabic script . the proposed system has two components . one is a phonemic based transcription of sounds for acoustic modelling in automatic speech recognizers and for text to speech synthesizer , using ascii based symbols , rather than international phonetic alphabet symbols . the other is a hybrid system that provides a minimally-ambiguous lexical representation that explicitly includes vocalic information ; such a representation is needed for language modelling , text to speech synthesis and machine translation .

user modeling by using bag-of-behaviors for building a dialog system sensitive to the interlocutors internal state
when using spoken dialog systems in actual environments , users sometimes abandon the dialog without making any input utterance . to help these users before they give up , the system should know why they could not make an utterance . thus , we have examined a method to estimate the state of a dialog user by capturing the users non-verbal behavior even when the users utterance is not observed . the proposed method is based on vector quantization of multi-modal features such as non-verbal speech , feature points of the face , and gaze . the histogram of the vq code is used as a feature for determining the state . we call this feature the bagof-behaviors . according to the experimental results , we prove that the proposed method surpassed the results of conventional approaches and discriminated the target users states with an accuracy of more than 70 % .

evaluation of a system for noun concepts acquisition from utterances about images ( sinca ) using daily conversation data
for a robot working in an open environment , a task-oriented language capability will not be sufficient . in order to adapt to the environment , such a robot will have to learn language dynamically . we developed a system for noun concepts acquisition from utterances about images , sinca in short . it is a language acquisition system without knowledge of grammar and vocabulary , which learns noun concepts from user utterances . we recorded a video of a childs daily life to collect dialogue data that was spoken to and around him . the child is a member of a family consisting of the parents and his sister . we evaluated the performance of sinca using the collected data . in this paper , we describe the algorithms of sinca and an evaluation experiment . we work on japanese language acquisition , however our method can easily be adapted to other languages .

ngoc-quang luong laurent besacier
this paper describes our word-level qe system for wmt 2014 shared task on spanish - english pair . compared to wmt 2013 , this years task is different due to the lack of smt setting information and additional resources . we report how we overcome this challenge to retain most of the important features which performed well last year in our system . novel features related to the availability of multiple systems output ( new point of this year ) are also proposed and experimented along with baseline set . the system is optimized by several ways : tuning the classification threshold , combining with wmt 2013 data , and refining using feature selection strategy on our development set , before dealing with the test set for submission .

annotation tools based on the annotation graph api
annotation graphs provide an efficient and expressive data model for linguistic annotations of time-series data . this paper reports progress on a complete open-source software infrastructure supporting the rapid development of tools for transcribing and annotating time-series data . this generalpurpose infrastructure uses annotation graphs as the underlying model , and allows developers to quickly create special-purpose annotation tools using common components . an application programming interface , an i/o library , and graphical user interfaces are described . our experience has shown us that it is a straightforward task to create new special-purpose annotation tools based on this general-purpose infrastructure .

verb noun construction mwe token supervised classification
we address the problem of classifying multiword expression tokens in running text . we focus our study on verb-noun constructions ( vnc ) that vary in their idiomaticity depending on context . vnc tokens are classified as either idiomatic or literal . we present a supervised learning approach to the problem . we experiment with different features . our approach yields the best results to date on mwe classification combining different linguistically motivated features , the overall performance yields an f-measure of 84.58 % corresponding to an fmeasure of 89.96 % for idiomaticity identification and classification and 62.03 % for literal identification and classification .

in the act of translation
the paper offers an effective way of teacher-student computer-based collaboration in translation class . we show how a quantitative-qualitative method of analysis supported by word alignment technology can be applied to student translations for use in the classroom . the combined use of natural-language processing and manual techniques enables students to co-emerge during highly motivated collaborative sessions . within the advocated approach , students are proactive seekers for a better translation ( grade ) in a teacher-centered computerbased peer-assisted translation class .

dependency-based decipherment for resource-limited machine
we introduce dependency relations into deciphering foreign languages and show that dependency relations help improve the state-ofthe-art deciphering accuracy by over 500 % . we learn a translation lexicon from large amounts of genuinely non parallel data with decipherment to improve a phrase-based machine translation system trained with limited parallel data . in experiments , we observe bleu gains of 1.2 to 1.8 across three different test sets .

cross-language parser adaptation between related languages stav formln a aplikovan lingvistiky
the present paper describes an approach to adapting a parser to a new language . presumably the target language is much poorer in linguistic resources than the source language . the technique has been tested on two european languages due to test data availability ; however , it is easily applicable to any pair of sufficiently related languages , including some of the indic language group . our adaptation technique using existing annotations in the source language achieves performance equivalent to that obtained by training on 1546 trees in the target language .

incremental reference resolution : the task , metrics for evaluation , and a bayesian filtering model that is sensitive to disfluencies
in this paper we do two things : a ) we discuss in general terms the task of incremental reference resolution ( irr ) , in particular resolution of exophoric reference , and specify metrics for measuring the performance of dialogue system components tackling this task , and b ) we present a simple bayesian filtering model of irr that performs reasonably well just using words directly ( no structure information and no hand-coded semantics ) : it picks the right referent out of 12 for around 50 % of realworld dialogue utterances in our test corpus . it is also able to learn to interpret not only words but also hesitations , just as humans have shown to do in similar situations , namely as markers of references to hard-to-describe entities .

measuring the relative compositionality of verb-noun ( v-n ) collocations by technology - hyderabad ,
measuring the relative compositionality of multi-word expressions ( mwes ) is crucial to natural language processing . various collocation based measures have been proposed to compute the relative compositionality of mwes . in this paper , we define novel measures ( both collocation based and context based measures ) to measure the relative compositionality of mwes of v-n type . we show that the correlation of these features with the human ranking is much superior to the correlation of the traditional features with the human ranking . we then integrate the proposed features and the traditional features using a svm based ranking function to rank the collocations of v-n type based on their relative compositionality . we then show that the correlation between the ranks computed by the svm based ranking function and human ranking is significantly better than the correlation between ranking of individual features and human ranking .

a more precise analysis of punctuation for broad-coverage surface realization with ccg
this paper describes a more precise analysis of punctuation for a bi-directional , broad coverage english grammar extracted from the ccgbank ( hockenmaier and steedman , 2007 ) . we discuss various approaches which have been proposed in the literature to constrain overgeneration with punctuation , and illustrate how aspects of briscoes ( 1994 ) influential approach , which relies on syntactic features to constrain the appearance of balanced and unbalanced commas and dashes to appropriate sentential contexts , is unattractive for ccg . as an interim solution to constrain overgeneration , we propose a rule-based filter which bars illicit sequences of punctuation and cases of improperly unbalanced apposition . using the openccg toolkit , we demonstrate that our punctuation-augmented grammar yields substantial increases in surface realization coverage and quality , helping to achieve state-of-the-art bleu scores .

exploring consensus in machine translation for quality estimation
this paper presents the use of consensus among machine translation ( mt ) systems for the wmt14 quality estimation shared task . consensus is explored here by comparing the mt system output against several alternative machine translations using standard evaluation metrics . figures extracted from such metrics are used as features to complement baseline prediction models . the hypothesis is that knowing whether the translation of interest is similar or dissimilar to translations from multiple different mt systems can provide useful information regarding the quality of such a translation .

bayesian kernel methods for natural language processing
kernel methods are heavily used in natural language processing ( nlp ) . frequentist approaches like support vector machines are the state-of-the-art in many tasks . however , these approaches lack efficient procedures for model selection , which hinders the usage of more advanced kernels . in this work , we propose the use of a bayesian approach for kernel methods , gaussian processes , which allow easy model fitting even for complex kernel combinations . our goal is to employ this approach to improve results in a number of regression and classification tasks in nlp .

a machine learning approach to the automatic evaluation of machine translation
we present a machine learning approach to evaluating the wellformedness of output of a machine translation system , using classifiers that learn to distinguish human reference translations from machine translations . this approach can be used to evaluate an mt system , tracking improvements over time ; to aid in the kind of failure analysis that can help guide system development ; and to select among alternative output strings . the method presented is fully automated and independent of source language , target language and domain .

recognising textual entailment with logical inference
we use logical inference techniques for recognising textual entailment . as the performance of theorem proving turns out to be highly dependent on not readily available background knowledge , we incorporate model building , a technique borrowed from automated reasoning , and show that it is a useful robust method to approximate entailment . finally , we use machine learning to combine these deep semantic analysis techniques with simple shallow word overlap ; the resulting hybrid model achieves high accuracy on the rte testset , given the state of the art . our results also show that the different techniques that we employ perform very differently on some of the subsets of the rte corpus and as a result , it is useful to use the nature of the dataset as a feature .

robust logistic regression using shift parameters
annotation errors can significantly hurt classifier performance , yet datasets are only growing noisier with the increased use of amazon mechanical turk and techniques like distant supervision that automatically generate labels . in this paper , we present a robust extension of logistic regression that incorporates the possibility of mislabelling directly into the objective . this model can be trained through nearly the same means as logistic regression , and retains its efficiency on highdimensional datasets . we conduct experiments on named entity recognition data and find that our approach can provide a significant improvement over the standard model when annotation errors are present .

clique-based clustering for improving named entity recognition systems
we propose a system which builds , in a semi-supervised manner , a resource that aims at helping a ner system to annotate corpus-specific named entities . this system is based on a distributional approach which uses syntactic dependencies for measuring similarities between named entities . the specificity of the presented method however , is to combine a clique-based approach and a clustering technique that amounts to a soft clustering method . our experiments show that the resource constructed by using this cliquebased clustering system allows to improve different ner systems .

regular expression learning for information extraction
regular expressions have served as the dominant workhorse of practical information extraction for several years . however , there has been little work on reducing the manual effort involved in building high-quality , complex regular expressions for information extraction tasks . in this paper , we propose relie , a novel transformation-based algorithm for learning such complex regular expressions . we evaluate the performance of our algorithm on multiple datasets and compare it against the crf algorithm . we show that relie , in addition to being an order of magnitude faster , outperforms crf under conditions of limited training data and cross-domain data . finally , we show how the accuracy of crf can be improved by using features extracted by relie .

ku : word sense disambiguation by substitution
data sparsity is one of the main factors that make word sense disambiguation ( wsd ) difficult . to overcome this problem we need to find effective ways to use resources other than sense labeled data . in this paper i describe a wsd system that uses a statistical language model based on a large unannotated corpus . the model is used to evaluate the likelihood of various substitutes for a word in a given context . these likelihoods are then used to determine the best sense for the word in novel contexts . the resulting system participated in three tasks in the semeval 2007 workshop . the wsd of prepositions task proved to be challenging for the system , possibly illustrating some of its limitations : e.g . not all words have good substitutes . the system achieved promising results for the english lexical sample and english lexical substitution tasks .

hit-wsd : using search engine for multilingual chinese-english lexical sample task
we have participated in the multilingual chinese-english lexical sample task of semeval-2007 . our system disambiguates senses of chinese words and finds the correct translation in english by using the web as wsd knowledge source . since all the statistic data is obtained from search engine , the method is considered to be unsupervised and does not require any sense-tagged corpus .

automatic recognition of logical relations for english , chinese and japanese in the glarf framework
we present glarf , a framework for representing three linguistic levels and systems for generating this representation . we focus on a logical level , like lfgs f-structure , but compatible with penn treebanks . while less finegrained than typical semantic role labeling approaches , our logical structure has several advantages : ( 1 ) it includes all words in all sentences , regardless of part of speech or semantic domain ; and ( 2 ) it is easier to produce accurately . our systems achieve 90 % for english/japanese news and 74.5 % for chinese news these f-scores are nearly the same as those achieved for treebank-based parsing .

extraction programs : a unified approach to translation rule extraction sdl language technologies division
we provide a general algorithmic schema for translation rule extraction and show that several popular extraction methods ( including phrase pair extraction , hierarchical phrase pair extraction , and ghkm extraction ) can be viewed as specific instances of this schema . this work is primarily intended as a survey of the dominant extraction paradigms , in which we make explicit the close relationship between these approaches , and establish a language for future hybridizations . this facilitates a generic and extensible implementation of alignment-based extraction methods .

empirical lower bounds on the complexity of translational equivalence
this paper describes a study of the patterns of translational equivalence exhibited by a variety of bitexts . the study found that the complexity of these patterns in every bitext was higher than suggested in the literature . these findings shed new light on why syntactic constraints have not helped to improve statistical translation models , including finitestate phrase-based models , tree-to-string models , and tree-to-tree models . the paper also presents evidence that inversion transduction grammars can not generate some translational equivalence relations , even in relatively simple real bitexts in syntactically similar languages with rigid word order . instructions for replicating our experiments are at http : //nlp.cs.nyu.edu/genpar/acl06

verbal valency frame detection and selection in czech and english
we present a supervised learning method for verbal valency frame detection and selection , i.e. , a specific kind of word sense disambiguation for verbs based on subcategorization information , which amounts to detecting mentions of events in text . we use the rich dependency annotation present in the prague dependency treebanks for czech and english , taking advantage of several analysis tools ( taggers , parsers ) developed on these datasets previously . the frame selection is based on manually created lexicons accompanying these treebanks , namely on pdt-vallex for czech and engvallex for english . the results show that verbal predicate detection is easier for czech , but in the subsequent frame selection task , better results have been achieved for english .

reranking and self-training for parser adaptation
statistical parsers trained and tested on the penn wall street journal ( wsj ) treebank have shown vast improvements over the last 10 years . much of this improvement , however , is based upon an ever-increasing number of features to be trained on ( typically ) the wsj treebank data . this has led to concern that such parsers may be too finely tuned to this corpus at the expense of portability to other genres . such worries have merit . the standard charniak parser checks in at a labeled precisionrecall f -measure of 89.7 % on the penn wsj test set , but only 82.9 % on the test set from the brown treebank corpus . this paper should allay these fears . in particular , we show that the reranking parser described in charniak and johnson ( 2005 ) improves performance of the parser on brown to 85.2 % . furthermore , use of the self-training techniques described in ( mcclosky et al , 2006 ) raise this to 87.8 % ( an error reduction of 28 % ) again without any use of labeled brown data . this is remarkable since training the parser and reranker on labeled brown data achieves only 88.4 % .

a non-monotonic arc-eager transition system for dependency parsing
previous incremental parsers have used monotonic state transitions . however , transitions can be made to revise previous decisions quite naturally , based on further information . we show that a simple adjustment to the arc-eager transition system to relax its monotonicity constraints can improve accuracy , so long as the training data includes examples of mistakes for the nonmonotonic transitions to repair . we evaluate the change in the context of a stateof-the-art system , and obtain a statistically significant improvement ( p < 0.001 ) on the english evaluation and 5/10 of the conll languages .

to annotate more accurately or to annotate more
the common accepted wisdom is that blind double annotation followed by adjudication of disagreements is necessary to create training and test corpora that result in the best possible performance . we provide evidence that this is unlikely to be the case . rather , the greatest value for your annotation dollar lies in single annotating more data .

cipher type detection human language technology
manual analysis and decryption of enciphered documents is a tedious and error prone work . ofteneven after spending large amounts of time on a particular cipherno decipherment can be found . automating the decryption of various types of ciphers makes it possible to sift through the large number of encrypted messages found in libraries and archives , and to focus human effort only on a small but potentially interesting subset of them . in this work , we train a classifier that is able to predict which encipherment method has been used to generate a given ciphertext . we are able to distinguish 50 different cipher types ( specified by the american cryptogram association ) with an accuracy of 58.5 % . this is a 11.2 % absolute improvement over the best previously published classifier .

determining word sense dominance using a thesaurus
the degree of dominance of a sense of a word is the proportion of occurrences of that sense in text . we propose four new methods to accurately determine word sense dominance using raw text and a published thesaurus . unlike the mccarthy et al ( 2004 ) system , these methods can be used on relatively small target texts , without the need for a similarly-sensedistributed auxiliary text . we perform an extensive evaluation using artificially generated thesaurus-sense-tagged data . in the process , we create a wordcategory cooccurrence matrix , which can be used for unsupervised word sense disambiguation and estimating distributional similarity of word senses , as well .

representation and treatment of multiword expressions in basque
this paper describes the representation of basque multiword lexical units and the automatic processing of multiword expressions . after discussing and stating which kind of multiword expressions we consider to be processed at the current stage of the work , we present the representation schema of the corresponding lexical units in a generalpurpose lexical database . due to its expressive power , the schema can deal not only with fixed expressions but also with morphosyntactically flexible constructions . it also allows us to lemmatize word combinations as a unit and yet to parse the components individually if necessary . moreover , we describe habil , a tool for the automatic processing of these expressions , and we give some evaluation results . this work must be placed in a general framework of written basque processing tools , which currently ranges from the tokenization and segmentation of single words up to the syntactic tagging of general texts .

extracting parallel phrases from comparable data
mining parallel data from comparable corpora is a promising approach for overcoming the data sparseness in statistical machine translation and other nlp applications . even if two comparable documents have few or no parallel sentence pairs , there is still potential for parallelism in the sub-sentential level . the ability to detect these phrases creates a valuable resource , especially for low-resource languages . in this paper we explore three phrase alignment approaches to detect parallel phrase pairs embedded in comparable sentences : the standard phrase extraction algorithm , which relies on the viterbi path ; a phrase extraction approach that does not rely on the viterbi path , but uses only lexical features ; and a binary classifier that detects parallel phrase pairs when presented with a large collection of phrase pair candidates . we evaluate the effectiveness of these approaches in detecting alignments for phrase pairs that have a known alignment in comparable sentence pairs . the results show that the non-viterbi alignment approach outperforms the other two approaches on f1 measure .

on dual decomposition and linear programming relaxations for natural language processing
this paper introduces dual decomposition as a framework for deriving inference algorithms for nlp problems . the approach relies on standard dynamic-programming algorithms as oracle solvers for sub-problems , together with a simple method for forcing agreement between the different oracles . the approach provably solves a linear programming ( lp ) relaxation of the global inference problem . it leads to algorithms that are simple , in that they use existing decoding algorithms ; efficient , in that they avoid exact algorithms for the full model ; and often exact , in that empirically they often recover the correct solution in spite of using an lp relaxation . we give experimental results on two problems : 1 ) the combination of two lexicalized parsing models ; and 2 ) the combination of a lexicalized parsing model and a trigram part-of-speech tagger .

real-time correction of closed-captions
live closed-captions for deaf and hard of hearing audiences are currently produced by stenographers , or by voice writers using speech recognition . both techniques can produce captions with errors . we are currently developing a correction module that allows a user to intercept the real-time caption stream and correct it before it is broadcast . we report results of preliminary experiments on correction rate and actual user performance using a prototype correction module connected to the output of a speech recognition captioning system .

in unsupervised learning
unsupervised learning of grammar is a problem that can be important in many areas ranging from text preprocessing for information retrieval and classification to machine translation . we describe an mdl based grammar of a language that contains morphology and lexical categories . we use an unsupervised learner of morphology to bootstrap the acquisition of lexical categories and use these two learning processes iteratively to help and constrain each other . to be able to do so , we need to make our existing morphological analysis less fine grained . we present an algorithm for collapsing morphological classes ( signatures ) by using syntactic context . our experiments demonstrate that this collapse preserves the relation between morphology and lexical categories within new signatures , and thereby minimizes the description length of the model .

re-ranking models based-on small training data for spoken language
the design of practical language applications by means of statistical approaches requires annotated data , which is one of the most critical constraint . this is particularly true for spoken dialog systems since considerably domain-specific conceptual annotation is needed to obtain accurate language understanding models . since data annotation is usually costly , methods to reduce the amount of data are needed . in this paper , we show that better feature representations serve the above purpose and that structure kernels provide the needed improved representation . given the relatively high computational cost of kernel methods , we apply them to just re-rank the list of hypotheses provided by a fast generative model . experiments with support vector machines and different kernels on two different dialog corpora show that our re-ranking models can achieve better results than state-of-the-art approaches when small data is available .

factored soft source syntactic constraints for hierarchical machine raytheon bbn technologies raytheon bbn technologies raytheon bbn technologies
this paper describes a factored approach to incorporating soft source syntactic constraints into a hierarchical phrase-based translation system . in contrast to traditional approaches that directly introduce syntactic constraints to translation rules by explicitly decorating them with syntactic annotations , which often exacerbate the data sparsity problem and cause other problems , our approach keeps translation rules intact and factorizes the use of syntactic constraints through two separate models : 1 ) a syntax mismatch model that associates each nonterminal of a translation rule with a distribution of tags that is used to measure the degree of syntactic compatibility of the translation rule on source spans ; 2 ) a syntax-based reordering model that predicts whether a pair of sibling constituents in the constituent parse tree of the source sentence should be reordered or not when translated to the target language . the features produced by both models are used as soft constraints to guide the translation process . experiments on chinese-english translation show that the proposed approach significantly improves a strong string-to-dependency translation system on multiple evaluation sets .

extracting definitions and hypernym relations relying on syntactic dependencies and support vector machines luigi di caro
in this paper we present a technique to reveal definitions and hypernym relations from text . instead of using pattern matching methods that rely on lexico-syntactic patterns , we propose a technique which only uses syntactic dependencies between terms extracted with a syntactic parser . the assumption is that syntactic information are more robust than patterns when coping with length and complexity of the sentences . afterwards , we transform such syntactic contexts in abstract representations , that are then fed into a support vector machine classifier . the results on an annotated dataset of definitional sentences demonstrate the validity of our approach overtaking current state-of-the-art techniques .

automatic evaluation of summaries using n-gram
following the recent adoption by the machine translation community of automatic evaluation using the bleu/nist scoring process , we conduct an in-depth study of a similar idea for evaluating summaries . the results show that automatic evaluation using unigram cooccurrences between summary pairs correlates surprising well with human evaluations , based on various statistical metrics ; while direct application of the bleu evaluation procedure does not always give good results .

determining the sentiment of opinions
identifying sentiments ( the affective parts of opinions ) is a challenging problem . we present a system that , given a topic , automatically finds the people who hold opinions about that topic and the sentiment of each opinion . the system contains a module for determining word sentiment and another for combining sentiments within a sentence . we experiment with various models of classifying and combining sentiment at word and sentence levels , with promising results .

negation detection in swedish clinical text
negex , a rule-based algorithm that detects negations in english clinical text , was translated into swedish and evaluated on clinical text written in swedish . the negex algorithm detects negations through the use of trigger phrases , which indicate that a preceding or following concept is negated . a list of english trigger phrases was translated into swedish , taking grammatical differences between the two languages into account . this translation was evaluated on a set of 436 manually classified sentences from swedish health records . the results showed a precision of 70 % and a recall of 81 % for sentences containing the trigger phrases and a negative predictive value of 96 % for sentences not containing any trigger phrases . the precision was significantly lower for the swedish adaptation than published results on the english version , but since many negated propositions were identified through a limited set of trigger phrases , it could nevertheless be concluded that the same trigger phrase approach is possible in a swedish context , even though it needs to be further developed .

how comparable are parallel corpora measuring the distribution of general vocabulary and connectives bruno cartoni sandrine zufferey thomas meyer andrei popescu-belis
in this paper , we question the homogeneity of a large parallel corpus by measuring the similarity between various sub-parts . we compare results obtained using a general measure of lexical similarity based on 2 and by counting the number of discourse connectives . we argue that discourse connectives provide a more sensitive measure , revealing differences that are not visible with the general measure . we also provide evidence for the existence of specific characteristics defining translated texts as opposed to nontranslated ones , due to a universal tendency for explicitation .

improving dependency parsing with subtrees from auto-parsed data
this paper presents a simple and effective approach to improve dependency parsing by using subtrees from auto-parsed data . first , we use a baseline parser to parse large-scale unannotated data . then we extract subtrees from dependency parse trees in the auto-parsed data . finally , we construct new subtree-based features for parsing algorithms . to demonstrate the effectiveness of our proposed approach , we present the experimental results on the english penn treebank and the chinese penn treebank . these results show that our approach significantly outperforms baseline systems . and , it achieves the best accuracy for the chinese data and an accuracy which is competitive with the best known systems for the english data .

simulating early-termination search for verbose spoken queries
building search engines that can respond to spoken queries with spoken content requires that the system not just be able to find useful responses , but also that it know when it has heard enough about what the user wants to be able to do so . this paper describes a simulation study with queries spoken by non-native speakers that suggests that indicates that finding relevant content is often possible within a half minute , and that combining features based on automatically recognized words with features designed for automated prediction of query difficulty can serve as a useful basis for predicting when that useful content has been found .

speech retrieval in unknown languages : a pilot study
most cross-lingual speech retrieval assumes intensive knowledge about all involved languages . however , such resource may not exist for some less popular languages . some applications call for speech retrieval in unknown languages . in this work , we leverage on a quasi-language-independent subword recognizer trained on multiple languages , to obtain an abstracted representation of speech data in an unknown language . languageindependent query expansion is achieved either by allowing a wide lattice output for an audio query , or by taking advantage of distinctive features in speech articulation to propose subwords most similar to the given subwords in a query . we propose using a retrieval model based on finite state machines for fuzzy matching of speech sound patterns , and further for speech retrieval . a pilot study of speech retrieval in unknown languages is presented , using english , spanish and russian as training languages , and croatian as the unknown target language .

underspecified beta reduction
for ambiguous sentences , traditional semantics construction produces large numbers of higher-order formulas , which must then be -reduced individually . underspecified versions can produce compact descriptions of all readings , but it is not known how to perform -reduction on these descriptions . we show how to do this using -reduction constraints in the constraint language for -structures ( clls ) .

hedge trimmer : a parse-and-trim approach to headline generation
this paper presents hedge trimmer , a headline generation system that creates a headline for a newspaper story using linguistically-motivated heuristics to guide the choice of a potential headline . we present feasibility tests used to establish the validity of an approach that constructs a headline by selecting words in order from a story . in addition , we describe experimental results that demonstrate the effectiveness of our linguistically-motivated approach over a hmm-based model , using both human evaluation and automatic metrics for comparing the two approaches .

wide-coverage parsing of speech transcripts
this paper discusses the performance difference of wide-coverage parsers on small-domain speech transcripts . two parsers ( c & c ccg and rasp ) are tested on the speech transcripts of two different domains ( parent-child language , and picture descriptions ) . the performance difference between the domain-independent parsers and two domain-trained parsers ( mstparser and megrasp ) is substantial , with a difference of at least 30 percent point in accuracy . despite this gap , some of the grammatical relations can still be recovered reliably .

dialogue act tagging for instant messaging chat sessions
instant messaging chat sessions are realtime text-based conversations which can be analyzed using dialogue-act models . we describe a statistical approach for modelling and detecting dialogue acts in instant messaging dialogue . this involved the collection of a small set of task-based dialogues and annotating them with a revised tag set . we then dealt with segmentation and synchronisation issues which do not arise in spoken dialogue . the model we developed combines naive bayes and dialogue-act n-grams to obtain better than 80 % accuracy in our tagging experiment .

heterogeneous transfer learning for image clustering via the social web
in this paper , we present a new learning scenario , heterogeneous transfer learning , which improves learning performance when the data can be in different feature spaces and where no correspondence between data instances in these spaces is provided . in the past , we have classified chinese text documents using english training data under the heterogeneous transfer learning framework . in this paper , we present image clustering as an example to illustrate how unsupervised learning can be improved by transferring knowledge from auxiliary heterogeneous data obtained from the social web . image clustering is useful for image sense disambiguation in query-based image search , but its quality is often low due to imagedata sparsity problem . we extend plsa to help transfer the knowledge from social web data , which have mixed feature representations . experiments on image-object clustering and scene clustering tasks show that our approach in heterogeneous transfer learning based on the auxiliary data is indeed effective and promising .

text modification for bulgarian sign language users slavina lozanova ivelina stoyanova svetlozara leseva svetla koeva boian savtchev
the paper discusses the main issues regarding the reading skills and comprehension proficiency in written bulgarian of people with communication difficulties , and deaf people , in particular . we consider several key components of text comprehension which pose a challenge for deaf readers and propose a rule-based system for automatic modification of bulgarian texts intended to facilitate comprehension by deaf people , to assist education , etc . in order to demonstrate the benefits of such a system and to evaluate its performance , we have carried out a study among a group of deaf people who use bulgarian sign language ( bulsl ) as their primary language ( primary bulsl users ) , which compares the comprehensibility of original texts and their modified versions . the results shows a considerable improvement in readability when using modified texts , but at the same time demonstrates that the level of comprehension is still low , and that a complex set of modifications will have to be implemented to attain satisfactory results .

bootstrapping semantic parsers from conversations computer science & engineering
conversations provide rich opportunities for interactive , continuous learning . when something goes wrong , a system can ask for clarification , rewording , or otherwise redirect the interaction to achieve its goals . in this paper , we present an approach for using conversational interactions of this type to induce semantic parsers . we demonstrate learning without any explicit annotation of the meanings of user utterances . instead , we model meaning with latent variables , and introduce a loss function to measure how well potential meanings match the conversation . this loss drives the overall learning approach , which induces a weighted ccg grammar that could be used to automatically bootstrap the semantic analysis component in a complete dialog system . experiments on darpa communicator conversational logs demonstrate effective learning , despite requiring no explicit meaning annotations .

automatic interpretation of the english possessive
the english s possessive construction occurs frequently in text and can encode several different semantic relations ; however , it has received limited attention from the computational linguistics community . this paper describes the creation of a semantic relation inventory covering the use of s , an inter-annotator agreement study to calculate how well humans can agree on the relations , a large collection of possessives annotated according to the relations , and an accurate automatic annotation system for labeling new examples . our 21,938 example dataset is by far the largest annotated possessives dataset we are aware of , and both our automatic classification system , which achieves 87.4 % accuracy in our classification experiment , and our annotation data are publicly available .

latent domain translation models in mix-of-domains haystack
this paper addresses the problem of selecting adequate training sentence pairs from a mix-ofdomains parallel corpus for a translation task represented by a small in-domain parallel corpus . we propose a novel latent domain translation model which includes domain priors , domaindependent translation models and language models . the goal of learning is to estimate the probability of a sentence pair in mix-domain corpus to be in- or out-domain using in-domain corpus statistics as prior . we derive an em training algorithm and provide solutions for estimating out-domain models ( given only in- and mix-domain data ) . we report on experiments in data selection ( intrinsic ) and machine translation ( extrinsic ) on a large parallel corpus consisting of a mix of a rather diverse set of domains . our results show that our latent domain invitation approach outperforms the existing baselines significantly . we also provide analysis of the merits of our approach relative to existing approaches . large parallel corpora are important for training statistical mt systems . besides size , the relevance of a parallel training corpus to the translation task at hand can be decisive for system performance , cf . ( axelrod et al. , 2011 ; koehn and haddow , 2012 ) .

composite kernels for relation extraction
the automatic extraction of relations between entities expressed in natural language text is an important problem for ir and text understanding . in this paper we show how different kernels for parse trees can be combined to improve the relation extraction quality . on a public benchmark dataset the combination of a kernel for phrase grammar parse trees and for dependency parse trees outperforms all known tree kernel approaches alone suggesting that both types of trees contain complementary information for relation extraction .

probabilistic ontology trees for belief tracking in dialog systems
we introduce a novel approach for robust belief tracking of user intention within a spoken dialog system . the space of user intentions is modeled by a probabilistic extension of the underlying domain ontology called a probabilistic ontology tree ( pot ) . pots embody a principled approach to leverage the dependencies among domain concepts and incorporate corroborating or conflicting dialog observations in the form of interpreted user utterances across dialog turns . we tailor standard inference algorithms to the pot framework to efficiently compute the user intentions in terms of m-best most probable explanations . we empirically validate the efficacy of our pot and compare it to a hierarchical frame-based approach in experiments with users of a tourism information system .

extracting syntactic features from a korean treebank
in this paper , we present a system which can extract syntactic feature structures from a korean treebank ( sejong treebank ) to develop a feature-based lexicalized tree adjoining grammars .

paraphrasing with search engine query logs
this paper proposes a method that extracts paraphrases from search engine query logs . the method first extracts paraphrase query-title pairs based on an assumption that a search query and its corresponding clicked document titles may mean the same thing . it then extracts paraphrase query-query and title-title pairs from the query-title paraphrases with a pivot approach . paraphrases extracted in each step are validated with a binary classifier . we evaluate the method using a query log from baidu1 , a chinese search engine . experimental results show that the proposed method is effective , which extracts more than 3.5 million pairs of paraphrases with a precision of over 70 % . the results also show that the extracted paraphrases can be used to generate high-quality paraphrase patterns .

application of prize based on sentence length in chunk-based automatic evaluation of machine translation
as described in this paper , we propose a new automatic evaluation metric for machine translation . our metric is based on chunking between the reference and candidate translation . moreover , we apply a prize based on sentence-length to the metric , dissimilar from penalties in bleu or nist . we designate this metric as automatic evaluation of machine translation in which the prize is applied to a chunkbased metric ( apac ) . through metaevaluation experiments and comparison with several metrics , we confirmed that our metric shows stable correlation with human judgment .

modeling syntactic and semantic structures in hierarchical phrase-based hal daume iii
incorporating semantic structure into a linguistics-free translation model is challenging , since semantic structures are closely tied to syntax . in this paper , we propose a two-level approach to exploiting predicate-argument structure reordering in a hierarchical phrase-based translation model . first , we introduce linguistically motivated constraints into a hierarchical model , guiding translation phrase choices in favor of those that respect syntactic boundaries . second , based on such translation phrases , we propose a predicate-argument structure reordering model that predicts reordering not only between an argument and its predicate , but also between two arguments . experiments on chinese-to-english translation demonstrate that both advances significantly improve translation accuracy .

lihla : shared task system description
in this paper we describe lihla , a lexical aligner which uses bilingual probabilistic lexicons generated by a freely available set of tools ( natools ) and languageindependent heuristics to find links between single words and multiword units in sentence-aligned parallel texts . the method has achieved an alignment error rate of 22.72 % and 44.49 % on english inuktitut and romanianenglish parallel sentences , respectively .

cross-lingual induction of selectional preferences with bilingual vector spaces
we describe a cross-lingual method for the induction of selectional preferences for resourcepoor languages , where no accurate monolingual models are available . the method uses bilingual vector spaces to translate foreign language predicate-argument structures into a resource-rich language like english . the only prerequisite for constructing the bilingual vector space is a large unparsed corpus in the resource-poor language , although the model can profit from ( even noisy ) syntactic knowledge . our experiments show that the cross-lingual predictions correlate well with human ratings , clearly outperforming monolingual baseline models .

integrated shallow and deep parsing : topp meets hpsg
we present a novel , data-driven method for integrated shallow and deep parsing . mediated by an xml-based multi-layer annotation architecture , we interleave a robust , but accurate stochastic topological field parser of german with a constraintbased hpsg parser . our annotation-based method for dovetailing shallow and deep phrasal constraints is highly flexible , allowing targeted and fine-grained guidance of constraint-based parsing . we conduct systematic experiments that demonstrate substantial performance gains.1

prosodic cues to discourse segment boundaries in human-computer
theories of discourse structure hypothesize a hierarchical structure of discourse segments , typically tree-structured . while substantial work has been done on identifying and automatically recognizing the textual and prosodic correlates of discourse structure in monologue , comparable cues for dialogue or multiparty conversation , and in particular humancomputer dialogue remain relatively less studied . in this paper , we explore prosodic cues to discourse segmentation in humancomputer dialogue . using data drawn from 60 hours of interactions with a voice-only conversational spoken language system , we identify pitch and intensity features that signal segment boundaries . specifically , based on 473 pairs of segment-final and segmentinitiating utterances , we find significant increases for segment-initial utterances in maximum pitch , average pitch , and average intensity , while segment-final utterances show significantly lower minimum pitch . these results suggest that even in the artificial environment of human-computer dialogue , prosodic cues robustly signal discourse segment structure , comparably to the contrastive uses of pitch and amplitude identified in natural monologues . keywords dialogue systems , discourse structure , prosody in understanding

event detection and summarization in weblogs with temporal collocations
this paper deals with the relationship between weblog content and time . with the proposed temporal mutual information , we analyze the collocations in time dimension , and the interesting collocations related to special events . the temporal mutual information is employed to observe the strength of term-to-term associations over time . an event detection algorithm identifies the collocations that may cause an event in a specific timestamp . an event summarization algorithm retrieves a set of collocations which describe an event . we compare our approach with the approach without considering the time interval . the experimental results demonstrate that the temporal collocations capture the real world semantics and real world events over time .

multi-tagging for lexicalized-grammar parsing
with performance above 97 % accuracy for newspaper text , part of speech ( pos ) tagging might be considered a solved problem . previous studies have shown that allowing the parser to resolve pos tag ambiguity does not improve performance . however , for grammar formalisms which use more fine-grained grammatical categories , for example tag and ccg , tagging accuracy is much lower . in fact , for these formalisms , premature ambiguity resolution makes parsing infeasible . we describe a multi-tagging approach which maintains a suitable level of lexical category ambiguity for accurate and efficient ccg parsing . we extend this multitagging approach to the pos level to overcome errors introduced by automatically assigned pos tags . although pos tagging accuracy seems high , maintaining some pos tag ambiguity in the language processing pipeline results in more accurate ccg supertagging .

automatic detection of on-deverbal event ouns for quick lexicon production
in this work we present the results of experimental work on the development of lexical class-based lexica by automatic means . our purpose is to assess the use of linguistic lexical-class based information as a feature selection methodology for the use of classifiers in quick lexical development . the results show that the approach can help reduce the human effort required in the development of language resources significantly .

semafor : frame argument resolution with log-linear models
this paper describes the semafor systems performance in the semeval 2010 task on linking events and their participants in discourse . our entry is based upon semafor 1.0 ( das et al , 2010a ) , a frame-semantic probabilistic parser built from log-linear models . the extended system models null instantiations , including non-local argument reference . performance is evaluated on the task data with and without gold-standard overt arguments . in both settings , it fares the best of the submitted systems with respect to recall and f 1 .

graphical models over multiple strings
we study graphical modeling in the case of stringvalued random variables . whereas a weighted finite-state transducer can model the probabilistic relationship between two strings , we are interested in building up joint models of three or more strings . this is needed for inflectional paradigms in morphology , cognate modeling or language reconstruction , and multiple-string alignment . we propose a markov random field in which each factor ( potential function ) is a weighted finite-state machine , typically a transducer that evaluates the relationship between just two of the strings . the full joint distribution is then a product of these factors . though decoding is actually undecidable in general , we can still do efficient joint inference using approximate belief propagation ; the necessary computations and messages are all finitestate . we demonstrate the methods by jointly predicting morphological forms .

monolingual web-based factoid question answering in chinese ,
in this paper we extend the application of our statistical pattern classification approach to question answering ( qa ) which has previously been applied successfully to english and japanese to develop two prototype qa systems in chinese and swedish . we show what data is necessary to achieve this and also evaluate the performance of the two new systems using a translation of the trec 2003 factoid qa task . while performance for chinese and swedish is found to be lower than that for the more developed english and japanese systems we explain why this is the case and offer solutions for their improvement . all systems form the basis of our publicly accessible web-based multilingual qa system at http : //asked.jp .

semantic parsing using content and context : a case study from requirements elicitation
we present a model for the automatic semantic analysis of requirements elicitation documents . our target semantic representation employs live sequence charts , a multi-modal visual language for scenariobased programming , which can be directly translated into executable code . the architecture we propose integrates sentencelevel and discourse-level processing in a generative probabilistic framework for the analysis and disambiguation of individual sentences in context . we show empirically that the discourse-based model consistently outperforms the sentence-based model when constructing a system that reflects all the static ( entities , properties ) and dynamic ( behavioral scenarios ) requirements in the document .

a tale about pro and monsters
while experimenting with tuning on long sentences , we made an unexpected discovery : that pro falls victim to monsters overly long negative examples with very low bleu+1 scores , which are unsuitable for learning and can cause testing bleu to drop by several points absolute . we propose several effective ways to address the problem , using length- and bleu+1based cut-offs , outlier filters , stochastic sampling , and random acceptance . the best of these fixes not only slay and protect against monsters , but also yield higher stability for pro as well as improved testtime bleu scores . thus , we recommend them to anybody using pro , monsterbeliever or not .

generating natural-language video descriptions using text-mined knowledge
we present a holistic data-driven technique that generates natural-language descriptions for videos . we combine the output of state-ofthe-art object and activity detectors with realworld knowledge to select the most probable subject-verb-object triplet for describing a video . we show that this knowledge , automatically mined from web-scale text corpora , enhances the triplet selection algorithm by providing it contextual information and leads to a four-fold increase in activity identification . unlike previous methods , our approach can annotate arbitrary videos without requiring the expensive collection and annotation of a similar training video corpus . we evaluate our technique against a baseline that does not use text-mined knowledge and show that humans prefer our descriptions 61 % of the time .

dependency parsing with reference to slovene , spanish and swedish natural language processing natural language processing
we describe a parser used in the conll 2006 shared task , multingual dependency parsing . the parser first identifies syntactic dependencies and then labels those dependencies using a maximum entropy classifier . we consider the impact of feature engineering and the choice of machine learning algorithm , with particular focus on slovene , spanish and swedish .

unsupervised spectral learning of wcfg as low-rank matrix completion
we derive a spectral method for unsupervised learning of weighted context free grammars . we frame wcfg induction as finding a hankel matrix that has low rank and is linearly constrained to represent a function computed by inside-outside recursions . the proposed algorithm picks the grammar that agrees with a sample and is the simplest with respect to the nuclear norm of the hankel matrix .

discourse connectors for latent subjectivity in sentiment analysis
document-level sentiment analysis can benefit from fine-grained subjectivity , so that sentiment polarity judgments are based on the relevant parts of the document . while finegrained subjectivity annotations are rarely available , encouraging results have been obtained by modeling subjectivity as a latent variable . however , latent variable models fail to capitalize on our linguistic knowledge about discourse structure . we present a new method for injecting linguistic knowledge into latent variable subjectivity modeling , using discourse connectors . connector-augmented transition features allow the latent variable model to learn the relevance of discourse connectors for subjectivity transitions , without subjectivity annotations . this yields significantly improved performance on documentlevel sentiment analysis in english and spanish . we also describe a simple heuristic for automatically identifying connectors when no predefined list is available .

evaluating the word sense disambiguation performance of statistical
we present the first known empirical test of an increasingly common speculative claim , by evaluating a representative chinese-toenglish smt model directly on word sense disambiguation performance , using standard wsd evaluation methodology and datasets from the senseval-3 chinese lexical sample task . much effort has been put in designing and evaluating dedicated word sense disambiguation ( wsd ) models , in particular with the senseval series of workshops . at the same time , the recent improvements in the bleu scores of statistical machine translation ( smt ) suggests that smt models are good at predicting the right translation of the words in source language sentences . surprisingly however , the wsd accuracy of smt models has never been evaluated and compared with that of the dedicated wsd models . we present controlled experiments showing the wsd accuracy of current typical smt models to be significantly lower than that of all the dedicated wsd models considered . this tends to support the view that despite recent speculative claims to the contrary , current smt models do have limitations in comparison with dedicated wsd models , and that smt should benefit from the better predictions made by the wsd models .

an alternate approach towards meaningful lyric generation in tamil ananth ramakrishnan a sobha lalitha devi
this paper presents our on-going work to improve the lyric generation component of the automatic lyric generation system for the tamil language . an earlier version of the system used an n-gram based model to generate lyrics that match the given melody . this paper identifies some of the deficiencies in the melody analysis and text generation components of the earlier system and explains the new approach used to tackle those drawbacks . the two central approaches discussed in this paper are : ( 1 ) an improved mapping scheme for matching melody with words and ( 2 ) knowledge-based text generation algorithm based on an existing ontology and tamil morphology generator .

open-source tools for morphology , lemmatization , pos tagging and named entity recognition
we present two recently released opensource taggers : nametag is a free software for named entity recognition ( ner ) which achieves state-of-the-art performance on czech ; morphodita ( morphological dictionary and tagger ) performs morphological analysis ( with lemmatization ) , morphological generation , tagging and tokenization with state-of-the-art results for czech and a throughput around 10-200k words per second . the taggers can be trained for any language for which annotated data exist , but they are specifically designed to be efficient for inflective languages , both tools are free software under lgpl license and are distributed along with trained linguistic models which are free for non-commercial use under the cc by-nc-sa license . the releases include standalone tools , c++ libraries with java , python and perl bindings and web services .

what good are nominalkomposita for noun compounds : of nominal compositions using linguistic restrictors patrick ziering lonneke van der plas
finding a definition of compoundhood that is cross-lingually valid is a non-trivial task as shown by linguistic literature . we present an iterative method for defining and extracting english noun compounds in a multilingual setting . we show how linguistic criteria can be used to extract compounds automatically and vice versa how the results of this extraction can shed new lights on linguistic theories about compounding . the extracted compound nouns and their multilingual contexts are a rich source that serves several purposes . in an additional case study we show how the database serves to predict the internal structure of tripartite noun compounds using spelling variations across languages , which leads to a precision of over 91 % .

answering opinion questions with random walks on graphs
opinion question answering ( opinion qa ) , which aims to find the authors sentimental opinions on a specific target , is more challenging than traditional factbased question answering problems . to extract the opinion oriented answers , we need to consider both topic relevance and opinion sentiment issues . current solutions to this problem are mostly ad-hoc combinations of question topic information and opinion information . in this paper , we propose an opinion pagerank model and an opinion hits model to fully explore the information from different relations among questions and answers , answers and answers , and topics and opinions . by fully exploiting these relations , the experiment results show that our proposed algorithms outperform several state of the art baselines on benchmark data set . a gain of over 10 % in f scores is achieved as compared to many other systems .

normalization and paraphrasing using symbolic methods
we describe an ongoing work in information extraction which is seen as a text normalization task . the normalized representation can be used to detect paraphrases in texts . normalization and paraphrase detection tasks are built on top of a robust analyzer for english and are exclusively achieved using symbolic methods . both grammar development rules and information extraction rules are expressed within the same formalism and are developed in an integrated way . the experiment we describe in the paper is evaluated and presents encouraging results .

the vi framework program in europe : centro per la ricerca scientifica e tecnologica itc-irst
significant progress has been made in the field of human language technologies . various tasks like continuous speech recognition for large vocabulary , speaker and language identification , spoken information inquiry , information extraction and cross-language retrieval in restricted domains are today feasible and different prototypes and systems are running . the spoken translation problem on the other hand is still a significant challenge : good text translation was hard enough to pull off . speech to speech mt was beyond going to the moon it was mars [ steve silbermann , wired magazine ] . considering the major achievements of the last years obtained in the field and the related challenges , a question arise : what next is it possible to foresee in the next decade real services and applications how can we reach this goal shall we rethink the approach shall we need much more critical mass how about data to answer to these questions a new preparatory action , tc_star_p , funded in the v framework , has been settled in europe . goals , objective and activities of this preparatory action will also be discussed in this paper

incremental decoding for phrase-based statistical machine translation
in this paper we focus on the incremental decoding for a statistical phrase-based machine translation system . in incremental decoding , translations are generated incrementally for every word typed by a user , instead of waiting for the entire sentence as input . we introduce a novel modification to the beam-search decoding algorithm for phrase-based mt to address this issue , aimed at efficient computation of future costs and avoiding search errors . our objective is to do a faster translation during incremental decoding without significant reduction in the translation quality .

detecting parser errors using web-based semantic filters
nlp systems for tasks such as question answering and information extraction typically rely on statistical parsers . but the efficacy of such parsers can be surprisingly low , particularly for sentences drawn from heterogeneous corpora such as the web . we have observed that incorrect parses often result in wildly implausible semantic interpretations of sentences , which can be detected automatically using semantic information obtained from the web . based on this observation , we introduce web-based semantic filteringa novel , domain-independent method for automatically detecting and discarding incorrect parses . we measure the effectiveness of our filtering system , called woodward , on two test collections . on a set of trec questions , it reduces error by 67 % . on a set of more complex penn treebank sentences , the reduction in error rate was 20 % .

cocqa : co-training over questions and answers with an application to predicting question subjectivity orientation
an increasingly popular method for finding information online is via the community question answering ( cqa ) portals such as yahoo ! answers , naver , and baidu knows . searching the cqa archives , and ranking , filtering , and evaluating the submitted answers requires intelligent processing of the questions and answers posed by the users . one important task is automatically detecting the questions subjectivity orientation : namely , whether a user is searching for subjective or objective information . unfortunately , real user questions are often vague , ill-posed , poorly stated . furthermore , there has been little labeled training data available for real user questions . to address these problems , we present cocqa , a co-training system that exploits the association between the questions and contributed answers for question analysis tasks . the co-training approach allows cocqa to use the effectively unlimited amounts of unlabeled data readily available in cqa archives . in this paper we study the effectiveness of cocqa for the question subjectivity classification task by experimenting over thousands of real users questions .

a preliminary look into the use of named entity information for bioscience text tokenization
tokenization in the bioscience domain is often difficult . new terms , technical terminology , and nonstandard orthography , all common in bioscience text , contribute to this difficulty . this paper will introduce the tasks of tokenization , normalization before introducing bacchant , a system built for bioscience text normalization . casting tokenization / normalization as a problem of punctuation classification motivates using machine learning methods in the implementation of this system . the evaluation of bacchant 's performance included error analysis of the system 's performance inside and outside of named entities ( nes ) from the genia corpus , which led to the creation of a normalization system trained solely on data from inside nes , bacchant-n. evaluation of this new system indicated that normalization systems trained on data inside nes perform better than systems trained both inside and outside nes , motivating a merging of tokenization and named entity tagging processes as opposed to the standard pipelining approach .

toward using morphology in french-english phrase-based smt
we describe the system used in our submission to the wmt-2009 french-english translation task . we use the moses phrasebased statistical machine translation system with two simple modications of the decoding input and word-alignment strategy based on morphology , and analyze their impact on translation quality .

directional distributional similarity for lexical expansion
distributional word similarity is most commonly perceived as a symmetric relation . yet , one of its major applications is lexical expansion , which is generally asymmetric . this paper investigates the nature of directional ( asymmetric ) similarity measures , which aim to quantify distributional feature inclusion . we identify desired properties of such measures , specify a particular one based on averaged precision , and demonstrate the empirical benefit of directional measures for expansion .

a computational framework for non-lexicalist semantics
under a lexicalist approach to semantics , a verb completely encodes its syntactic and semantic structures , along with the relevant syntax-tosemantics mapping ; polysemy is typically attributed to the existence of different lexical entries . a lexicon organized in this fashion contains much redundant information and is unable to capture cross-categorial morphological derivations . the solution is to spread the semantic load of lexical entries to other morphemes not typically taken to bear semantic content . this approach follows current trends in linguistic theory , and more perspicuously accounts for alternations in argument structure . i demonstrate how such a framework can be computationally realized with a feature-based , agenda-driven chart parser for the minimalist program .

combining acoustic confidences and pragmatic plausibility for classifying spoken chess move instructions
this paper describes a machine learning approach to classifying n-best speech recognition hypotheses as either correctly or incorrectly recognised . the learners are trained on a combination of acoustic confidence features and move evaluation scores in a chess-playing scenario . the results show significant improvements over sharp baselines that use confidence rejection thresholds for classification .

instance sense induction from attribute sets
this paper investigates the new problem of automatic sense induction for instance names using automatically extracted attribute sets . several clustering strategies and data sources are described and evaluated . we also discuss the drawbacks of the evaluation metrics commonly used in similar clustering tasks . the results show improvements in most metrics with respect to the baselines , especially for polysemous instances .

incremental hypothesis alignment for building confusion networks with application to machine translation system combination
confusion network decoding has been the most successful approach in combining outputs from multiple machine translation ( mt ) systems in the recent darpa gale and nist open mt evaluations . due to the varying word order between outputs from different mt systems , the hypothesis alignment presents the biggest challenge in confusion network decoding . this paper describes an incremental alignment method to build confusion networks based on the translation edit rate ( ter ) algorithm . this new algorithm yields significant bleu score improvements over other recent alignment methods on the gale test sets and was used in bbns submission to the wmt08 shared translation task .

gathering and generating paraphrases from twitter with application to normalization wei xu+ alan ritter ralph grishman+
we present a new and unique paraphrase resource , which contains meaningpreserving transformations between informal user-generated text . sentential paraphrases are extracted from a comparable corpus of temporally and topically related messages on twitter which often express semantically identical information through distinct surface forms . we demonstrate the utility of this new resource on the task of paraphrasing and normalizing noisy text , showing improvement over several state-of-the-art paraphrase and normalization systems 1 .

finding non-local dependencies : beyond pattern matching
we describe an algorithm for recovering non-local dependencies in syntactic dependency structures . the patternmatching approach proposed by johnson ( 2002 ) for a similar task for phrase structure trees is extended with machine learning techniques . the algorithm is essentially a classifier that predicts a nonlocal dependency given a connected fragment of a dependency structure and a set of structural features for this fragment . evaluating the algorithm on the penn treebank shows an improvement of both precision and recall , compared to the results presented in ( johnson , 2002 ) .

large scale relation detection
we present a technique for reading sentences and producing sets of hypothetical relations that the sentence may be expressing . the technique uses large amounts of instance-level background knowledge about the relations in order to gather statistics on the various ways the relation may be expressed in language , and was inspired by the observation that half of the linguistic forms used to express relations occur very infrequently and are simply not considered by systems that use too few seed examples . some very early experiments are presented that show promising results .

detecting transliterated orthographic variants via two similarity metrics keihanna science city
we propose a detection method for orthographic variants caused by transliteration in a large corpus . the method employs two similarities . one is string similarity based on edit distance . the other is contextual similarity by a vector space model . experimental results show that the method performed a 0.889 f-measure in an open test .

an error analysis tool for natural language processing and applied
in this paper we present a simple to use web based error analysis tool to help computational linguists , researchers building language applications , and non-technical personnel managing development of language tools to analyze the predictions made by their machine learning models . the only expectation is that the users of the tool convert their data into an intuitive xml format . once the xml is ready , several error analysis functionalities that promote principled feature engineering are a click away .

morphological analysis and generation for arabic dialects
we present magead , a morphological analyzer and generator for the arabic language family . our work is novel in that it explicitly addresses the need for processing the morphology of the dialects . magead provides an analysis to a root+pattern representation , it has separate phonological and orthographic representations , and it allows for combining morphemes from different dialects .

using maximum entropy to extract biomedical named entities
current ner approaches include : dictionary-based , rule-based , or machine learning . since there is no consolidated nomenclature for most biomedical nes , most ner systems relying on limited dictionaries or rules do not perform satisfactorily . in this paper , we apply maximum entropy ( me ) to construct our ner framework . we represent shallow linguistic information as linguistic features in our me model . on the genia 3.02 corpus , our system achieves satisfactory f-scores of 74.3 % in protein and 70.0 % overall without using any dictionary . our system performs significantly better than dictionary-based systems . using partial match criteria , our system achieves an f-score of 81.3 % . using appropriate domain knowledge to modify the boundaries , our system has the potential to achieve an f-score of over 80 % .

universal morphological analysis using structured nearest neighbor
in this paper , we consider the problem of unsupervised morphological analysis from a new angle . past work has endeavored to design unsupervised learning methods which explicitly or implicitly encode inductive biases appropriate to the task at hand . we propose instead to treat morphological analysis as a structured prediction problem , where languages with labeled data serve as training examples for unlabeled languages , without the assumption of parallel data . we define a universal morphological feature space in which every language and its morphological analysis reside . we develop a novel structured nearest neighbor prediction method which seeks to find the morphological analysis for each unlabeled language which lies as close as possible in the feature space to a training language . we apply our model to eight inflecting languages , and induce nominal morphology with substantially higher accuracy than a traditional , mdlbased approach . our analysis indicates that accuracy continues to improve substantially as the number of training languages increases .

wordnet-based semantic relatedness measures in automatic speech recognition for meetings
this paper presents the application of wordnet-based semantic relatedness measures to automatic speech recognition ( asr ) in multi-party meetings . different word-utterance context relatedness measures and utterance-coherence measures are defined and applied to the rescoring of n best lists . no significant improvements in terms of word-error-rate ( wer ) are achieved compared to a large word-based ngram baseline model . we discuss our results and the relation to other work that achieved an improvement with such models for simpler tasks .

extracting contextual evaluativity
recent work on evaluativity or sentiment in the language sciences has focused on the contributions that lexical items provide . in this paper , we discuss contextual evaluativity , stance that is inferred from lexical meaning and pragmatic environments . focusing on assessor-grounding claims like we liked him because he so clearly disliked margaret thatcher , we build a corpus and construct a system employing compositional principles of evaluativity calculation to derive that we dislikes margaret thatcher . the resulting system has an f-score of 0.90 on our dataset , outperforming reasonable baselines , and indicating the viability of inferencing in the evaluative domain .

discrete optimization as an alternative to sequential processing in nlg
we present an nlg system that uses integer linear programming to integrate different decisions involved in the generation process . our approach provides an alternative to pipeline-based sequential processing which has become prevalent in todays nlg applications .

correlation between rouge and human evaluation of extractive meeting
automatic summarization evaluation is critical to the development of summarization systems . while rouge has been shown to correlate well with human evaluation for content match in text summarization , there are many characteristics in multiparty meeting domain , which may pose potential problems to rouge . in this paper , we carefully examine how well the rouge scores correlate with human evaluation for extractive meeting summarization . our experiments show that generally the correlation is rather low , but a significantly better correlation can be obtained by accounting for several unique meeting characteristics , such as disfluencies and speaker information , especially when evaluating system-generated summaries .

parsing conversational speech using enhanced segmentation
the lack of sentence boundaries and presence of disfluencies pose difficulties for parsing conversational speech . this work investigates the effects of automatically detecting these phenomena on a probabilistic parsers performance . we demonstrate that a state-of-the-art segmenter , relative to a pause-based segmenter , gives more than 45 % of the possible error reduction in parser performance , and that presentation of interruption points to the parser improves performance over using sentence boundaries alone .

combining cbir and nlp for multilingual terminology alignment and cross-language image indexing
in this paper , an overview of an approach for cross-language image indexing and multilingual terminology alignment is presented . content-based image retrieval ( cbir ) is proposed as a means to find similar images in target language documents in the web and natural language processing is used to reduce the search space and find the image index . as the experiments are carried out in specialized domains , a systematic and recursive use of the approach is used to align multilingual terminology by creating repositories of images with their respective cross-language indices .

the infinite hmm for unsupervised pos tagging
we extend previous work on fully unsupervised part-of-speech tagging . using a non-parametric version of the hmm , called the infinite hmm ( ihmm ) , we address the problem of choosing the number of hidden states in unsupervised markov models for pos tagging . we experiment with two non-parametric priors , the dirichlet and pitman-yor processes , on the wall street journal dataset using a parallelized implementation of an ihmm inference algorithm . we evaluate the results with a variety of clustering evaluation metrics and achieve equivalent or better performances than previously reported . building on this promising result we evaluate the output of the unsupervised pos tagger as a direct replacement for the output of a fully supervised pos tagger for the task of shallow parsing and compare the two evaluations .

improving mt word alignment using aligned multi-stage parses
we use hand-coded rules and graph-aligned logical dependencies to reorder english text towards chinese word order . we obtain a 1.5 % higher f-score for giza++ compared to running with unprocessed text . we describe this research and its implications for smt .

task-focused summarization of email
we describe smartmail , a prototype system for automatically identifying action items ( tasks ) in email messages . smartmail presents the user with a task-focused summary of a message . the summary consists of a list of action items extracted from the message . the user can add these action items to their to do list .

exploiting syntactic patterns as clues in zero-anaphora resolution
we approach the zero-anaphora resolution problem by decomposing it into intra-sentential and inter-sentential zeroanaphora resolution . for the former problem , syntactic patterns of the appearance of zero-pronouns and their antecedents are useful clues . taking japanese as a target language , we empirically demonstrate that incorporating rich syntactic pattern features in a state-of-the-art learning-based anaphora resolution model dramatically improves the accuracy of intra-sentential zero-anaphora , which consequently improves the overall performance of zeroanaphora resolution .

word segmentation standard in chinese , japanese and korean seoul korea seok mun pak
word segmentation is a process to divide a sentence into meaningful units called word unit [ iso/dis 24614-1 ] . what is a word unit is judged by principles for its internal integrity and external use constraints . a word units internal structure is bound by principles of lexical integrity , unpredictability and so on in order to represent one syntactically meaningful unit . principles for external use include language economy and frequency such that word units could be registered in a lexicon or any other storage for practical reduction of processing complexity for the further syntactic processing after word segmentation . such principles for word segmentation are applied for chinese , japanese and korean , and impacts of the standard are discussed .

generalizing inflection tables into paradigms with finite state operations
extracting and performing an alignment of the longest common subsequence in inflection tables has been shown to be a fruitful approach to supervised learning of morphological paradigms . however , finding the longest subsequence common to multiple strings is well known to be an intractable problem . additional constraints on the solution sought complicate the problem furthersuch as requiring that the particular subsequence extracted , if there is ambiguity , be one that is best alignable in an inflection table . in this paper we present and discuss the design of a tool that performs the extraction through some advanced techniques in finite state calculus and does so efficiently enough for the practical purposes of inflection table generalization .

assessing the challenge of fine-grained named entity recognition and classification asif ekbal , eva sourjikova , anette frank and simone paolo ponzetto
named entity recognition and classification ( nerc ) is a well-studied nlp task typically focused on coarse-grained named entity ( ne ) classes . nerc for more fine-grained semantic ne classes has not been systematically studied . this paper quantifies the difficulty of fine-grained nerc ( fg-nerc ) when performed at large scale on the people domain . we apply unsupervised acquisition methods to construct a gold standard dataset for fg-nerc . this dataset is used to benchmark methods for classifying nes at various levels of fine-grainedness using classical nerc techniques and global contextual information inspired fromword sense disambiguation approaches . our results indicate high difficulty of the task and provide a strong baseline for future research .

for learning from ambiguous supervision
we present a probabilistic generative model for learning semantic parsers from ambiguous supervision . our approach learns from natural language sentences paired with world states consisting of multiple potential logical meaning representations . it disambiguates the meaning of each sentence while simultaneously learning a semantic parser that maps sentences into logical form . compared to a previous generative model for semantic alignment , it also supports full semantic parsing . experimental results on the robocup sportscasting corpora in both english and korean indicate that our approach produces more accurate semantic alignments than existing methods and also produces competitive semantic parsers and improved language generators .

event detection and summarization in weblogs with temporal collocations
this paper deals with the relationship between weblog content and time . with the proposed temporal mutual information , we analyze the collocations in time dimension , and the interesting collocations related to special events . the temporal mutual information is employed to observe the strength of term-to-term associations over time . an event detection algorithm identifies the collocations that may cause an event in a specific timestamp . an event summarization algorithm retrieves a set of collocations which describe an event . we compare our approach with the approach without considering the time interval . the experimental results demonstrate that the temporal collocations capture the real world semantics and real world events over time .

generating english determiners in phrase-based translation with synthetic translation options
we propose a technique for improving the quality of phrase-based translation systems by creating synthetic translation optionsphrasal translations that are generated by auxiliary translation and postediting processesto augment the default phrase inventory learned from parallel data . we apply our technique to the problem of producing english determiners when translating from russian and czech , languages that lack definiteness morphemes . our approach augments the english side of the phrase table using a classifier to predict where english articles might plausibly be added or removed , and then we decode as usual . doing so , we obtain significant improvements in quality relative to a standard phrase-based baseline and to a to post-editing complete translations with the classifier .

two monolingual parses are better than one ( synchronous parse )
we describe a synchronous parsing algorithm that is based on two successive monolingual parses of an input sentence pair . although the worst-case complexity of this algorithm is and must be o ( n 6 ) for binary scfgs , its average-case run-time is far better . we demonstrate that for a number of common synchronous parsing problems , the two-parse algorithm substantially outperforms alternative synchronous parsing strategies , making it efficient enough to be utilized without resorting to a pruned search .

recognising the predicateargument structure of tagalog
this paper describes research on parsing tagalog text for predicateargument structure ( pas ) . we first outline the linguistic phenomenon and corpus annotation process , then detail a series of pas parsing experiments .

can we translate letters
current statistical machine translation systems handle the translation process as the transformation of a string of symbols into another string of symbols . normally the symbols dealt with are the words in different languages , sometimes with some additional information included , like morphological data . in this work we try to push the approach to the limit , working not on the level of words , but treating both the source and target sentences as a string of letters . we try to find out if a nearly unmodified state-of-the-art translation system is able to cope with the problem and whether it is capable to further generalize translation rules , for example at the level of word suffixes and translation of unseen words . experiments are carried out for the translation of catalan to spanish .

unsupervised part-of-speech tagging employing efficient graph clustering
an unsupervised part-of-speech ( pos ) tagging system that relies on graph clustering methods is described . unlike in current state-of-the-art approaches , the kind and number of different tags is generated by the method itself . we compute and merge two partitionings of word graphs : one based on context similarity of high frequency words , another on log-likelihood statistics for words of lower frequencies . using the resulting word clusters as a lexicon , a viterbi pos tagger is trained , which is refined by a morphological component . the approach is evaluated on three different languages by measuring agreement with existing taggers .

analysis and detection of reading miscues for interactive literacy tutors
the colorado literacy tutor ( clt ) is a technology-based literacy program , designed on the basis of cognitive theory and scientifically motivated reading research , which aims to improve literacy and student achievement in public schools . one of the critical components of the clt is a speech recognition system which is used to track the childs progress during oral reading and to provide sufficient information to detect reading miscues . in this paper , we extend on prior work by examining a novel labeling of childrens oral reading audio data in order to better understand the factors that contribute most significantly to speech recognition errors . while these events make up nearly 8 % of the data , they are shown to account for approximately 30 % of the word errors in a state-of-the-art speech recognizer . next , we consider the problem of detecting miscues during oral reading . using features derived from the speech recognizer , we demonstrate that 67 % of reading miscues can be detected at a false alarm rate of 3 % .

annotating a japanese text corpus with
in this paper , we discuss how to annotate coreference and predicate-argument relations in japanese written text . there have been research activities for building japanese text corpora annotated with coreference and predicate-argument relations as are done in the kyoto text corpus version 4.0 ( kawahara et al , 2002 ) and the gdatagged corpus ( hasida , 2005 ) . however , there is still much room for refining their specifications . for this reason , we discuss issues in annotating these two types of relations , and propose a new specification for each . in accordance with the specification , we built a large-scaled annotated corpus , and examined its reliability . as a result of our current work , we have released an annotated corpus named the naist text corpus1 , which is used as the evaluation data set in the coreference and zero-anaphora resolution tasks in iida et al ( 2005 ) and iida et al ( 2006 ) .

adaptive hter estimation for document-specific mt post-editing
we present an adaptive translation quality estimation ( qe ) method to predict the human-targeted translation error rate ( hter ) for a document-specific machine translation model . we first introduce features derived internal to the translation decoding process as well as externally from the source sentence analysis . we show the effectiveness of such features in both classification and regression of mt quality . by dynamically training the qe model for the document-specific mt model , we are able to achieve consistency and prediction quality across multiple documents , demonstrated by the higher correlation coefficient and f-scores in finding good sentences . additionally , the proposed method is applied to ibm english-to-japanese mt post editing field study and we observe strong correlation with human preference , with a 10 % increase in human translators productivity .

lexical computing ltd
we present an english lexical database which is fuller , more accurate and more consistent than any other . we believe this to be so because the project has been well-planned , with a 12-month intensive planning phase prior to the lexicography beginning ; well-resourced , employing a team of fifteen highly experienced lexicographers for a thirty-month main phase ; it has had access to the latest corpus and dictionary-editing technology ; it has not been constrained to meet any goals other than an accurate description of the language ; and it has been led by a team with singular experience in delivering high-quality and innovative resources . the lexicon will be complete in summer 2010 and will be available for nlp groups , on terms designed to encourage its research use .

named entity recognition : different approaches
the talk deals with different approaches used for named entity recognition and how they are used in developing a robust named entity recognizer . the talk includes the development of tagset for ner and manual annotation of text .

a tree adjoining grammar analysis of the syntax and semantics of
in this paper , we argue that in it-clefts as in it was ohno who won , the cleft pronoun ( it ) and the cleft clause ( who won ) form a discontinuous syntactic constituent , and a semantic unit as a definite description , presenting arguments from percus ( 1997 ) and hedberg ( 2000 ) . we propose a syntax of it-clefts using tree-local multicomponent tree adjoining grammar and a compositional semantics on the proposed syntax using synchronous tree adjoining grammar .

comparing user simulation models for dialog strategy learning intelligent systems program
this paper explores what kind of user simulation model is suitable for developing a training corpus for using markov decision processes ( mdps ) to automatically learn dialog strategies . our results suggest that with sparse training data , a model that aims to randomly explore more dialog state spaces with certain constraints actually performs at the same or better than a more complex model that simulates realistic user behaviors in a statistical way .

zdenek z abokrtsky
the valency lexicon of czech verbs , version 1.0 ( vallex 1.0 ) is a collection of linguistically annotated data and documentation , resulting from an attempt at formal description of valency frames of czech verbs . vallex 1.0 is closely related to prague dependency treebank . in this paper , the context in which vallex came into existence is briefly outlined , and also three similar projects for english verbs are mentioned . the core of the paper is the description of the logical structure of the vallex data . finally , we suggest a few directions of the future research .

learning to fuse disparate sentences linguistic information processing ( bllip )
we present a system for fusing sentences which are drawn from the same source document but have different content . unlike previous work , our approach is supervised , training on real-world examples of sentences fused by professional journalists in the process of editing news articles . like filippova and strube ( 2008 ) , our system merges dependency graphs using integer linear programming . however , instead of aligning the inputs as a preprocess , we integrate the tasks of finding an alignment and selecting a merged sentence into a joint optimization problem , and learn parameters for this optimization using a structured online algorithm . evaluation by human judges shows that our technique produces fused sentences that are both informative and readable .

text alignment for real-time crowd captioning
the primary way of providing real-time captioning for deaf and hard of hearing people is to employ expensive professional stenographers who can type as fast as natural speaking rates . recent work has shown that a feasible alternative is to combine the partial captions of ordinary typists , each of whom types part of what they hear . in this paper , we describe an improved method for combining partial captions into a final output based on weighted a search and multiple sequence alignment ( msa ) . in contrast to prior work , our method allows the tradeoff between accuracy and speed to be tuned , and provides formal error bounds . our method outperforms the current state-of-the-art on word error rate ( wer ) ( 29.6 % ) , bleu score ( 41.4 % ) , and f-measure ( 36.9 % ) . the end goal is for these captions to be used by people , and so we also compare how these metrics correlate with the judgments of 50 study participants , which may assist others looking to make further progress on this problem .

determining compositionality of word expressions using word space models
this research focuses on determining semantic compositionality of word expressions using word space models ( wsms ) . we discuss previous works employing wsms and present differences in the proposed approaches which include types of wsms , corpora , preprocessing techniques , methods for determining compositionality , and evaluation testbeds . we also present results of our own approach for determining the semantic compositionality based on comparing distributional vectors of expressions and their components . the vectors were obtained by latent semantic analysis ( lsa ) applied to the ukwac corpus . our results outperform those of all the participants in the distributional semantics and compositionality ( disco ) 2011 shared task .

flexible spoken dialogue system based on user models and dynamic generation of voicexml scripts kazunori komatani fumihiro adachi shinichi ueno
we realize a telephone-based collaborative natural language dialogue system . since natural language involves very various expressions , a large number of voicexml scripts need to be prepared to handle all possible input patterns . we realize flexible dialogue management for various user utterances by generating voicexml scripts dynamically . moreover , we address appropriate user modeling in order to generate cooperative responses to each user . specifically , we set up three dimensions of user models : skill level to the system , knowledge level on the target domain and the degree of hastiness . the models are automatically derived by decision tree learning using real dialogue data collected by the system . experimental evaluation shows that the cooperative responses adapted to individual users serve as good guidance for novice users without increasing the dialogue duration for skilled users .

a comparative evaluation of deep and shallow approaches to the automatic detection of common grammatical errors
this paper compares a deep and a shallow processing approach to the problem of classifying a sentence as grammatically wellformed or ill-formed . the deep processing approach uses the xle lfg parser and english grammar : two versions are presented , one which uses the xle directly to perform the classification , and another one which uses a decision tree trained on features consisting of the xles output statistics . the shallow processing approach predicts grammaticality based on n-gram frequency statistics : we present two versions , one which uses frequency thresholds and one which uses a decision tree trained on the frequencies of the rarest n-grams in the input sentence . we find that the use of a decision tree improves on the basic approach only for the deep parser-based approach . we also show that combining both the shallow and deep decision tree features is effective . our evaluation is carried out using a large test set of grammatical and ungrammatical sentences . the ungrammatical test set is generated automatically by inserting grammatical errors into well-formed bnc sentences .

on the parameterized complexity of linear context-free rewriting systems
we study the complexity of uniform membership for linear context-free rewriting systems , i.e. , the problem where we are given a string w and a grammar g and are asked whether w l ( g ) . in particular , we use parameterized complexity theory to investigate how the complexity depends on various parameters . while we focus primarily on rank and fan-out , derivation length is also considered .

ksc-pal : a peer learning agent that encourages students to take the barbara di eugenio
we present an innovative application of dialogue processing concepts to educational technology . in a previous corpus analysis of peer learning dialogues , we found that initiative and initiative shifts are indicative of learning , and of learning-conducive episodes . we have incorporated this finding in ksc-pal , a peer learning agent . ksc-pal promotes learning by encouraging shifts in task initiative .

first joint workshop on statistical parsing of morphologically rich languages initial explorations in two-phase turkish dependency parsing by ilknur durgar el-kahlout ahmet afsn akn
this paper describes a two-phase turkish dependency parsing which separates dependency and labeling into two similar to ( mcdonald et al. , 2006b ) . first , in order to solve the long distance dependency attachment problem , the sentences are split into constituents and the dependencies are estimated on shorter sentences . later , for better estimation of labels , conditional random fields ( crfs ) are used with previously learned chunk and several dependency and morphosyntactic features . finally , a post-processing step is applied to correct some of labels , if necessary .

vi-xfst : a visual regular expression development environment for xerox finite state tool
this paper describes vi-xfst , a visual interface and a development environment , for developing finite state language processing applications using the xerox finite state tool , xfst . vi-xfst lets a user construct complex regular expressions via a drag-anddrop visual interface , treating simpler regular expressions as lego blocks . it also enables the visualization of the structure of the regular expression components , providing a birds eye view of the overall system , enabling a user to easily understand and track the structural and functional relationships among the components involved . since the structure of a large regular expression ( built in terms of other regular expressions ) is now transparent , users can also interact with regular expressions at any level of detail , easily navigating among them for testing . vi-xfst also keeps track of dependencies among the regular expressions at a very finegrained level . so when a certain regular expression is modified as a result of testing , only the dependent regular expressions are recompiled resulting in an improvement in development process time , by avoiding file level recompiles which usually causes redundant regular expression compilations .

bounding and comparing methods for correlation clustering beyond ilp
we evaluate several heuristic solvers for correlation clustering , the np-hard problem of partitioning a dataset given pairwise affinities between all points . we experiment on two practical tasks , document clustering and chat disentanglement , to which ilp does not scale . on these datasets , we show that the clustering objective often , but not always , correlates with external metrics , and that local search always improves over greedy solutions . we use semi-definite programming ( sdp ) to provide a tighter bound , showing that simple algorithms are already close to optimality .

fbk nk : a wordnet-based system for multi-way classification of semantic relations
we describe a wordnet-based system for the extraction of semantic relations between pairs of nominals appearing in english texts . the system adopts a lightweight approach , based on training a bayesian network classifier using large sets of binary features . our features consider : i ) the context surrounding the annotated nominals , and ii ) different types of knowledge extracted from wordnet , including direct and explicit relations between the annotated nominals , and more general and implicit evidence ( e.g . semantic boundary collocations ) . the system achieved a macro-averaged f1 of 68.02 % on the multi-way classification of semantic relations between pairs of nominals task ( task # 8 ) at semeval-2010 .

multiparty turn taking in situated dialog
we report on an empirical study of a multiparty turn-taking model for physically situated spoken dialog systems . we present subjective and objective performance measures that show how the model , supported with a basic set of sensory competencies and turn-taking policies , can enable interactions with multiple participants in a collaborative task setting . the analysis brings to the fore several phenomena and frames challenges for managing multiparty turn taking in physically situated interaction .

evaluating a crosslinguistic grammar resource : a case study of wambaya
this paper evaluates the lingo grammar matrix , a cross-linguistic resource for the development of precision broad coverage grammars , by applying it to the australian language wambaya . despite large typological differences between wambaya and the languages on which the development of the resource was based , the grammar matrix is found to provide a significant jump-start in the creation of the grammar for wambaya : with less than 5.5 person-weeks of development , the wambaya grammar was able to assign correct semantic representations to 76 % of the sentences in a naturally occurring text . while the work on wambaya identified some areas of refinement for the grammar matrix , 59 % of the matrix-provided types were invoked in the final wambaya grammar , and only 4 % of the matrix-provided types required modification .

stochastic inversion transduction grammars for obtaining word phrases for phrase-based statistical machine translation
an important problem that is related to phrase-based statistical translation models is the obtaining of word phrases from an aligned bilingual training corpus . in this work , we propose obtaining word phrases by means of a stochastic inversion translation grammar . experiments on the shared task proposed in this workshop with the europarl corpus have been carried out and good results have been obtained .

efficient online summarization of microblogging streams
the large amounts of data generated on microblogging services are making summarization challenging . previous research has mostly focused on working in batches or with filtered streams . input data has to be saved and analyzed several times , in order to detect underlying events and then summarize them . we improve the efficiency of this process by designing an online abstractive algorithm . processing is done in a single pass , removing the need to save any input data and improving the running time . an online approach is also able to generate the summaries in real time , using the latest information . the algorithm we propose uses a word graph , along with optimization techniques such as decaying windows and pruning . it outperforms the baseline in terms of summary quality , as well as time and memory efficiency .

human gene name normalization using text matching with automatically extracted synonym dictionaries division of oncology , childrens hospital of philadelphia
the identification of genes in biomedical text typically consists of two stages : identifying gene mentions and normalization of gene names . we have created an automated process that takes the output of named entity recognition ( ner ) systems designed to identify genes and normalizes them to standard referents . the system identifies human gene synonyms from online databases to generate an extensive synonym lexicon . the lexicon is then compared to a list of candidate gene mentions using various string transformations that can be applied and chained in a flexible order , followed by exact string matching or approximate string matching . using a gold standard of medline abstracts manually tagged and normalized for mentions of human genes , a combined tagging and normalization system achieved 0.669 f-measure ( 0.718 precision and 0.626 recall ) at the mention level , and 0.901 f-measure ( 0.957 precision and 0.857 recall ) at the document level for documents used for tagger training .

exploiting social relations and sentiment for stock prediction
in this paper we first exploit cash-tags ( $ followed by stocks ticker symbols ) in twitter to build a stock network , where nodes are stocks connected by edges when two stocks co-occur frequently in tweets . we then employ a labeled topic model to jointly model both the tweets and the network structure to assign each node and each edge a topic respectively . this semantic stock network ( ssn ) summarizes discussion topics about stocks and stock relations . we further show that social sentiment about stock ( node ) topics and stock relationship ( edge ) topics are predictive of each stocks market . for prediction , we propose to regress the topic-sentiment time-series and the stocks price time series . experimental results demonstrate that topic sentiments from close neighbors are able to help improve the prediction of a stock markedly .

optimizing chinese word segmentation for machine translation
previous work has shown that chinese word segmentation is useful for machine translation to english , yet the way different segmentation strategies affect mt is still poorly understood . in this paper , we demonstrate that optimizing segmentation for an existing segmentation standard does not always yield better mt performance . we find that other factors such as segmentation consistency and granularity of chinese words can be more important for machine translation . based on these findings , we implement methods inside a conditional random field segmenter that directly optimize segmentation granularity with respect to the mt task , providing an improvement of 0.73 bleu . we also show that improving segmentation consistency using external lexicon and proper noun features yields a 0.32 bleu increase .

linguistically-motivated grammar extraction ,
proposed a model to measure grammar coverage and designed a pcfg parser to measure efficiency of the grammar . to generalize grammars , a grammar binarization method was proposed to increase the coverage of a probabilistic contextfree grammar . in the mean time linguistically-motivated feature constraints were added into grammar rules to maintain precision of the grammar . the generalized grammar increases grammar coverage from 93 % to 99 % and bracketing f-score from 87 % to 91 % in parsing chinese sentences . to cope with error propagations due to word segmentation and part-of-speech tagging errors , we also proposed a grammar blending method to adapt to such errors . the blended grammar can reduce about 20~30 % of parsing errors due to error assignment of pos made by a word segmentation system .

corpus annotation by generation
as the interest in annotated corpora is spreading , there is increasing concern with using existing language technology for corpus processing . in this paper we explore the idea of using natural language generation systems for corpus annotation . resources for generation systems often focus on areas of linguistic variability that are under-represented in analysis-directed approaches . therefore , making use of generation resources promises some significant extensions in the kinds of annotation information that can be captured . we focus here on exploring the use of the kpml ( komet-penman multilingual ) generation system for corpus annotation . we describe the kinds of linguistic information covered in kpml and show the steps involved in creating a standard xml corpus representation from kpmls generation output .

optimizing textual entailment recognition using particle swarm fbk - irst
this paper introduces a new method to improve tree edit distance approach to textual entailment recognition , using particle swarm optimization . currently , one of the main constraints of recognizing textual entailment using tree edit distance is to tune the cost of edit operations , which is a difficult and challenging task in dealing with the entailment problem and datasets . we tried to estimate the cost of edit operations in tree edit distance algorithm automatically , in order to improve the results for textual entailment . automatically estimating the optimal values of the cost operations over all rte development datasets , we proved a significant enhancement in accuracy obtained on the test sets .

using kybots for extracting events in biomedical texts koldo gojenola ( ) maite oronoz ( ) german rigau ( )
in this paper we describe a rule-based system developed for the bionlp 2011 genia event detection task . the system applies kybots ( knowledge yielding robots ) on annotated texts to extract bio-events involving proteins or genes . the main goal of this work is to verify the usefulness and portability of the kybot technology to the domain of biomedicine .

analysis with boxer
boxer is an open-domain software component for semantic analysis of text , based on combinatory categorial grammar ( ccg ) and discourse representation theory ( drt ) . used together with the c & c tools , boxer reaches more than 95 % coverage on newswire texts . the semantic representations produced by boxer , known as discourse representation structures ( drss ) , incorporate a neo-davidsonian representations for events , using the verbnet inventory of thematic roles . the resulting drss can be translated to ordinary first-order logic formulas and be processing by standard theorem provers for first-order logic . boxers performance on the shared task for comparing semantic represtations was promising . it was able to produce complete drss for all seven texts . manually inspecting the output revealed that : ( a ) the computed predicate argument structure was generally of high quality , in particular dealing with hard constructions involving control or coordination ; ( b ) discourse structure triggered by conditionals , negation or discourse adverbs was overall correctly computed ; ( c ) some measure and time expressions are correctly analysed , others arent ; ( d ) several shallow analyses are given for lexical phrases that require deep analysis ; ( e ) bridging references and pronouns are not resolved in most cases . boxer is distributed with the c & c tools and freely available for research purposes . 277 278 bos

were not in kansas anymore : detecting domain changes in streams
domain adaptation , the problem of adapting a natural language processing system trained in one domain to perform well in a different domain , has received significant attention . this paper addresses an important problem for deployed systems that has received little attention detecting when such adaptation is needed by a system operating in the wild , i.e. , performing classification over a stream of unlabeled examples . our method uses adistance , a metric for detecting shifts in data streams , combined with classification margins to detect domain shifts . we empirically show effective domain shift detection on a variety of data sets and shift conditions .

um-checker : a hybrid system for english grammatical error correction
this paper describes the nlp2ct grammatical error detection and correction system for the conll 2013 shared task , with a focus on the errors of article or determiner ( artordet ) , noun number ( nn ) , preposition ( prep ) , verb form ( vform ) and subject-verb agreement ( sva ) . a hybrid model is adopted for this special task . the process starts with spellchecking as a preprocessing step to correct any possible erroneous word . we used a maximum entropy classifier together with manually rule-based filters to detect the grammatical errors in english . a language model based on the google n-gram corpus was employed to select the best correction candidate from a confusion matrix . we also explored a graphbased label propagation approach to overcome the sparsity problem in training the model . finally , a number of deterministic rules were used to increase the precision and recall . the proposed model was evaluated on the test set consisting of 50 essays and with about 500 words in each essay . our system achieves the 5 th and 3 rd f1 scores on official test set among all 17 participating teams based on goldstandard edits before and after revision , respectively .

an efficient clustering algorithm for class-based language models takuya matsuzaki yusuke miyao crest , jst ( japan science and technology corporation )
this paper defines a general form for classbased probabilistic language models and proposes an efficient algorithm for clustering based on this . our evaluation experiments revealed that our method decreased computation time drastically , while retaining accuracy .

dialogue systems for virtual environments
we present an on-going research project carried out at the universidad nacional de cordoba in argentina . this project investigates theoretical and practical research questions related to the development of a dialogue system situated in a virtual environment . we describe the pln research group in which this project is being developed and , in particular , we spell out the areas of expertise of the authors . moreover , we discuss relevant past , current and future collaborations of the research group .

parallelism in coordination as an instance of syntactic priming : evidence from corpus-based modeling
experimental research in psycholinguistics has demonstrated a parallelism effect in coordination : speakers are faster at processing the second conjunct of a coordinate structure if it has the same internal structure as the first conjunct . we show that this phenomenon can be explained by the prevalence of parallel structures in corpus data . we demonstrate that parallelism is not limited to coordination , but also applies to arbitrary syntactic configurations , and even to documents . this indicates that the parallelism effect is an instance of a general syntactic priming mechanism in human language processing .

personalized recommendation of user comments via factor models
in recent years , the amount of user-generated opinionated texts ( e.g. , reviews , user comments ) continues to grow at a rapid speed : featured news stories on a major event easily attract thousands of user comments on a popular online news service . how to consume subjective information of this volume becomes an interesting and important research question . in contrast to previous work on review analysis that tried to filter or summarize information for a generic average user , we explore a different direction of enabling personalized recommendation of such information . for each user , our task is to rank the comments associated with a given article according to personalized user preference ( i.e. , whether the user is likely to like or dislike the comment ) . to this end , we propose a factor model that incorporates rater-comment and rater-author interactions simultaneously in a principled way . our full model significantly outperforms strong baselines as well as related models that have been considered in previous work .

normalizing tweets with edit scripts and recurrent neural embeddings
tweets often contain a large proportion of abbreviations , alternative spellings , novel words and other non-canonical language . these features are problematic for standard language analysis tools and it can be desirable to convert them to canonical form . we propose a novel text normalization model based on learning edit operations from labeled data while incorporating features induced from unlabeled data via character-level neural text embeddings . the text embeddings are generated using an simple recurrent network . we find that enriching the feature set with text embeddings substantially lowers word error rates on an english tweet normalization dataset . our model improves on stateof-the-art with little training data and without any lexical resources .

managing dialogue interaction : a multi-layered approach
we present evidence for the importance of low-level phenomena in dialogue interaction and use this to motivate a multi-layered approach to dialogue processing . we describe an architecture that separates content-level communicative processes from interaction-level phenomena ( such as feedback , grounding , turn-management ) , and provide details of specific implementations of a number of such phenomena .

domain-specific coreference resolution with lexicalized features
most coreference resolvers rely heavily on string matching , syntactic properties , and semantic attributes of words , but they lack the ability to make decisions based on individual words . in this paper , we explore the benefits of lexicalized features in the setting of domain-specific coreference resolution . we show that adding lexicalized features to off-the-shelf coreference resolvers yields significant performance gains on four domain-specific data sets and with two types of coreference resolution architectures .

the lingo redwoods treebank
the lingo redwoods initiative is a seed activity in the design and development of a new type of treebank . while several medium- to large-scale treebanks exist for english ( and for other major languages ) , pre-existing publicly available resources exhibit the following limitations : ( i ) annotation is mono-stratal , either encoding topological ( phrase structure ) or tectogrammatical ( dependency ) information , ( ii ) the depth of linguistic information recorded is comparatively shallow , ( iii ) the design and format of linguistic representation in the treebank hard-wires a small , predefined range of ways in which information can be extracted from the treebank , and ( iv ) representations in existing treebanks are static and over the ( often year- or decade-long ) evolution of a large-scale treebank tend to fall behind the development of the field . lingo redwoods aims at the development of a novel treebanking methodology , rich in nature and dynamic both in the ways linguistic data can be retrieved from the treebank in varying granularity and in the constant evolution and regular updating of the treebank itself . since october 2001 , the project is working to build the foundations for this new type of treebank , to develop a basic set of tools for treebank construction and maintenance , and to construct an initial set of 10,000 annotated trees to be distributed together with the tools under an open-source license .

best analysis selection in inflectional languages
ambiguity is the fundamental property of natural language . perhaps , the most burdensome case of ambiguity manifests itself on the syntactic level of analysis . in order to face up to the high number of obtained derivation trees , this paper describes several techniques for evaluation of the figures of merit , which define a sort order on parsing trees . the presented methods are based on language specific features of synthetical languages and they improve the results of simple stochastic approaches .

shallow semantic parsing of chinese
in this paper we address the question of assigning semantic roles to sentences in chinese . we show that good semantic parsing results for chinese can be achieved with a small 1100-sentence training set . in order to extract features from chinese , we describe porting the collins parser to chinese , resulting in the best performance currently reported on chinese syntactic parsing ; we include our headrules in the appendix . finally , we compare english and chinese semantic-parsing performance . while slight differences in argument labeling make a perfect comparison impossible , our results nonetheless suggest significantly better performance for chinese . we show that much of this difference is due to grammatical differences between english and chinese , such as the prevalence of passive in english , and the strict word order constraints on adjuncts in chinese .

automated error detection in digitized cultural heritage documents
the work reported in this paper aims at performance optimization in the digitization of documents pertaining to the cultural heritage domain . a hybrid method is proposed , combining statistical classification algorithms and linguistic knowledge to automatize post-ocr error detection and correction . the current paper deals with the integration of linguistic modules and their impact on error detection .

a multi-dimensional bayesian approach to lexical style
we adapt the popular lda topic model ( blei et al , 2003 ) to the representation of stylistic lexical information , evaluating our model on the basis of human-interpretability at the word and text level . we show , in particular , that this model can be applied to the task of inducing stylistic lexicons , and that a multi-dimensional approach is warranted given the correlations among stylistic dimensions .

wrapping of trees
we explore the descriptive power , in terms of syntactic phenomena , of a formalism that extends treeadjoining grammar ( tag ) by adding a fourth level of hierarchical decomposition to the three levels tag already employs . while extending the descriptive power minimally , the additional level of decomposition allows us to obtain a uniform account of a range of phenomena that has heretofore been difficult to encompass , an account that employs unitary elementary structures and eschews synchronized derivation operations , and which is , in many respects , closer to the spirit of the intuitions underlying tag-based linguistic theory than previously considered extensions to tag .

using wizard-of-oz simulations to bootstrap reinforcement-learningbased dialog management systems
this paper describes a method for bootstrapping a reinforcement learningbased dialog manager using a wizard-ofoz trial . the state space and action set are discovered through the annotation , and an initial policy is generated using a supervised learning algorithm . the method is tested and shown to create an initial policy which performs significantly better and with less effort than a handcrafted policy , and can be generated using a small number of dialogs .

on arabic error correction
the qalb-2014 shared task focuses on correcting errors in texts written in modern standard arabic . in this paper , we describe the columbia university entry in the shared task . our system consists of several components that rely on machinelearning techniques and linguistic knowledge . we submitted three versions of the system : these share several core elements but each version also includes additional components . we describe our underlying approach and the special aspects of the different versions of our submission . our system ranked first out of nine participating teams .

how synchronous are adjuncts in translation data
the argument-adjunct distinction is central to most syntactic and semantic theories . as optional elements that refine ( the meaning of ) a phrase , adjuncts are important for recursive , compositional accounts of syntax , semantics and translation . in formal accounts of machine translation , adjuncts are often treated as modifiers applying synchronously in source and target derivations . but how well can the assumption of synchronous adjunction explain translation equivalence in actual parallel data in this paper we present the first empirical study of translation equivalence of adjuncts on a variety of frenchenglish parallel corpora , while varying word alignments so we can gauge the effect of errors in them . we show that for proper measurement of the types of translation equivalence of adjuncts , we must work with non-contiguous , many-to-many relations , thereby amending the traditional direct correspondence assumption . our empirical results show that 70 % of manually identified adjuncts have adjunct translation equivalents in training data , against roughly 50 % for automatically identified adjuncts .

applying prosodic speech features in mental health care : an exploratory study in a life-review intervention for depression human media interaction human media interaction
the present study aims to investigate the application of prosodic speech features in a psychological intervention based on lifereview . several studies have shown that speech features can be used as indicators of depression severity , but these studies are mainly based on controlled speech recording tasks instead of natural conversations . the present exploratory study investigated speech features as indicators of depression in conversations of a therapeutic intervention . the changes in the prosodic speech features pitch , duration of pauses , and total duration of the participants speaking time were studied over four sessions of a life-review intervention for three older participants . the ecological validity of the dynamics observed for prosodic speech features could not be established in the present study . the changes in speech features differed from what can be expected in an intervention that is effective in decreasing depression and were inconsistent with each other for each of the participants . we suggest future research to investigate changes within the intervention sessions , to relate the changes in feature values to the topical content of the speech , and to relate the speech features directly to depression scores .

umcc_dlsi_semsim : multilingual system for measuring semantic textual similarity
in this paper we describe the specifications and results of umcc_dlsi system , which was involved in semeval-2014 addressing two subtasks of semantic textual similarity ( sts , task 10 , for english and spanish ) , and one subtask of cross-level semantic similarity ( task 3 ) . as a supervised system , it was provided by different types of lexical and semantic features to train a classifier which was used to decide the correct answers for distinct subtasks . these features were obtained applying the hungarian algorithm over a semantic network to create semantic alignments among words . regarding the spanish subtask of task 10 two runs were submitted , where our run2 was the best ranked with a general correlation of 0.807. however , for english subtask our best run ( run1 of our 3 runs ) reached 16th place of 38 of the official ranking , obtaining a general correlation of 0.682. in terms of task 3 , only addressing paragraph to sentence subtask , our best run ( run1 of 2 runs ) obtained a correlation value of 0.760 reaching 3rd place of 34 .

in ontology-based question answering
ambiguities are ubiquitous in natural language and pose a major challenge for the automatic interpretation of natural language expressions . in this paper we focus on different types of lexical ambiguities that play a role in the context of ontology-based question answering , and explore strategies for capturing and resolving them . we show that by employing underspecification techniques and by using ontological reasoning in order to filter out inconsistent interpretations as early as possible , the overall number of interpretations can be effectively reduced by 44 % .

using large monolingual and bilingual corpora to improve coordination disambiguation
resolving coordination ambiguity is a classic hard problem . this paper looks at coordination disambiguation in complex noun phrases ( nps ) . parsers trained on the penn treebank are reporting impressive numbers these days , but they dont do very well on this problem ( 79 % ) . we explore systems trained using three types of corpora : ( 1 ) annotated ( e.g . the penn treebank ) , ( 2 ) bitexts ( e.g . europarl ) , and ( 3 ) unannotated monolingual ( e.g . google n-grams ) . size matters : ( 1 ) is a million words , ( 2 ) is potentially billions of words and ( 3 ) is potentially trillions of words . the unannotated monolingual data is helpful when the ambiguity can be resolved through associations among the lexical items . the bilingual data is helpful when the ambiguity can be resolved by the order of words in the translation .

gloss-based semantic similarity metrics for predominant sense acquisition
in recent years there have been various approaches aimed at automatic acquisition of predominant senses of words . this information can be exploited as a powerful backoff strategy for word sense disambiguation given the zipfian distribution of word senses . approaches which do not require manually sense-tagged data have been proposed for english exploiting lexical resources available , notably wordnet . in these approaches distributional similarity is coupled with a semantic similarity measure which ties the distributionally related words to the sense inventory . the semantic similarity measures that have been used have all taken advantage of the hierarchical information in wordnet . we investigate the applicability to japanese and demonstrate the feasibility of a measure which uses only information in the dictionary definitions , in contrast with previous work on english which uses hierarchical information in addition to dictionary definitions . we extend the definition based semantic similarity measure with distributional similarity applied to the words in different definitions . this increases the recall of our method and in some cases , precision as well .

a very large dictionary with paradigmatic , syntagmatic , and paronymic links between entries
a very large russian dictionary is described . it contains currently 3.6 million links between its 120,000 entries . the links are syntagmatic ( collocations ) , paradigmatic ( wordnet-like ) , or paronymic ( words similar in letters or in morphs ) . the entries of the dictionary are sin-gle- or multiwords belonging to four main pos . the entries represent so-called gram-memes rather than lexemes : e.g. , nouns are represented as singular and plural ; verbs are split into finite forms + infinitive , participle , and gerund . the multiword entries in turn can be collocationsidiomatic freewhose parts are also entries of the same dictionary .

translating with non-contiguous phrases
this paper presents a phrase-based statistical machine translation method , based on non-contiguous phrases , i.e . phrases with gaps . a method for producing such phrases from a word-aligned corpora is proposed . a statistical translation model is also presented that deals such phrases , as well as a training method based on the maximization of translation accuracy , as measured with the nist evaluation metric . translations are produced by means of a beam-search decoder . experimental results are presented , that demonstrate how the proposed method allows to better generalize from the training data .

a brazilian portuguese phonological-prosodic algorithm applied to language acquisition : a case study
the paper presents a system for transcribing and annotating phonological information in brazilian portuguese , including syllabification . an application of this system for the assessment of language understanding and production is described , following a child longitudinally , comparing expected production with observed production .

automatic analysis of plot for story rewriting
a method for automatic plot analysis of narrative texts that uses components of both traditional symbolic analysis of natural language and statistical machine-learning is presented for the story rewriting task . in the story rewriting task , an exemplar story is read to the pupils and the pupils rewrite the story in their own words . this allows them to practice language skills such as spelling , diction , and grammar without being stymied by content creation . often the pupil improperly recalls the story . our method of automatic plot analysis enables the tutoring system to automatically analyze the students story for both general coherence and specific missing events .

toward learning and evaluation of dialogue policies with text examples
we present a dialogue collection and enrichment framework that is designed to explore the learning and evaluation of dialogue policies for simple conversational characters using textual training data . to facilitate learning and evaluation , our framework enriches a collection of role-play dialogues with additional training data , including paraphrases of user utterances , and multiple independent judgments by external referees about the best policy response for the character at each point . as a case study , we use this framework to train a policy for a limited domain tactical questioning character , reaching promising performance . we also introduce an automatic policy evaluation metric that recognizes the validity of multiple conversational responses at each point in a dialogue . we use this metric to explore the variability in human opinion about optimal policy decisions , and to automatically evaluate several learned policies in our example domain .

pos tagging of english-hindi code-mixed social media content jatin sharma kalika bali monojit choudhury
code-mixing is frequently observed in user generated content on social media , especially from multilingual users . the linguistic complexity of such content is compounded by presence of spelling variations , transliteration and non-adherance to formal grammar . we describe our initial efforts to create a multi-level annotated corpus of hindi-english codemixed text collated from facebook forums , and explore language identification , back-transliteration , normalization and pos tagging of this data . our results show that language identification and transliteration for hindi are two major challenges that impact pos tagging accuracy .

choosing the right translation : a syntactically informed classification approach
one style of multi-engine machine translation architecture involves choosing the best of a set of outputs from different systems . choosing the best translation from an arbitrary set , even in the presence of human references , is a difficult problem ; it may prove better to look at mechanisms for making such choices in more restricted contexts . in this paper we take a classificationbased approach to choosing between candidates from syntactically informed translations . the idea is that using multiple parsers as part of a classifier could help detect syntactic problems in this context that lead to bad translations ; these problems could be detected on either the source sideperhaps sentences with difficult or incorrect parses could lead to bad translationsor on the target sideperhaps the output quality could be measured in a more syntactically informed way , looking for syntactic abnormalities . we show that there is no evidence that the source side information is useful . however , a target-side classifier , when used to identify particularly bad translation candidates , can lead to significant improvements in bleu score . improvements are even greater when combined with existing language and alignment model approaches .

predicting fine-grained social roles with selectional preferences
selectional preferences , the tendencies of predicates to select for certain semantic classes of arguments , have been successfully applied to a number of tasks in computational linguistics including word sense disambiguation , semantic role labeling , relation extraction , and textual inference . here we leverage the information encoded in selectional preferences to the task of predicting fine-grained categories of authors on the social media platform twitter . first person uses of verbs that select for a given social role as subject ( e.g . i teach ... for teacher ) are used to quickly build up binary classifiers for that role .

estimation of consistent probabilistic context-free grammars
we consider several empirical estimators for probabilistic context-free grammars , and show that the estimated grammars have the so-called consistency property , under the most general conditions . our estimators include the widely applied expectation maximization method , used to estimate probabilistic context-free grammars on the basis of unannotated corpora . this solves a problem left open in the literature , since for this method the consistency property has been shown only under restrictive assumptions on the rules of the source grammar .

web-based interfaces for natural language processing tools
we have built web interfaces to a number of natural language processing technologies . these interfaces allow students to experiment with different inputs and view corresponding output and inner workings of the systems . when possible , the interfaces also enable the student to modify the knowledge bases of the systems and view the resulting change in behavior . such interfaces are important because they allow students without computer science background to learn by doing . web interfaces also sidestep issues of platform dependency in software packages , available computer lab times , etc . we discuss our basic approach and lessons learned .

design challenges and misconceptions in named entity recognition
we analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust ner system . in particular , we address issues such as the representation of text chunks , the inference approach needed to combine local ner decisions , the sources of prior knowledge and how to use them within an ner system . in the process of comparing several solutions to these challenges we reach some surprising conclusions , as well as develop an ner system that achieves 90.8 f1 score on the conll-2003 ner shared task , the best reported result for this dataset .

online methods for multi-domain learning and adaptation
nlp tasks are often domain specific , yet systems can learn behaviors across multiple domains . we develop a new multi-domain online learning framework based on parameter combination from multiple classifiers . our algorithms draw from multi-task learning and domain adaptation to adapt multiple source domain classifiers to a new target domain , learn across multiple similar domains , and learn across a large number of disparate domains . we evaluate our algorithms on two popular nlp domain adaptation tasks : sentiment classification and spam filtering .

confidence in structured-prediction using confidence-weighted models
confidence-weighted linear classifiers ( cw ) and its successors were shown to perform well on binary and multiclass nlp problems . in this paper we extend the cw approach for sequence learning and show that it achieves state-of-the-art performance on four noun phrase chucking and named entity recognition tasks . we then derive few algorithmic approaches to estimate the predictions correctness of each label in the output sequence . we show that our approach provides a reliable relative correctness information as it outperforms other alternatives in ranking label-predictions according to their error . we also show empirically that our methods output close to absolute estimation of error . finally , we show how to use this information to improve active learning .

predicting code-switching in multilingual communication for for advanced study
immigrant communities host multilingual speakers who switch across languages and cultures in their daily communication practices . although there are in-depth linguistic descriptions of code-switching across different multilingual communication settings , there is a need for automatic prediction of code-switching in large datasets . we use emoticons and multi-word expressions as novel features to predict code-switching in a large online discussion forum for the turkish-dutch immigrant community in the netherlands . our results indicate that multi-word expressions are powerful features to predict code-switching .

an expert lexicon approach to identifying english phrasal verbs
phrasal verbs are an important feature of the english language . properly identifying them provides the basis for an english parser to decode the related structures . phrasal verbs have been a challenge to natural language processing ( nlp ) because they sit at the borderline between lexicon and syntax . traditional nlp frameworks that separate the lexicon module from the parser make it difficult to handle this problem properly . this paper presents a finite state approach that integrates a phrasal verb expert lexicon between shallow parsing and deep parsing to handle morpho-syntactic interaction . with precision/recall combined performance benchmarked consistently at 95.8 % -97.5 % , the phrasal verb identification problem has basically been solved with the presented method .

identification of patients with acute lung injury from free-text chest x-ray reports cosmin adrian bejan
identification of complex clinical phenotypes among critically ill patients is a major challenge in clinical research . the overall research goal of our work is to develop automated approaches that accurately identify critical illness phenotypes to prevent the resource intensive manual abstraction approach . in this paper , we describe a text processing method that uses natural language processing ( nlp ) and supervised text classification methods to identify patients who are positive for acute lung injury ( ali ) based on the information available in free-text chest x-ray reports . to increase the classification performance we enhanced the baseline unigram representation with bigram and trigram features , enriched the n-gram features with assertion analysis , and applied statistical feature selection . we used 10-fold cross validation for evaluation and our best performing classifier achieved 81.70 % precision ( positive predictive value ) , 75.59 % recall ( sensitivity ) , 78.53 % f-score , 74.61 % negative predictive value , 76.80 % specificity in identifying patients with ali .

gaiku : generating haiku with word associations norms
creativeness / a pleasing field / of bloom word associations are an important element of linguistic creativity . traditional lexical knowledge bases such as wordnet formalize a limited set of systematic relations among words , such as synonymy , polysemy and hypernymy . such relations maintain their systematicity when composed into lexical chains . we claim that such relations can not explain the type of lexical associations common in poetic text . we explore in this paper the usage of word association norms ( wans ) as an alternative lexical knowledge source to analyze linguistic computational creativity . we specifically investigate the haiku poetic genre , which is characterized by heavy reliance on lexical associations . we first compare the density of wan-based word associations in a corpus of english haiku poems to that of wordnet-based associations as well as in other non-poetic genres . these experiments confirm our hypothesis that the non-systematic lexical associations captured in wans play an important role in poetic text . we then present gaiku , a system to automatically generate haikus from a seed word and using wan-associations . human evaluation indicate that generated haikus are of lesser quality than human haikus , but a high proportion of generated haikus can confuse human readers , and a few of them trigger intriguing reactions .

a machine learning based approach to evaluating retrieval systems
test collections are essential to evaluate information retrieval ( ir ) systems . the relevance assessment set has been recognized as the key bottleneck in test collection building , especially on very large sized document collections . this paper addresses the problem of efficiently selecting documents to be included in the assessment set . we will show how machine learning techniques can fit this task . this leads to smaller pools than traditional round robin pooling , thus reduces significantly the manual assessment workload . experimental results on trec collections1 consistently demonstrate the effectiveness of our approach according to different evaluation criteria .

a joint source-channel model for machine transliteration
most foreign names are transliterated into chinese , japanese or korean with approximate phonetic equivalents . the transliteration is usually achieved through intermediate phonemic mapping . this paper presents a new framework that allows direct orthographical mapping ( dom ) between two different languages , through a joint source-channel model , also called n-gram transliteration model ( tm ) . with the n-gram tm model , we automate the orthographic alignment process to derive the aligned transliteration units from a bilingual dictionary . the n-gram tm under the dom framework greatly reduces system development effort and provides a quantum leap in improvement in transliteration accuracy over that of other state-of-the-art machine learning algorithms . the modeling framework is validated through several experiments for english-chinese language pair .

automated content scoring of spoken responses in an assessment for teachers of english educational testing service
this paper presents and evaluates approaches to automatically score the content correctness of spoken responses in a new language test for teachers of english as a foreign language who are non-native speakers of english . most existing tests of english spoken proficiency elicit responses that are either very constrained ( e.g. , reading a passage aloud ) or are of a predominantly spontaneous nature ( e.g. , stating an opinion on an issue ) . however , the assessment discussed in this paper focuses on essential speaking skills that english teachers need in order to be effective communicators in their classrooms and elicits mostly responses that fall in between these extremes and are moderately predictable . in order to automatically score the content accuracy of these spoken responses , we propose three categories of robust features , inspired from flexible text matching , n-grams , as well as string edit distance metrics . the experimental results indicate that even based on speech recognizer output , most of the feature correlations with human expert rater scores are in the range of r = 0.4 to r = 0.5 , and further , that a scoring model for predicting human rater proficiency scores that includes our content features can significantly outperform a baseline without these features ( r = 0.56 vs. r = 0.33 ) .

ehu-alm : similarity-feature based approach for student response basque foundation for science
we present a 5-way supervised system based on syntactic-semantic similarity features . the model deploys : text overlap measures , wordnet-based lexical similarities , graphbased similarities , corpus-based similarities , syntactic structure overlap and predicateargument overlap measures . these measures are applied to question , reference answer and student answer triplets . we take into account the negation in the syntactic and predicateargument overlap measures . our system uses the domain-specific data as one dataset to build a robust system . the results show that our system is above the median and mean on all the evaluation scenarios of the semeval2013 task # 7 .

feedback cleaning of machine translation rules using automatic evaluation atr spoken language translation
when rules of transfer-based machine translation ( mt ) are automatically acquired from bilingual corpora , incorrect/redundant rules are generated due to acquisition errors or translation variety in the corpora . as a new countermeasure to this problem , we propose a feedback cleaning method using automatic evaluation of mt quality , which removes incorrect/redundant rules as a way to increase the evaluation score . bleu is utilized for the automatic evaluation . the hillclimbing algorithm , which involves features of this task , is applied to searching for the optimal combination of rules . our experiments show that the mt quality improves by 10 % in test sentences according to a subjective evaluation . this is considerable improvement over previous methods .

simple syntactic and morphological processing can help english-hindi statistical machine translation cdac mumbai ( formerly ncst )
in this paper , we report our work on incorporating syntactic and morphological information for english to hindi statistical machine translation . two simple and computationally inexpensive ideas have proven to be surprisingly effective : ( i ) reordering the english source sentence as per hindi syntax , and ( ii ) using the suffixes of hindi words . the former is done by applying simple transformation rules on the english parse tree . the latter , by using a simple suffix separation program . with only a small amount of bilingual training data and limited tools for hindi , we achieve reasonable performance and substantial improvements over the baseline phrase-based system . our approach eschews the use of parsing or other sophisticated linguistic tools for the target language ( hindi ) making it a useful framework for statistical machine translation from english to indian languages in general , since such tools are not widely available for indian languages currently .

cognitively salient relations for multilingual lexicography
providing sets of semantically related words in the lexical entries of an electronic dictionary should help language learners quickly understand the meaning of the target words . relational information might also improve memorisation , by allowing the generation of structured vocabulary study lists . however , an open issue is which semantic relations are cognitively most salient , and should therefore be used for dictionary construction . in this paper , we present a concept description elicitation experiment conducted with german and italian speakers . the analysis of the experimental data suggests that there is a small set of concept-classdependent relation types that are stable across languages and robust enough to allow discrimination across broad concept domains . our further research will focus on harvesting instantiations of these classes from corpora .

constructing coherent event hierarchies from news stories
news describe real-world events of varying granularity , and recognition of internal structure of events is important for automated reasoning over events . we propose an approach for constructing coherent event hierarchies from news by enforcing document-level coherence over pairwise decisions of spatiotemporal containment . evaluation on a news corpus annotated with event hierarchies shows that enforcing global spatiotemporal coreference of events leads to significant improvements ( 7.6 % f 1 -score ) in the accuracy of pairwise decisions .

unsupervised coreference resolution by utilizing the most informative relations
in this paper we present a novel method for unsupervised coreference resolution . we introduce a precision-oriented inference method that scores a candidate entity of a mention based on the most informative mention pair relation between the given mention entity pair . we introduce an informativeness score for determining the most precise relation of a mention entity pair regarding the coreference decisions . the informativeness score is learned robustly during few iterations of the expectation maximization algorithm . the proposed unsupervised system outperforms existing unsupervised methods on all benchmark data sets .

polynomial to linear : efficient classification with conjunctive features
this paper proposes a method that speeds up a classifier trained with many conjunctive features : combinations of ( primitive ) features . the key idea is to precompute as partial results the weights of primitive feature vectors that appear frequently in the target nlp task . a trie compactly stores the primitive feature vectors with their weights , and it enables the classifier to find for a given feature vector its longest prefix feature vector whose weight has already been computed . experimental results for a japanese dependency parsing task show that our method speeded up the svm and llm classifiers of the parsers , which achieved accuracy of 90.84/90.71 % , by a factor of 10.7/11.6 .

cross language text categorization by acquiring multilingual domain models from comparable corpora
in a multilingual scenario , the classical monolingual text categorization problem can be reformulated as a cross language tc task , in which we have to cope with two or more languages ( e.g . english and italian ) . in this setting , the system is trained using labeled examples in a source language ( e.g . english ) , and it classifies documents in a different target language ( e.g . italian ) . in this paper we propose a novel approach to solve the cross language text categorization problem based on acquiring multilingual domain models from comparable corpora in a totally unsupervised way and without using any external knowledge source ( e.g . bilingual dictionaries ) . these multilingual domain models are exploited to define a generalized similarity function ( i.e . a kernel function ) among documents in different languages , which is used inside a support vector machines classification framework . the results show that our approach is a feasible and cheap solution that largely outperforms a baseline .

issue framing as a generalizable phenomenon
framingportraying an issue from one perspective to the necessary exclusion of alternative perspectivesis a central concept in political communication . it is also a powerful political tool , as evidenced through experiments and single-issue studies beyond the lab . yet compared to its significance , we know very little about framing as a generalizable phenomenon . do framing dynamics , such as the evolution of one frame into another , play out the same way for all issues under what conditions does framing influence public opinion and policy understanding the general patterns of framing dynamics and effects is thus hugely important . it is also a serious challenge , thanks to the volume of text data , the dynamic nature of language , and variance in applicable frames across issues ( e.g. , the innocence frame of the death penalty debate is irrelevant for discussing smoking bans ) . to address this challenge , i describe a collaborative project with justin gross , philip resnik , and noah smith . we advance a unified policy frames codebook , in which issue-specific frames ( e.g. , innocence ) are nested within high-level categories of frames ( e.g. , fairness ) that cross cut issues . through manual annotation bolstered by supervised learning , we can track the relative use of different frame cues within a given issue over time and in an apples-to-apples way across issues . preliminary findings suggest our work may help unlock the black box of framing , pointing to generalizable conditions under which we should expect to see different types of framing dynamics and framing effects . 71

explaining the stars : weighted multiple-instance learning for aspect-based sentiment analysis
this paper introduces a model of multipleinstance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from usercontributed texts such as product reviews . each variable-length text is represented by several independent feature vectors ; one word vector per sentence or paragraph . for learning from texts with known aspect ratings , the model performs multipleinstance regression ( mir ) and assigns importance weights to each of the sentences or paragraphs of a text , uncovering their contribution to the aspect ratings . next , the model is used to predict aspect ratings in previously unseen texts , demonstrating interpretability and explanatory power for its predictions . we evaluate the model on seven multi-aspect sentiment analysis data sets , improving over four mir baselines and two strong bag-of-words linear models , namely svr and lasso , by more than 10 % relative in terms of mse .

experiments with pos-based restructuring and alignment-based reordering for statistical machine translation
this paper presents the methods which are based on the part-of-speech ( pos ) and auto alignment information to improve the quality of machine translation result and the word alignment . we utilize different types of pos tag to restructure source sentences and use an alignment-based reordering method to improve the alignment . after applying the reordering method , we use two phrase tables in the decoding part to keep the translation performance . our experiments on korean-chinese show that our methods can improve the alignment and translation results . since the proposed approach reduces the size of the phrase table , multi-tables are considered . the combination of all these methods together would get the best translation result .

a report on the first native language identification shared task
native language identification , or nli , is the task of automatically classifying the l1 of a writer based solely on his or her essay written in another language . this problem area has seen a spike in interest in recent years as it can have an impact on educational applications tailored towards non-native speakers of a language , as well as authorship profiling . while there has been a growing body of work in nli , it has been difficult to compare methodologies because of the different approaches to pre-processing the data , different sets of languages identified , and different splits of the data used . in this shared task , the first ever for native language identification , we sought to address the above issues by providing a large corpus designed specifically for nli , in addition to providing an environment for systems to be directly compared . in this paper , we report the results of the shared task . a total of 29 teams from around the world competed across three different sub-tasks .

a constraint satisfaction approach to dependency parsing erik tjong kim sang
we present an adaptation of constraint satisfaction inference ( canisius et al , 2006b ) for predicting dependency trees . three different classifiers are trained to predict weighted soft-constraints on parts of the complex output . from these constraints , a standard weighted constraint satisfaction problem can be formed , the solution to which is a valid dependency tree .

sentence processing in a vectorial model of working memory
this paper presents a vectorial incremental parsing model defined using independently posited operations over activationbased working memory and weight-based episodic memory . this model has the attractive property that it hypothesizes only one unary preterminal rule application and only one binary branching rule application per time step , which allows it to be smoothly integrated into a vector-based recurrence that propagates structural ambiguity from one time step to the next . predictions of this model are calculated on a center-embedded sentence processing task and shown to exhibit decreased processing accuracy in center-embedded constructions .

situated models of meaning for sports video retrieval
situated models of meaning ground words in the non-linguistic context , or situation , to which they refer . applying such models to sports video retrieval requires learning appropriate representations for complex events . we propose a method that uses data mining to discover temporal patterns in video , and pair these patterns with associated closed captioning text . this paired corpus is used to train a situated model of meaning that significantly improves video retrieval performance .

annotating dialogue acts to construct dialogue systems for consulting
this paper introduces a new corpus of consulting dialogues , which is designed for training a dialogue manager that can handle consulting dialogues through spontaneous interactions from the tagged dialogue corpus . we have collected 130 h of consulting dialogues in the tourist guidance domain . this paper outlines our taxonomy of dialogue act annotation that can describe two aspects of an utterances : the communicative function ( speech act ) , and the semantic content of the utterance . we provide an overview of the kyoto tour guide dialogue corpus and a preliminary analysis using the dialogue act tags .

sentiment vector space model for lyric-based song sentiment classification
lyric-based song sentiment classification seeks to assign songs appropriate sentiment labels such as light-hearted and heavy-hearted . four problems render vector space model ( vsm ) -based text classification approach ineffective : 1 ) many words within song lyrics actually contribute little to sentiment ; 2 ) nouns and verbs used to express sentiment are ambiguous ; 3 ) negations and modifiers around the sentiment keywords make particular contributions to sentiment ; 4 ) song lyric is usually very short . to address these problems , the sentiment vector space model ( s-vsm ) is proposed to represent song lyric document . the preliminary experiments prove that the svsm model outperforms the vsm model in the lyric-based song sentiment classification task .

dependency parsing for weibo : an efficient probabilistic logic programming approach
dependency parsing is a core task in nlp , and it is widely used by many applications such as information extraction , question answering , and machine translation . in the era of social media , a big challenge is that parsers trained on traditional newswire corpora typically suffer from the domain mismatch issue , and thus perform poorly on social media data . we present a new gfl/fudg-annotated chinese treebank with more than 18k tokens from sina weibo ( the chinese equivalent of twitter ) . we formulate the dependency parsing problem as many small and parallelizable arc prediction tasks : for each task , we use a programmable probabilistic firstorder logic to infer the dependency arc of a token in the sentence . in experiments , we show that the proposed model outperforms an off-the-shelf stanford chinese parser , as well as a strong maltparser baseline that is trained on the same in-domain data .

cambridge : parser evaluation using textual entailment by grammatical relation comparison
this paper describes the cambridge submission to the semeval-2010 parser evaluation using textual entailment ( pete ) task . we used a simple definition of entailment , parsing both t and h with the c & c parser and checking whether the core grammatical relations ( subject and object ) produced for h were a subset of those for t. this simple system achieved the top score for the task out of those systems submitted . we analyze the errors made by the system and the potential role of the task in parser evaluation .

evaluation of a japanese cfg derived from a syntactically annotated corpus with respect to dependency measures tomoya noro chimato koike taiichi hashimoto takenobu tokunaga hozumi tanaka
parsing is one of the important processes for natural language processing and , in general , a large-scale cfg is used to parse a wide variety of sentences . for many languages , a cfg is derived from a large-scale syntactically annotated corpus , and many parsing algorithms using cfgs have been proposed . however , we could not apply them to japanese since a japanese syntactically annotated corpus has not been available as of yet . in order to solve the problem , we have been building a large-scale japanese syntactically annotated corpus . in this paper , we show the evaluation results of a cfg derived from our corpus and compare it with results of some japanese dependency analyzers .

the framenet data and software
the framenet project has developed a lexical knowledge base providing a unique level of detail as to the the possible syntactic realizations of the specific semantic roles evoked by each predicator , for roughly 7,000 lexical units , on the basis of annotating more than 100,000 example sentences extracted from corpora . an interim version of the framenet data was released in october , 2002 and is being widely used . a new , more portable version of the framenet software is also being made available to researchers elsewhere , including the spanish framenet project . this demo and poster will briefly explain the principles of frame semantics and demonstrate the new unified tools for lexicon building and annotation and also framesql , a search tool for finding patterns in annotated sentences . we will discuss the content and format of the data releases and how the software and data can be used by other nlp researchers .

towards gene recognition from rare and ambiguous abbreviations using a filtering approach and philipp cimiano cognitive interaction technology boehringer ingelheim pharma gmbh
retrieving information about highly ambiguous gene/protein homonyms is a challenge , in particular where their non-protein meanings are more frequent than their protein meaning ( e. g. , sah or hf ) . due to their limited coverage in common benchmarking data sets , the performance of existing gene/protein recognition tools on these problematic cases is hard to assess . we uniformly sample a corpus of eight ambiguous gene/protein abbreviations from medliner and provide manual annotations for each mention of these abbreviations . based on this resource , we show that available gene recognition tools such as conditional random fields ( crf ) trained on biocreative 2 ner data or gnat tend to underperform on this phenomenon . we propose to extend existing gene recognition approaches by combining a crf and a support vector machine . in a crossentity evaluation and without taking any entity-specific information into account , our model achieves a gain of 6 points f 1 -measure over our best baseline which checks for the occurrence of a long form of the abbreviation and more than 9 points over all existing tools investigated .

semantic mapping using automatic word alignment and semantic role
to facilitate the application of semantics in statistical machine translation , we propose a broad-coverage predicate-argument structure mapping technique using automated resources . our approach utilizes automatic syntactic and semantic parsers to generate chinese-english predicate-argument structures . the system produced a many-to-many argument mapping for all propbank argument types by computing argument similarity based on automatic word alignment , achieving 80.5 % f-score on numbered argument mapping and 64.6 % f-score on all arguments . by measuring predicate-argument structure similarity based on the argument mapping , and formulating the predicate-argument structure mapping problem as a linear-assignment problem , the system achieved 84.9 % f-score using automatic srl , only 3.7 % f-score lower than using gold standard srl . the mapping output covered 49.6 % of the annotated chinese predicates ( which contains predicateadjectives that often have no parallel annotations in english ) and 80.7 % of annotated english predicates , suggesting its potential as a valuable resource for improving word alignment and reranking mt output .

on wordnet semantic classes and dependency parsing
this paper presents experiments with wordnet semantic classes to improve dependency parsing . we study the effect of semantic classes in three dependency parsers , using two types of constituencyto-dependency conversions of the english penn treebank . overall , we can say that the improvements are small and not significant using automatic pos tags , contrary to previously published results using gold pos tags ( agirre et al , 2011 ) . in addition , we explore parser combinations , showing that the semantically enhanced parsers yield a small significant gain only on the more semantically oriented lth treebank conversion .

annotation and data mining of the penn discourse treebank
the penn discourse treebank ( pdtb ) is a new resource built on top of the penn wall street journal corpus , in which discourse connectives are annotated along with their arguments . its use of standoff annotation allows integration with a stand-off version of the penn treebank ( syntactic structure ) and propbank ( verbs and their arguments ) , which adds value for both linguistic discovery and discourse modeling . here we describe the pdtb and some experiments in linguistic discovery based on the pdtb alone , as well as on the linked ptb and pdtb corpora .

building a discourse-tagged corpus in the framework of rhetorical structure theory mary ellen okurowski
we describe our experience in developing a discourse-annotated corpus for community-wide use . working in the framework of rhetorical structure theory , we were able to create a large annotated resource with very high consistency , using a well-defined methodology and protocol . this resource is made publicly available through the linguistic data consortium to enable researchers to develop empirically grounded , discourse-specific applications .

composing and updating verb argument expectations : a distributional semantic model
the aim of this paper is to present a computational model of the dynamic composition and update of verb argument expectations using distributional memory , a state-of-the-art framework for distributional semantics . the experimental results conducted on psycholinguistic data sets show that the model is able to successfully predict the changes on the patient argument thematic fit produced by different types of verb agents .

an efficient , generic approach to extracting multi-word expressions from dependency trees centrum voor computerlingustiek
the varro toolkit offers an intuitive mechanism for extracting syntactically motivated multi-word expressions ( mwes ) from dependency treebanks by looking for recurring connected subtrees instead of subsequences in strings . this approach can find mwes that are in varying orders and have words inserted into their components . this paper also proposes description length gain as a statistical correlation measure well-suited to tree structures .

integrating bayesian co-segmentation models with pivot-based smt
recent research on multilingual statistical machine translation ( smt ) focuses on the usage of pivot languages in order to overcome resource limitations for certain language pairs . this paper proposes a new method to translate a dialect language into a foreign language by integrating transliteration approaches based on bayesian co-segmentation ( bcs ) models with pivot-based smt approaches . the advantages of the proposed method with respect to standard smt approaches are three fold : ( 1 ) it uses a standard language as the pivot language and acquires knowledge about the relation between dialects and the standard language automatically , ( 2 ) it reduces the translation task complexity by using monotone decoding techniques , ( 3 ) it reduces the number of features in the log-linear model that have to be estimated from bilingual data . experimental results translating four japanese dialects ( kumamoto , kyoto , okinawa , osaka ) into four indo-european languages ( english , german , russian , hindi ) and two asian languages ( chinese , korean ) revealed that the proposed method improves the translation quality of dialect translation tasks and outperforms standard pivot translation approaches concatenating smt engines for the majority of the investigated language pairs .

probabilistic parsing strategies
we present new results on the relation between context-free parsing strategies and their probabilistic counter-parts . we provide a necessary condition and a sufficient condition for the probabilistic extension of parsing strategies . these results generalize existing results in the literature that were obtained by considering parsing strategies in isolation .

a multi-layer chinese word segmentation system optimized for
state-of-the-art chinese word segmentation systems have achieved high performance when training data and testing data are from the same domain . however , they suffer from the generalizability problem when applied on test data from different domains . we introduce a multi-layer chinese word segmentation system which can integrate the outputs from multiple heterogeneous segmentation systems . by training a second layer of large margin classifier on top of the outputs from several conditional random fields classifiers , it can utilize a small amount of in-domain training data to improve the performance . experimental results show consistent improvement on f1 scores and oov recall rates by applying the approach .

unt : subfinder : combining knowledge sources for automatic lexical substitution
this paper describes the university of north texas subfinder system . the system is able to provide the most likely set of substitutes for a word in a given context , by combining several techniques and knowledge sources . subfinder has successfully participated in the best and out of ten ( oot ) tracks in the semeval lexical substitution task , consistently ranking in the first or second place .

building a training corpus for word sense disambiguation in english-to-vietnamese machine translation
the most difficult task in machine translation is the elimination of ambiguity in human languages . a certain word in english as well as vietnamese often has different meanings which depend on their syntactical position in the sentence and the actual context . in order to solve this ambiguation , formerly , people used to resort to many hand-coded rules . nevertheless , manually building these rules is a time-consuming and exhausting task . so , we suggest an automatic method to solve the above-mentioned problem by using semantically tagged corpus . in this paper , we mainly present building a semantically tagged bilingual corpus to word sense disambiguation ( wsd ) in english texts . to assign semantic tags , we have taken advantage of bilingual texts via word alignments with semantic class names of lloce ( longman lexicon of contemporary english ) . so far , we have built 5,000,000-word bilingual corpus in which 1,000,000 words have been semantically annotated with the accuracy of 70 % . we have evaluated our result of semantic tagging by comparing with semcor on susanne part of our corpus . this semantically annotated corpus will be used to extract disambiguation rules automatically by tbl ( transformationbased learning ) method .

speech recognition of czech - inclusion of rare words helps
large vocabulary continuous speech recognition of inflective languages , such as czech , russian or serbo-croatian , is heavily deteriorated by excessive out of vocabulary rate . in this paper , we tackle the problem of vocabulary selection , language modeling and pruning for inflective languages . we show that by explicit reduction of out of vocabulary rate we can achieve significant improvements in recognition accuracy while almost preserving the model size . reported results are on czech speech corpora .

using very simple statistics for review search : an exploration
we report on work in progress on using very simple statistics in an unsupervised fashion to re-rank search engine results when review-oriented queries are issued ; the goal is to bring opinionated or subjective results to the top of the results list . we find that our proposed technique performs comparably to methods that rely on sophisticated pre-encoded linguistic knowledge , and that both substantially improve the initial results produced by the yahoo ! search engine .

dfki keywe : ranking keyphrases extracted from scientific articles dfki - language technology dfki - language technology
a central issue for making the content of a scientific document quickly accessible to a potential reader is the extraction of keyphrases , which capture the main topic of the document . keyphrases can be extracted automatically by generating a list of keyphrase candidates , ranking these candidates , and selecting the top-ranked candidates as keyphrases . we present the keywe system , which uses an adapted nominal group chunker for candidate extraction and a supervised ranking algorithm based on support vector machines for ranking the extracted candidates . the system was evaluated on data provided for the semeval 2010 shared task on keyphrase extraction .

semi-automatic practical ontology construction by using a republic of korea
this paper presents the semi-automatic construction method of a practical ontology by using various resources . in order to acquire a reasonably practical ontology in a limited time and with less manpower , we extend the kadokawa thesaurus by inserting additional semantic relations into its hierarchy , which are classified as case relations and other semantic relations . the former can be obtained by converting valency information and case frames from previously-built computational dictionaries used in machine translation . the latter can be acquired from concept co-occurrence information , which is extracted automatically from large corpora . the ontology stores rich semantic constraints among 1,110 concepts , and enables a natural language processing system to resolve semantic ambiguities by making inferences with the concept network of the ontology . in our practical machine translation system , our ontology-based word sense disambiguation method achieved an 8.7 % improvement over methods which do not use an ontology for korean translation .

the grammar matrix : an open-source starter-kit for the rapid development of cross-linguistically consistent broad-coverage precision
the grammar matrix is an open-source starter-kit for the development of broadcoverage hpsgs . by using a type hierarchy to represent cross-linguistic generalizations and providing compatibility with other open-source tools for grammar engineering , evaluation , parsing and generation , it facilitates not only quick start-up but also rapid growth towards the wide coverage necessary for robust natural language processing and the precision parses and semantic representations necessary for natural language understanding .

acquiring an ontology for a fundamental vocabulary
in this paper we describe the extraction of thesaurus information from parsed dictionary definition sentences . the main data for our experiments comes from lexeed , a japanese semantic dictionary , and the hinoki treebank built on it . the dictionary is parsed using a head-driven phrase structure grammar of japanese . knowledge is extracted from the semantic representation ( minimal recursion semantics ) . this makes the extraction process language independent .

regular tree grammars as a formalism for scope underspecification
we propose the use of regular tree grammars ( rtgs ) as a formalism for the underspecified processing of scope ambiguities . by applying standard results on rtgs , we obtain a novel algorithm for eliminating equivalent readings and the first efficient algorithm for computing the best reading of a scope ambiguity . we also show how to derive rtgs from more traditional underspecified descriptions .

the use of spatial relations in referring expression generation
there is a prevailing assumption in the literature on referring expression generation that relations are used in descriptions only as a last resort , typically on the basis that including the second entity in the relation introduces an additional cognitive load for either speaker or hearer . in this paper , we describe an experiemt that attempts to test this assumption ; we determine that , even in simple scenes where the use of relations is not strictly required in order to identify an entity , relations are in fact often used . we draw some conclusions as to what this means for the development of algorithms for the generation of referring expressions .

sentiment classification with graph co-regularization
sentiment classification aims to automatically predict sentiment polarity ( e.g. , positive or negative ) of user-generated sentiment data ( e.g. , reviews , blogs ) . to obtain sentiment classification with high accuracy , supervised techniques require a large amount of manually labeled data . the labeling work can be time-consuming and expensive , which makes unsupervised ( or semisupervised ) sentiment analysis essential for this application . in this paper , we propose a novel algorithm , called graph co-regularized non-negative matrix tri-factorization ( gnmtf ) , from the geometric perspective . gnmtf assumes that if two words ( or documents ) are sufficiently close to each other , they tend to share the same sentiment polarity . to achieve this , we encode the geometric information by constructing the nearest neighbor graphs , in conjunction with a nonnegative matrix tri-factorization framework . we derive an efficient algorithm for learning the factorization , analyze its complexity , and provide proof of convergence . our empirical study on two open data sets validates that gnmtf can consistently improve the sentiment classification accuracy in comparison to the state-of-the-art methods .

quantitative analysis of treebanks using frequent subtree mining methods centrum voor computerlingustiek , ku leuven
the first task of statistical computational linguistics , or any other type of datadriven processing of language , is the extraction of counts and distributions of phenomena . this is much more difficult for the type of complex structured data found in treebanks and in corpora with sophisticated annotation than for tokenized texts . recent developments in data mining , particularly in the extraction of frequent subtrees from treebanks , offer some solutions . we have applied a modified version of the treeminer algorithm to a small treebank and present some promising results .

wordnet : :similarity - measuring the relatedness of concepts
wordnet : :similarity is a freely available software package that makes it possible to measure the semantic similarity and relatedness between a pair of concepts ( or synsets ) . it provides six measures of similarity , and three measures of relatedness , all of which are based on the lexical database wordnet . these measures are implemented as perl modules which take as input two concepts , and return a numeric value that represents the degree to which they are similar or related .

structured sparsity in structured prediction
linear models have enjoyed great success in structured prediction in nlp . while a lot of progress has been made on efficient training with several loss functions , the problem of endowing learners with a mechanism for feature selection is still unsolved . common approaches employ ad hoc filtering or l1regularization ; both ignore the structure of the feature space , preventing practicioners from encoding structural prior knowledge . we fill this gap by adopting regularizers that promote structured sparsity , along with efficient algorithms to handle them . experiments on three tasks ( chunking , entity recognition , and dependency parsing ) show gains in performance , compactness , and model interpretability .

agent-based modeling of language evolution torvald lekvam bj ack lars bungum
agent-based models of language evolution have received a lot of attention in the last two decades . researchers wish to understand the origin of language , and aim to compensate for the lacking empirical evidence by utilizing methods from computer science and artificial life . the paper looks at the main theories of language evolution : biological evolution , learning , and cultural evolution . in particular , the baldwin effect in a naming game model is elaborated on by describing a set of experimental simulations . this is on-going work and ideas for further investigating the social aspects of language evolution are also discussed .

which words are hard to recognize prosodic , lexical , and disfluency factors that increase asr error rates
many factors are thought to increase the chances of misrecognizing a word in asr , including low frequency , nearby disfluencies , short duration , and being at the start of a turn . however , few of these factors have been formally examined . this paper analyzes a variety of lexical , prosodic , and disfluency factors to determine which are likely to increase asr error rates . findings include the following . ( 1 ) for disfluencies , effects depend on the type of disfluency : errors increase by up to 15 % ( absolute ) for words near fragments , but decrease by up to 7.2 % ( absolute ) for words near repetitions . this decrease seems to be due to longer word duration . ( 2 ) for prosodic features , there are more errors for words with extreme values than words with typical values . ( 3 ) although our results are based on output from a system with speaker adaptation , speaker differences are a major factor influencing error rates , and the effects of features such as frequency , pitch , and intensity may vary between speakers .

a statistical machine translation model based on a synthetic
recently , various synchronous grammars are proposed for syntax-based machine translation , e.g . synchronous context-free grammar and synchronous tree ( sequence ) substitution grammar , either purely formal or linguistically motivated . aiming at combining the strengths of different grammars , we describes a synthetic synchronous grammar ( ssg ) , which tentatively in this paper , integrates a synchronous context-free grammar ( scfg ) and a synchronous tree sequence substitution grammar ( stssg ) for statistical machine translation . the experimental results on nist mt05 chinese-to-english test set show that the ssg based translation system achieves significant improvement over three baseline systems .

parsing graphs with hyperedge replacement grammars karl moritz hermann
hyperedge replacement grammar ( hrg ) is a formalism for generating and transforming graphs that has potential applications in natural language understanding and generation . a recognition algorithm due to lautemann is known to be polynomial-time for graphs that are connected and of bounded degree . we present a more precise characterization of the algorithms complexity , an optimization analogous to binarization of contextfree grammars , and some important implementation details , resulting in an algorithm that is practical for natural-language applications . the algorithm is part of bolinas , a new software toolkit for hrg processing .

stop-probability estimates computed on a large corpus improve unsupervised dependency parsing
even though the quality of unsupervised dependency parsers grows , they often fail in recognition of very basic dependencies . in this paper , we exploit a prior knowledge of stop-probabilities ( whether a given word has any children in a given direction ) , which is obtained from a large raw corpus using the reducibility principle . by incorporating this knowledge into dependency model with valence , we managed to considerably outperform the state-of-theart results in terms of average attachment score over 20 treebanks from conll 2006 and 2007 shared tasks .

kcdc : word sense induction by using grammatical dependencies and sentence phrase structure
word sense induction and discrimination ( wsid ) identifies the senses of an ambiguous word and assigns instances of this word to one of these senses . we have build a wsid system that exploits syntactic and semantic features based on the results of a natural language parser component . to achieve high robustness and good generalization capabilities , we designed our system to work on a restricted , but grammatically rich set of features . based on the results of the evaluations our system provides a promising performance and robustness .

acquisition of lexical paraphrases from texts
automatic acquisition of paraphrase knowledge for content words is proposed . using only a non-parallel text corpus , we compute the paraphrasability metrics between two words from their similarity in context . we then filter words such as proper nouns from external knowledge . finally , we use a heuristic in further filtering to improve the accuracy of the automatic acquisition . in this paper , we report the results of acquisition experiments .

syntactic and semantic kernels for short text pair categorization
automatic detection of general relations between short texts is a complex task that can not be carried out only relying on language models and bag-of-words . therefore , learning methods to exploit syntax and semantics are required . in this paper , we present a new kernel for the representation of shallow semantic information along with a comprehensive study on kernel methods for the exploitation of syntactic/semantic structures for short text pair categorization . our experiments with support vector machines on question/answer classification show that our kernels can be used to greatly improve system accuracy .

tacit contracts for wheelchairs
in this paper , we propose a novel approach to infer dialogue acts using the notion of tacit contracts . we describe the interpersonal linguistic features that our analysis grammar can identify in uttered texts and present an inference procedure that strictly separates the semantic and pragmatic steps of utterance understanding , thereby meeting a higher degree of modularity , a prerequisite for extending robot functionality .

rover : improving system combination with classification
we present an improved system combination technique , rover . our approach obtains significant improvements over rover , and is consistently better across varying numbers of component systems . a classifier is trained on features from the system lattices , and selects the final word hypothesis by learning cues to choose the system that is most likely to be correct at each word location . this approach achieves the best result published to date on the tc-star 2006 english speech recognition evaluation set .

representing uncertainty about complex user goals in statistical
we point out several problems in scalingup statistical approaches to spoken dialogue systems to enable them to deal with complex but natural user goals , such as disjunctive and negated goals and preferences . in particular , we explore restrictions imposed by current independence assumptions in pomdp dialogue models . this position paper proposes the use of automatic belief compression methods to remedy these problems .

unsupervised morphological segmentation with log-linear models
morphological segmentation breaks words into morphemes ( the basic semantic units ) . it is a key component for natural language processing systems . unsupervised morphological segmentation is attractive , because in every language there are virtually unlimited supplies of text , but very few labeled resources . however , most existing model-based systems for unsupervised morphological segmentation use directed generative models , making it difficult to leverage arbitrary overlapping features that are potentially helpful to learning . in this paper , we present the first log-linear model for unsupervised morphological segmentation . our model uses overlapping features such as morphemes and their contexts , and incorporates exponential priors inspired by the minimum description length ( mdl ) principle . we present efficient algorithms for learning and inference by combining contrastive estimation with sampling . our system , based on monolingual features only , outperforms a state-of-the-art system by a large margin , even when the latter uses bilingual information such as phrasal alignment and phonetic correspondence . on the arabic penn treebank , our system reduces f1 error by 11 % compared to morfessor .

recurrent convolutional neural networks for discourse compositionality
the compositionality of meaning extends beyond the single sentence . just as words combine to form the meaning of sentences , so do sentences combine to form the meaning of paragraphs , dialogues and general discourse . we introduce both a sentence model and a discourse model corresponding to the two levels of compositionality . the sentence model adopts convolution as the central operation for composing semantic vectors and is based on a novel hierarchical convolutional neural network . the discourse model extends the sentence model and is based on a recurrent neural network that is conditioned in a novel way both on the current sentence and on the current speaker . the discourse model is able to capture both the sequentiality of sentences and the interaction between different speakers . without feature engineering or pretraining and with simple greedy decoding , the discourse model coupled to the sentence model obtains state of the art performance on a dialogue act classification experiment .

unsupervised discriminative language model training for machine translation using simulated confusion sets
an unsupervised discriminative training procedure is proposed for estimating a language model ( lm ) for machine translation ( mt ) . an english-to-english synchronous context-free grammar is derived from a baseline mt system to capture translation alternatives : pairs of words , phrases or other sentence fragments that potentially compete to be the translation of the same source-language fragment . using this grammar , a set of impostor sentences is then created for each english sentence to simulate confusions that would arise if the system were to process an ( unavailable ) input whose correct english translation is that sentence . an lm is then trained to discriminate between the original sentences and the impostors . the procedure is applied to the iwslt chinese-to-english translation task , and promising improvements on a state-ofthe-art mt system are demonstrated .

experiments in preposition error detection educational testing service
evaluation and annotation are two of the greatest challenges in developing nlp instructional or diagnostic tools to mark grammar and usage errors in the writing of non-native speakers . past approaches have commonly used only one rater to annotate a corpus of learner errors to compare to system output . in this paper , we show how using only one rater can skew system evaluation and then we present a sampling approach that makes it possible to evaluate a system more efficiently .

manual and automatic evaluation of summaries
in this paper we discuss manual and automatic evaluations of summaries using data from the document understanding conference 2001 ( duc-2001 ) . we first show the instability of the manual evaluation . specifically , the low interhuman agreement indicates that more reference summaries are needed . to investigate the feasibility of automated summary evaluation based on the recent bleu method from machine translation , we use accumulative n-gram overlap scores between system and human summaries . the initial results provide encouraging correlations with human judgments , based on the spearman rank-order correlation coefficient . however , relative ranking of systems needs to take into account the instability .

using syntactic information to identify plagiarism
using keyword overlaps to identify plagiarism can result in many false negatives and positives : substitution of synonyms for each other reduces the similarity between works , making it difficult to recognize plagiarism ; overlap in ambiguous keywords can falsely inflate the similarity of works that are in fact different in content . plagiarism detection based on verbatim similarity of works can be rendered ineffective when works are paraphrased even in superficial and immaterial ways . considering linguistic information related to creative aspects of writing can improve identification of plagiarism by adding a crucial dimension to evaluation of similarity : documents that share linguistic elements in addition to content are more likely to be copied from each other . in this paper , we present a set of low-level syntactic structures that capture creative aspects of writing and show that information about linguistic similarities of works improves recognition of plagiarism ( over tfidf-weighted keywords alone ) when combined with similarity measurements based on tfidf-weighted keywords .

zero-shot entity extraction from web pages
in order to extract entities of a fine-grained category from semi-structured data in web pages , existing information extraction systems rely on seed examples or redundancy across multiple web pages . in this paper , we consider a new zero-shot learning task of extracting entities specified by a natural language query ( in place of seeds ) given only a single web page . our approach defines a log-linear model over latent extraction predicates , which select lists of entities from the web page . the main challenge is to define features on widely varying candidate entity lists . we tackle this by abstracting list elements and using aggregate statistics to define features . finally , we created a new dataset of diverse queries and web pages , and show that our system achieves significantly better accuracy than a natural baseline .

building a biowordnet by using wordnets data formats and wordnets software infrastructure a failure story
in this paper , we describe our efforts to build on wordnet resources , using wordnet lexical data , the data format that it comes with and wordnets software infrastructure in order to generate a biomedical extension of wordnet , the biowordnet . we began our efforts on the assumption that the software resources were stable and reliable . in the course of our work , it turned out that this belief was far too optimistic . we discuss the stumbling blocks that we encountered , point out an error in the wordnet software with implications for research based on it , and conclude that building on the legacy of wordnet data structures and its associated software might preclude sustainable extensions that go beyond the domain of general english .

role of morpho-syntactic features in estonian proficiency classification
we developed an approach to predict the proficiency level of estonian language learners based on the cefr guidelines . we performed learner classification by studying morphosyntactic variation and lexical richness in texts produced by learners of estonian as a second language . we show that our features which exploit the rich morphology of estonian by focusing on the nominal case and verbal mood are useful predictors for this task . we also show that re-formulating the classification problem as a multi-stage cascaded classification improves the classification accuracy . finally , we also studied the effect of training data size on classification accuracy and found that more training data is beneficial in only some of the cases .

using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic structure
adaptor grammars are a non-parametric bayesian extension of probabilistic context-free grammars ( pcfgs ) which in effect learn the probabilities of entire subtrees . in practice , this means that an adaptor grammar learns the structures useful for generating the training data as well as their probabilities . we present several different adaptor grammars that learn to segment phonemic input into words by modeling different linguistic properties of the input . one of the advantages of a grammar-based framework is that it is easy to combine grammars , and we use this ability to compare models that capture different kinds of linguistic structure . we show that incorporating both unsupervised syllabification and collocation-finding into the adaptor grammar significantly improves unsupervised word-segmentation accuracy over that achieved by adaptor grammars that model only one of these linguistic phenomena .

streaming for large scale nlp : language modeling
in this paper , we explore a streaming algorithm paradigm to handle large amounts of data for nlp problems . we present an efficient low-memory method for constructing high-order approximate n-gram frequency counts . the method is based on a deterministic streaming algorithm which efficiently computes approximate frequency counts over a stream of data while employing a small memory footprint . we show that this method easily scales to billion-word monolingual corpora using a conventional ( 8 gb ram ) desktop machine . statistical machine translation experimental results corroborate that the resulting high-n approximate small language model is as effective as models obtained from other count pruning methods .

coordination structures in dependency treebanks
paratactic syntactic structures are notoriously difficult to represent in dependency formalisms . this has painful consequences such as high frequency of parsing errors related to coordination . in other words , coordination is a pending problem in dependency analysis of natural languages . this paper tries to shed some light on this area by bringing a systematizing view of various formal means developed for encoding coordination structures . we introduce a novel taxonomy of such approaches and apply it to treebanks across a typologically diverse range of 26 languages . in addition , empirical observations on convertibility between selected styles of representations are shown too .

simplified feature set for arabic named entity recognition
this paper introduces simplified yet effective features that can robustly identify named entities in arabic text without the need for morphological or syntactic analysis or gazetteers . a crf sequence labeling model is trained on features that primarily use character n-gram of leading and trailing letters in words and word n-grams . the proposed features help overcome some of the morphological and orthographic complexities of arabic . in comparing to results in the literature using arabic specific features such pos tags on the same dataset and same crf implementation , the results in this paper are lower by 2 f-measure points for locations , but are better by 8 points for organizations and 9 points for persons .

ngram-based statistical machine translation enhanced with multiple weighted reordering hypotheses
this paper describes the 2007 ngram-based statistical machine translation system developed at the talp research center of the upc ( universitat polite`cnica de catalunya ) in barcelona . emphasis is put on improvements and extensions of the previous years system , being highlighted and empirically compared . mainly , these include a novel word ordering strategy based on : ( 1 ) statistically monotonizing the training source corpus and ( 2 ) a novel reordering approach based on weighted reordering graphs . in addition , this system introduces a target language model based on statistical classes , a feature for out-of-domain units and an improved optimization procedure . the paper provides details of this system participation in the acl 2007 second workshop on statistical machine translation . results on three pairs of languages are reported , namely from spanish , french and german into english ( and the other way round ) for both the in-domain and out-of-domain tasks .

exploiting owl ontologies in the multilingual generation of object descriptions
we present three ways in which a natural language generator that produces textual descriptions of objects from symbolic information can exploit owl ontologies , using m-piros multilingual generation system as a concrete example .

information extraction using the structured language model
the paper presents a data-driven approach to information extraction ( viewed as template lling ) using the structured language model ( slm ) as a statistical parser . the task of template lling is cast as constrained parsing using the slm . the model is automatically trained from a set of sentences annotated with frame/slot labels and spans . training proceeds in stages : rst a constrained syntactic parser is trained such that the parses on training data meet the speci ed semantic spans , then the non-terminal labels are enriched to contain semantic information and nally a constrained syntactic+semantic parser is trained on the parse trees resulting from the previous stage . despite the small amount of training data used , the model is shown to outperform the slot level accuracy of a simple semantic grammar authored manually for the mipad | personal information management | task .

exploiting domain structure for named entity recognition
named entity recognition ( ner ) is a fundamental task in text mining and natural language understanding . current approaches to ner ( mostly based on supervised learning ) perform well on domains similar to the training domain , but they tend to adapt poorly to slightly different domains . we present several strategies for exploiting the domain structure in the training data to learn a more robust named entity recognizer that can perform well on a new domain . first , we propose a simple yet effective way to automatically rank features based on their generalizabilities across domains . we then train a classifier with strong emphasis on the most generalizable features . this emphasis is imposed by putting a rank-based prior on a logistic regression model . we further propose a domain-aware cross validation strategy to help choose an appropriate parameter for the rank-based prior . we evaluated the proposed method with a task of recognizing named entities ( genes ) in biology text involving three species . the experiment results show that the new domainaware approach outperforms a state-ofthe-art baseline method in adapting to new domains , especially when there is a great difference between the new domain and the training domain .

semantic dependency parsing using n-best semantic role sequences and roleset information
in this paper , we describe a syntactic and semantic dependency parsing system submitted to the shared task of conll 2008. the proposed system consists of five modules : syntactic dependency parser , predicate identifier , local semantic role labeler , global role sequence candidate generator , and role sequence selector . the syntactic dependency parser is based on malt parser and the sequence candidate generator is based on cky style algorithm . the remaining three modules are implemented by using maximum entropy classifiers . the proposed system achieves 76.90 of labeled f1 for the overall task , 84.82 of labeled attachment , and 68.71 of labeled f1 on the wsj+brown test set .

accurate non-hierarchical phrase-based translation
a principal weakness of conventional ( i.e. , non-hierarchical ) phrase-based statistical machine translation is that it can only exploit continuous phrases . in this paper , we extend phrase-based decoding to allow both source and target phrasal discontinuities , which provide better generalization on unseen data and yield significant improvements to a standard phrase-based system ( moses ) . more interestingly , our discontinuous phrasebased system also outperforms a state-of-the-art hierarchical system ( joshua ) by a very significant margin ( +1.03 bleu on average on five chineseenglish nist test sets ) , even though both joshua and our system support discontinuous phrases . since the key difference between these two systems is that ours is not hierarchicali.e. , our system uses a string-based decoder instead of cky , and it imposes no hard hierarchical reordering constraints during training and decodingthis paper sets out to challenge the commonly held belief that the tree-based parameterization of systems such as hiero and joshua is crucial to their good performance against moses .

grammar approximation by representative sublanguage : a new model for language learning
we propose a new language learning model that learns a syntactic-semantic grammar from a small number of natural language strings annotated with their semantics , along with basic assumptions about natural language syntax . we show that the search space for grammar induction is a complete grammar lattice , which guarantees the uniqueness of the learned grammar .

learning to rank definitions to generate quizzes for interactive
this paper proposes the idea of ranking definitions of a person ( a set of biographical facts ) to automatically generate who is this quizzes . the definitions are ordered according to how difficult they make it to name the person . such ranking would enable users to interactively learn about a person through dialogue with a system with improved understanding and lasting motivation , which is useful for educational systems . in our approach , we train a ranker that learns from data the appropriate ranking of definitions based on features that encode the importance of keywords in a definition as well as its content . experimental results show that our approach is significantly better in ranking definitions than baselines that use conventional information retrieval measures such as tf*idf and pointwisemutual information ( pmi ) .

statistical language modeling with performance benchmarks using various levels of syntactic-semantic information
statistical language models using n-gram approach have been under the criticism of neglecting large-span syntactic-semantic information that influences the choice of the next word in a language . one of the approaches that helped recently is the use of latent semantic analysis to capture the semantic fabric of the document and enhance the n-gram model . similarly there have been some approaches that used syntactic analysis to enhance the n-gram models . in this paper , we explain a framework called syntactically enhanced latent semantic analysis and its application in statistical language modeling . this approach augments each word with its syntactic descriptor in terms of the part-of-speech tag , phrase type or the supertag . we observe that given this syntactic knowledge , the model outperforms lsa based models significantly in terms of perplexity measure . we also present some observations on the effect of the knowledge of content or function word type in language modeling . this paper also poses the problem of better syntax prediction to achieve the benchmarks .

who wrote what where : analyzing the content of human and automatic summaries
abstractive summarization has been a longstanding and long-term goal in automatic summarization , because systems that can generate abstracts demonstrate a deeper understanding of language and the meaning of documents than systems that merely extract sentences from those documents . genest ( 2009 ) showed that summaries from the top automatic summarizers are judged as comparable to manual extractive summaries , and both are judged to be far less responsive than manual abstracts , as the state of the art approaches the limits of extractive summarization , it becomes even more pressing to advance abstractive summarization . however , abstractive summarization has been sidetracked by questions of what qualifies as important information , and how do we find it the guided summarization task introduced at the text analysis conference 2010 attempts to neutralize both of these problems by introducing topic categories and lists of aspects that a responsive summary should address . this design results in more similar human models , giving the automatic summarizers a more focused target to pursue , and also provides detailed diagnostics of summary content , which can can help build better meaningoriented summarization systems .

event extraction as dependency parsing
nested event structures are a common occurrence in both open domain and domain specific extraction tasks , e.g. , a crime event can cause a investigation event , which can lead to an arrest event . however , most current approaches address event extraction with highly local models that extract each event and argument independently . we propose a simple approach for the extraction of such structures by taking the tree of event-argument relations and using it directly as the representation in a reranking dependency parser . this provides a simple framework that captures global properties of both nested and flat event structures . we explore a rich feature space that models both the events to be parsed and context from the original supporting text . our approach obtains competitive results in the extraction of biomedical events from the bionlp09 shared task with a f1 score of 53.5 % in development and 48.6 % in testing .

an overview of microsoft web n - gram corpus and applications
this document describes the properties and some applications of the microsoft web ngram corpus . the corpus is designed to have the following characteristics . first , in contrast to static data distribution of previous corpus releases , this n-gram corpus is made publicly available as an xml web service so that it can be updated as deemed necessary by the user community to include new words and phrases constantly being added to the web . secondly , the corpus makes available various sections of a web document , specifically , the body , title , and anchor text , as separates models as text contents in these sections are found to possess significantly different statistical properties and therefore are treated as distinct languages from the language modeling point of view . the usages of the corpus are demonstrated here in two nlp tasks : phrase segmentation and word breaking .

event detection and summarization in weblogs with temporal collocations
this paper deals with the relationship between weblog content and time . with the proposed temporal mutual information , we analyze the collocations in time dimension , and the interesting collocations related to special events . the temporal mutual information is employed to observe the strength of term-to-term associations over time . an event detection algorithm identifies the collocations that may cause an event in a specific timestamp . an event summarization algorithm retrieves a set of collocations which describe an event . we compare our approach with the approach without considering the time interval . the experimental results demonstrate that the temporal collocations capture the real world semantics and real world events over time .

dependency trees and the strong generative capacity of ccg
we propose a novel algorithm for extracting dependencies from the derivations of a large fragment of ccg . unlike earlier proposals , our dependency structures are always tree-shaped . we then use these dependency trees to compare the strong generative capacities of ccg and tag and obtain surprising results : both formalisms generate the same languages of derivation trees but the mechanisms they use to bring the words in these trees into a linear order are incomparable .

identifying pronominal verbs : towards automatic disambiguation of the clitic se in portuguese magali sanches duran , carolina evaristo scarton , sandra maria alusio , carlos ramisch
a challenging topic in portuguese language processing is the multifunctional and ambiguous use of the clitic pronoun se , which impacts nlp tasks such as syntactic parsing , semantic role labeling and machine translation . aiming to give a step forward towards the automatic disambiguation of se , our study focuses on the identification of pronominal verbs , which correspond to one of the six uses of se as a clitic pronoun , when se is considered a constitutive particle of the verb lemma to which it is bound , as a multiword unit . our strategy to identify such verbs is to analyze the results of a corpus search and to rule out all the other possible uses of se . this process evidenced the features needed in a computational lexicon to automatically perform the disambiguation task . the availability of the resulting lexicon of pronominal verbs on the web enables their inclusion in broader lexical resources , such as the portuguese versions of wordnet , propbank and verbnet . moreover , it will allow the revision of parsers and dictionaries already in use .

natural language analysis of patent claims
we propose a nlp methodology for analyzing patent claims that combines symbolic grammar formalisms with dataintensive methods while enhancing analysis robustness . the output of our analyzer is a shallow interlingual representation that captures both the structure and content of a claim text . the methodology can be used in any patent-related application , such as machine translation , improving readability of patent claims , information retrieval , extraction , summarization , generation , etc . the methodology should be universal in the sense that it could be applied to any language , other parts of patent documentation and text as such .

multi-component tree adjoining grammars , dependency graph models , and linguistic analyses
recent work identifies two properties that appear particularly relevant to the characterization of graph-based dependency models of syntactic structure : the absence of interleaving substructures ( well-nestedness ) and a bound on a type of discontinuity ( gap-degree 1 ) successfully describe more than 99 % of the structures in two dependency treebanks . 2 bodirsky et al establish that every dependency structure with these two properties can be recast as a lexicalized tree adjoining grammar ( ltag ) derivation and vice versa . however , multicomponent extensions of tag ( mc-tag ) , argued to be necessary on linguistic grounds , induce dependency structures that do not conform to these two properties . in this paper , we observe that several types of mc-tag as used for linguistic analysis are more restrictive than the formal system is in principle . in particular , tree-local mc-tag , tree-local mc-tag with flexible composition , and special cases of set-local tag as used to describe certain linguistic phenomena satisfy the well-nested and gap degree 1 criteria . we also observe that gap degree can distinguish between prohibited and allowed wh-extractions in english , and report some preliminary work comparing the predictions of the graph approach and the mctag approach to scrambling .

sentence diagrams : their evaluation and combination
the purpose of our work is to explore the possibility of using sentence diagrams produced by schoolchildren as training data for automatic syntactic analysis . we have implemented a sentence diagram editor that schoolchildren can use to practice morphology and syntax . we collect their diagrams , combine them into a single diagram for each sentence and transform them into a form suitable for training a particular syntactic parser . in this study , the object language is czech , where sentence diagrams are part of elementary school curriculum , and the target format is the annotation scheme of the prague dependency treebank . we mainly focus on the evaluation of individual diagrams and on their combination into a merged better version .

using query patterns to learn the duration of events
we present the first approach to learning the durations of events without annotated training data , employing web query patterns to infer duration distributions . for example , we learn that war lasts years or decades , while look lasts seconds or minutes . learning aspectual information is an important goal for computational semantics and duration information may help enable rich document understanding . we first describe and improve a supervised baseline that relies on event duration annotations . we then show how web queries for linguistic patterns can help learn the duration of events without labeled data , producing fine-grained duration judgments that surpass the supervised system . we evaluate on the timebank duration corpus , and also investigate how an events participants ( arguments ) effect its duration using a corpus collected through amazons mechanical turk . we make available a new database of events and their duration distributions for use in research involving the temporal and aspectual properties of events .

discourse and dialogue processing in spoken intelligent tutoring systems
in recent years , the development of intelligent tutoring dialogue systems has become more prevalent , in an attempt to close the performance gap between human and computer tutors . tutoring applications differ in many ways , however , from the types of applications for which spoken dialogue systems are typically developed . this talk will illustrate some of the opportunities and challenges in this area , focusing on issues such as affective reasoning , discourse and dialogue analysis , and performance evaluation .

e-assessment using latent semantic analysis in the computer science domain : a pilot study
latent semantic analysis ( lsa ) is a statistical natural language processing ( nlp ) technique for inferring meaning from a text . existing lsa-based applications focus on formative assessment in general domains . the suitability of lsa for summative assessment in the domain of computer science is not well known . the results from the pilot study reported in this paper encourage us to pursue further research in the use of lsa in the narrow , technical domain of computer science . this paper explains the theory behind lsa , describes some existing lsa applications , and presents some results using lsa for automatic marking of short essays for a graduate class in architectures of computing systems .

domain-specific image captioning
we present a data-driven framework for image caption generation which incorporates visual and textual features with varying degrees of spatial structure . we propose the task of domain-specific image captioning , where many relevant visual details can not be captured by off-the-shelf general-domain entity detectors . we extract previously-written descriptions from a database and adapt them to new query images , using a joint visual and textual bag-of-words model to determine the correctness of individual words . we implement our model using a large , unlabeled dataset of womens shoes images and natural language descriptions ( berg et al. , 2010 ) . using both automatic and human evaluations , we show that our captioning method effectively deletes inaccurate words from extracted captions while maintaining a high level of detail in the generated output .

frolog : an accommodating text-adventure game
frolog is a text-adventure game whose goal is to serve as a laboratory for testing pragmatic theories of accommodation . to this end , rather than implementing ad-hoc mechanisms for each task that is necessary in such a conversational agent , frolog integrates recently developed tools from computational linguistics , theorem proving and artificial intelligence planning .

cogex : a logic prover for question answering language computer corporation
recent trec results have demonstrated the need for deeper text understanding methods . this paper introduces the idea of automated reasoning applied to question answering and shows the feasibility of integrating a logic prover into a question answering system . the approach is to transform questions and answer passages into logic representations . world knowledge axioms as well as linguistic axioms are supplied to the prover which renders a deep understanding of the relationship between question text and answer text . moreover , the trace of the proofs provide answer justifications . the results show that the prover boosts the performance of the qa system on trec questions by 30 % .

multiword expressions in the wild the mwetoolkit comes in handy
the mwetoolkit is a tool for automatic extraction of multiword expressions ( mwes ) from monolingual corpora . it both generates and validates mwe candidates . the generation is based on surface forms , while for the validation , a series of criteria for removing noise are provided , such as some ( language independent ) association measures.1 in this paper , we present the use of the mwetoolkit in a standard configuration , for extracting mwes from a corpus of general-purpose english . the functionalities of the toolkit are discussed in terms of a set of selected examples , comparing it with related work on mwe extraction .

improving grammaticality in statistical sentence generation : introducing a dependency spanning tree algorithm with an argument
abstract-like text summarisation requires a means of producing novel summary sentences . in order to improve the grammaticality of the generated sentence , we model a global ( sentence ) level syntactic structure . we couch statistical sentence generation as a spanning tree problem in order to search for the best dependency tree spanning a set of chosen words . we also introduce a new search algorithm for this task that models argument satisfaction to improve the linguistic validity of the generated tree . we treat the allocation of modifiers to heads as a weighted bipartite graph matching ( or assignment ) problem , a well studied problem in graph theory . using bleu to measure performance on a string regeneration task , we found an improvement , illustrating the benefit of the spanning tree approach armed with an argument satisfaction model .

building a bilingual wordnet-like lexicon : the new approach and algorithms
a bilingual concept mrd is of significance for ie , mt , wsd and the like . however , it is reasonably difficult to build such a lexicon for there exist two ontologies , also , the evolution of such a lexicon is quite challenging . in this paper , we would like to put forth the new approach to building a bilingual wordnet-like lexicon and to dwell on some of the pivotal algorithms . a characteristic of this new approach is to emphasize the inheritance and transformation of the existent monolingual lexicon . on the one hand , we have extracted all the common knowledge in wordnet as the semantic basis for further use . on the other hand , we have developed a visualized developing tool for the lexicographers to interactively operate on to express the bilingual semantics . the bilingual lexicon has thus gradually come into being in this natural process . icl now has benefited a lot by employing this new approach to build ccd ( chinese concept dictionary ) , a bilingual wordnet-like lexicon , in peking university .

annotating social acts : authority claims and alignment moves in wikipedia talk pages
we present the aawd corpus , a collection of 365 discussions drawn from wikipedia talk pages and annotated with labels capturing two kinds of social acts : alignment moves and authority claims . we describe these social acts and our annotation process , and analyze the resulting data set for interactions between participant status and social acts and between the social acts themselves .

divide and translate : improving long distance reordering in statistical
this paper proposes a novel method for long distance , clause-level reordering in statistical machine translation ( smt ) . the proposed method separately translates clauses in the source sentence and reconstructs the target sentence using the clause translations with non-terminals . the nonterminals are placeholders of embedded clauses , by which we reduce complicated clause-level reordering into simple wordlevel reordering . its translation model is trained using a bilingual corpus with clause-level alignment , which can be automatically annotated by our alignment algorithm with a syntactic parser in the source language . we achieved significant improvements of 1.4 % in bleu and 1.3 % in ter by using moses , and 2.2 % in bleu and 3.5 % in ter by using our hierarchical phrase-based smt , for the english-to-japanese translation of research paper abstracts in the medical domain .

inner-outer bracket models for word alignment using hidden blocks
most statistical translation systems are based on phrase translation pairs , or blocks , which are obtained mainly from word alignment . we use blocks to infer better word alignment and improved word alignment which , in turn , leads to better inference of blocks . we propose two new probabilistic models based on the innerouter segmentations and use em algorithms for estimating the models parameters . the first model recovers ibm model-1 as a special case . both models outperform bidirectional ibm model-4 in terms of word alignment accuracy by 10 % absolute on the f-measure . using blocks obtained from the models in actual translation systems yields statistically significant improvements in chinese-english smt evaluation .

is machine translation ripe for cross-lingual sentiment classification
recent advances in machine translation ( mt ) have brought forth a new paradigm for building nlp applications in low-resource scenarios . to build a sentiment classifier for a language with no labeled resources , one can translate labeled data from another language , then train a classifier on the translated text . this can be viewed as a domain adaptation problem , where labeled translations and test data have some mismatch . various prior work have achieved positive results using this approach . in this opinion piece , we take a step back and make some general statements about crosslingual adaptation problems . first , we claim that domain mismatch is not caused by mt errors , and accuracy degradation will occur even in the case of perfect mt . second , we argue that the cross-lingual adaptation problem is qualitatively different from other ( monolingual ) adaptation problems in nlp ; thus new adaptation algorithms ought to be considered . this paper will describe a series of carefullydesigned experiments that led us to these conclusions .

inter-annotator agreement for ere annotation
this paper describes a system for interannotator agreement analysis of ere annotation , focusing on entity mentions and how the higher-order annotations such as events are dependent on those entity mentions . the goal of this approach is to provide both ( 1 ) quantitative scores for the various levels of annotation , and ( 2 ) information about the types of annotation inconsistencies that might exist . while primarily designed for inter-annotator agreement , it can also be considered a system for evaluation of ere annotation .

a corpus-based account of regular polysemy : the case of
in this paper we investigate polysemous adjectives whose meaning varies depending on the nouns they modify ( e.g. , fast ) . we acquire the meanings of these adjectives from a large corpus and propose a probabilistic model which provides a ranking on the set of possible interpretations . we identify lexical semantic information automatically by exploiting the consistent correspondences between surface syntactic cues and lexical meaning . we evaluate our results against paraphrase judgments elicited experimentally from humans and show that the models ranking of meanings correlates reliably with human intuitions : meanings that are found highly probable by the model are also rated as plausible by the subjects .

learning the scope of negation via shallow semantic parsing
in this paper we present a simplified shallow semantic parsing approach to learning the scope of negation ( son ) . this is done by formulating it as a shallow semantic parsing problem with the negation signal as the predicate and the negation scope as its arguments . our parsing approach to son learning differs from the state-of-the-art chunking ones in two aspects . first , we extend son learning from the chunking level to the parse tree level , where structured syntactic information is available . second , we focus on determining whether a constituent , rather than a word , is negated or not , via a simplified shallow semantic parsing framework . evaluation on the bioscope corpus shows that structured syntactic information is effective in capturing the domination relationship between a negation signal and its dominated arguments . it also shows that our parsing approach much outperforms the state-of-the-art chunking ones .

co-parsing with competitive models
we present an asymmetric approach to a run-time combination of two parsers where one component serves as a predictor to the other one . predictions are integrated by means of weighted constraints and therefore are subject to preferential decisions . previously , the same architecture has been successfully used with predictors providing partial or inferior information about the parsing problem . it has now been applied to a situation where the predictor produces exactly the same type of information at a fully competitive quality level . results show that the combined system outperforms its individual components , even though their performance in isolation is already fairly high .

melb-kb : nominal classification as noun compound interpretation su nam kim and timothy baldwin
in this paper , we outline our approach to interpreting semantic relations in nominal pairs in semeval-2007 task # 4 : classification of semantic relations between nominals . we build on two baseline approaches to interpreting noun compounds : sense collocation , and constituent similarity . these are consolidated into an overall system in combination with co-training , to expand the training data . our two systems attained an average f-score over the test data of 58.7 % and 57.8 % , respectively .

efficient hierarchical entity classifier using conditional random fields
in this paper we develop an automatic classifier for a very large set of labels , the wordnet synsets . we employ conditional random fields ( crfs ) because of their flexibility to include a wide variety of nonindependent features . training crfs on a big number of labels proved a problem because of the large training cost . by taking into account the hypernym/hyponym relation between synsets in wordnet , we reduced the complexity of training from o ( tm2ng ) to o ( t ( logm ) 2ng ) with only a limited loss in accuracy .

informed ways of improving data-driven dependency parsing for german
we investigate a series of targeted modifications to a data-driven dependency parser of german and show that these can be highly effective even for a relatively well studied language like german if they are made on a ( linguistically and methodologically ) informed basis and with a parser implementation that allows for fast and robust training and application . making relatively small changes to a range of very different system components , we were able to increase labeled accuracy on a standard test set ( from the conll 2009 shared task ) , ignoring gold standard partof-speech tags , from 87.64 % to 89.40 % . the study was conducted in less than five weeks and as a secondary project of all four authors . effective modifications include the quality and combination of autoassigned morphosyntactic features entering machine learning , the internal feature handling as well as the inclusion of global constraints and a combination of different parsing strategies .

evangelising language technology : a practically-focussed undergraduate program
this paper describes an undergraduate program in language technology that we have developed at macquarie university . we question the industrial relevance of much that is taught in nlp courses , and emphasize the need for a practical orientation as a means to growing the size of the field . we argue that a more evangelical approach , both with regard to students and industry , is required . the paper provides an overview of the material we cover , and makes some observations for the future on the basis of our experiences so far .

taxonomy learning using term specificity and similarity
learning taxonomy for technical terms is difficult and tedious task , especially when new terms should be included . the goal of this paper is to assign taxonomic relations among technical terms . we propose new approach to the problem that relies on term specificity and similarity measures . term specificity and similarity are necessary conditions for taxonomy learning , because highly specific terms tend to locate in deep levels and semantically similar terms are close to each other in taxonomy . we analyzed various features used in previous researches in view of term specificity and similarity , and applied optimal features for term specificity and similarity to our method .

querying xml documents with multi-dimensional markup
xml documents annotated by different nlp tools accommodate multidimensional markup in a single hierarchy . to query such documents one has to account for different possible nesting structures of the annotations and the original markup of a document . we propose an expressive pattern language with extended semantics of the sequence pattern , supporting negation , permutation and regular patterns that is especially appropriate for querying xml annotated documents with multi-dimensional markup . the concept of fuzzy matching allows matching of sequences that contain textual fragments and known xml elements independently of how concurrent annotations and original markup are merged . we extend the usual notion of sequence as a sequence of siblings allowing matching of sequence elements on the different levels of nesting and abstract so from the hierarchy of the xml document . extended sequence semantics in combination with other language patterns allows more powerful and expressive queries than queries based on regular patterns .

a re-examination on features in regression based approach to automatic mt evaluation
machine learning methods have been extensively employed in developing mt evaluation metrics and several studies show that it can help to achieve a better correlation with human assessments . adopting the regression svm framework , this paper discusses the linguistic motivated feature formulation strategy . we argue that blind combination of available features does not yield a general metrics with high correlation rate with human assessments . instead , certain simple intuitive features serve better in establishing the regression svm evaluation model . with six features selected , we show evidences to support our view through a few experiments in this paper .

usf : chunking for aspect term identification & polarity classification
this paper describes the systems submitted by the university of san francisco ( usf ) to semeval-2014 task 4 , aspect based sentiment analysis ( absa ) , which provides labeled data in two domains , laptops and restaurants . for the constrained condition of both the aspect term extraction and aspect term polarity tasks , we take a supervised machine learning approach using a combination of lexical , syntactic , and baseline sentiment features . our extraction approach is inspired by a chunking approach , based on its strong past results on related tasks . our system performed slightly below average compared to other submissions , possibly because we use a simpler classification model than prior work . our polarity labeling approach uses two baseline hand-built sentiment classifiers as features in addition to lexical and syntactic features , and performed in the top ten of other constrained systems on both domains .

adaptive parser-centric text normalization
text normalization is an important first step towards enabling many natural language processing ( nlp ) tasks over informal text . while many of these tasks , such as parsing , perform the best over fully grammatically correct text , most existing text normalization approaches narrowly define the task in the word-to-word sense ; that is , the task is seen as that of mapping all out-of-vocabulary non-standard words to their in-vocabulary standard forms . in this paper , we take a parser-centric view of normalization that aims to convert raw informal text into grammatically correct text . to understand the real effect of normalization on the parser , we tie normalization performance directly to parser performance . additionally , we design a customizable framework to address the often overlooked concept of domain adaptability , and illustrate that the system allows for transfer to new domains with a minimal amount of data and effort . our experimental study over datasets from three domains demonstrates that our approach outperforms not only the state-of-the-art wordto-word normalization techniques , but also manual word-to-word annotations .

teaching the basics of nlp and ml in an introductory course
in this paper we discuss our experience of teaching basic natural language processing ( nlp ) and machine learning ( ml ) in an introductory course to information science . we discuss the challenges we faced while incorporating nlp and ml to the curriculum followed by a presentation of how we met these challenges . the overall response ( of students ) to the inclusion of this new topic to the curriculum has been positive . students this semester are pursuing nlp/ml projects , formulating their own tasks ( some of which are novel and presented towards the end of the paper ) , collecting and annotating data and building models for their task .

joint processing and discriminative training for sittichai jiampojamarn colin cherry grzegorz kondrak
we present a discriminative structureprediction model for the letter-to-phoneme task , a crucial step in text-to-speech processing . our method encompasses three tasks that have been previously handled separately : input segmentation , phoneme prediction , and sequence modeling . the key idea is online discriminative training , which updates parameters according to a comparison of the current system output to the desired output , allowing us to train all of our components together . by folding the three steps of a pipeline approach into a unified dynamic programming framework , we are able to achieve substantial performance gains . our results surpass the current state-of-the-art on six publicly available data sets representing four different languages .

non-parametric bayesian segmentation of japanese noun phrases
a key factor of high quality word segmentation for japanese is a high-coverage dictionary , but it is costly to manually build such a lexical resource . although external lexical resources for human readers are potentially good knowledge sources , they have not been utilized due to differences in segmentation criteria . to supplement a morphological dictionary with these resources , we propose a new task of japanese noun phrase segmentation . we apply non-parametric bayesian language models to segment each noun phrase in these resources according to the statistical behavior of its supposed constituents in text . for inference , we propose a novel block sampling procedure named hybrid type-based sampling , which has the ability to directly escape a local optimum that is not too distant from the global optimum . experiments show that the proposed method efficiently corrects the initial segmentation given by a morphological analyzer .

syntax and semantics in quality estimation of machine translation
we employ syntactic and semantic information in estimating the quality of machine translation from a new data set which contains source text from english customer support forums and target text consisting of its machine translation into french . these translations have been both post-edited and evaluated by professional translators . we find that quality estimation using syntactic and semantic information on this data set can hardly improve over a baseline which uses only surface features . however , the performance can be improved when they are combined with such surface features . we also introduce a novel metric to measure translation adequacy based on predicate-argument structure match using word alignments . while word alignments can be reliably used , the two main factors affecting the performance of all semantic-based methods seems to be the low quality of semantic role labelling ( especially on ill-formed text ) and the lack of nominal predicate annotation .

the stanford typed dependencies representation
this paper examines the stanford typed dependencies representation , which was designed to provide a straightforward description of grammatical relations for any user who could benefit from automatic text understanding . for such purposes , we argue that dependency schemes must follow a simple design and provide semantically contentful information , as well as offer an automatic procedure to extract the relations . we consider the underlying design principles of the stanford scheme from this perspective , and compare it to the gr and parc representations . finally , we address the question of the suitability of the stanford scheme for parser evaluation .

generating and interpreting referring expressions as belief state
planning-based approaches to reference provide a uniform treatment of linguistic decisions , from content selection to lexical choice . in this paper , we show how the issues of lexical ambiguity , vagueness , unspecific descriptions , ellipsis , and the interaction of subsective modifiers can be expressed using a belief-state planner modified to support context-dependent actions . because the number of distinct denotations it searches grows doublyexponentially with the size of the referential domain , we present representational and search strategies that make generation and interpretation tractable .

the new thot toolkit for fully-automatic and interactive statistical
we present the new thot toolkit for fullyautomatic and interactive statistical machine translation ( smt ) . initial public versions of thot date back to 2005 and did only include estimation of phrase-based models . by contrast , the new version offers several new features that had not been previously incorporated . the key innovations provided by the toolkit are computeraided translation , including post-editing and interactive smt , incremental learning and robust generation of alignments at phrase level . in addition to this , the toolkit also provides standard smt features such as fully-automatic translation , scalable and parallel algorithms for model training , client-server implementation of the translation functionality , etc . the toolkit can be compiled in unix-like and windows platforms and it is released under the gnu lesser general public license ( lgpl ) .

using derivation trees for informative treebank inter-annotator and mohamed maamouri linguistic data consortium beatrice santorini and
this paper discusses the extension of a system developed for automatic discovery of treebank annotation inconsistencies over an entire corpus to the particular case of evaluation of inter-annotator agreement . this system makes for a more informative iaa evaluation than other systems because it pinpoints the inconsistencies and groups them by their structural types . we evaluate the system on two corpora - ( 1 ) a corpus of english web text , and ( 2 ) a corpus of modern british english .

learning a part-of-speech tagger from two hours of annotation
most work on weakly-supervised learning for part-of-speech taggers has been based on unrealistic assumptions about the amount and quality of training data . for this paper , we attempt to create true low-resource scenarios by allowing a linguist just two hours to annotate data and evaluating on the languages kinyarwanda and malagasy . given these severely limited amounts of either type supervision ( tag dictionaries ) or token supervision ( labeled sentences ) , we are able to dramatically improve the learning of a hidden markov model through our method of automatically generalizing the annotations , reducing noise , and inducing word-tag frequency information .

a new approach and evaluation methodology
this paper addresses the mitigation of medical errors due to the confusion of sound-alike and look-alike drug names . our approach involves application of two new methods one based on orthographic similarity ( lookalike ) and the other based on phonetic similarity ( sound-alike ) . we present a new recall-based evaluation methodology for determining the effectiveness of different similarity measures on drug names . we show that the new orthographic measure ( bi-sim ) outperforms other commonly used measures of similarity on a set containing both look-alike and sound-alike pairs , and that the feature-based phonetic approach ( aline ) outperforms orthographic approaches on a test set containing solely sound-alike confusion pairs . however , an approach that combines several different measures achieves the best results on both test sets .

unsupervised discovery of morphologically related words based on
we present an algorithm that takes an unannotated corpus as its input , and returns a ranked list of probable morphologically related pairs as its output . the algorithm tries to discover morphologically related pairs by looking for pairs that are both orthographically and semantically similar , where orthographic similarity is measured in terms of minimum edit distance , and semantic similarity is measured in terms of mutual information . the procedure does not rely on a morpheme concatenation model , nor on distributional properties of word substrings ( such as affix frequency ) . experiments with german and english input give encouraging results , both in terms of precision ( proportion of good pairs found at various cutoff points of the ranked list ) , and in terms of a qualitative analysis of the types of morphological patterns discovered by the algorithm .

classifying negative findings in biomedical publications
publication bias refers to the phenomenon that statistically significant , positive results are more likely to be published than non-significant , negative results . currently , researchers have to manually identify negative results in a large number of publications in order to examine publication biases . this paper proposes an nlp approach for automatically classifying negated sentences in biomedical abstracts as either reporting negative findings or not . using multinomial nave bayes algorithm and bag-ofwords features enriched by parts-ofspeeches and constituents , we built a classifier that reached 84 % accuracy based on 5-fold cross validation on a balanced data set .

hmm based chunker for hindi
this paper presents an hmm-based chunk tagger for hindi . various tagging schemes for marking chunk boundaries are discussed along with their results . contextual information is incorporated into the chunk tags in the form of partof-speech ( pos ) information . this information is also added to the tokens themselves to achieve better precision . error analysis is carried out to reduce the number of common errors . it is found that for certain classes of words , using the pos information is more effective than using a combination of word and pos tag as the token . finally , chunk labels are also marked on the chunks .

tcdscss : dimensionality reduction to evaluate texts of varying lengths - an ir approach
this paper provides system description of the cross-level semantic similarity task for the semeval-2014 workshop . crosslevel semantic similarity measures the degree of relatedness between texts of varying lengths such as paragraph to sentence and sentence to phrase . latent semantic analysis was used to evaluate the cross-level semantic relatedness between the texts to achieve above baseline scores , tested on the training and test datasets . we also tried using a bag-of-vectors approach to evaluate the semantic relatedness . this bag-of-vectors approach however did not produced encouraging results .

collective named entity disambiguation using graph ranking and clique partitioning approaches and robert gaizauskas
disambiguating named entities ( ne ) in running text to their correct interpretations in a specific knowledge base ( kb ) is an important problem in nlp . this paper presents two collective disambiguation approaches using a graph representation where possible kb candidates for ne textual mentions are represented as nodes and the coherence relations between different ne candidates are represented by edges . each node has a local confidence score and each edge has a weight . the first approach uses page-rank ( pr ) to rank all nodes and selects a candidate based on pr score combined with local confidence score . the second approach uses an adapted clique partitioning technique to find the most weighted clique and expands this clique until all ne textual mentions are disambiguated . experiments on 27,819 ne textual mentions show the effectiveness of both approaches , outperforming both baseline and state-of-the-art approaches .

construction of domain dictionary for fundamental vocabulary
for natural language understanding , it is essential to reveal semantic relations between words . to date , only the is-a relation has been publicly available . toward deeper natural language understanding , we semiautomatically constructed the domain dictionary that represents the domain relation between japanese fundamental words . this is the first japanese domain resource that is fully available . besides , our method does not require a document collection , which is indispensable for keyword extraction techniques but is hard to obtain . as a task-based evaluation , we performed blog categorization . also , we developed a technique for estimating the domain of unknown words .

simple but effective feedback generation to tutor abstract problem solving
to generate natural language feedback for an intelligent tutoring system , we developed a simple planning model with a distinguishing feature : its plan operators are derived automatically , on the basis of the association rules mined from our tutorial dialog corpus . automatically mined rules are also used for realization . we evaluated 5 different versions of a system that tutors on an abstract sequence learning task . the version that uses our planning framework is significantly more effective than the other four versions . we compared this version to the human tutors we employed in our tutorial dialogs , with intriguing results .

refining grammars for parsing with hierarchical semantic knowledge
this paper proposes a novel method to refine the grammars in parsing by utilizing semantic knowledge from hownet . based on the hierarchical state-split approach , which can refine grammars automatically in a data-driven manner , this study introduces semantic knowledge into the splitting process at two steps . firstly , each part-of-speech node will be annotated with a semantic tag of its terminal word . these new tags generated in this step are semantic-related , which can provide a good start for splitting . secondly , a knowledge-based criterion is used to supervise the hierarchical splitting of these semantic-related tags , which can alleviate overfitting . the experiments are carried out on both chinese and english penn treebank show that the refined grammars with semantic knowledge can improve parsing performance significantly . especially with respect to chinese , our parser achieves an f 1 score of 87.5 % , which is the best published result we are aware of .

towards a game-theoretic approach to content determination
this paper argues for a game-theoretic approach to content determination that uses text-type specific strategies in order to determine the optimal content for various user types . by means of content determination for the description of numerical data the benefits of a game-theoretic treatment of content determination are outlined .

streaming cross document entity coreference resolution
previous research in cross-document entity coreference has generally been restricted to the offline scenario where the set of documents is provided in advance . as a consequence , the dominant approach is based on greedy agglomerative clustering techniques that utilize pairwise vector comparisons and thus require o ( n2 ) space and time . in this paper we explore identifying coreferent entity mentions across documents in high-volume streaming text , including methods for utilizing orthographic and contextual information . we test our methods using several corpora to quantitatively measure both the efficacy and scalability of our streaming approach . we show that our approach scales to at least an order of magnitude larger data than previous reported methods .

towards a prototyping tool for behavior oriented authoring of conversational agents for educational applications
our goal is to develop tools for facilitating the authoring of conversational agents for educational applications , and in particular to enable noncomputational linguists to accomplish this task efficiently . such a tool would benefit both learning researchers , allowing them to study dialogue in new ways , and educational technology researchers , allowing them to quickly build dialogue based help systems for tutoring systems . we argue in favor of a user-centered design methodology . we present our work-in-progress design for authoring , which is motivated by our previous tool development experiences and preliminary contextual interviews and then refined through user testing and iterative design .

towards segment-based recognition of argumentation structure in short texts applied computational linguistics
despite recent advances in discourse parsing and causality detection , the automatic recognition of argumentation structure of authentic texts is still a very challenging task . to approach this problem , we collected a small corpus of german microtexts in a text generation experiment , resulting in texts that are authentic but of controlled linguistic and rhetoric complexity . we show that trained annotators can determine the argumentation structure on these microtexts reliably . we experiment with different machine learning approaches for automatic argumentation structure recognition on various levels of granularity of the scheme . given the complex nature of such a discourse understanding tasks , the first results presented here are promising , but invite for further investigation .

an empirical evaluation of a statistical dialog system in public use
this paper provides a first assessment of a statistical dialog system in public use . in our dialog system there are four main recognition tasks , or slots bus route names , bus-stop locations , dates , and times . whereas a conventional system tracks a single value for each slot i.e. , the speech recognizers top hypothesis our statistical system tracks a distribution of many possible values over each slot . past work in lab studies has showed that this distribution improves robustness to speech recognition errors ; but to our surprise , we found the distribution yielded an increase in accuracy for only two of the four slots , and actually decreased accuracy in the other two . in this paper , we identify root causes for these differences in performance , including intrinsic properties of n-best lists , parameter settings , and the quality of statistical models . we synthesize our findings into a set of guidelines which aim to assist researchers and practitioners employing statistical techniques in future dialog systems .

coupling a linguistic formalism and a script language
this article presents a novel syntactic parser architecture , in which a linguistic formalism can be enriched with all sorts of constraints , included extra-linguistic ones , thanks to the seamless coupling of the formalism with a programming language .

dependency-based statistical machine translation
we present a czech-english statistical machine translation system which performs tree-to-tree translation of dependency structures . the only bilingual resource required is a sentence-aligned parallel corpus . all other resources are monolingual . we also refer to an evaluation method and plan to compare our systems output with a benchmark system .

alignment link projection using transformation-based learning
we present a new word-alignment approach that learns errors made by existing word alignment systems and corrects them . by adapting transformationbased learning to the problem of word alignment , we project new alignment links from already existing links , using features such as pos tags . we show that our alignment link projection approach yields a significantly lower alignment error rate than that of the best performing alignment system ( 22.6 % relative reduction on englishspanish data and 23.2 % relative reduction on english-chinese data ) .

discriminative features in reversible stochastic attribute-value grammars
reversible stochastic attribute-value grammars ( de kok et al , 2011 ) use one model for parse disambiguation and fluency ranking . such a model encodes preferences with respect to syntax , fluency , and appropriateness of logical forms , as weighted features . reversible models are built on the premise that syntactic preferences are shared between parse disambiguation and fluency ranking . given that reversible models also use features that are specific to parsing or generation , there is the possibility that the model is trained to rely on these directional features . if this is true , the premise that preferences are shared between parse disambiguation and fluency ranking does not hold . in this work , we compare and apply feature selection techniques to extract the most discriminative features from directional and reversible models . we then analyse the contributions of different classes of features , and show that reversible models do rely on task-independent features .

one distributional memory , many semantic spaces
we propose an approach to corpus-based semantics , inspired by cognitive science , in which different semantic tasks are tackled using the same underlying repository of distributional information , collected once and for all from the source corpus . task-specific semantic spaces are then built on demand from the repository . a straightforward implementation of our proposal achieves state-of-the-art performance on a number of unrelated tasks .

semantic parsing with structured svm ensemble classification models
we present a learning framework for structured support vector models in which boosting and bagging methods are used to construct ensemble models . we also propose a selection method which is based on a switching model among a set of outputs of individual classifiers when dealing with natural language parsing problems . the switching model uses subtrees mined from the corpus and a boosting-based algorithm to select the most appropriate output . the application of the proposed framework on the domain of semantic parsing shows advantages in comparison with the original large margin methods .

language models and reranking for machine translation
complex language models can not be easily integrated in the first pass decoding of a statistical machine translation system the decoder queries the lm a very large number of times ; the search process in the decoding builds the hypotheses incrementally and can not make use of lms that analyze the whole sentence . we present in this paper the language computers system for wmt06 that employs lmpowered reranking on hypotheses generated by phrase-based smt systems

phrase dependency parsing for opinion mining
in this paper , we present a novel approach for mining opinions from product reviews , where it converts opinion mining task to identify product features , expressions of opinions and relations between them . by taking advantage of the observation that a lot of product features are phrases , a concept of phrase dependency parsing is introduced , which extends traditional dependency parsing to phrase level . this concept is then implemented for extracting relations between product features and expressions of opinions . experimental evaluations show that the mining task can benefit from phrase dependency parsing .

building a korean web corpus for analyzing learner language
post-positional particles are a significant source of errors for learners of korean . following methodology that has proven effective in handling english preposition errors , we are beginning the process of building a machine learner for particle error detection in l2 korean writing . as a first step , however , we must acquire data , and thus we present a methodology for constructing large-scale corpora of korean from the web , exploring the feasibility of building corpora appropriate for a given topic and grammatical construction .

employing event inference to improve semi-supervised chinese
although semi-supervised model can extract the event mentions matching frequent event patterns , it suffers much from those event mentions , which match infrequent patterns or have no matching pattern . to solve this issue , this paper introduces various kinds of linguistic knowledge-driven event inference mechanisms to semi-supervised chinese event extraction . these event inference mechanisms can capture linguistic knowledge from four aspects , i.e . semantics of argument role , compositional semantics of trigger , consistency on coreference events and relevant events , to further recover missing event mentions from unlabeled texts . evaluation on the ace 2005 chinese corpus shows that our event inference mechanisms significantly outperform the refined state-of-the-art semi-supervised chinese event extraction system in f1-score by 8.5 % .

automated suggestions for miscollocations
one of the most common and persistent error types in second language writing is collocation errors , such as learn knowledge instead of gain or acquire knowledge , or make damage rather than cause damage . in this work-inprogress report , we propose a probabilistic model for suggesting corrections to lexical collocation errors . the probabilistic model incorporates three features : word association strength ( mi ) , semantic similarity ( via wordnet ) and the notion of shared collocations ( or intercollocability ) . the results suggest that the combination of all three features outperforms any single feature or any combination of two features .

random walk inference and learning in a large scale knowledge base
we consider the problem of performing learning and inference in a large scale knowledge base containing imperfect knowledge with incomplete coverage . we show that a soft inference procedure based on a combination of constrained , weighted , random walks through the knowledge base graph can be used to reliably infer new beliefs for the knowledge base . more specifically , we show that the system can learn to infer different target relations by tuning the weights associated with random walks that follow different paths through the graph , using a version of the path ranking algorithm ( lao and cohen , 2010b ) . we apply this approach to a knowledge base of approximately 500,000 beliefs extracted imperfectly from the web by nell , a never-ending language learner ( carlson et al , 2010 ) . this new system improves significantly over nells earlier horn-clause learning and inference method : it obtains nearly double the precision at rank 100 , and the new learning method is also applicable to many more inference tasks .

named entity transliteration and discovery from multilingual comparable
named entity recognition ( ner ) is an important part of many natural language processing tasks . most current approaches employ machine learning techniques and require supervised data . however , many languages lack such resources . this paper presents an algorithm to automatically discover named entities ( nes ) in a resource free language , given a bilingual corpora in which it is weakly temporally aligned with a resource rich language . we observe that nes have similar time distributions across such corpora , and that they are often transliterated , and develop an algorithm that exploits both iteratively . the algorithm makes use of a new , frequency based , metric for time distributions and a resource free discriminative approach to transliteration . we evaluate the algorithm on an english-russian corpus , and show high level of nes discovery in russian .

from chunks to function-argument structure : a similarity-based seminar fur sprachwissenschaft
chunk parsing has focused on the recognition of partial constituent structures at the level of individual chunks . little attention has been paid to the question of how such partial analyses can be combined into larger structures for complete utterances . such larger structures are not only desirable for a deeper syntactic analysis . they also constitute a necessary prerequisite for assigning function-argument structure . the present paper offers a similaritybased algorithm for assigning functional labels such as subject , object , head , complement , etc . to complete syntactic structures on the basis of prechunked input . the evaluation of the algorithm has concentrated on measuring the quality of functional labels . it was performed on a german and an english treebank using two different annotation schemes at the level of function-argument structure . the results of 89.73 % correct functional labels for german and 90.40 % for english validate the general approach .

incrementality in syntactic processing : computational models and experimental evidence
it is a well-known intuition that human sentence understanding works in an incremental fashion , with a seemingly constant update of the interpretation through the left-to-right processing of a string . such intuitions are backed up by experimental evidence dating from at least as far back as marslen-wilson ( 1973 ) , showing that under many circumstances , interpretations are indeed updated very quickly .

learning to match names across languages the mitre corporation the mitre corporation the mitre corporation
we report on research on matching names in different scripts across languages . we explore two trainable approaches based on comparing pronunciations . the first , a cross-lingual approach , uses an automatic name-matching program that exploits rules based on phonological comparisons of the two languages carried out by humans . the second , monolingual approach , relies only on automatic comparison of the phonological representations of each pair . alignments produced by each approach are fed to a machine learning algorithm . results show that the monolingual approach results in machine-learning based comparison of person-names in english and chinese at an accuracy of over 97.0 f-measure .

phrase-based backoff models for machine translation of highly inflected
we propose a backoff model for phrasebased machine translation that translates unseen word forms in foreign-language text by hierarchical morphological abstractions at the word and the phrase level . the model is evaluated on the europarl corpus for german-english and finnishenglish translation and shows improvements over state-of-the-art phrase-based models .

using bilingual information for cross-language document
cross-language document summarization is defined as the task of producing a summary in a target language ( e.g . chinese ) for a set of documents in a source language ( e.g . english ) . existing methods for addressing this task make use of either the information from the original documents in the source language or the information from the translated documents in the target language . in this study , we propose to use the bilingual information from both the source and translated documents for this task . two summarization methods ( simfusion and corank ) are proposed to leverage the bilingual information in the graph-based ranking framework for cross-language summary extraction . experimental results on the duc2001 dataset with manually translated reference chinese summaries show the effectiveness of the proposed methods .

latent features in automatic tense translation between chinese
on the task of determining the tense to use when translating a chinese verb into english , current systems do not perform as well as human translators . the main focus of the present paper is to identify features that human translators use , but which are not currently automatically extractable . the goal is twofold : to test a particular hypothesis about what additional information human translators might be using , and as a pilot to determine where to focus effort on developing automatic extraction methods for features that are somewhat beyond the reach of current feature extraction . the paper shows that incorporating several latent features into the tense classifier boosts the tense classifiers performance , and a tense classifier using only the latent features outperforms one using only the surface features . our findings confirm the utility of the latent features in automatic tense classification , explaining the gap between automatic classification systems and the human brain .

genre-based paragraph classification for sentiment analysis
we present a taxonomy and classification system for distinguishing between different types of paragraphs in movie reviews : formal vs. functional paragraphs and , within the latter , between description and comment . the classification is used for sentiment extraction , achieving improvement over a baseline without paragraph classification .

optimizing grammars for minimum dependency length
we examine the problem of choosing word order for a set of dependency trees so as to minimize total dependency length . we present an algorithm for computing the optimal layout of a single tree as well as a numerical method for optimizing a grammar of orderings over a set of dependency types . a grammar generated by minimizing dependency length in unordered trees from the penn treebank is found to agree surprisingly well with english word order , suggesting that dependency length minimization has influenced the evolution of english .

play your way to an annotated corpus : games with a purpose and anaphoric annotation
the lack of large-scale corpora annotated with semantic information has been a serious bottleneck for computational semantics , slowing down not only the development of more advanced statistical methods , but also our empirical understanding of the phenomena . the creation of the ontonotes corpus will finally bring computational semantics to the point where computational syntax was in 1993 - but in the meantime , we have come to appreciate the limitations of that methodology both theoretically and as a way of gathering judgments . in this talk , i will discuss an ongoing effort to use the games with a purpose methodology to create a large-scale anaphorically annotated corpus in which multiple judgments are maintained about the interpretation of each anaphoric expression .

posslt : a korean to english spoken language translation system
the posslt 1 is a korean to english spoken language translation ( slt ) system . like most other slt systems , automatic speech recognition ( asr ) , machine translation ( mt ) , and text-to-speech ( tts ) are coupled in a cascading manner in our posslt . however , several novel techniques are applied to improve overall translation quality and speed . models used in posslt are trained on a travel domain conversational corpus .

detecting linguistic idiosyncratic interests in autism using distributional semantic models , jan van santen
children with autism spectrum disorder often exhibit idiosyncratic patterns of behaviors and interests . in this paper , we focus on measuring the presence of idiosyncratic interests at the linguistic level in children with autism using distributional semantic models . we model the semantic space of childrens narratives by calculating pairwise word overlap , and we compare the overlap found within and across diagnostic groups . we find that the words used by children with typical development tend to be used by other children with typical development , while the words used by children with autism overlap less with those used by children with typical development and even less with those used by other children with autism . these findings suggest that children with autism are veering not only away from the topic of the target narrative but also in idiosyncratic semantic directions potentially defined by their individual topics of interest .

joint learning and inference for grammatical error correction
state-of-the-art systems for grammatical error correction are based on a collection of independently-trained models for specific errors . such models ignore linguistic interactions at the sentence level and thus do poorly on mistakes that involve grammatical dependencies among several words . in this paper , we identify linguistic structures with interacting grammatical properties and propose to address such dependencies via joint inference and joint learning . we show that it is possible to identify interactions well enough to facilitate a joint approach and , consequently , that joint methods correct incoherent predictions that independentlytrained classifiers tend to produce . furthermore , because the joint learning model considers interacting phenomena during training , it is able to identify mistakes that require making multiple changes simultaneously and that standard approaches miss . overall , our model significantly outperforms the illinois system that placed first in the conll-2013 shared task on grammatical error correction .

evaluating and integrating treebank parsers on a biomedical corpus
it is not clear a priori how well parsers trained on the penn treebank will parse significantly different corpora without retraining . we carried out a competitive evaluation of three leading treebank parsers on an annotated corpus from the human molecular biology domain , and on an extract from the penn treebank for comparison , performing a detailed analysis of the kinds of errors each parser made , along with a quantitative comparison of syntax usage between the two corpora . our results suggest that these tools are becoming somewhat over-specialised on their training domain at the expense of portability , but also indicate that some of the errors encountered are of doubtful importance for information extraction tasks . furthermore , our inital experiments with unsupervised parse combination techniques showed that integrating the output of several parsers can ameliorate some of the performance problems they encounter on unfamiliar text , providing accuracy and coverage improvements , and a novel measure of trustworthiness . supplementary materials are available at http : //textmining.cryst.bbk . ac.uk/acl05/ .

machine translation using probabilistic synchronous dependency insertion grammars yuan ding martha palmer
syntax-based statistical machine translation ( mt ) aims at applying statistical models to structured data . in this paper , we present a syntax-based statistical machine translation system based on a probabilistic synchronous dependency insertion grammar . synchronous dependency insertion grammars are a version of synchronous grammars defined on dependency trees . we first introduce our approach to inducing such a grammar from parallel corpora . second , we describe the graphical model for the machine translation task , which can also be viewed as a stochastic tree-to-tree transducer . we introduce a polynomial time decoding algorithm for the model . we evaluate the outputs of our mt system using the nist and bleu automatic mt evaluation software . the result shows that our system outperforms the baseline system based on the ibm models in both translation speed and quality .

exploiting n-best hypotheses for smt self-enhancement
word and n-gram posterior probabilities estimated on n-best hypotheses have been used to improve the performance of statistical machine translation ( smt ) in a rescoring framework . in this paper , we extend the idea to estimate the posterior probabilities on n-best hypotheses for translation phrase-pairs , target language n-grams , and source word reorderings . the smt system is self-enhanced with the posterior knowledge learned from nbest hypotheses in a re-decoding framework . experiments on nist chinese-to-english task show performance improvements for all the strategies . moreover , the combination of the three strategies achieves further improvements and outperforms the baseline by 0.67 bleu score on nist-2003 set , and 0.64 on nist2005 set , respectively .

automatic identification of semantic relations in italian complex nominals dipartimento di studi linguistici e orientali
this paper addresses the problem of the identification of the semantic relations in italian complex nominals ( cns ) of the type n+p+n . we exploit the fact that the semantic relation , which is underspecified in most cases , is partially made explicit by the preposition . we develop an annotation framework around five different semantic relations , which we use to create a corpus of 1700 italian cns , obtaining an inter-annotator agreement of k=.695 . exploiting this data , for each preposition p we train a classifier to assign one of the five semantic relations to any cn of the type n+p+n , by using both string and supersense features . to obtain supersenses , we experiment with a sequential tagger as well as a plain lookup in multiwordnet , and find that using information obtained from the former yields better results .

learning to interpret novel noun-noun compounds : evidence from a category learning experiment barry devereux & fintan costello
the ability to correctly interpret and produce noun-noun compounds such as wind farm or carbon tax is an important part of the acquisition of language in various domains of discourse . one approach to the interpretation of noun-noun compounds assumes that people make use of distributional information about how the constituent words of compounds tend to combine ; another assumes that people make use of information about the two constituent concepts features to produce interpretations . we present an experiment that examines how people acquire both the distributional information and conceptual information relevant to compound interpretation . a plausible model of the interpretation process is also presented .

an intrinsic stopping criterion for committee-based active learning
as supervised machine learning methods are increasingly used in language technology , the need for high-quality annotated language data becomes imminent . active learning ( al ) is a means to alleviate the burden of annotation . this paper addresses the problem of knowing when to stop the al process without having the human annotator make an explicit decision on the matter . we propose and evaluate an intrinsic criterion for committee-based al of named entity recognizers .

language identification in code-switching scenario riyaz ahmad bhat
this paper describes a crf based token level language identification system entry to language identification in codeswitched ( cs ) data task of codeswitch 2014. our system hinges on using conditional posterior probabilities for the individual codes ( words ) in code-switched data to solve the language identification task . we also experiment with other linguistically motivated language specific as well as generic features to train the crf based sequence labeling algorithm achieving reasonable results .

unsupervised linguistically-driven reliable dependency parses detection and self-training for adaptation to the biomedical domain
in this paper , a new selftraining method for domain adaptation is illustrated , where the selection of reliable parses is carried out by an unsupervised linguistically driven algorithm , ulisse . the method has been tested on biomedical texts with results showing a significant improvement with respect to considered baselines , which demonstrates its ability to capture both reliability of parses and domain specificity of linguistic constructions .

text summarization model based on maximum coverage problem and its variant
we discuss text summarization in terms of maximum coverage problem and its variant . we explore some decoding algorithms including the ones never used in this summarization formulation , such as a greedy algorithm with performance guarantee , a randomized algorithm , and a branch-andbound method . on the basis of the results of comparative experiments , we also augment the summarization model so that it takes into account the relevance to the document cluster . through experiments , we showed that the augmented model is superior to the best-performing method of duc04 on rouge-1 without stopwords .

utilizing contextually relevant terms in bilingual lexicon extraction
this paper demonstrates one efficient technique in extracting bilingual word pairs from non-parallel but comparable corpora . instead of using the common approach of taking high frequency words to build up the initial bilingual lexicon , we show contextually relevant terms that co-occur with cognate pairs can be efficiently utilized to build a bilingual dictionary . the result shows that our models using this technique have significant improvement over baseline models especially when highestranked translation candidate per word is considered .

combining speech retrieval results with generalized additive models
rapid and inexpensive techniques for automatic transcription of speech have the potential to dramatically expand the types of content to which information retrieval techniques can be productively applied , but limitations in accuracy and robustness must be overcome before that promise can be fully realized . combining retrieval results from systems built on various errorful representations of the same collection offers some potential to address these challenges . this paper explores that potential by applying generalized additive models to optimize the combination of ranked retrieval results obtained using transcripts produced automatically for the same spoken content by substantially different recognition systems . topic-averaged retrieval effectiveness better than any previously reported for the same collection was obtained , and even larger gains are apparent when using an alternative measure emphasizing results on the most difficult topics .

a semantic study on yami ontology in traditional songs taiwan taiwan taiwan
the purpose of this study was to provide an example of how to build a yami ontology from traditional songs by employing protg , an open-source tool for editing and managing ontologies developed by stanford university . following conceptual blending theory ( fauconnier and turner , 1998 ) , we found that yami people use the conceptual metaphor of fishing in traditional songs when praising the hosts diligence in a ceremony celebrating the completion of a workhouse . the process of building ontologies is explored and illustrated . the proposed construction of an ontology for yami traditional songs can serve as a fundamental template , using the corpus available online from the yami documentation website ( http : //yamiproject.cs.pu.edu.tw/yami ) to build ontologies for other domains .

story understanding through multi-representation model construction
we present an implemented model of story understanding and apply it to the understanding of a childrens story . we argue that understanding a story consists of building multirepresentation models of the story and that story models are efficiently constructed using a satisfiability solver . we present a computer program that contains multiple representations of commonsense knowledge , takes a narrative as input , transforms the narrative and representations of commonsense knowledge into a satisfiability problem , runs a satisfiability solver , and produces models of the story as output . the narrative , models , and representations are expressed in the language of shanahans event calculus .

incremental ltag parsing
we present a very efficient statistical incremental parser for ltag-spinal , a variant of ltag . the parser supports the full adjoining operation , dynamic predicate coordination , and non-projective dependencies , with a formalism of provably stronger generative capacity as compared to cfg . using gold standard pos tags as input , on section 23 of the ptb , the parser achieves an f-score of 89.3 % for syntactic dependency defined on ltag derivation trees , which are deeper than the dependencies extracted from ptb alone with head rules ( for example , in magermans style ) .

automatic extraction of english-chinese transliteration pairs using dynamic window and tokenizer
recently , many studies have been focused on extracting transliteration pairs from bilingual texts . most of these studies are based on the statistical transliteration model . the paper discusses the limitations of previous approaches and proposes novel approaches called dynamic window and tokenizer to overcome these limitations . experimental results show that the average rates of word and character precision are 99.0 % and 99.78 % , respectively .

syntax-to-morphology mapping in factored phrase-based statistical machine translation from english to turkish
we present a novel scheme to apply factored phrase-based smt to a language pair with very disparate morphological structures . our approach relies on syntactic analysis on the source side ( english ) and then encodes a wide variety of local and non-local syntactic structures as complex structural tags which appear as additional factors in the training data . on the target side ( turkish ) , we only perform morphological analysis and disambiguation but treat the complete complex morphological tag as a factor , instead of separating morphemes . we incrementally explore capturing various syntactic substructures as complex tags on the english side , and evaluate how our translations improve in bleu scores . our maximal set of source and target side transformations , coupled with some additional techniques , provide an 39 % relative improvement from a baseline 17.08 to 23.78 bleu , all averaged over 10 training and test sets . now that the syntactic analysis on the english side is available , we also experiment with more long distance constituent reordering to bring the english constituent order close to turkish , but find that these transformations do not provide any additional consistent tangible gains when averaged over the 10 sets .

induction of a simple morphology for highly-inflecting languages
this paper presents an algorithm for the unsupervised learning of a simple morphology of a natural language from raw text . a generative probabilistic model is applied to segment word forms into morphs . the morphs are assumed to be generated by one of three categories , namely prefix , suffix , or stem , and we make use of some observed asymmetries between these categories . the model learns a word structure , where words are allowed to consist of lengthy sequences of alternating stems and affixes , which makes the model suitable for highly-inflecting languages . the ability of the algorithm to find real morpheme boundaries is evaluated against a gold standard for both finnish and english . in comparison with a state-of-the-art algorithm the new algorithm performs best on the finnish data , and on roughly equal level on the english data .

integrating phrase-based reordering features into a chart-based decoder for machine translation
hiero translation models have two limitations compared to phrase-based models : 1 ) limited hypothesis space ; 2 ) no lexicalized reordering model . we propose an extension of hiero called phrasalhiero to address hieros second problem . phrasal-hiero still has the same hypothesis space as the original hiero but incorporates a phrase-based distance cost feature and lexicalized reodering features into the chart decoder . the work consists of two parts : 1 ) for each hiero translation derivation , find its corresponding discontinuous phrase-based path . 2 ) extend the chart decoder to incorporate features from the phrase-based path . we achieve significant improvement over both hiero and phrase-based baselines for arabicenglish , chinese-english and germanenglish translation .

character-sense association and compounding template similarity : automatic semantic classification of chinese compounds
this paper presents a character-based model of automatic sense determination for chinese compounds . the model adopts a sense approximation approach using synonymous compounds retrieved by measuring similarity of semantic template in compounding . the similarity measure is derived from an association network among characters and senses , which is built from a formatted mrd . adopting the taxonomy of cilin , a system of deep semantic classification ( at least to the small classes ) for v-v compounds is implemented and evaluated to test the model . the experiment reports a high precision rate ( about 38 % in outside test and 61 % in inside test ) against the baseline one ( about 18 % ) .

other-initiated self-repairs in estonian information dialogues : solving communication problems in cooperation
the paper gives an overview of repair sequences used in estonian spoken information dialogues . 62 calls for information , travel bureaus , shops or outpatients departments are analysed . several repair types are considered . our further aim is to develop a dialogue system which can interact with the user in estonian following the norms and rules of humanhuman communication

authorship attribution of micro-messages roy schwartz oren tsur ari rappoport
work on authorship attribution has traditionally focused on long texts . in this work , we tackle the question of whether the author of a very short text can be successfully identified . we use twitter as an experimental testbed . we introduce the concept of an authors unique signature , and show that such signatures are typical of many authors when writing very short texts . we also present a new authorship attribution feature ( flexible patterns ) and demonstrate a significant improvement over our baselines . our results show that the author of a single tweet can be identified with good accuracy in an array of flavors of the authorship attribution task .

measuring ideological proportions in political speeches
we seek to measure political candidates ideological positioning from their speeches . to accomplish this , we infer ideological cues from a corpus of political writings annotated with known ideologies . we then represent the speeches of u.s. presidential candidates as sequences of cues and lags ( filler distinguished only by its length in words ) . we apply a domain-informed bayesian hmm to infer the proportions of ideologies each candidate uses in each campaign . the results are validated against a set of preregistered , domain expertauthored hypotheses .

emotions evoked by common words and phrases : using mechanical turk to create an emotion lexicon
even though considerable attention has been given to semantic orientation of words and the creation of large polarity lexicons , research in emotion analysis has had to rely on limited and small emotion lexicons . in this paper , we show how we create a high-quality , moderate-sized emotion lexicon using mechanical turk . in addition to questions about emotions evoked by terms , we show how the inclusion of a word choice question can discourage malicious data entry , help identify instances where the annotator may not be familiar with the target term ( allowing us to reject such annotations ) , and help obtain annotations at sense level ( rather than at word level ) . we perform an extensive analysis of the annotations to better understand the distribution of emotions evoked by terms of different parts of speech . we identify which emotions tend to be evoked simultaneously by the same term and show that certain emotions indeed go hand in hand .

active learning-based elicitation for semi-supervised word alignment
semi-supervised word alignment aims to improve the accuracy of automatic word alignment by incorporating full or partial manual alignments . motivated by standard active learning query sampling frameworks like uncertainty- , margin- and query-by-committee sampling we propose multiple query strategies for the alignment link selection task . our experiments show that by active selection of uncertain and informative links , we reduce the overall manual effort involved in elicitation of alignment link data for training a semisupervised word aligner .

relaxed cross-lingual projection of constituent syntax
we propose a relaxed correspondence assumption for cross-lingual projection of constituent syntax , which allows a supposed constituent of the target sentence to correspond to an unrestricted treelet in the source parse . such a relaxed assumption fundamentally tolerates the syntactic non-isomorphism between languages , and enables us to learn the target-language-specific syntactic idiosyncrasy rather than a strained grammar directly projected from the source language syntax . based on this assumption , a novel constituency projection method is also proposed in order to induce a projected constituent treebank from the source-parsed bilingual corpus . experiments show that , the parser trained on the projected treebank dramatically outperforms previous projected and unsupervised parsers .

multi-document relationship fusion via constraints on probabilistic databases
previous multi-document relationship extraction and fusion research has focused on single relationships . shifting the focus to multiple relationships allows for the use of mutual constraints to aid extraction . this paper presents a fusion method which uses a probabilistic database model to pick relationships which violate few constraints . this model allows improved performance on constructing corporate succession timelines from multiple documents with respect to a multi-document fusion baseline .

in spoken dialogue systems
this paper deals with user corrections and aware sites of system errors in the toot spoken dialogue system . we rst describe our corpus , and give details on our procedure to label corrections and aware sites . then , we show that corrections and aware sites exhibit some prosodic and other properties which set them apart from `normal ' utterances . it appears that some correction types , such as simple repeats , are more likely to be correctly recognized than other types , such as paraphrases . we also present evidence that system dialogue strategy a ects users ' choice of correction type , suggesting that strategy-speci c methods of detecting or coaching users on corrections may be useful . aware sites tend to be shorter than other utterances , and are also more dif cult to recognize correctly for the asr system .

a novel discourse parser based on support vector machine classification
this paper introduces a new algorithm to parse discourse within the framework of rhetorical structure theory ( rst ) . our method is based on recent advances in the field of statistical machine learning ( multivariate capabilities of support vector machines ) and a rich feature space . rst offers a formal framework for hierarchical text organization with strong applications in discourse analysis and text generation . we demonstrate automated annotation of a text with rst hierarchically organised relations , with results comparable to those achieved by specially trained human annotators . using a rich set of shallow lexical , syntactic and structural features from the input text , our parser achieves , in linear time , 73.9 % of professional annotators human agreement f-score . the parser is 5 % to 12 % more accurate than current state-of-the-art parsers .

interlinear glossing and its role in theoretical and descriptive studies of african and other lesser-documented languages
in a manuscript william labov ( 1987 ) states that although linguistics is a field with a long historical tradition and with a high degree of consensus on basic categories , it experiences a fundamental devision concerning the role that quantitative methods should play as part of the research progress . linguists differ in the role they assign to the use of natural language examples in linguistic research and in the publication of its results . in this paper we suggest that the general availability of richly annotated , multi-lingual data directly suited for scientific publications could have a positive impact on the way we think about language , and how we approach linguistics.we encourage the systematic generation of linguistic data beyond what emerges from fieldwork and other descriptive studies and introduce an online glossing tool for textual data annotation . we argue that the availability of such an online tool will facilitate the generation of in-depth annotated linguistic examples as part of linguistic research . this in turn will allow the build-up of linguistic resources which can be used independent of the research focus and of the theoretical framework applied . the tool we would like to present is a non-expert-user system designed in particular for the work with lesser documented languages . it has been used for the documentation of several african languages , and has served for two projects involving universities in africa .

wordnet based features for predicting brain activity associated with meanings of nouns ahmad babaeian jelodar , mehrdad alizadeh , and shahram khadivi
different studies have been conducted for predicting human brain activity associated with the semantics of nouns . corpus based approaches have been used for deriving feature vectors of concrete nouns , to model the brain activity associated with that noun . in this paper a computational model is proposed in which , the feature vectors for each concrete noun is computed by the wordnet similarity of that noun with the 25 sensory-motor verbs suggested by psychologists . the feature vectors are used for training a linear model to predict functional mri images of the brain associated with nouns . the wordnet extracted features are also combined with corpus based semantic features of the nouns . the combined features give better results in predicting human brain activity related to concrete nouns .

recognizing noisy romanized japanese words in learner english
this paper describes a method for recognizing romanized japanese words in learner english . they become noise and problematic in a variety of tasks including part-of-speech tagging , spell checking , and error detection because they are mostly unknown words . a problem one encounters when recognizing romanized japanese words in learner english is that the spelling rules of romanized japanese words are often violated by spelling errors . to address the problem , the described method uses a clustering algorithm reinforced by a small set of rules . experiments show that it achieves an -measure of 0.879 and outperforms other methods . they also show that it only requires the target text and a fair size of english word list .

cora : a web-based annotation tool for historical and other non-standard language data
we present cora , a web-based annotation tool for manual annotation of historical and other non-standard language data . it allows for editing the primary data and modifying token boundaries during the annotation process . further , it supports immediate retraining of taggers on newly annotated data .

learning to lemmatise polish noun phrases
we present a novel approach to noun phrase lemmatisation where the main phase is cast as a tagging problem . the idea draws on the observation that the lemmatisation of almost all polish noun phrases may be decomposed into transformation of singular words ( tokens ) that make up each phrase . we perform evaluation , which shows results similar to those obtained earlier by a rule-based system , while our approach allows to separate chunking from lemmatisation .

automated disease normalization with low rank approximations robert leaman zhiyong lu national library of medicine
while machine learning methods for named entity recognition ( mention-level detection ) have become common , machine learning methods have rarely been applied to normalization ( concept-level identification ) . recent research introduced a machine learning method for normalization based on pairwise learning to rank . this method , dnorm , uses a linear model to score the similarity between mentions and concept names , and has several desirable properties , including learning term variation directly from training data . in this manuscript we employ a dimensionality reduction technique based on low-rank matrix approximation , similar to latent semantic indexing . we compare the performance of the low rank method to previous work , using disease name normalization in the ncbi disease corpus as the test case , and demonstrate increased performance as the matrix rank increases . we further demonstrate a significant reduction in the number of parameters to be learned and discuss the implications of this result in the context of algorithm scalability .

the evolution of a statistical nlp course
this paper describes the evolution of a statistical nlp course , which i have been teaching every year for the past three years . the paper will focus on major changes made to the course ( including the course design , assignments , and the use of discussion board ) and highlight the lessons learned from this experience .

joint and conditional estimation of tagging and parsing models
this paper compares two different ways of estimating statistical language models . many statistical nlp tagging and parsing models are estimated by maximizing the ( joint ) likelihood of the fully-observed training data . however , since these applications only require the conditional probability distributions , these distributions can in principle be learnt by maximizing the conditional likelihood of the training data . perhaps somewhat surprisingly , models estimated by maximizing the joint were superior to models estimated by maximizing the conditional , even though some of the latter models intuitively had access to more information .

evaluating multilanguage-comparability of subjectivity analysis division of electrical and computer engineering
subjectivity analysis is a rapidly growing field of study . along with its applications to various nlp tasks , much work have put efforts into multilingual subjectivity learning from existing resources . multilingual subjectivity analysis requires language-independent criteria for comparable outcomes across languages . this paper proposes to measure the multilanguage-comparability of subjectivity analysis tools , and provides meaningful comparisons of multilingual subjectivity analysis from various points of view .

factored markov translation with robust modeling information sciences institue
phrase-based translation models usually memorize local translation literally and make independent assumption between phrases which makes it neither generalize well on unseen data nor model sentencelevel effects between phrases . in this paper we present a new method to model correlations between phrases as a markov model and meanwhile employ a robust smoothing strategy to provide better generalization . this method defines a recursive estimation process and backs off in parallel paths to infer richer structures . our evaluation shows an 1.13.2 % bleu improvement over competitive baselines for chinese-english and arabic-english translation .

learning the taxonomy of function words for parsing
completely data-driven grammar training is prone to over-fitting . human-defined word class knowledge is useful to address this issue . however , the manual word class taxonomy may be unreliable and irrational for statistical natural language processing , aside from its insufficient linguistic phenomena coverage and domain adaptivity . in this paper , a formalized representation of function word subcategorization is developed for parsing in an automatic manner . the function word classification representing intrinsic features of syntactic usages is used to supervise the grammar induction , and the structure of the taxonomy is learned simultaneously . the grammar learning process is no longer a unilaterally supervised training by hierarchical knowledge , but an interactive process between the knowledge structure learning and the grammar training . the established taxonomy implies the stochastic significance of the diversified syntactic features . the experiments on both penn chinese treebank and tsinghua treebank show that the proposed method improves parsing performance by 1.6 % and 7.6 % respectively over the baseline .

context vector disambiguation for bilingual lexicon extraction from
this paper presents an approach that extends the standard approach used for bilingual lexicon extraction from comparable corpora . we focus on the unresolved problem of polysemous words revealed by the bilingual dictionary and introduce a use of a word sense disambiguation process that aims at improving the adequacy of context vectors . on two specialized frenchenglish comparable corpora , empirical experimental results show that our method improves the results obtained by two stateof-the-art approaches .

a large scale distributed syntactic , semantic and lexical language model for machine translation
this paper presents an attempt at building a large scale distributed composite language model that simultaneously accounts for local word lexical information , mid-range sentence syntactic structure , and long-span document semantic content under a directed markov random field paradigm . the composite language model has been trained by performing a convergent n-best list approximate em algorithm that has linear time complexity and a followup em algorithm to improve word prediction power on corpora with up to a billion tokens and stored on a supercomputer . the large scale distributed composite language model gives drastic perplexity reduction over ngrams and achieves significantly better translation quality measured by the bleu score and readability when applied to the task of re-ranking the n-best list from a state-of-theart parsing-based machine translation system .

automatic identification of rhetorical roles using conditional random fields for legal document summarization
in this paper , we propose a machine learning approach to rhetorical role identification from legal documents . in our approach , we annotate roles in sample documents with the help of legal experts and take them as training data . conditional random field model has been trained with the data to perform rhetorical role identification with reinforcement of rich feature sets . the understanding of structure of a legal document and the application of mathematical model can brings out an effective summary in the final stage . other important new findings in this work include that the training of a model for one sub-domain can be extended to another sub-domains with very limited augmentation of feature sets . moreover , we can significantly improve extraction-based summarization results by modifying the ranking of sentences with the importance of specific roles .

english-to-czech statistical machine translation
this paper describes our two contributions to wmt08 shared task : factored phrase-based model using moses and a probabilistic treetransfer model at a deep syntactic layer .

identifying opinion holders and targets with dependency parser in chinese news texts
in this paper , we propose to identify opinion holders and targets with dependency parser in chinese news texts , i.e . to identify opinion holders by means of reporting verbs and to identify opinion targets by considering both opinion holders and opinion-bearing words . the experiments with ntcir-7 moats chinese test data show that our approach provides better performance than the baselines and most systems reported at ntcir-7 .

scanning methods and language modeling for binary switch typing
we present preliminary experiments of a binary-switch , static-grid typing interface making use of varying language model contributions . our motivation is to quantify the degree to which language models can make the simplest scanning interfaces such as showing one symbol at a time rather than a scanning a grid competitive in terms of typing speed . we present a grid scanning method making use of optimal huffman binary codes , and demonstrate the impact of higher order language models on its performance . we also investigate the scanning methods of highlighting just one cell in a grid at any given time or showing one symbol at a time without a grid , and show that they yield commensurate performance when using higher order n-gram models , mainly due to lower error rate and a lower rate of missed targets .

tips : a translingual information processing system
searching online information is increasingly a daily activity for many people . the multilinguality of online content is also increasing ( e.g . the proportion of english web users , which has been decreasing as a fraction the increasing population of web users , dipped below 50 % in the summer of 2001 ) . to improve the ability of an english speaker to search mutlilingual content , we built a system that supports cross-lingual search of an arabic newswire collection and provides on demand translation of arabic web pages into english . the cross-lingual search engine supports a fast search capability ( sub-second response for typical queries ) and achieves state-of-the-art performance in the high precision region of the result list . the on demand statistical machine translation uses the direct translation model along with a novel statistical arabic morphological analyzer to yield state-of-the-art translation quality . the on demand smt uses an efficient dynamic programming decoder that achieves reasonable speed for translating web documents . overview morphologically rich languages like arabic ( beesley , k. 1996 ) present significant challenges to many natural language processing applications as the one described above because a word often conveys complex meanings decomposable into several morphemes ( i.e . prefix , stem , suffix ) . by segmenting words into morphemes , we can improve the performance of natural language systems including machine translation ( brown et al .

generic noun phrases and annotation of coreference and bridging relations in the prague dependency treebank
this paper discusses the problem of annotating coreference relations with generic expressions in a large scale corpus . we present and analyze some existing theories of genericity , compare them to the approaches to generics that are used in the state-of-the-art coreference annotation guidelines and discuss how coreference of generic expressions is processed in the manual annotation of the prague dependency treebank . after analyzing some typical problematic issues we propose some partial solutions that can be used to enhance the quality and consistency of the annotation .

dependency structure analysis and sentence boundary detection in spontaneous japanese kazuya shitaoka kiyotaka uchimoto tatsuya kawahara hitoshi isahara and communications technology
this paper describes a project to detect dependencies between japanese phrasal units called bunsetsus , and sentence boundaries in a spontaneous speech corpus . in monologues , the biggest problem with dependency structure analysis is that sentence boundaries are ambiguous . in this paper , we propose two methods for improving the accuracy of sentence boundary detection in spontaneous japanese speech : one is based on statistical machine translation using dependency information and the other is based on text chunking using svm . an f-measure of 84.9 was achieved for the accuracy of sentence boundary detection by using the proposed methods . the accuracy of dependency structure analysis was also improved from 75.2 % to 77.2 % by using automatically detected sentence boundaries . the accuracy of dependency structure analysis and that of sentence boundary detection were also improved by interactively using both automatically detected dependency structures and sentence boundaries .

unsupervised language-independent name translation mining from
the automatic generation of entity profiles from unstructured text , such as knowledge base population , if applied in a multi-lingual setting , generates the need to align such profiles from multiple languages in an unsupervised manner . this paper describes an unsupervised and language-independent approach to mine name translation pairs from entity profiles , using wikipedia infoboxes as a stand-in for high quality entity profile extraction . pairs are initially found using expressions that are written in language-independent forms ( such as dates and numbers ) , and new translations are then mined from these pairs . the algorithm then iteratively bootstraps from these translations to learn more pairs and more translations . the algorithm maintains a high precision , over 95 % , for the majority of its iterations , with a slightly lower precision of 85.9 % and an f-score of 76 % . a side effect of the name mining algorithm is the unsupervised creation of a translation lexicon between the two languages , with an accuracy of 64 % . we also duplicate three state-of-the-art name translation mining methods and use two existing name translation gazetteers to compare with our approach . comparisons show our approach can effectively augment the results from each of these alternative methods and resources .

unsupervised dependency parsing without gold part-of-speech tags
we show that categories induced by unsupervised word clustering can surpass the performance of gold part-of-speech tags in dependency grammar induction . unlike classic clustering algorithms , our method allows a word to have different tags in different contexts . in an ablative analysis , we first demonstrate that this context-dependence is crucial to the superior performance of gold tags requiring a word to always have the same part-ofspeech significantly degrades the performance of manual tags in grammar induction , eliminating the advantage that human annotation has over unsupervised tags . we then introduce a sequence modeling technique that combines the output of a word clustering algorithm with context-colored noise , to allow words to be tagged differently in different contexts . with these new induced tags as input , our state-ofthe-art dependency grammar inducer achieves 59.1 % directed accuracy on section 23 ( all sentences ) of the wall street journal ( wsj ) corpus 0.7 % higher than using gold tags .

a multi-resolution framework for information extraction from free text
extraction of relations between entities is an important part of information extraction on free text . previous methods are mostly based on statistical correlation and dependency relations between entities . this paper re-examines the problem at the multiresolution layers of phrase , clause and sentence using dependency and discourse relations . our multi-resolution framework are ( anchor and relation ) uses clausal relations in 2 ways : 1 ) to filter noisy dependency paths ; and 2 ) to increase reliability of dependency path extraction . the resulting system outperforms the previous approaches by 3 % , 7 % , 4 % on muc4 , muc6 and ace rdc domains respectively .

building language resources : ways to move forward
there are perhaps seven thousand languages in the world , ranging from the largest with hundreds of millions of speakers , to the smallest , with one speaker . on a different axis , languages can be ranked according to the quantity and quality of computational resources . not surprisingly , there are correlations between these two axes : languages like english and mandarin have substantial resources , while many of the smallest languages are completely undocumented . nevertheless , the correlation is not perfect ; there are languages with a million speakers which are more or less unwritten , and there are very large languages some of the languages of india , for example which are relatively resource-poor . unfortunately , what counts as resource-rich ( or even resource-adequate ) in computational linguistics is a moving target . for languages to move in the direction of resource richness , considerable effort ( people and money ) have to be provided over a prolonged period of time . one can sit back and wait for this to happen , or give up ; alternatively , one can map out a realistic way forward , building on the strengths of each languages situation . among the strengths which may prove useful to building computational resources for languages are the following : long traditions of grammatical and lexical description traditions of literacy and literature local expertise in linguistics and computing the world-wide community of linguists and computer experts resource availability in related languages at the same time , there are weaknesses and other problems some language specific , some more general which need to be considered : lack of consensus on ways of representing the language ( scripts , character encoding ) complexities inherent in particular languages ( complex scripts , complex morphologies , variant orthographies , diglossia , dialectal variation ) economic and educational realities in the countries where the language is spoken political attitudes towards some languages , particularly minority languages the 'not invented here ' syndrome software obsolescence , and the potential obsolescence of language data this talk will look at ways in which the strengths enumerated above might be leveraged , while avoiding the potential weaknesses .

machine-assisted rhetorical structure annotation applied computational linguistics
manually annotating the rhetorical structure of texts is very labour-intensive . at the same time , high-quality automatic analysis is currently out of reach . we thus propose to split the manual annotation in two phases : the simpler marking of lexical connectives and their relations , and the more difficult decisions on overall tree structure . to this end , we developed an environment of two analysis tools and xml-based declarative resources . our conano tool allows for efficient , interactive annotation of connectives , scopes and relations . this intermediate result is exported to odonnells rst tool , which facilitates completing the tree structure .

data-driven generation of emphatic facial displays mary ellen foster
we describe an implementation of datadriven selection of emphatic facial displays for an embodied conversational agent in a dialogue system . a corpus of sentences in the domain of the target dialogue system was recorded , and the facial displays used by the speaker were annotated . the data from those recordings was used in a range of models for generating facial displays , each model making use of a different amount of context or choosing displays differently within a context . the models were evaluated in two ways : by cross-validation against the corpus , and by asking users to rate the output . the predictions of the cross-validation study differed from the actual user ratings . while the cross-validation gave the highest scores to models making a majority choice within a context , the user study showed a significant preference for models that produced more variation . this preference was especially strong among the female subjects .

learning bayesian networks for semantic frame composition in a spoken dialog system
a stochastic approach based on dynamic bayesian networks ( dbns ) is introduced for spoken language understanding . dbn-based models allow to infer and then to compose semantic frame-based tree structures from speech transcriptions . experimental results on the french media dialog corpus show the appropriateness of the technique which both lead to good tree identification results and can provide the dialog system with n-best lists of scored hypotheses .

using unknown word techniques to learn known words
unknown words are a hindrance to the performance of hand-crafted computational grammars of natural language . however , words with incomplete and incorrect lexical entries pose an even bigger problem because they can be the cause of a parsing failure despite being listed in the lexicon of the grammar . such lexical entries are hard to detect and even harder to correct . we employ an error miner to pinpoint words with problematic lexical entries . an automated lexical acquisition technique is then used to learn new entries for those words which allows the grammar to parse previously uncovered sentences successfully . we test our method on a large-scale grammar of dutch and a set of sentences for which this grammar fails to produce a parse . the application of the method enables the grammar to cover 83.76 % of those sentences with an accuracy of 86.15 % .

automatic extraction of complex predicates in bengali dipankar das santanu pal tapabrata mondal tanmoy chakraborty
this paper presents the automatic extraction of complex predicates ( cps ) in bengali with a special focus on compound verbs ( verb + verb ) and conjunct verbs ( noun /adjective + verb ) . the lexical patterns of compound and conjunct verbs are extracted based on the information of shallow morphology and available seed lists of verbs . lexical scopes of compound and conjunct verbs in consecutive sequence of complex predicates ( cps ) have been identified . the fine-grained error analysis through confusion matrix highlights some insufficiencies of lexical patterns and the impacts of different constraints that are used to identify the complex predicates ( cps ) . system achieves f-scores of 75.73 % , and 77.92 % for compound verbs and 89.90 % and 89.66 % for conjunct verbs respectively on two types of bengali corpus .

empirical analysis of exploiting review helpfulness for extractive summarization of online reviews
we propose a novel unsupervised extractive approach for summarizing online reviews by exploiting review helpfulness ratings . in addition to using the helpfulness ratings for review-level filtering , we suggest using them as the supervision of a topic model for sentence-level content scoring . the proposed method is metadata-driven , requiring no human annotation , and generalizable to different kinds of online reviews . our experiment based on a widely used multi-document summarization framework shows that our helpfulness-guided review summarizers significantly outperform a traditional content-based summarizer in both human evaluation and automated evaluation .

dependency parsing with energy-based reinforcement learning
we present a model which integrates dependency parsing with reinforcement learning based on markov decision process . at each time step , a transition is picked up to construct the dependency tree in terms of the long-run reward . the optimal policy for choosing transitions can be found with the sarsa algorithm . in sarsa , an approximation of the stateaction function can be obtained by calculating the negative free energies for the restricted boltzmann machine . the experimental results on conll-x multilingual data show that the proposed model achieves comparable results with the current state-of-the-art methods .

topic model analysis of metaphor frequency for psycholinguistic stimuli vicky tzuyin lai
psycholinguistic studies of metaphor processing must control their stimuli not just for word frequency but also for the frequency with which a term is used metaphorically . thus , we consider the task of metaphor frequency estimation , which predicts how often target words will be used metaphorically . we develop metaphor classifiers which represent metaphorical domains through latent dirichlet allocation , and apply these classifiers to the target words , aggregating their decisions to estimate the metaphorical frequencies . training on only 400 sentences , our models are able to achieve 61.3 % accuracy on metaphor classification and 77.8 % accuracy on high vs. low metaphorical frequency estimation .

a rule-driven dynamic programming decoder for statistical mt
the paper presents an extension of a dynamic programming ( dp ) decoder for phrase-based smt ( koehn , 2004 ; och and ney , 2004 ) that tightly integrates pos-based re-order rules ( crego and marino , 2006 ) into a left-to-right beam-search algorithm , rather than handling them in a pre-processing or re-order graph generation step . the novel decoding algorithm can handle tens of thousands of rules efficiently . an improvement over a standard phrase-based decoder is shown on an arabicenglish translation task with respect to translation accuracy and speed for large re-order window sizes .

semantic normalisation : a framework and an experiment
we present a normalisation framework for linguistic representations and illustrate its use by normalising the stanford dependency graphs ( sds ) produced by the stanford parser into labelled stanford dependency graphs ( lsds ) . the normalised representations are evaluated both on a testsuite of constructed examples and on free text . the resulting representations improve on standard predicate/argument structures produced by srl by combining role labelling with the semantically oriented features of sds . furthermore , the proposed normalisation framework opens the way to stronger normalisation processes which should be useful in reducing the burden on inference .

codex : combining an svm classifier and character n-gram language models for sentiment analysis on twitter text
this paper briefly reports our system for the semeval-2013 task 2 : sentiment analysis in twitter . we first used an svm classifier with a wide range of features , including bag of word features ( unigram , bigram ) , pos features , stylistic features , readability scores and other statistics of the tweet being analyzed , domain names , abbreviations , emoticons in the twitter text . then we investigated the effectiveness of these features . we also used character n-gram language models to address the problem of high lexical variation in twitter text and combined the two approaches to obtain the final results . our system is robust and achieves good performance on the twitter test data as well as the sms test data .

a salience-based approach to gesture-speech alignment
one of the first steps towards understanding natural multimodal language is aligning gesture and speech , so that the appropriate gestures ground referential pronouns in the speech . this paper presents a novel technique for gesture-speech alignment , inspired by saliencebased approaches to anaphoric pronoun resolution . we use a hybrid between data-driven and knowledge-based mtehods : the basic structure is derived from a set of rules about gesture salience , but the salience weights themselves are learned from a corpus . our system achieves 95 % recall and precision on a corpus of transcriptions of unconstrained multimodal monologues , significantly outperforming a competitive baseline .

when conset meets synset : a preliminary survey of an ontological lexical resource based on chinese characters
this paper describes an on-going project concerning with an ontological lexical resource based on the abundant conceptual information grounded on chinese characters . the ultimate goal of this project is set to construct a cognitively sound and computationally effective character-grounded machine-understandable resource . philosophically , chinese ideogram has its ontological status , but its applicability to the nlp task has not been expressed explicitly in terms of language resource . we thus propose the first attempt to locate chinese characters within the context of ontology . having the primary success in applying it to some nlp tasks , we believe that the construction of this knowledge resource will shed new light on theoretical setting as well as the construction of chinese lexical semantic resources .

unsupervised dependency parsing with acoustic cues
unsupervised parsing is a difficult task that infants readily perform . progress has been made on this task using text-based models , but few computational approaches have considered how infants might benefit from acoustic cues . this paper explores the hypothesis that word duration can help with learning syntax . we describe how duration information can be incorporated into an unsupervised bayesian dependency parser whose only other source of information is the words themselves ( without punctuation or parts of speech ) . our results , evaluated on both adult-directed and child-directed utterances , show that using word duration can improve parse quality relative to words-only baselines . these results support the idea that acoustic cues provide useful evidence about syntactic structure for language-learning infants , and motivate the use of word duration cues in nlp tasks with speech .

iiith : domain specific word sense disambiguation
we describe two systems that participated in semeval-2010 task 17 ( all-words word sense disambiguation on a specific domain ) and were ranked in the third and fourth positions in the formal evaluation . domain adaptation techniques using the background documents released in the task were used to assign ranking scores to the words and their senses . the test data was disambiguated using the personalized pagerank algorithm which was applied to a graph constructed from the whole of wordnet in which nodes are initialized with ranking scores of words and their senses . in the competition , our systems achieved comparable accuracy of 53.4 and 52.2 , which outperforms the most frequent sense baseline ( 50.5 ) .

reasoning in metaphor understanding : the att-meta approach and system
a detailed approach has been developed for core aspects of the task of understanding a broad class of metaphorical utterances . the utterances in question are those that depend on known metaphorical mappings but that nevertheless contain elements not mapped by those mappings . a reasoning system has been implemented that partially instantiates the theoretical approach . the system , called att-meta , will be demonstrated . the paper briefly indicates how the system works , and outlines some specific aspects of the system , approach and the overall project .

umcc_dlsi- ( eps ) : paraphrases detection based on semantic
this paper describes the specifications and results of umcc_dlsi- ( eps ) system , which participated in the first evaluating phrasal semantics of semeval-2013 . our supervised system uses different kinds of semantic features to train a bagging classifier used to select the correct similarity option . related to the different features we can highlight the resource wordnet used to extract semantic relations among words and the use of different algorithms to establish semantic similarities . our system obtains promising results with a precision value around 78 % for the english corpus and 71.84 % for the italian corpus .

fine grained classification of named entities
while named entity extraction is useful in many natural language applications , the coarse categories that most ne extractors work with prove insufficient for complex applications such as question answering and ontology generation . we examine one coarse category of named entities , persons , and describe a method for automatically classifying person instances into eight finergrained subcategories . we present a supervised learning method that considers the local context surrounding the entity as well as more global semantic information derived from topic signatures and wordnet . we reinforce this method with an algorithm that takes advantage of the presence of entities in multiple contexts .

extracting opinion expressions and their polarities exploration of
we investigate systems that identify opinion expressions and assigns polarities to the extracted expressions . in particular , we demonstrate the benefit of integrating opinion extraction and polarity classification into a joint model using features reflecting the global polarity structure . the model is trained using large-margin structured prediction methods . the system is evaluated on the mpqa opinion corpus , where we compare it to the only previously published end-to-end system for opinion expression extraction and polarity classification . the results show an improvement of between 10 and 15 absolute points in f-measure .

is it really that difficult to parse german
this paper presents a comparative study of probabilistic treebank parsing of german , using the negra and tuba-d/z treebanks . experiments with the stanford parser , which uses a factored pcfg and dependency model , show that , contrary to previous claims for other parsers , lexicalization of pcfg models boosts parsing performance for both treebanks . the experiments also show that there is a big difference in parsing performance , when trained on the negra and on the tubad/z treebanks . parser performance for the models trained on tuba-d/z are comparable to parsing results for english with the stanford parser , when trained on the penn treebank . this comparison at least suggests that german is not harder to parse than its west-germanic neighbor language english .

joint arc-factored parsing of syntactic and semantic dependencies
in this paper we introduce a joint arc-factored model for syntactic and semantic dependency parsing . the semantic role labeler predicts the full syntactic paths that connect predicates with their arguments . this process is framed as a linear assignment task , which allows to control some well-formedness constraints . for the syntactic part , we define a standard arc-factored dependency model that predicts the full syntactic tree . finally , we employ dual decomposition techniques to produce consistent syntactic and predicate-argument structures while searching over a large space of syntactic configurations . in experiments on the conll-2009 english benchmark we observe very competitive results .

how to avoid burning ducks : combining linguistic analysis and corpus statistics for german compound processing
compound splitting is an important problem in many nlp applications which must be solved in order to address issues of data sparsity . previous work has shown that linguistic approaches for german compound splitting produce a correct splitting more often , but corpus-driven approaches work best for phrase-based statistical machine translation from german to english , a worrisome contradiction . we address this situation by combining linguistic analysis with corpus-driven statistics and obtaining better results in terms of both producing splittings according to a gold standard and statistical machine translation performance .

lexicographic semirings for exact automata encoding of sequence models
in this paper we introduce a novel use of the lexicographic semiring and motivate its use for speech and language processing tasks . we prove that the semiring allows for exact encoding of backoff models with epsilon transitions . this allows for off-line optimization of exact models represented as large weighted finite-state transducers in contrast to implicit ( on-line ) failure transition representations . we present preliminary empirical results demonstrating that , even in simple intersection scenarios amenable to the use of failure transitions , the use of the more powerful lexicographic semiring is competitive in terms of time of intersection .

extending statistical machine translation with discriminative and trigger-based lexicon models
in this work , we propose two extensions of standard word lexicons in statistical machine translation : a discriminative word lexicon that uses sentence-level source information to predict the target words and a trigger-based lexicon model that extends ibm model with a second trigger , allowing for a more fine-grained lexical choice of target words . the models capture dependencies that go beyond the scope of conventional smt models such as phraseand language models . we show that the models improve translation quality by 1 % in bleu over a competitive baseline on a large-scale task .

from protein-protein interaction to molecular event extraction
this document describes the methods and results for our participation in the bionlp09 shared task # 1 on event extraction . it also contains some error analysis and a brief discussion of the results . previous shared tasks in the bionlp community have focused on extracting gene and protein names , and on finding ( direct ) protein-protein interactions ( ppi ) . this years task was slightly different , since the protein names were already manually annotated in the text . the new challenge was to extract biological events involving these given gene and gene products . we modified a publicly available system ( akaneppi ) to apply it to this new , but similar , protein interaction task . akaneppi has previously achieved state-of-the-art performance on all existing public ppi corpora , and only small changes were needed to achieve competitive results on this event extraction task . our official result was an f-score of 36.9 % , which was ranked as number six among submissions from 24 different groups . we later balanced the recall/precision by including more predictions than just the most confident one in ambiguous cases , and this raised the f-score on the test-set to 42.6 % . the new akane program can be used freely for academic purposes .

arabic diacritization through full morphological tagging
we present a diacritization system for written arabic which is based on a lexical resource . it combines a tagger and a lexeme language model . it improves on the best results reported in the literature .

feature engineering and post-processing for temporal expression recognition using conditional random fields
we present the results of feature engineering and post-processing experiments conducted on a temporal expression recognition task . the former explores the use of different kinds of tagging schemes and of exploiting a list of core temporal expressions during training . the latter is concerned with the use of this list for postprocessing the output of a system based on conditional random fields . we find that the incorporation of knowledge sources both for training and postprocessing improves recall , while the use of extended tagging schemes may help to offset the ( mildly ) negative impact on precision . each of these approaches addresses a different aspect of the overall recognition performance . taken separately , the impact on the overall performance is low , but by combining the approaches we achieve both high precision and high recall scores .

modeling and predicting quality in spoken human-computer interaction
in this work we describe the modeling and prediction of interaction quality ( iq ) in spoken dialogue systems ( sds ) using support vector machines . the model can be employed to estimate the quality of the ongoing interaction at arbitrary points in a spoken humancomputer interaction . we show that the use of 52 completely automatic features characterizing the system-user exchange significantly outperforms state-of-the-art approaches . the model is evaluated on publically available data from the cmu lets go bus information system . it reaches a performance of 61.6 % unweighted average recall when discriminating between 5 classes ( good to very poor ) . it can be further shown that incorporating knowledge about the users emotional state does hardly improve the performance .

multilingual lexical network from the archives of the digital
we are describing the construction process of a specialized multilingual lexical resource dedicated for the archive of the digital silk road dsr . the dsr project creates digital archives of cultural heritage along the historical silk road ; more than 116 of basic references on silk road have been digitized and made available online . these books are written in various languages and attract people from different linguistic background , therefore , we are trying to build a multilingual repository for the terminology of the dsr to help its users , and increase the accessibility of these books . the construction of a terminological database using a classical approach is difficult and expensive . instead , we are introducing specialized lexical resources that can be constructed by the community and its resources ; we call it multilingual preterminological graphs mpgs . we build such graphs by analyzing the access log files of the website of the digital silk road . we aim at making this graph as a seed repository so multilingual volunteers can contribute . we have used the access log files of the dsr since its beginning in 2003 , and obtained an initial graph of around 116,000 terms . as an application , we have used this graph to obtain a preterminological multilingual database that has a number of applications .

japanese query alteration based on semantic similarity
we propose a unified approach to web search query alterations in japanese that is not limited to particular character types or orthographic similarity between a query and its alteration candidate . our model is based on previous work on english query correction , but makes some crucial improvements : ( 1 ) we augment the query-candidate list to include orthographically dissimilar but semantically similar pairs ; and ( 2 ) we use kernel-based lexical semantic similarity to avoid the problem of data sparseness in computing querycandidate similarity . we also propose an efficient method for generating query-candidate pairs for model training and testing . we show that the proposed method achieves about 80 % accuracy on the query alteration task , improving over previously proposed methods that use semantic similarity .

a comparison of pivot methods for phrase-based statistical machine
we compare two pivot strategies for phrase-based statistical machine translation ( smt ) , namely phrase translation and sentence translation . the phrase translation strategy means that we directly construct a phrase translation table ( phrase-table ) of the source and target language pair from two phrase-tables ; one constructed from the source language and english and one constructed from english and the target language . we then use that phrase-table in a phrase-based smt system . the sentence translation strategy means that we first translate a source language sentence into n english sentences and then translate these n sentences into target language sentences separately . then , we select the highest scoring sentence from these target sentences . we conducted controlled experiments using the europarl corpus to evaluate the performance of these pivot strategies as compared to directly trained smt systems . the phrase translation strategy significantly outperformed the sentence translation strategy . its relative performance was 0.92 to 0.97 compared to directly trained smt systems .

guiding semi-supervision with constraint-driven learning
over the last few years , two of the main research directions in machine learning of natural language processing have been the study of semi-supervised learning algorithms as a way to train classifiers when the labeled data is scarce , and the study of ways to exploit knowledge and global information in structured learning tasks . in this paper , we suggest a method for incorporating domain knowledge in semi-supervised learning algorithms . our novel framework unifies and can exploit several kinds of task specic constraints . the experimental results presented in the information extraction domain demonstrate that applying constraints helps the model to generate better feedback during learning , and hence the framework allows for high performance learning with significantly less training data than was possible before on these tasks .

modeling commonality among related classes in relation extraction
this paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem in relation extraction by modeling the commonality among related classes . for each class in the hierarchy either manually predefined or automatically clustered , a linear discriminative function is determined in a topdown way using a perceptron algorithm with the lower-level weight vector derived from the upper-level weight vector . as the upper-level class normally has much more positive training examples than the lower-level class , the corresponding linear discriminative function can be determined more reliably . the upperlevel discriminative function then can effectively guide the discriminative function learning in the lower-level , which otherwise might suffer from limited training data . evaluation on the ace rdc 2003 corpus shows that the hierarchical strategy much improves the performance by 5.6 and 5.1 in f-measure on least- and medium- frequent relations respectively . it also shows that our system outperforms the previous best-reported system by 2.7 in f-measure on the 24 subtypes using the same feature set .

type-based mcmc for sampling tree fragments from forests
this paper applies type-based markov chain monte carlo ( mcmc ) algorithms to the problem of learning synchronous context-free grammar ( scfg ) rules from a forest that represents all possible rules consistent with a fixed word alignment . while type-based mcmc has been shown to be effective in a number of nlp applications , our setting , where the tree structure of the sentence is itself a hidden variable , presents a number of challenges to type-based inference . we describe methods for defining variable types and efficiently indexing variables in order to overcome these challenges . these methods lead to improvements in both log likelihood and bleu score in our experiments .

dijkstra-wsa : a graph-based approach to word sense alignment
in this paper , we present dijkstra-wsa , a novel graph-based algorithm for word sense alignment . we evaluate it on four different pairs of lexical-semantic resources with different characteristics ( wordnet-omegawiki , wordnet-wiktionary , germanet-wiktionary and wordnet-wikipedia ) and show that it achieves competitive performance on 3 out of 4 datasets . dijkstra-wsa outperforms the state of the art on every dataset if it is combined with a back-off based on gloss similarity . we also demonstrate that dijkstra-wsa is not only flexibly applicable to different resources but also highly parameterizable to optimize for precision or recall .

on-the-fly translator assistant
this paper describes a new methodology for developing cat tools that assist translators of technical and scientific texts by ( i ) on-the-fly highlight of nominal and verbal terminology in a source language ( sl ) document that lifts possible syntactic ambiguity and thus essentially raises the document readability and ( ii ) simultaneous translation of all sl document one- and multicomponent lexical units . the methodology is based on a language-independent hybrid extraction technique used for document analysis , and language-dependent shallow linguistic knowledge . it is targeted at intelligent output and computationally attractive properties . the approach is illustrated by its implementation into a cat tool for the russian-english language pair . such tools can also be integrated into full mt systems .

estimating word alignment quality for smt reordering tasks sara stymne j org tiedemann joakim nivre
previous studies of the effect of word alignment on translation quality in smt generally explore link level metrics only and mostly do not show any clear connections between alignment and smt quality . in this paper , we specifically investigate the impact of word alignment on two pre-reordering tasks in translation , using a wider range of quality indicators than previously done . experiments on germanenglish translation show that reordering may require alignment models different from those used by the core translation system . sparse alignments with high precision on the link level , for translation units , and on the subset of crossing links , like intersected hmm models , are preferred . unlike smt performance the desired alignment characteristics are similar for small and large training data for the pre-reordering tasks . moreover , we confirm previous research showing that the fuzzy reordering score is a useful and cheap proxy for performance on smt reordering tasks .

generating expository dialogue from monologue :
generating expository dialogue from monologue is a task that poses an interesting and rewarding challenge for natural language processing . this short paper has three aims : firstly , to motivate the importance of this task , both in terms of the benefits of expository dialogue as a way to present information and in terms of potential applications ; secondly , to introduce a parallel corpus of monologues and dialogues which enables a data-driven approach to this challenge ; and , finally , to describe work-in-progress on semi-automatic construction of monologueto-dialogue ( m2d ) generation rules .

graph-based text representation for novelty detection
we discuss several feature sets for novelty detection at the sentence level , using the data and procedure established in task 2 of the trec 2004 novelty track . in particular , we investigate feature sets derived from graph representations of sentences and sets of sentences . we show that a highly connected graph produced by using sentence-level term distances and pointwise mutual information can serve as a source to extract features for novelty detection . we compare several feature sets based on such a graph representation . these feature sets allow us to increase the accuracy of an initial novelty classifier which is based on a bagof-word representation and kl divergence . the final result ties with the best system at trec 2004 .

making relative sense : from word-graphs to semantic frames robert porzel berenike loos vanessa micelli
scaling up from controlled single domain spoken dialogue systems towards conversational , multi-domain and multimodal dialogue systems poses new challenges for the reliable processing of less restricted user utterances . in this paper we explore the feasibility to employ a general purpose ontology for various tasks involved in processing the users utterances .

a model for fine-grained alignment of multilingual texts
while alignment of texts on the sentential level is often seen as being too coarse , and word alignment as being too fine-grained , bi- or multilingual texts which are aligned on a level inbetween are a useful resource for many purposes . starting from a number of examples of non-literal translations , which tend to make alignment difficult , we describe an alignment model which copes with these cases by explicitly coding them . the model is based on predicateargument structures and thus covers the middle ground between sentence and word alignment . the model is currently used in a recently initiated project of a parallel english-german treebank ( fuse ) , which can in principle be extended with additional languages .

exact inference for generative probabilistic non-projective dependency parsing
we describe a generative model for nonprojective dependency parsing based on a simplified version of a transition system that has recently appeared in the literature . we then develop a dynamic programming parsing algorithm for our model , and derive an insideoutside algorithm that can be used for unsupervised learning of non-projective dependency trees .

paraphrasing for automatic evaluation
this paper studies the impact of paraphrases on the accuracy of automatic evaluation . given a reference sentence and a machine-generated sentence , we seek to find a paraphrase of the reference sentence that is closer in wording to the machine output than the original reference . we apply our paraphrasing method in the context of machine translation evaluation . our experiments show that the use of a paraphrased synthetic reference refines the accuracy of automatic evaluation . we also found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation .

long-distance dependency resolution in automatically acquired wide-coverage pcfg-based lfg approximations
this paper shows how finite approximations of long distance dependency ( ldd ) resolution can be obtained automatically for wide-coverage , robust , probabilistic lexical-functional grammar ( lfg ) resources acquired from treebanks . we extract lfg subcategorisation frames and paths linking ldd reentrancies from f-structures generated automatically for the penn-ii treebank trees and use them in an ldd resolution algorithm to parse new text . unlike ( collins , 1999 ; johnson , 2002 ) , in our approach resolution of ldds is done at f-structure ( attribute-value structure representations of basic predicate-argument or dependency structure ) without empty productions , traces and coindexation in cfg parse trees . currently our best automatically induced grammars achieve 80.97 % f-score for fstructures parsing section 23 of the wsj part of the penn-ii treebank and evaluating against the dcu 1051 and 80.24 % against the parc 700 dependency bank ( king et al , 2003 ) , performing at the same or a slightly better level than state-of-the-art hand-crafted grammars ( kaplan et al , 2004 ) .

text generation from keywords
we describe a method for generating sentences from keywords or headwords . this method consists of two main parts , candidate-text construction and evaluation . the construction part generates text sentences in the form of dependency trees by using complementary information to replace information that is missing because of a knowledge gap and other missing function words to generate natural text sentences based on a particular monolingual corpus . the evaluation part consists of a model for generating an appropriate text when given keywords . this model considers not only word n-gram information , but also dependency information between words . furthermore , it considers both string information and morphological information .

language independent connectivity strength features for phrase pivot statistical machine translation
an important challenge to statistical machine translation ( smt ) is the lack of parallel data for many language pairs . one common solution is to pivot through a third language for which there exist parallel corpora with the source and target languages . although pivoting is a robust technique , it introduces some low quality translations . in this paper , we present two language-independent features to improve the quality of phrase-pivot based smt . the features , source connectivity strength and target connectivity strength reflect the quality of projected alignments between the source and target phrases in the pivot phrase table . we show positive results ( 0.6 bleu points ) on persian-arabic smt as a case study .

prosody-based topic segmentation for mandarin broadcast news
automatic topic segmentation , separation of a discourse stream into its constituent stories or topics , is a necessary preprocessing step for applications such as information retrieval , anaphora resolution , and summarization . while significant progress has been made in this area for text sources and for english audio sources , little work has been done in automatic , acoustic feature-based segmentation of other languages . in this paper , we focus on prosody-based topic segmentation of mandarin chinese . as a tone language , mandarin presents special challenges for applicability of intonation-based techniques , since the pitch contour is also used to establish lexical identity . we demonstrate that intonational cues such as reduction in pitch and intensity at topic boundaries and increase in duration and pause still provide significant contrasts in mandarin chinese . we also build a decision tree classifier that , based only on word and local context prosodic information without reference to term similarity , cue phrase , or sentence-level information , achieves boundary classification accuracy of 89-95.8 % on a large standard test set .

fast and robust joint models for biomedical event extraction sebastian riedel andrew mccallum
extracting biomedical events from literature has attracted much recent attention . the bestperforming systems so far have been pipelines of simple subtask-specific local classifiers . a natural drawback of such approaches are cascading errors introduced in early stages of the pipeline . we present three joint models of increasing complexity designed to overcome this problem . the first model performs joint trigger and argument extraction , and lends itself to a simple , efficient and exact inference algorithm . the second model captures correlations between events , while the third model ensures consistency between arguments of the same event . inference in these models is kept tractable through dual decomposition . the first two models outperform the previous best joint approaches and are very competitive with respect to the current state-of-theart . the third model yields the best results reported so far on the bionlp 2009 shared task , the bionlp 2011 genia task and the bionlp 2011 infectious diseases task .

the hiero machine translation system :
hierarchical organization is a well known property of language , and yet the notion of hierarchical structure has been largely absent from the best performing machine translation systems in recent community-wide evaluations . in this paper , we discuss a new hierarchical phrase-based statistical machine translation system ( chiang , 2005 ) , presenting recent extensions to the original proposal , new evaluation results in a community-wide evaluation , and a novel technique for fine-grained comparative analysis of mt systems .

dutch word sense disambiguation : optimizing the localness of context
we describe a new version of the dutch word sense disambiguation system trained and tested on a corrected version of the senseval-2 data . the system is an ensemble of word experts ; each word expert is a memory-based classifier of which the parameters are automatically determined through cross-validation on training material . the original best-performing system , which used only local context features for disambiguation , is further refined by performing additional parallel crossvalidation experiments for optimizing algorithmic parameters and the amount of local context available to each of the word experts memory-based kernels . this procedure produces an accuracy of 84.8 % on test material , improving on a baseline score of 77.2 % and the previous senseval-2 score of 84.2 % . we show that cross-validation overfits ; had the local context been held constant at two left and right neighbouring words , the system would have scored 85.0 % .

a stochastic finite-state morphological parser for turkish hasim sak & tunga gung
this paper presents the first stochastic finite-state morphological parser for turkish . the non-probabilistic parser is a standard finite-state transducer implementation of two-level morphology formalism . a disambiguated text corpus of 200 million words is used to stochastize the morphotactics transducer , then it is composed with the morphophonemics transducer to get a stochastic morphological parser . we present two applications to evaluate the effectiveness of the stochastic parser ; spelling correction and morphology-based language modeling for speech recognition .

the utility of manual and automatic linguistic error codes for identifying neurodevelopmental disorders eric morley , brian roark and jan van santen
we investigate the utility of linguistic features for automatically differentiating between children with varying combinations of two potentially comorbid neurodevelopmental disorders : autism spectrum disorder and specific language impairment . we find that certain manual codes for linguistic errors are useful for distinguishing between diagnostic groups . we investigate the relationship between coding detail and diagnostic classification performance , and find that a simple coding scheme is of high diagnostic utility . we propose a simple method to automate the pared down coding scheme , and find that these automatic codes are of diagnostic utility .

active dual supervision : reducing the cost of annotating examples and features
when faced with the task of building machine learning or nlp models , it is often worthwhile to turn to active learning to obtain human annotations at minimal costs . traditional active learning schemes query a human for labels of intelligently chosen examples . however , human effort can also be expended in collecting alternative forms of annotations . for example , one may attempt to learn a text classifier by labeling class-indicating words , instead of , or in addition to , documents . learning from two different kinds of supervision brings a new , unexplored dimension to the problem of active learning . in this paper , we demonstrate the value of such active dual supervision in the context of sentiment analysis . we show how interleaving queries for both documents and words significantly reduces human effort more than what is possible through traditional one-dimensional active learning , or by passive combinations of supervisory inputs .

a sentiment-aligned topic model for product aspect rating prediction
aspect-based opinion mining has attracted lots of attention today . in this paper , we address the problem of product aspect rating prediction , where we would like to extract the product aspects , and predict aspect ratings simultaneously . topic models have been widely adapted to jointly model aspects and sentiments , but existing models may not do the prediction task well due to their weakness in sentiment extraction . the sentiment topics usually do not have clear correspondence to commonly used ratings , and the model may fail to extract certain kinds of sentiments due to skewed data . to tackle this problem , we propose a sentiment-aligned topic model ( satm ) , where we incorporate two types of external knowledge : productlevel overall rating distribution and wordlevel sentiment lexicon . experiments on real dataset demonstrate that satm is effective on product aspect rating prediction , and it achieves better performance compared to the existing approaches .

a generative constituent-context model for improved grammar induction
we present a generative distributional model for the unsupervised induction of natural language syntax which explicitly models constituent yields and contexts . parameter search with em produces higher quality analyses than previously exhibited by unsupervised systems , giving the best published unsupervised parsing results on the atis corpus . experiments on penn treebank sentences of comparable length show an even higher f1 of 71 % on nontrivial brackets . we compare distributionally induced and actual part-of-speech tags as input data , and examine extensions to the basic model . we discuss errors made by the system , compare the system to previous models , and discuss upper bounds , lower bounds , and stability for this task .

position specific posterior lattices for indexing speech
the paper presents the position specific posterior lattice , a novel representation of automatic speech recognition lattices that naturally lends itself to efficient indexing of position information and subsequent relevance ranking of spoken documents using proximity . in experiments performed on a collection of lecture recordings mit icampus data the spoken document ranking accuracy was improved by 20 % relative over the commonly used baseline of indexing the 1-best output from an automatic speech recognizer . the mean average precision ( map ) increased from 0.53 when using 1-best output to 0.62 when using the new lattice representation . the reference used for evaluation is the output of a standard retrieval engine working on the manual transcription of the speech collection . albeit lossy , the pspl lattice is also much more compact than the asr 3-gram lattice from which it is computed which translates in reduced inverted index size as well at virtually no degradation in word-error-rate performance . since new paths are introduced in the lattice , the oracle accuracy increases over the original asr lattice .

sometimes average is best : the importance of averaging for prediction using mcmc inference in topic modeling
markov chain monte carlo ( mcmc ) approximates the posterior distribution of latent variable models by generating many samples and averaging over them . in practice , however , it is often more convenient to cut corners , using only a single sample or following a suboptimal averaging strategy . we systematically study different strategies for averaging mcmc samples and show empirically that averaging properly leads to significant improvements in prediction .

constituent parsing with incremental sigmoid belief networks
we introduce a framework for syntactic parsing with latent variables based on a form of dynamic sigmoid belief networks called incremental sigmoid belief networks . we demonstrate that a previous feed-forward neural network parsing model can be viewed as a coarse approximation to inference with this class of graphical model . by constructing a more accurate but still tractable approximation , we significantly improve parsing accuracy , suggesting that isbns provide a good idealization for parsing . this generative model of parsing achieves state-of-theart results on wsj text and 8 % error reduction over the baseline neural network parser .

ubbnbc wsd system description
the nave bayes classification proves to be a good performing tool in word sense disambiguation , although it has not yet been applied to the romanian language . the aim of this paper is to present our wsd system , based on the nbc algorithm , that performed quite well in senseval 3 .

an n-gram frequency database reference to handle mwe extraction in nlp
the identification and extraction of multiword expressions ( mwes ) currently deliver satisfactory results . however , the integration of these results into a wider application remains an issue . this is mainly due to the fact that the association measures ( ams ) used to detect mwes require a critical amount of data and that the mwe dictionaries can not account for all the lexical and syntactic variations inherent in mwes . in this study , we use an alternative technique to overcome these limitations . it consists in defining an n-gram frequency database that can be used to compute ams on-thefly , allowing the extraction procedure to efficiently process all the mwes in a text , even if they have not been previously observed .

identifying intention posts in discussion forums
this paper proposes to study the problem of identifying intention posts in online discussion forums . for example , in a discussion forum , a user wrote i plan to buy a camera , which indicates a buying intention . this intention can be easily exploited by advertisers . to the best of our knowledge , there is still no reported study of this problem . our research found that this problem is particularly suited to transfer learning because in different domains , people express the same intention in similar ways . we then propose a new transfer learning method which , unlike a general transfer learning algorithm , exploits several special characteristics of the problem . experimental results show that the proposed method outperforms several strong baselines , including supervised learning in the target domain and a recent transfer learning method .

sentimerge : combining sentiment lexicons in a bayesian framework
many approaches to sentiment analysis rely on a lexicon that labels words with a prior polarity . this is particularly true for languages other than english , where labelled training data is not easily available . existing efforts to produce such lexicons exist , and to avoid duplicated effort , a principled way to combine multiple resources is required . in this paper , we introduce a bayesian probabilistic model , which can simultaneously combine polarity scores from several data sources and estimate the quality of each source . we apply this algorithm to a set of four german sentiment lexicons , to produce the sentimerge lexicon , which we make publically available . in a simple classification task , we show that this lexicon outperforms each of the underlying resources , as well as a majority vote model .

automatically constructing a corpus of sentential paraphrases
an obstacle to research in automatic paraphrase identification and generation is the lack of large-scale , publiclyavailable labeled corpora of sentential paraphrases . this paper describes the creation of the recently-released microsoft research paraphrase corpus , which contains 5801 sentence pairs , each hand-labeled with a binary judgment as to whether the pair constitutes a paraphrase . the corpus was created using heuristic extraction techniques in conjunction with an svm-based classifier to select likely sentence-level paraphrases from a large corpus of topicclustered news data . these pairs were then submitted to human judges , who confirmed that 67 % were in fact semantically equivalent . in addition to describing the corpus itself , we explore a number of issues that arose in defining guidelines for the human raters .

applying the tarsqi toolkit to augment text mining of ehrs
we present a preliminary attempt to apply the tarsqi toolkit to the medical domain , specifically electronic health records , for use in answering temporally motivated questions .

noun-noun compound machine translation : a feasibility study on shallow processing
the translation of compound nouns is a major issue in machine translation due to their frequency of occurrence and high productivity . various shallow methods have been proposed to translate compound nouns , notable amongst which are memory-based machine translation and word-to-word compositional machine translation . this paper describes the results of a feasibility study on the ability of these methods to translate japanese and english noun-noun compounds .

effective utterance classification with unsupervised phonotactic models
this paper describes a method for utterance classification that does not require manual transcription of training data . the method combines domain independent acoustic models with off-the-shelf classifiers to give utterance classification performance that is surprisingly close to what can be achieved using conventional word-trigram recognition requiring manual transcription . in our method , unsupervised training is first used to train a phone n-gram model for a particular domain ; the output of recognition with this model is then passed to a phone-string classifier . the classification accuracy of the method is evaluated on three different spoken language system domains .

a minimum message length approach for argument interpretation
we describe a mechanism which receives as input a segmented argument composed of nl sentences , and generates an interpretation . our mechanism relies on the minimum message length principle for the selection of an interpretation among candidate options . this enables our mechanism to cope with noisy input in terms of wording , beliefs and argument structure ; and reduces its reliance on a particular knowledge representation . the performance of our system was evaluated by distorting automatically generated arguments , and passing them to the system for interpretation . in 75 % of the cases , the interpretations produced by the system matched precisely or almost-precisely the representation of the original arguments .

assessing dialog system user simulation evaluation measures using
previous studies evaluate simulated dialog corpora using evaluation measures which can be automatically extracted from the dialog systems logs . however , the validity of these automatic measures has not been fully proven . in this study , we first recruit human judges to assess the quality of three simulated dialog corpora and then use human judgments as the gold standard to validate the conclusions drawn from the automatic measures . we observe that it is hard for the human judges to reach good agreement when asked to rate the quality of the dialogs from given perspectives . however , the human ratings give consistent ranking of the quality of simulated corpora generated by different simulation models . when building prediction models of human judgments using previously proposed automatic measures , we find that we can not reliably predict human ratings using a regression model , but we can predict human rankings by a ranking model .

analysis and development of urdu pos tagged corpus
in this paper , two corpora of urdu ( with 110k and 120k words ) tagged with different pos tagsets are used to train tnt and tree taggers . error analysis of both taggers is done to identify frequent confusions in tagging . based on the analysis of tagging , and syntactic structure of urdu , a more refined tagset is derived . the existing tagged corpora are tagged with the new tagset to develop a single corpus of 230k words and the tnt tagger is retrained . the results show improvement in tagging accuracy for individual corpora to 94.2 % and also for the merged corpus to 91 % . implications of these results are discussed .

the fingerprint of human referring expressions and their surface realization with graph transducers
the algorithm is-fp takes up the idea from the is-fbn algorithm developed for the shared task 2007. both algorithms learn the individual attribute selection style for each human that provided referring expressions to the corpus . the is-fp algorithm was developed with two additional goals ( 1 ) to improve the indentification time that was poor for the fbn algorithm and ( 2 ) to push the dice score even higher . in order to generate a word string for the selected attributes , we build based on individual preferences a surface syntactic dependency tree as input . we derive the individual preferences from the training set . finally , a graph transducer maps the input strucutre to a deep morphologic structure .

extracting the unextractable : a case study on verb-particles
this paper proposes a series of techniques for extracting english verbparticle constructions from raw text corpora . we initially propose three basic methods , based on tagger output , chunker output and a chunk grammar , respectively , with the chunk grammar method optionally combining with an attachment resolution module to determine the syntactic structure of verbpreposition pairs in ambiguous constructs . we then combine the three methods together into a single classifier , and add in a number of extra lexical and frequentistic features , producing a final f-score of 0.865 over the wsj .

character-level dependencies in chinese : usefulness and learning
we investigate the possibility of exploiting character-based dependency for chinese information processing . as chinese text is made up of character sequences rather than word sequences , word in chinese is not so natural a concept as in english , nor is word easy to be defined without argument for such a language . therefore we propose a character-level dependency scheme to represent primary linguistic relationships within a chinese sentence . the usefulness of character dependencies are verified through two specialized dependency parsing tasks . the first is to handle trivial character dependencies that are equally transformed from traditional word boundaries . the second furthermore considers the case that annotated internal character dependencies inside a word are involved . both of these results from character-level dependency parsing are positive . this study provides an alternative way to formularize basic characterand word-level representation for chinese .

incremental parsing , or incremental grammar
standard grammar formalisms are defined without reflection of the incremental and serial nature of language processing , and incrementality must therefore be reflected by independently defined parsing and/or generation techniques . we argue that this leads to a poor setup for modelling dialogue , with its rich speaker-hearer interaction , and instead propose context-based parsing and generation models defined in terms of an inherently incremental grammar formalism ( dynamic syntax ) , which allow a straightforward model of otherwise problematic dialogue phenomena such as shared utterances , ellipsis and alignment .

discriminative methods for transliteration
we present two discriminative methods for name transliteration . the methods correspond to local and global modeling approaches in modeling structured output spaces . both methods do not require alignment of names in different languages their features are computed directly from the names themselves . we perform an experimental evaluation of the methods for name transliteration from three languages ( arabic , korean , and russian ) into english , and compare the methods experimentally to a state-of-theart joint probabilistic modeling approach . we find that the discriminative methods outperform probabilistic modeling , with the global discriminative modeling approach achieving the best performance in all languages .

multilingual document clustering : an heuristic approach based on cognate named entities
this paper presents an approach for multilingual document clustering in comparable corpora . the algorithm is of heuristic nature and it uses as unique evidence for clustering the identification of cognate named entities between both sides of the comparable corpora . one of the main advantages of this approach is that it does not depend on bilingual or multilingual resources . however , it depends on the possibility of identifying cognate named entities between the languages used in the corpus . an additional advantage of the approach is that it does not need any information about the right number of clusters ; the algorithm calculates it . we have tested this approach with a comparable corpus of news written in english and spanish . in addition , we have compared the results with a system which translates selected document features . the obtained results are encouraging .

enforcing transitivity in coreference resolution
a desirable quality of a coreference resolution system is the ability to handle transitivity constraints , such that even if it places high likelihood on a particular mention being coreferent with each of two other mentions , it will also consider the likelihood of those two mentions being coreferent when making a final assignment . this is exactly the kind of constraint that integer linear programming ( ilp ) is ideal for , but , surprisingly , previous work applying ilp to coreference resolution has not encoded this type of constraint . we train a coreference classifier over pairs of mentions , and show how to encode this type of constraint on top of the probabilities output from our pairwise classifier to extract the most probable legal entity assignments . we present results on two commonly used datasets which show that enforcement of transitive closure consistently improves performance , including improvements of up to 3.6 % using the b3 scorer , and up to 16.5 % using cluster f-measure .

using first and second language models to correct preposition errors in second language authoring matthieu hermet alain dsilets
in this paper , we investigate a novel approach to correcting grammatical and lexical errors in texts written by second language authors . contrary to previous approaches which tend to use unilingual models of the user 's second language ( l2 ) , this new approach uses a simple roundtrip machine translation method which leverages information about both the authors first ( l1 ) and second languages . we compare the repair rate of this roundtrip translation approach to that of an existing approach based on a unilingual l2 model with shallow syntactic pruning , on a series of preposition choice errors . we find no statistically significant difference between the two approaches , but find that a hybrid combination of both does perform significantly better than either one in isolation . finally , we illustrate how the translation approach has the potential of repairing very complex errors which would be hard to treat without leveraging knowledge of the author 's l1 .

using similarity scoring to improve the bilingual dictionary for word
we describe an approach to improve the bilingual cooccurrence dictionary that is used for word alignment , and evaluate the improved dictionary using a version of the competitive linking algorithm . we demonstrate a problem faced by the competitive linking algorithm and present an approach to ameliorate it . in particular , we rebuild the bilingual dictionary by clustering similar words in a language and assigning them a higher cooccurrence score with a given word in the other language than each single word would have otherwise . experimental results show a significant improvement in precision and recall for word alignment when the improved dicitonary is used .

syntactic identification of occurrences of multiword expressions in text using a lexicon with dependency structures
we deal with syntactic identification of occurrences of multiword expression ( mwe ) from an existing dictionary in a text corpus . the mwes we identify can be of arbitrary length and can be interrupted in the surface sentence . we analyse and compare three approaches based on linguistic analysis at a varying level , ranging from surface word order to deep syntax . the evaluation is conducted using two corpora : the prague dependency treebank and czech national corpus . we use the dictionary of multiword expressions semlex , that was compiled by annotating the prague dependency treebank and includes deep syntactic dependency trees of all mwes .

combining morphosyntactic enriched representation with n-best reranking in statistical translation
the purpose of this work is to explore the integration of morphosyntactic information into the translation model itself , by enriching words with their morphosyntactic categories . we investigate word disambiguation using morphosyntactic categories , n-best hypotheses reranking , and the combination of both methods with word or morphosyntactic n-gram language model reranking . experiments are carried out on the english-to-spanish translation task . using the morphosyntactic language model alone does not results in any improvement in performance . however , combining morphosyntactic word disambiguation with a word based 4-gram language model results in a relative improvement in the bleu score of 2.3 % on the development set and 1.9 % on the test set .

a novel distributional approach to multilingual conceptual metaphor
we present a novel approach to the problem of multilingual conceptual metaphor recognition . our approach extends recent work in conceptual metaphor discovery by combining a complex methodology for facet-based concept induction with a distributional vector space model of linguistic and conceptual metaphor . in the evaluation of our system in english , spanish , russian , and farsi , we experiment with several state-of-the-art vector space models and demonstrate a clear benefit to the fine-grained concept representation that forms the basis of our methodology for conceptual metaphor recognition .

part of speech tagger for assamese text
assamese is a morphologically rich , agglutinative and relatively free word order indic language . although spoken by nearly 30 million people , very little computational linguistic work has been done for this language . in this paper , we present our work on part of speech ( pos ) tagging for assamese using the well-known hidden markov model . since no well-defined suitable tagset was available , we develop a tagset of 172 tags in consultation with experts in linguistics . for successful tagging , we examine relevant linguistic issues in assamese . for unknown words , we perform simple morphological analysis to determine probable tags . using a manually tagged corpus of about 10000 words for training , we obtain a tagging accuracy of nearly 87 % for test inputs .

chinese classifier assignment using svms
in chinese , nouns need numeral classifiers to express quantity . in this paper , we explore the relationship between classifiers and nouns . we extract a set of lexical , syntactic and ontological features and the corresponding noun-classifier pairs from a corpus and then train svms to assign classifers to nouns . we analyse which features are most important for this task .

a web-based demonstrator of a multi-lingual phrase-based itc-irst - centro per la ricerca scientifica e tecnologica
this paper describes a multi-lingual phrase-based statistical machine translation system accessible by means of a web page . the user can issue translation requests from arabic , chinese or spanish into english . the same phrase-based statistical technology is employed to realize the three supported language-pairs . new language-pairs can be easily added to the demonstrator . the web-based interface allows the use of the translation system to any computer connected to the internet .

multilingual editing of linguistic problems
multilinguality has been an essential feature of the international linguistic olympiad since its conception . although deemed most desirable , the production of a problem set in several parallel versions and the verification of their equivalence is a time-consuming and errorprone task . this paper tells about the efforts to develop tools and methods which increase its efficiency and reliability .

automatic identification of infrequent word sense
in this paper we show that an unsupervised method for ranking word senses automatically can be used to identify infrequently occurring senses . we demonstrate this using a ranking of noun senses derived from the bnc and evaluating on the sense-tagged text available in both semcor and the senseval-2 english all-words task . we show that the method does well at identifying senses that do not occur in a corpus , and that those that are erroneously filtered but do occur typically have a lower frequency than the other senses . this method should be useful for word sense disambiguation systems , allowing effort to be concentrated on more frequent senses ; it may also be useful for other tasks such as lexical acquisition . whilst the results on balanced corpora are promising , our chief motivation for the method is for application to domain specific text . for text within a particular domain many senses from a generic inventory will be rare , and possibly redundant . since a large domain specific corpus of sense annotated data is not available , we evaluate our method on domain-specific corpora and demonstrate that sense types identified for removal are predominantly senses from outside the domain .

breaking bad : extraction of verb-particle constructions from a parallel subtitles corpus
the automatic extraction of verb-particle constructions ( vpcs ) is of particular interest to the nlp community . previous studies have shown that word alignment methods can be used with parallel corpora to successfully extract a range of multi-word expressions ( mwes ) . in this paper the technique is applied to a new type of corpus , made up of a collection of subtitles of movies and television series , which is parallel in english and spanish . building on previous research , it is shown that a precision level of 94 4.7 % can be achieved in english vpc extraction . this high level of precision is achieved despite the difficulties of aligning and tagging subtitles data . moreover , many of the extracted vpcs are not present in online lexical resources , highlighting the benefits of using this unique corpus type , which contains a large number of slang and other informal expressions . an added benefit of using the word alignment process is that translations are also automatically extracted for each vpc . a precision rate of 758.5 % is found for the translations of english vpcs into spanish . this study thus shows that vpcs are a particularly good subset of the mwe spectrum to attack using word alignment methods , and that subtitles data provide a range of interesting expressions that do not exist in other corpus types .

tagging urdu text with parts of speech : a tagger comparison
in this paper , four state-of-art probabilistic taggers i.e . tnt tagger , treetagger , rf tagger and svm tool , are applied to the urdu language . for the purpose of the experiment , a syntactic tagset is proposed . a training corpus of 100,000 tokens is used to train the models . using the lexicon extracted from the training corpus , svm tool shows the best accuracy of 94.15 % . after providing a separate lexicon of 70,568 types , svm tool again shows the best accuracy of 95.66 % .

person name disambiguation based on topic model
in this paper we describe our participation in the sighan 2010 task3 ( person name disambiguation ) and detail our approaches . person name disambiguation is typically viewed as an unsupervised clustering problem where the aim is to partition a names contexts into different clusters , each representing a real world people . the key point of clustering is the similarity measure of context , which depends upon the features selection and representation . two clustering algorithms , hac and dbscan , are investigated in our system . the experiments show that the topic features learned by lda outperforms token features and more robust .

representing verbal semantics with diagrams an adaptation of the uml for lexical semantics
the paper presents a new way of accounting for the meaning of verbs in natural languages , using a diagrammatic notation based on the unified modeling language ( uml ) . we will introduce the new framework by outlining some modeling elements and indicating major differences to the uml . an extended example will be discussed in more detail . we will then focus on the cognitive background of the framework , and in particular address the question why the usage of graphical elements within a linguistic modeling language proves to be very fruitful . finally , we will briefly indicate the potential of the new framework and its applicability .

hippocratic abbreviation expansion
incorrect normalization of text can be particularly damaging for applications like text-to-speech synthesis ( tts ) or typing auto-correction , where the resulting normalization is directly presented to the user , versus feeding downstream applications . in this paper , we focus on abbreviation expansion for tts , which requires a do no harm , high precision approach yielding few expansion errors at the cost of leaving relatively many abbreviations unexpanded . in the context of a largescale , real-world tts scenario , we present methods for training classifiers to establish whether a particular expansion is apt . we achieve a large increase in correct abbreviation expansion when combined with the baseline text normalization component of the tts system , together with a substantial reduction in incorrect expansions .

npmi driven recognition of nested terms
in the paper , we propose a new method of identifying terms nested within candidates for the terms extracted from domain texts . the list of all terms is then ranked by the process of automatic term recognition . our method of identifying nested terms is based on two aspects : grammatical correctness and normalised pointwise mutual information ( npmi ) counted for all bigrams on the basis of a corpus . npmi is typically used for recognition of strong word connections but in our solution we use it to recognise the weakest points within phrases to suggest the best place for division of a phrase into two parts . by creating only two nested phrases in each step we introduce a binary hierarchical term structure . in the paper , we test the impact of the proposed nested terms recognition method applied together with the c-value ranking method to the automatic term recognition task .

japanese zero pronoun resolution
anaphora resolution is one of the most important research topics in natural language processing . in english , overt pronouns such as she and definite noun phrases such as the company are anaphors that refer to preceding entities ( antecedents ) . in japanese , anaphors are often omitted , and these omissions are called zero pronouns . there are two major approaches to zero pronoun resolution : the heuristic approach and the machine learning approach . since we have to take various factors into consideration , it is difficult to find a good combination of heuristic rules . therefore , the machine learning approach is attractive , but it requires a large amount of training data . in this paper , we propose a method that combines ranking rules and machine learning . the ranking rules are simple and effective , while machine learning can take more factors into account . from the results of our experiments , this combination gives better performance than either of the two previous approaches .

concreteness and subjectivity as dimensions of lexical meaning
we quantify the lexical subjectivity of adjectives using a corpus-based method , and show for the first time that it correlates with noun concreteness in large corpora . these cognitive dimensions together influence how word meanings combine , and we exploit this fact to achieve performance improvements on the semantic classification of adjective-noun pairs .

addressing the resource bottleneck to create large-scale annotated texts
large-scale linguistically annotated resources have become available in recent years . this is partly due to sophisticated automatic and semiautomatic approaches that work well on specific tasks such as part-ofspeech tagging . for more complex linguistic phenomena like anaphora resolution there are no tools that result in high-quality annotations without massive user intervention . annotated corpora of the size needed for modern computational linguistics research can not however be created by small groups of hand annotators . the anawiki project strikes a balance between collecting high-quality annotations from experts and applying a game-like approach to collecting linguistic annotation from the general web population . more generally , anawiki is a project that explores to what extend expert annotations can be substituted by a critical mass of non-expert judgements . 375 376 chamberlain , poesio , and kruschwitz

an empirical examination of challenges in chinese parsing
aspects of chinese syntax result in a distinctive mix of parsing challenges . however , the contribution of individual sources of error to overall difficulty is not well understood . we conduct a comprehensive automatic analysis of error types made by chinese parsers , covering a broad range of error types for large sets of sentences , enabling the first empirical ranking of chinese error types by their performance impact . we also investigate which error types are resolved by using gold part-of-speech tags , showing that improving chinese tagging only addresses certain error types , leaving substantial outstanding challenges .

resolving pronominal references in chinese with the hobbs algorithm
this study addresses pronominal anaphora resolution , including zero pronouns , in chinese . a syntactic , rule-based pronoun resolution algorithm , the hobbs algorithm was run on gold standard hand parses from the penn chinese treebank . while first proposed for english , the algorithm counts for its success on two characteristics that chinese and english have in common . both languages are svo , and both are fixed word order languages . no changes were made to adapt the algorithm to chinese . the accuracy of the algorithm on overt , third-person pronouns at the matrix level was 77.6 % , and the accuracy for resolving matrix-level zero pronouns was 73.3 % . in contrast , the accuracy of the algorithm on pronouns that appeared in subordinate constructions was only 43.3 % , providing support for miltsakakis two-mechanism proposal for resolving inter- vs. intra-sentential anaphors .

a re-examination of lexical association measures
we review lexical association measures ( ams ) that have been employed by past work in extracting multiword expressions . our work contributes to the understanding of these ams by categorizing them into two groups and suggesting the use of rank equivalence to group ams with the same ranking performance . we also examine how existing ams can be adapted to better rank english verb particle constructions and light verb constructions . specifically , we suggest normalizing ( pointwise ) mutual information and using marginal frequencies to construct penalization terms . we empirically validate the effectiveness of these modified ams in detection tasks in english , performed on the penn treebank , which shows significant improvement over the original ams .

annotating chinese collocations with multi information
this paper presents the design and construction of an annotated chinese collocation bank as the resource to support systematic research on chinese collocations . with the help of computational tools , the bi-gram and n-gram collocations corresponding to 3,643 headwords are manually identified . furthermore , annotations for bi-gram collocations include dependency relation , chunking relation and classification of collocation types . currently , the collocation bank annotated 23,581 bigram collocations and 2,752 n-gram collocations extracted from a 5-million-word corpus . through statistical analysis on the collocation bank , some characteristics of chinese bigram collocations are examined which is essential to collocation research , especially for chinese .

the infinite pcfg using hierarchical dirichlet processes
we present a nonparametric bayesian model of tree structures based on the hierarchical dirichlet process ( hdp ) . our hdp-pcfg model allows the complexity of the grammar to grow as more training data is available . in addition to presenting a fully bayesian model for the pcfg , we also develop an efficient variational inference procedure . on synthetic data , we recover the correct grammar without having to specify its complexity in advance . we also show that our techniques can be applied to full-scale parsing applications by demonstrating its effectiveness in learning state-split grammars .

icarus an extensible graphical search tool for dependency treebanks
we present icarus , a versatile graphical search tool to query dependency treebanks . search results can be inspected both quantitatively and qualitatively by means of frequency lists , tables , or dependency graphs . icarus also ships with plugins that enable it to interface with tool chains running either locally or remotely .

translation acquisition using synonym sets
we propose a new method for translation acquisition which uses a set of synonyms to acquire translations from comparable corpora . the motivation is that , given a certain query term , it is often possible for a user to specify one or more synonyms . using the resulting set of query terms has the advantage that we can overcome the problem that a single query terms context vector does not always reliably represent a terms meaning due to the context vectors sparsity . our proposed method uses a weighted average of the synonyms context vectors , that is derived by inferring the mean vector of the von mises-fisher distribution . we evaluate our method , using the synsets from the cross-lingually aligned japanese and english wordnet . the experiments show that our proposed method significantly improves translation accuracy when compared to a previous method for smoothing context vectors .

addressing class imbalance for improved recognition of implicit
in this paper we address the problem of skewed class distribution in implicit discourse relation recognition . we examine the performance of classifiers for both binary classification predicting if a particular relation holds or not and for multi-class prediction . we review prior work to point out that the problem has been addressed differently for the binary and multi-class problems . we demonstrate that adopting a unified approach can significantly improve the performance of multi-class prediction . we also propose an approach that makes better use of the full annotations in the training set when downsampling is used . we report significant absolute improvements in performance in multi-class prediction , as well as significant improvement of binary classifiers for detecting the presence of implicit temporal , comparison and contingency relations .

extraction of multi-word expressions from small parallel corpora
we present a general methodology for extracting multi-word expressions ( of various types ) , along with their translations , from small parallel corpora . we automatically align the parallel corpus and focus on misalignments ; these typically indicate expressions in the source language that are translated to the target in a noncompositional way . we then use a large monolingual corpus to rank and filter the results . evaluation of the quality of the extraction algorithm reveals significant improvements over nave alignment-based methods . external evaluation shows an improvement in the performance of machine translation that uses the extracted dictionary .

predicting human-targeted translation edit rate via untrained human annotators
in the field of machine translation , automatic metrics have proven quite valuable in system development for tracking progress and measuring the impact of incremental changes . however , human judgment still plays a large role in the context of evaluating mt systems . for example , the gale project uses humantargeted translation edit rate ( hter ) , wherein the mt output is scored against a post-edited version of itself ( as opposed to being scored against an existing human reference ) . this poses a problem for mt researchers , since hter is not an easy metric to calculate , and would require hiring and training human annotators to perform the editing task . in this work , we explore soliciting those edits from untrained human annotators , via the online service amazon mechanical turk . we show that the collected data allows us to predict hter-ranking of documents at a significantly higher level than the ranking obtained using automatic metrics .

an analysis of the calque phenomena based on comparable corpora
in this short paper we show how comparable corpora can be constructed in order to analyze the notion of calque . we then investigate the way comparable corpora contribute to a better linguistic analysis of the calque effect and how it can help improve error correction for non-native language productions .

unsupervised construction of a lexicon and a repository of variation patterns for arabic modal multiword expressions
we present an unsupervised approach to build a lexicon of arabic modal multiword expressions ( am-mwes ) and a repository of their variation patterns . these novel resources are likely to boost the automatic identification and extraction of am-mwes1 .

automatic discovery of adposition typology rishiraj saha roy
natural languages ( nl ) can be classified as prepositional or postpositional based on the order of the noun phrase and the adposition . categorizing a language by its adposition typology helps in addressing several challenges in linguistics and natural language processing ( nlp ) . understanding the adposition typologies for less-studied languages by manual analysis of large text corpora can be quite expensive , yet automatic discovery of the same has received very little attention till date . this research presents a simple unsupervised technique to automatically predict the adposition typology for a language . most of the function words of a language are adpositions , and we show that function words can be effectively separated from content words by leveraging differences in their distributional properties in a corpus . using this principle , we show that languages can be classified as prepositional or postpositional based on the rank correlations derived from entropies of word co-occurrence distributions . our claims are substantiated through experiments on 23 languages from ten diverse families , 19 of which are correctly classified by our technique .

using dedicom for completely unsupervised part-of-speech tagging
a standard and widespread approach to part-of-speech tagging is based on hidden markov models ( hmms ) . an alternative approach , pioneered by schtze ( 1993 ) , induces parts of speech from scratch using singular value decomposition ( svd ) . we introduce dedicom as an alternative to svd for part-of-speech induction . dedicom retains the advantages of svd in that it is completely unsupervised : no prior knowledge is required to induce either the tagset or the associations of types with tags . however , unlike svd , it is also fully compatible with the hmm framework , in that it can be used to estimate emission- and transition-probability matrices which can then be used as the input for an hmm . we apply the dedicom method to the conll corpus ( conll 2000 ) and compare the output of dedicom to the part-of-speech tags given in the corpus , and find that the correlation ( almost 0.5 ) is quite high . using dedicom , we also estimate part-ofspeech ambiguity for each type , and find that these estimates correlate highly with part-of-speech ambiguity as measured in the original corpus ( around 0.88 ) . finally , we show how the output of dedicom can be evaluated and compared against the more familiar output of supervised hmm-based tagging .

automatic temporal expression normalization with reference
temporal expressions in texts contain significant temporal information . understanding temporal information is very useful in many nlp applications , such as information extraction , documents summarization and question answering . therefore , the temporal expression normalization which is used for transforming temporal expressions to temporal information has absorbed many researchers attentions . but previous works , whatever the hand-crafted rules-based or the machine-learnt rules-based , all can not address the actual problem about temporal reference in real texts effectively . more specifically , the reference time choosing mechanism employed by these works is not adaptable to the universal implicit times in normalization . aiming at this issue , we introduce a new reference time choosing mechanism for temporal expression normalization , called reference time dynamic-choosing , which assigns the appropriate reference times to different classes of implicit temporal expressions dynamically when normalizing . and then , the solution to temporal expression defuzzification by scenario dependences among temporal expressions is discussed . finally , we evaluate the system on a substantial corpus collected by chinese news articles and obtained more promising results than compared methods .

better semantic frame based mt evaluation via inversion transduction grammars dekai wu lo chi-kiu meriem beloucif markus saers
we introduce an inversion transduction grammar based restructuring of the meant automatic semantic frame based mt evaluation metric , which , by leveraging itg language biases , is able to further improve upon meants already-high correlation with human adequacy judgments . the new metric , called imeant , uses bracketing itgs to biparse the reference and machine translations , but subject to obeying the semantic frames in both . resulting improvements support the presumption that itgs , which constrain the allowable permutations between compositional segments across the reference and mt output , score the phrasal similarity of the semantic role fillers more accurately than the simple word alignment heuristics ( bag-of-word alignment or maximum alignment ) used in previous version of meant . the approach successfully integrates ( 1 ) the previously demonstrated extremely high coverage of cross-lingual semantic frame alternations by itgs , with ( 2 ) the high accuracy of evaluating mt via weighted f-scores on the degree of semantic frame preservation .

an enhanced lesk word sense disambiguation algorithm through a distributional semantic model pierpaolo basile annalina caputo
this paper describes a new word sense disambiguation ( wsd ) algorithm which extends two well-known variations of the lesk wsd method . given a word and its context , lesk algorithm exploits the idea of maximum number of shared words ( maximum overlaps ) between the context of a word and each definition of its senses ( gloss ) in order to select the proper meaning . the main contribution of our approach relies on the use of a word similarity function defined on a distributional semantic space to compute the gloss-context overlap . as sense inventory we adopt babelnet , a large multilingual semantic network built exploiting both wordnet and wikipedia . besides linguistic knowledge , babelnet also represents encyclopedic concepts coming from wikipedia . the evaluation performed on semeval-2013 multilingual word sense disambiguation shows that our algorithm goes beyond the most frequent sense baseline and the simplified version of the lesk algorithm . moreover , when compared with the other participants in semeval-2013 task , our approach is able to outperform the best system for english .

syntactic construct : an aid for translating english nominal compound
this paper illustrates a way of using paraphrasal interpretation of english nominal compound for translating them into hindi . input nominal compound is first paraphrased automatically with the 8 prepositions as proposed by lauer ( 1995 ) for the task . english prepositions have one-to-one mapping to post-position in hindi . the english paraphrases are then translated into hindi using the mapping schema . we have got an accuracy of 71 % over a set of gold data of 250 nominal compound . the translation-strategy is motivated by the following observation : it is only 50 % of the cases that english nominal compound is translated into nominal compound in hindi . in other cases , they are translated into varied syntactic constructs . among them the most frequent construction type is modifier + postposition + head . the translation module also attempts to determine when a compound is translated using paraphrase and when it is translated into a nominal compound .

the perils and rewards of developing restricted domain applications
over the last three decades , the development of restricted domain applications has been an ongoing theme in computational linguistic research . speech recognition , machine translation , summarization , and question answering researchers have all built at one time or another restricted domain systems . in retrospect , it is long due to examine both the successes and failures of these previous attempts . in this talk , i will examine the circumstances in which the development of restricted domain applications has led to significant advances in the state of the art and the circumstances in which restricted domain research has had little impact on our field . i will use the lessons learned from previous attempts to building restricted domain natural language processing applications in order to examine the potential impact of current research in restricted domain question answering .

hybrid approach for coreference resolution
this paper describes our participation in the conll-2011 shared task for closed task . the approach used combines refined salience measure based pronominal resolution and crfs for non-pronominal resolution . in this work we also use machine learning based approach for identifying non-anaphoric pronouns .

using bilingual parallel corpora for cross-lingual textual entailment fbk - irst fbk - irst
this paper explores the use of bilingual parallel corpora as a source of lexical knowledge for cross-lingual textual entailment . we claim that , in spite of the inherent difficulties of the task , phrase tables extracted from parallel data allow to capture both lexical relations between single words , and contextual information useful for inference . we experiment with a phrasal matching method in order to : i ) build a system portable across languages , and ii ) evaluate the contribution of lexical knowledge in isolation , without interaction with other inference mechanisms . results achieved on an english-spanish corpus obtained from the rte3 dataset support our claim , with an overall accuracy above average scores reported by rte participants on monolingual data . finally , we show that using parallel corpora to extract paraphrase tables reveals their potential also in the monolingual setting , improving the results achieved with other sources of lexical knowledge .

paraphrasing japanese noun phrases using character-based indexing tokunaga takenobu tanaka hozumi
this paper proposes a novel method to extract paraphrases of japanese noun phrases from a set of documents . the proposed method consists of three steps : ( 1 ) retrieving passages using character-based index terms given a noun phrase as an input query , ( 2 ) filtering the retrieved passages with syntactic and semantic constraints , and ( 3 ) ranking the passages and reformatting them into grammatical forms . experiments were conducted to evaluate the method by using 53 noun phrases and three years worth of newspaper articles . the accuracy of the method needs to be further improved for fully automatic paraphrasing but the proposed method can extract novel paraphrases which past approaches could not .

shallow semantic parsing of chinese
in this paper we address the question of assigning semantic roles to sentences in chinese . we show that good semantic parsing results for chinese can be achieved with a small 1100-sentence training set . in order to extract features from chinese , we describe porting the collins parser to chinese , resulting in the best performance currently reported on chinese syntactic parsing ; we include our headrules in the appendix . finally , we compare english and chinese semantic-parsing performance . while slight differences in argument labeling make a perfect comparison impossible , our results nonetheless suggest significantly better performance for chinese  work of watanabe et al. , we explore the use of the mira algorithm of crammer et al as an alternative to mert . we first show that by parallel processing and exploiting more of the parse forest , we can obtain results using mira that match or surpass mert in terms of both translation quality and computational cost . we then test the method on two classes of features that address deficiencies in the hiero hierarchical phrasebased model : first , we simultaneously train a large number of marton and resniks soft syntactic constraints , and , second , we introduce a novel structural distortion model . in both cases we obtain significant improvements in translation performance . optimizing them in combination , for a total of 56 feature weights , we improve performance by 2.6 b on a subset of the nist 2006 arabic-english evaluation data .

a scaleable automated quality assurance technique for semantic computational bioscience program computational bioscience program
this paper presents an evaluation of an automated quality assurance technique for a type of semantic representation known as a predicate argument structure . these representations are crucial to the development of an important class of corpus known as a proposition bank . previous work ( cohen and hunter , 2006 ) proposed and tested an analytical technique based on a simple discovery procedure inspired by classic structural linguistic methodology . cohen and hunter applied the technique manually to a small set of representations . here we test the feasibility of automating the technique , as well as the ability of the technique to scale to a set of semantic representations and to a corpus many times larger than that used by cohen and hunter . we conclude that the technique is completely automatable , uncovers missing sense distinctions and other bad semantic representations , and does scale well , performing at an accuracy of 69 % for identifying bad representations . we also report on the implications of our findings for the correctness of the semantic representations in propbank .

a noisy-channel model of rational human sentence comprehension
language comprehension , as with all other cases of the extraction of meaningful structure from perceptual input , takes places under noisy conditions . if human language comprehension is a rational process in the sense of making use of all available information sources , then we might expect uncertainty at the level of word-level input to affect sentence-level comprehension . however , nearly all contemporary models of sentence comprehension assume clean inputthat is , that the input to the sentence-level comprehension mechanism is a perfectly-formed , completely certain sequence of input tokens ( words ) . this article presents a simple model of rational human sentence comprehension under noisy input , and uses the model to investigate some outstanding problems in the psycholinguistic literature for theories of rational human sentence comprehension . we argue that by explicitly accounting for inputlevel noise in sentence processing , our model provides solutions for these outstanding problems and broadens the scope of theories of human sentence comprehension as rational probabilistic inference .

annotating change of state for clinical events
understanding the event structure of sentences and whole documents is an important step in being able to extract meaningful information from the text . our task is the identification of phenotypes , specifically , pneumonia , from clinical narratives . in this paper , we consider the importance of identifying the change of state for events , in particular , events that measure and compare multiple states across time . change of state is important to the clinical diagnosis of pneumonia ; in the example there are bibasilar opacities that are unchanged , the presence of bibasilar opacities alone may suggest pneumonia , but not when they are unchanged , which suggests the need to modify events with change of state information . our corpus is comprised of chest xray reports , where we find many descriptions of change of state comparing the volume and density of the lungs and surrounding areas . we propose an annotation schema to capture this information as a tuple of < location , attribute , value , change-of-state , time-reference > .

a phrase-based unigram model for statistical machine translation
in this paper , we describe a phrase-based unigram model for statistical machine translation that uses a much simpler set of model parameters than similar phrase-based models . the units of translation are blocks - pairs of phrases . during decoding , we use a block unigram model and a word-based trigram language model . during training , the blocks are learned from source interval projections using an underlying word alignment . we show experimental results on block selection criteria based on unigram counts and phrase length .

types and records for predication
this paper studies the use of records and dependent types in gf ( grammatical framework ) to build a grammar for predication with an unlimited number of subcategories , also covering extraction and coordination . the grammar is implemented for chinese , english , finnish , and swedish , sharing the maximum of code to identify similarities and differences between the languages . equipped with a probabilistic model and a large lexicon , the grammar has also been tested in widecoverage machine translation . the first evaluations show improvements in parsing speed , coverage , and robustness in comparison to earlier gf grammars . the study confirms that dependent types , records , and functors are useful in both engineering and theoretical perspectives .

what prompts translators to modify draft translations an analysis of basic modification patterns for use in the automatic notification of awkwardly translated text library and information science course
in human translation , translators first make draft translations and then modify them . this paper analyses these modifications , in order to identify the features that trigger modification . our goal is to construct a system that notifies ( english-to-japanese ) volunteer translators of awkward translations . after manually classifying the basic modification patterns , we analysed the factors that trigger a change in verb voice from passive to active using svm . an experimental result shows good prospects for the automatic identification of candidates for modification .

hierarchical mt training using max-violation perceptron
large-scale discriminative training has become promising for statistical machine translation by leveraging the huge training corpus ; for example the recent effort in phrase-based mt ( yu et al , 2013 ) significantly outperforms mainstream methods that only train on small tuning sets . however , phrase-based mt suffers from limited reorderings , and thus its training can only utilize a small portion of the bitext due to the distortion limit . to address this problem , we extend yu et al ( 2013 ) to syntax-based mt by generalizing their latent variable violation-fixing perceptron from graphs to hypergraphs . experiments confirm that our method leads to up to +1.2 bleu improvement over mainstream methods such as mert and pro .

interpreting communicative goals in constrained domains
this article presents an approach to interpret the content of documents in constrained domains at the level of communicative goals . the kind of knowledge used contains descriptions of wellformed document contents and texts that can be produced from them . the automatic analysis of text content is followed by an interactive negotiation phase involving an expert of the class of documents . motivating reasons are given for an application of this approach , document normalization , and an implemented system is briefly introduced.1

graph-based clustering for computational linguistics : a survey
in this survey we overview graph-based clustering and its applications in computational linguistics . we summarize graph-based clustering as a five-part story : hypothesis , modeling , measure , algorithm and evaluation . we then survey three typical nlp problems in which graph-based clustering approaches have been successfully applied . finally , we comment on the strengths and weaknesses of graph-based clustering and envision that graph-based clustering is a promising solution for some emerging nlp problems .

toward finer-grained sentiment identification in product reviews
we propose categories of finer-grained polarity for a more effective aspect-based sentiment summary , and describe linguistic and ontological clues that may affect such fine-grained polarity . we argue that relevance for satisfaction , contrastive weight clues , and certain adverbials work to affect the polarity , as evidenced by the statistical analysis .

focused web corpus crawling
in web corpus construction , crawling is a necessary step , and it is probably the most costly of all , because it requires expensive bandwidth usage , and excess crawling increases storage requirements . excess crawling results from the fact that the web contains a lot of redundant content ( duplicates and near-duplicates ) , as well as other material not suitable or desirable for inclusion in web corpora or web indexes ( for example , pages with little text or virtually no text at all ) . an optimized crawler for web corpus construction would ideally avoid crawling such content in the first place , saving bandwidth , storage , and post-processing costs . in this paper , we show in three experiments that two simple scores are suitable to improve the ratio between corpus size and crawling effort for web corpus construction . the first score is related to overall text quality of the page containing the link , the other one is related to the likelihood that the local block enclosing a link is boilerplate .

opal : applying opinion mining techniques for the disambiguation of
the task of extracting the opinion expressed in text is challenging due to different reasons . one of them is that the same word ( in particular , adjectives ) can have different polarities depending on the context . this paper presents the experiments carried out by the opal team for the participation in the semeval 2010 task 18 disambiguation of sentiment ambiguous adjectives . our approach is based on three different strategies : a ) the evaluation of the polarity of the whole context using an opinion mining system ; b ) the assessment of the polarity of the local context , given by the combinations between the closest nouns and the adjective to be classified ; c ) rules aiming at refining the local semantics through the spotting of modifiers . the final decision for classification is taken according to the output of the majority of these three approaches . the method used yielded good results , the opal system run ranking fifth among 16 in micro accuracy and sixth in macro accuracy .

chasing hypernyms in vector spaces with entropy
in this paper , we introduce slqs , a new entropy-based measure for the unsupervised identification of hypernymy and its directionality in distributional semantic models ( dsms ) . slqs is assessed through two tasks : ( i . ) identifying the hypernym in hyponym-hypernym pairs , and ( ii . ) discriminating hypernymy among various semantic relations . in both tasks , slqs outperforms other state-of-the-art measures .

proposal for an extension of traditional named entities : from guidelines to evaluation , an overview
within the framework of the construction of a fact database , we defined guidelines to extract named entities , using a taxonomy based on an extension of the usual named entities definition . we thus defined new types of entities with broader coverage including substantivebased expressions . these extended named entities are hierarchical ( with types and components ) and compositional ( with recursive type inclusion and metonymy annotation ) . human annotators used these guidelines to annotate a 1.3m word broadcast news corpus in french . this article presents the definition and novelty of extended named entity annotation guidelines , the human annotation of a global corpus and of a mini reference corpus , and the evaluation of annotations through the computation of inter-annotator agreements . finally , we discuss our approach and the computed results , and outline further work .

assessment of utility in web mining for the domain of public health
this paper presents ongoing work on application of information extraction ( ie ) technology to domain of public health , in a real-world scenario . a central issue in ie is the quality of the results . we present two novel points . first , we distinguish the criteria for quality : the objective criteria that measure correctness of the systems analysis in traditional terms ( f-measure , recall and precision ) , and , on the other hand , subjective criteria that measure the utility of the results to the end-user . second , to obtain measures of utility , we build an environment that allows users to interact with the system by rating the analyzed content . we then build and compare several classifiers that learn from the users responses to predict the relevance scores for new events . we conduct experiments with learning to predict relevance , and discuss the results and their implications for text mining in the domain of public health .

confidence measure for word alignment
in this paper we present a confidence measure for word alignment based on the posterior probability of alignment links . we introduce sentence alignment confidence measure and alignment link confidence measure . based on these measures , we improve the alignment quality by selecting high confidence sentence alignments and alignment links from multiple word alignments of the same sentence pair . additionally , we remove low confidence alignment links from the word alignment of a bilingual training corpus , which increases the alignment f-score , improves chinese-english and arabic-english translation quality and significantly reduces the phrase translation table size .

unsupervised approaches for automatic keyword extraction using
this paper explores several unsupervised approaches to automatic keyword extraction using meeting transcripts . in the tfidf ( term frequency , inverse document frequency ) weighting framework , we incorporated partof-speech ( pos ) information , word clustering , and sentence salience score . we also evaluated a graph-based approach that measures the importance of a word based on its connection with other sentences or words . the system performance is evaluated in different ways , including comparison to human annotated keywords using f-measure and a weighted score relative to the oracle system performance , as well as a novel alternative human evaluation . our results have shown that the simple unsupervised tfidf approach performs reasonably well , and the additional information from pos and sentence score helps keyword extraction . however , the graph method is less effective for this domain . experiments were also performed using speech recognition output and we observed degradation and different patterns compared to human transcripts .

multi-document summarization using off the shelf compression software
this study examines the usefulness of common off the shelf compression software such as gzip in enhancing already existing summaries and producing summaries from scratch . since the gzip algorithm works by removing repetitive data from a file in order to compress it , we should be able to determine which sentences in a summary contain the least repetitive data by judging the gzipped size of the summary with the sentence compared to the gzipped size of the summary without the sentence . by picking the sentence that increased the size of the summary the most , we hypothesized that the summary will gain the sentence with the most new information . this hypothesis was found to be true in many cases and to varying degrees in this study .

a tale of two parsers : investigating and combining graph-based and transition-based dependency parsing using beam-search
graph-based and transition-based approaches to dependency parsing adopt very different views of the problem , each view having its own strengths and limitations . we study both approaches under the framework of beamsearch . by developing a graph-based and a transition-based dependency parser , we show that a beam-search decoder is a competitive choice for both methods . more importantly , we propose a beam-search-based parser that combines both graph-based and transitionbased parsing into a single system for training and decoding , showing that it outperforms both the pure graph-based and the pure transition-based parsers . testing on the english and chinese penn treebank data , the combined system gave state-of-the-art accuracies of 92.1 % and 86.2 % , respectively .

inducing domain-specific noun polarity guided by domain-independent polarity preferences of adjectives
in this paper , we discuss how domainspecific noun polarity lexicons can be induced . we focus on the generation of good candidates and compare two machine learning scenarios in order to establish an approach that produces high precision . candidates are generated on the basis of polarity preferences of adjectives derived from a large domain-independent corpus . the polarity preference of a word , here an adjective , reflects the distribution of positive , negative and neutral arguments the word takes ( here : its nominal head ) . given a noun modified by some adjectives , a vote among the polarity preferences of these adjectives establishes a good indicator of the polarity of the noun . in our experiments with five domains , we achieved f-measure of 59 % up to 88 % on the basis of two machine learning approaches carried out on top of the preference votes .

relation extraction using support vector machine
apply support vector machines to detect and classify the relations in automatic content extraction ( ace ) corpus . we use a set of features including lexical tokens , syntactic structures , and semantic entity types for relation detection and classification problem . besides these linguistic features , we successfully utilize the distance between two entities to improve the performance . in relation detection , we filter out the negative relation candidates using entity distance threshold . in relation classification , we use the entity distance as a feature for support vector classifier . the system is evaluated in terms of recall , precision , and fmeasure , and errors of the system are analyzed with proposed solution .

linguistic profiling of texts for the purpose of language verification
in order to control the quality of internet-based language corpora , we developed a method to verify automatically that texts are of ( near- ) native quality . for the locness and icle corpora , the method is rather successful in separating native and non-native learner texts . the equal error rate is about 10 % . however , for other domains , such as internet texts , separate classifiers have to be trained on the basis of suitable seed corpora .

simplicity is better : revisiting single kernel ppi extraction
it has been known that a combination of multiple kernels and addition of various resources are the best options for improving effectiveness of kernel-based ppi extraction methods . these supplements , however , involve extensive kernel adaptation and feature selection processes , which attenuate the original benefits of the kernel methods . this paper shows that we are able to achieve the best performance among the stateof-the-art methods by using only a single kernel , convolution parse tree kernel . in-depth analyses of the kernel reveal that the keys to the improvement are the tree pruning method and consideration of tree kernel decay factors . it is noteworthy that we obtained the performance without having to use any additional features , kernels or corpora .

chinese named entity recognition with cascaded hybrid model
we propose a high-performance cascaded hybrid model for chinese ner . firstly , we use boosting , a standard and theoretically wellfounded machine learning method to combine a set of weak classifiers together into a base system . secondly , we introduce various types of heuristic human knowledge into markov logic networks ( mlns ) , an effective combination of first-order logic and probabilistic graphical models to validate boosting ner hypotheses . experimental results show that the cascaded hybrid model significantly outperforms the state-of-the-art boosting model .

chinese segmentation with a word-based perceptron algorithm
standard approaches to chinese word segmentation treat the problem as a tagging task , assigning labels to the characters in the sequence indicating whether the character marks a word boundary . discriminatively trained models based on local character features are used to make the tagging decisions , with viterbi decoding finding the highest scoring segmentation . in this paper we propose an alternative , word-based segmentor , which uses features based on complete words and word sequences . the generalized perceptron algorithm is used for discriminative training , and we use a beamsearch decoder . closed tests on the first and second sighan bakeoffs show that our system is competitive with the best in the literature , achieving the highest reported f-scores for a number of corpora .

translating italian connectives into italian sign language politecnico di milano barbara di eugenio
we present a corpus analysis of how italian connectives are translated into lis , the italian sign language . since corpus resources are scarce , we propose an alignment method between the syntactic trees of the italian sentence and of its lis translation . this method , and clustering applied to its outputs , highlight the different ways a connective can be rendered in lis : with a corresponding sign , by affecting the location or shape of other signs , or being omitted altogether . we translate these findings into a computational model that will be integrated into the pipeline of an existing italian-lis rendering system . initial experiments to learn the four possible translations with decision trees give promising results .

multi-adaptive natural language generation using principal component
we present feedbackgen , a system that uses a multi-adaptive approach to natural language generation . with the term multi-adaptive , we refer to a system that is able to adapt its content to different user groups simultaneously , in our case adapting to both lecturers and students . we present a novel approach to student feedback generation , which simultaneously takes into account the preferences of lecturers and students when determining the content to be conveyed in a feedback summary . in this framework , we utilise knowledge derived from ratings on feedback summaries by extracting the most relevant features using principal component regression ( pcr ) analysis . we then model a reward function that is used for training a reinforcement learning agent . our results with students suggest that , from the students perspective , such an approach can generate more preferable summaries than a purely lecturer-adapted approach .

building an annotated japanese-chinese parallel corpus a part of nict multilingual corpora
we are constricting a japanese-chinese parallel corpus , which is a part of the nict multilingual corpora . the corpus is general domain , of large scale of about 40,000 sentence pairs , long sentences , annotated with detailed information and high quality . to the best of our knowledge , this will be the first annotated japanesechinese parallel corpus in the world . we created the corpus by selecting japanese sentences from mainichi newspaper and then manually translating them into chinese . we then annotated the corpus with morphological and syntactic structures and alignments at word and phrase levels . this paper describes the specification in human translation and the scheme of detailed information annotation , and the tools we developed in the corpus construction . the experience we obtained and points we paid special attentions are also introduced for share with other researches in corpora construction .

improving name discrimination : a language salad approach
this paper describes a method of discriminating ambiguous names that relies upon features found in corpora of a more abundant language . in particular , we discriminate ambiguous names in bulgarian , romanian , and spanish corpora using information derived from much larger quantities of english data . we also mix together occurrences of the ambiguous name found in english with the occurrences of the name in the language in which we are trying to discriminate . we refer to this as a language salad , and find that it often results in even better performance than when only using english or the language itself as the source of information for discrimination .

mining of parsed data to derive deverbal argument structure
the availability of large parsed corpora and improved computing resources now make it possible to extract vast amounts of lexical data . we describe the process of extracting structured data and several methods of deriving argument structure mappings for deverbal nouns that significantly improves upon non-lexicalized rule-based methods . for a typical model , the f-measure of performance improves from a baseline of about 0.72 to 0.81 .

homotopy-based semi-supervised hidden markov models
this paper explores the use of the homotopy method for training a semi-supervised hidden markov model ( hmm ) used for sequence labeling . we provide a novel polynomial-time algorithm to trace the local maximum of the likelihood function for hmms from full weight on the labeled data to full weight on the unlabeled data . we present an experimental analysis of different techniques for choosing the best balance between labeled and unlabeled data based on the characteristics observed along this path . furthermore , experimental results on the field segmentation task in information extraction show that the homotopy-based method significantly outperforms em-based semisupervised learning , and provides a more accurate alternative to the use of held-out data to pick the best balance for combining labeled and unlabeled data .

building a large-scale annotated chinese corpus
in this paper we address issues related to building a large-scale chinese corpus . we try to answer four questions : ( i ) how to speed up annotation , ( ii ) how to maintain high annotation quality , ( iii ) for what purposes is the corpus applicable , and finally ( iv ) what future work we anticipate .

discriminative language models as a tool for machine translation error analysis
in this paper , we propose a new method for effective error analysis of machine translation ( mt ) systems . in previous work on error analysis of mt , error trends are often shown by frequency . however , if we attempt to perform a more detailed analysis based on frequently erroneous word strings , the word strings also often occur in correct translations , and analyzing these correct sentences decreases the overall efficiency of error analysis . in this paper , we propose the use of regularized discriminative language models ( lms ) to allow for more focused mt error analysis . in experiments , we demonstrate that our method is more efficient than frequency-based analysis , and examine differences across systems , language pairs , and evaluation measures .

trend survey on japanese natural language processing studies over the last decade
using natural language processing , we carried out a trend survey on japanese natural language processing studies that have been done over the last ten years . we determined the changes in the number of papers published for each research organization and on each research area as well as the relationship between research organizations and research areas . this paper is useful for both recognizing trends in japanese nlp and constructing a method of supporting trend surveys using nlp .

utterance segmentation using combined approach based on bi-directional n-gram and maximum entropy
this paper proposes a new approach to segmentation of utterances into sentences using a new linguistic model based upon maximum-entropy-weighted bidirectional n-grams . the usual n-gram algorithm searches for sentence boundaries in a text from left to right only . thus a candidate sentence boundary in the text is evaluated mainly with respect to its left context , without fully considering its right context . using this approach , utterances are often divided into incomplete sentences or fragments . in order to make use of both the right and left contexts of candidate sentence boundaries , we propose a new linguistic modeling approach based on maximum-entropy-weighted bidirectional n-grams . experimental results indicate that the new approach significantly outperforms the usual n-gram algorithm for segmenting both chinese and english utterances .

interpreting consumer health questions : the role of anaphora and national library of medicine
while interest in biomedical question answering has been growing , research in consumer health question answering remains relatively sparse . in this paper , we focus on the task of consumer health question understanding . we present a rule-based methodology that relies on lexical and syntactic information as well as anaphora/ellipsis resolution to construct structured representations of questions ( frames ) . our results indicate the viability of our approach and demonstrate the important role played by anaphora and ellipsis in interpreting consumer health questions .

spontaneous speech understanding for robust multi-modal
this paper presents a speech understanding component for enabling robust situated human-robot communication . the aim is to gain semantic interpretations of utterances that serve as a basis for multi-modal dialog management also in cases where the recognized word-stream is not grammatically correct . for the understanding process , we designed semantic processable units , which are adapted to the domain of situated communication . our framework supports the specific characteristics of spontaneous speech used in combination with gestures in a real world scenario . it also provides information about the dialog acts . finally , we present a processing mechanism using these concept structures to generate the most likely semantic interpretation of the utterances and to evaluate the interpretation with respect to semantic coherence .

learning contrastive connectives in sentence realization ranking the ohio state univeristy
we look at the average frequency of contrastive connectives in the sparky restaurant corpus with respect to realization ratings by human judges . we implement a discriminative n-gram ranker to model these ratings and analyze the resulting n-gram weights to determine if our ranker learns this distribution . surprisingly , our ranker learns to avoid contrastive connectives . we look at possible explanations for this distribution , and recommend improvements to both the generator and ranker of the sentence plans/realizations .

extractive summarization and dialogue act modeling on email threads : an integrated probabilistic approach
in this paper , we present a novel supervised approach to the problem of summarizing email conversations and modeling dialogue acts . we assume that there is a relationship between dialogue acts and important sentences . based on this assumption , we introduce a sequential graphical model approach which simultaneously summarizes email conversation and models dialogue acts . we compare our model with sequential and non-sequential models , which independently conduct the tasks of extractive summarization and dialogue act modeling . an empirical evaluation shows that our approach significantly outperforms all baselines in classifying correct summary sentences without losing performance on dialogue act modeling task .

error driven paraphrase annotation using mechanical turk
the source text provided to a machine translation system is typically only one of many ways the input sentence could have been expressed , and alternative forms of expression can often produce a better translation . we introduce here error driven paraphrasing of source sentences : instead of paraphrasing a source sentence exhaustively , we obtain paraphrases for only the parts that are predicted to be problematic for the translation system . we report on an amazon mechanical turk study that explores this idea , and establishes via an oracle evaluation that it holds the potential to substantially improve translation quality .

coarse-grained candidate generation and fine-grained re-ranking for chinese abbreviation prediction
correctly predicting abbreviations given the full forms is important in many natural language processing systems . in this paper we propose a two-stage method to find the corresponding abbreviation given its full form . we first use the contextual information given a large corpus to get abbreviation candidates for each full form and get a coarse-grained ranking through graph random walk . this coarse-grained rank list fixes the search space inside the top-ranked candidates . then we use a similarity sensitive re-ranking strategy which can utilize the features of the candidates to give a fine-grained re-ranking and select the final result . our method achieves good results and outperforms the state-ofthe-art systems . one advantage of our method is that it only needs weak supervision and can get competitive results with fewer training data . the candidate generation and coarse-grained ranking is totally unsupervised . the re-ranking phase can use a very small amount of training data to get a reasonably good result .

( re ) ranking meets morphosyntax : state-of-the-art results
this paper describes the ims-szeged-cis contribution to the spmrl 2013 shared task . we participate in both the constituency and dependency tracks , and achieve state-of-theart for all languages . for both tracks we make significant improvements through high quality preprocessing and ( re ) ranking on top of strong baselines . our system came out first for both tracks .

ot syntax : decidability of generation-based optimization
in optimality-theoretic syntax , optimization with unrestricted expressive power on the side of the ot constraints is undecidable . this paper provides a proof for the decidability of optimization based on constraints expressed with reference to local subtrees ( which is in the spirit of ot theory ) . the proof builds on kaplan and wedekinds ( 2000 ) construction showing that lfg generation produces contextfree languages .

unsupervised relation extraction from web documents
the idex system is a prototype of an interactive dynamic information extraction ( ie ) system . a user of the system expresses an information request in the form of a topic description , which is used for an initial search in order to retrieve a relevant set of documents . on basis of this set of documents , unsupervised relation extraction and clustering is done by the system . the results of these operations can then be interactively inspected by the user . in this paper we describe the relation extraction and clustering components of the idex system . preliminary evaluation results of these components are presented and an overview is given of possible enhancements to improve the relation extraction and clustering components .

acquiring entailment pairs across languages and domains : a data analysis seminar fr computerlinguistik
entailment pairs are sentence pairs of a premise and a hypothesis , where the premise textually entails the hypothesis . such sentence pairs are important for the development of textual entailment systems . in this paper , we take a closer look at a prominent strategy for their automatic acquisition from newspaper corpora , pairing first sentences of articles with their titles . we propose a simple logistic regression model that incorporates and extends this heuristic and investigate its robustness across three languages and three domains . we manage to identify two predictors which predict entailment pairs with a fairly high accuracy across all languages . however , we find that robustness across domains within a language is more difficult to achieve .

population testing : extracting semantic information on near-synonymy from native speakers languages & civilizations ,
measuring differences between nearsynonyms constitutes a major challenge in the development of electronic dictionaries and natural language processing systems . this paper presents a pilot study on how population test method ( ptm ) may be used as an effective , empirical tool to define near-synonyms in a quantifiable manner . use of ptm presumes that all knowledge about lexical meaning in a language resides collectively in the mind ( s ) of its native speakers , and that this intersubjective understanding may be extracted via targeted surveys that encourage creative , thinking responses . in this paper we show ( 1 ) examples of such tests performed on a group of high school students in finland , ( 2 ) resulting data from the tests that is surprisingly quantifiable , and ( 3 ) a web-based visualization program we are developing to analyze and present the collected data .

topic-based bengali opinion summarization
in this paper the development of an opinion summarization system that works on bengali news corpus has been described . the system identifies the sentiment information in each document , aggregates them and represents the summary information in text . the present sys-tem follows a topic-sentiment model for sentiment identification and aggregation . topic-sentiment model is designed as discourse level theme identification and the topic-sentiment aggregation is achieved by theme clustering ( k-means ) and document level theme relational graph representation . the document level theme relational graph is finally used for candidate summary sentence selection by standard page rank algorithms used in information retrieval ( ir ) . as bengali is a resource constrained language , the building of annotated gold standard corpus and acquisition of linguistics tools for lexico-syntactic , syntactic and discourse level features extraction are described in this paper . the reported accuracy of the theme detection technique is 83.60 % ( precision ) , 76.44 % ( recall ) and 79.85 % ( f-measure ) . the summarization system has been evaluated with precision of 72.15 % , recall of 67.32 % and fmeasure of 69.65 % .

exploring correlation of dependency relation paths for answer extraction spoken language systems
in this paper , we explore correlation of dependency relation paths to rank candidate answers in answer extraction . using the correlation measure , we compare dependency relations of a candidate answer and mapped question phrases in sentence with the corresponding relations in question . different from previous studies , we propose an approximate phrase mapping algorithm and incorporate the mapping score into the correlation measure . the correlations are further incorporated into a maximum entropy-based ranking model which estimates path weights from training . experimental results show that our method significantly outperforms state-ofthe-art syntactic relation-based methods by up to 20 % in mrr .

skeletons in the parser : using a shallow parser to improve deep parsing
we describe a simple approach for integrating shallow and deep parsing . we use phrase structure bracketing obtained from the collins parser as filters to guide deep parsing . our experiments demonstrate that our technique yields substantial gains in speed along with modest improvements in accuracy .

filtering syntactic constraints for statistical machine translation
source language parse trees offer very useful but imperfect reordering constraints for statistical machine translation . a lot of effort has been made for soft applications of syntactic constraints . we alternatively propose the selective use of syntactic constraints . a classifier is built automatically to decide whether a node in the parse trees should be used as a reordering constraint or not . using this information yields a 0.8 bleu point improvement over a full constraint-based system .

a cross-task flexible transition model for arabic tokenization , affix
this paper describes cross-task flexible transition models ( ctf-tms ) and demonstrates their effectiveness for arabic natural language processing ( nlp ) . nlp pipelines often suffer from error propagation , as errors committed in lower-level tasks cascade through the remainder of the processing pipeline . by allowing a flexible order of operations across and within multiple nlp tasks , a ctf-tm can mitigate both cross-task and within-task error propagation . our arabic ctf-tm models tokenization , affix detection , affix labeling , partof-speech tagging , and dependency parsing , achieving state-of-the-art results . we present the details of our general framework , our arabic ctf-tm , and the setup and results of our experiments .

complex predicates annotation in a corpus of portuguese
we present an annotation scheme for the annotation of complex predicates , understood as constructions with more than one lexical unit , each contributing part of the information normally associated with a single predicate . we discuss our annotation guidelines of four types of complex predicates , and the treatment of several difficult cases , related to ambiguity , overlap and coordination . we then discuss the process of marking up the portuguese cintil corpus of 1m tokens ( written and spoken ) with a new layer of information regarding complex predicates . we also present the outcomes of the annotation work and statistics on the types of cps that we found in the corpus .

twitter users # codeswitch hashtags ! # moltoimportante # wow #
when code switching , individuals incorporate elements of multiple languages into the same utterance . while code switching has been studied extensively in formal and spoken contexts , its behavior and prevalence remains unexamined in many newer forms of electronic communication . the present study examines code switching in twitter , focusing on instances where an author writes a post in one language and then includes a hashtag in a second language . in the first experiment , we perform a large scale analysis on the languages used in millions of posts to show that authors readily incorporate hashtags from other languages , and in a manual analysis of a subset the hashtags , reveal prolific code switching , with code switching occurring for some hashtags in over twenty languages . in the second experiment , french and english posts from three bilingual cities are analyzed for their code switching frequency and its content .

analysis and repair of name tagger errors
name tagging is a critical early stage in many natural language processing pipelines . in this paper we analyze the types of errors produced by a tagger , distinguishing name classification and various types of name identification errors . we present a joint inference model to improve chinese name tagging by incorporating feedback from subsequent stages in an information extraction pipeline : name structure parsing , cross-document coreference , semantic relation extraction and event extraction . we show through examples and performance measurement how different stages can correct different types of errors . the resulting accuracy approaches that of individual human annotators .

linefeed insertion into japanese spoken monologue for captioning
to support the real-time understanding of spoken monologue such as lectures and commentaries , the development of a captioning system is required . in monologues , since a sentence tends to be long , each sentence is often displayed in multi lines on one screen , it is necessary to insert linefeeds into a text so that the text becomes easy to read . this paper proposes a technique for inserting linefeeds into a japanese spoken monologue text as an elemental technique to generate the readable captions . our method appropriately inserts linefeeds into a sentence by machine learning , based on the information such as dependencies , clause boundaries , pauses and line length . an experiment using japanese speech data has shown the effectiveness of our technique .

red : a reference dependency based mt evaluation metric
most of the widely-used automatic evaluation metrics consider only the local fragments of the references and translations , and they ignore the evaluation on the syntax level . current syntaxbased evaluation metrics try to introduce syntax information but suffer from the poor parsing results of the noisy machine translations . to alleviate this problem , we propose a novel dependency-based evaluation metric which only employs the dependency information of the references . we use two kinds of reference dependency structures : headword chain to capture the long distance dependency information , and fixed and floating structures to capture the local continuous ngram . experiment results show that our metric achieves higher correlations with human judgments than bleu , ter and hwcm on wmt 2012 and wmt 2013. by introducing extra linguistic resources and tuning parameters , the new metric gets the state-of-the-art performance which is better than meteor and sempos on system level , and is comparable with meteor on sentence level on wmt 2012 and wmt 2013 .

global learning of typed entailment rules
extensive knowledge bases of entailment rules between predicates are crucial for applied semantic inference . in this paper we propose an algorithm that utilizes transitivity constraints to learn a globally-optimal set of entailment rules for typed predicates . we model the task as a graph learning problem and suggest methods that scale the algorithm to larger graphs . we apply the algorithm over a large data set of extracted predicate instances , from which a resource of typed entailment rules has been recently released ( schoenmackers et al , 2010 ) . our results show that using global transitivity information substantially improves performance over this resource and several baselines , and that our scaling methods allow us to increase the scope of global learning of entailment-rule graphs .

self-annotation for fine-grained geospatial relation extraction andre blessing hinrich schutze
a great deal of information on the web is represented in both textual and structured form . the structured form is machinereadable and can be used to augment the textual data . we call this augmentation the annotation of texts with relations that are included in the structured data self-annotation . in this paper , we introduce self-annotation as a new supervised learning approach for developing and implementing a system that extracts finegrained relations between entities . the main benefit of self-annotation is that it does not require manual labeling . the input of the learned model is a representation of the free text , its output structured relations . thus , the model , once learned , can be applied to any arbitrary free text . we describe the challenges for the self-annotation process and give results for a sample relation extraction system . to deal with the challenge of finegrained relations , we implement and evaluate both shallow and deep linguistic analysis , focusing on german .

self-training with products of latent variable grammars
we study self-training with products of latent variable grammars in this paper . we show that increasing the quality of the automatically parsed data used for self-training gives higher accuracy self-trained grammars . our generative self-trained grammars reach f scores of 91.6 on the wsj test set and surpass even discriminative reranking systems without selftraining . additionally , we show that multiple self-trained grammars can be combined in a product model to achieve even higher accuracy . the product model is most effective when the individual underlying grammars are most diverse . combining multiple grammars that were self-trained on disjoint sets of unlabeled data results in a final test accuracy of 92.5 % on the wsj test set and 89.6 % on our broadcast news test set .

sinhala grapheme-to-phoneme conversion and rules for schwa epenthesis
this paper describes an architecture to convert sinhala unicode text into phonemic specification of pronunciation . the study was mainly focused on disambiguating schwa-/\/ and /a/ vowel epenthesis for consonants , which is one of the significant problems found in sinhala . this problem has been addressed by formulating a set of rules . the proposed set of rules was tested using 30,000 distinct words obtained from a corpus and compared with the same words manually transcribed to phonemes by an expert . the grapheme-to-phoneme ( g2p ) conversion model achieves 98 % accuracy .

linking entities to a knowledge base with query expansion
in this paper we present a novel approach to entity linking based on a statistical language model-based information retrieval with query expansion . we use both local contexts and global world knowledge to expand query language models . we place a strong emphasis on named entities in the local contexts and explore a positional language model to weigh them differently based on their distances to the query . our experiments on the tac-kbp 2010 data show that incorporating such contextual information indeed aids in disambiguating the named entities and consistently improves the entity linking performance . compared with the official results from kbp 2010 participants , our system shows competitive performance .

melb-mkb : lexical substitution system based on relatives in context
in this paper we describe the melb-mkb system , as entered in the semeval-2007 lexical substitution task . the core of our system was the relatives in context unsupervised approach , which ranked the candidate substitutes by web-lookup of the word sequences built combining the target context and each substitute . our system ranked third in the final evaluation , performing close to the top-ranked system .

animacy encoding in english : why and how program in applied linguistics program in applied linguistics
we report on two recent medium-scale initiatives annotating present day english corpora for animacy distinctions . we discuss the relevance of animacy for computational linguistics , specifically generation , the annotation categories used in the two studies and the interannotator reliability for one of the studies .

text-to-text semantic similarity for automatic short answer grading
in this paper , we explore unsupervised techniques for the task of automatic short answer grading . we compare a number of knowledge-based and corpus-based measures of text similarity , evaluate the effect of domain and size on the corpus-based measures , and also introduce a novel technique to improve the performance of the system by integrating automatic feedback from the student answers . overall , our system significantly and consistently outperforms other unsupervised methods for short answer grading that have been proposed in the past .

cross-language and cross-encyclopedia article linking using mixed-language topic model and hypernym translation richard tzong-han tsai
creating cross-language article links among different online encyclopedias is now an important task in the unification of multilingual knowledge bases . in this paper , we propose a cross-language article linking method using a mixed-language topic model and hypernym translation features based on an svm model to link english wikipedia and chinese baidu baike , the most widely used wiki-like encyclopedia in china . to evaluate our approach , we compile a data set from the top 500 baidu baike articles and their corresponding english wiki articles . the evaluation results show that our approach achieves 80.95 % in mrr and 87.46 % in recall . our method does not heavily depend on linguistic characteristics and can be easily extended to generate crosslanguage article links among different online encyclopedias in other languages .

an algebra for semantic construction in constraint-based grammars new museums site division of informatics
we develop a framework for formalizing semantic construction within grammars expressed in typed feature structure logics , including hpsg . the approach provides an alternative to the lambda calculus ; it maintains much of the desirable flexibility of unificationbased approaches to composition , while constraining the allowable operations in order to capture basic generalizations and improve maintainability .

using names and topics for new event detection
new event detection ( ned ) involves monitoring chronologically-ordered news streams to automatically detect the stories that report on new events . we compare two stories by finding three cosine similarities based on names , topics and the full text . these additional comparisons suggest treating the ned problem as a binary classification problem with the comparison scores serving as features . the classifier models we learned show statistically significant improvement over the baseline vector space model system on all the collections we tested , including the latest tdt5 collection . the presence of automatic speech recognizer ( asr ) output of broadcast news in news streams can reduce performance and render our named entity recognition based approaches ineffective . we provide a solution to this problem achieving statistically significant improvements .

adaptation of maximum entropy capitalizer : little data can help a lot
a novel technique for maximum a posteriori ( map ) adaptation of maximum entropy ( maxent ) and maximum entropy markov models ( memm ) is presented . the technique is applied to the problem of recovering the correct capitalization of uniformly cased text : a background capitalizer trained on 20mwds of wall street journal ( wsj ) text from 1987 is adapted to two broadcast news ( bn ) test sets one containing abc primetime live text and the other npr morning news/cnn morning edition text from 1996. the in-domain performance of the wsj capitalizer is 45 % better than that of the 1-gram baseline , when evaluated on a test set drawn from wsj 1994. when evaluating on the mismatched out-ofdomain test data , the 1-gram baseline is outperformed by 60 % ; the improvement brought by the adaptation technique using a very small amount of matched bn data 25-70kwds is about 20-25 % relative . overall , automatic capitalization error rate of 1.4 % is achieved on bn data .

how creative is your writing a linguistic creativity measure from computer science and cognitive psychology perspectives
we demonstrate that subjective creativity in sentence-writing can in part be predicted using computable quantities studied in computer science and cognitive psychology . we introduce a task in which a writer is asked to compose a sentence given a keyword . the sentence is then assigned a subjective creativity score by human judges . we build a linear regression model which , given the keyword and the sentence , predicts the creativity score . the model employs features on statistical language models from a large corpus , psychological word norms , and wordnet .

contextual recommendation based on text mining robert bosch llc
the potential benefit of integrating contextual information for recommendation has received much research attention recently , especially with the ever-increasing interest in mobile-based recommendation services . however , context based recommendation research is limited due to the lack of standard evaluation data with contextual information and reliable technology for extracting such information . as a result , there are no widely accepted conclusions on how , when and whether context helps . additionally , a system often suffers from the so called cold start problem due to the lack of data for training the initial context based recommendation model . this paper proposes a novel solution to address these problems with automated information extraction techniques . we also compare several approaches for utilizing context based on a new data set collected using the proposed solution . the experimental results demonstrate that 1 ) ie-based techniques can help create a large scale context data with decent quality from online reviews , at least for restaurant recommendations ; 2 ) context helps recommender systems rank items , however , does not help predict user ratings ; 3 ) simply using context to filter items hurts recommendation performance , while a new probabilistic latent relational model we proposed helps .

recognizing partial textual entailment omer levy torsten zesch ido dagan iryna gurevych
textual entailment is an asymmetric relation between two text fragments that describes whether one fragment can be inferred from the other . it thus can not capture the notion that the target fragment is almost entailed by the given text . the recently suggested idea of partial textual entailment may remedy this problem . we investigate partial entailment under the faceted entailment model and the possibility of adapting existing textual entailment methods to this setting . indeed , our results show that these methods are useful for recognizing partial entailment . we also provide a preliminary assessment of how partial entailment may be used for recognizing ( complete ) textual entailment .

spred : large-scale harvesting of semantic predicates dipartimento di informatica
we present spred , a novel method for the creation of large repositories of semantic predicates . we start from existing collocations to form lexical predicates ( e.g. , break ) and learn the semantic classes that best fit the argument . to do this , we extract all the occurrences in wikipedia which match the predicate and abstract its arguments to general semantic classes ( e.g. , break body part , break agreement , etc . ) . our experiments show that we are able to create a large collection of semantic predicates from the oxford advanced learners dictionary with high precision and recall , and perform well against the most similar approach .

structured lexical similarity via convolution kernels on dependency trees
a central topic in natural language processing is the design of lexical and syntactic features suitable for the target application . in this paper , we study convolution dependency tree kernels for automatic engineering of syntactic and semantic patterns exploiting lexical similarities . we define efficient and powerful kernels for measuring the similarity between dependency structures , whose surface forms of the lexical nodes are in part or completely different . the experiments with such kernels for question classification show an unprecedented results , e.g . 41 % of error reduction of the former state-of-the-art . additionally , semantic role classification confirms the benefit of semantic smoothing for dependency kernels .

separating fact from fear : tracking flu infections on twitter
twitter has been shown to be a fast and reliable method for disease surveillance of common illnesses like influenza . however , previous work has relied on simple content analysis , which conflates flu tweets that report infection with those that express concerned awareness of the flu . by discriminating these categories , as well as tweets about the authors versus about others , we demonstrate significant improvements on influenza surveillance using twitter .

computational simulations of second language construction learning yevgen matusevych afra alishahi ad backus culture studies and information sciences culture studies
there are few computational models of second language acquisition ( sla ) . at the same time , many questions in the field of sla remain unanswered . in particular , sla patterns are difficult to study due to the large amount of variation between human learners . we present a computational model of second language construction learning that allows manipulating specific parameters such as age of onset and amount of exposure . we use the model to study general developmental patterns of sla and two specific effects sometimes found in empirical studies : construction priming and a facilitatory effect of skewed frequencies in the input . our simulations replicate the expected sla patterns as well as the two effects . our model can be used in further studies of various sla phenomena .

a language-independent method for the extraction of rdf verbalization
with the rise of the semantic web more and more data become available encoded using the semantic web standard rdf . rdf is faced towards machines : designed to be easily processable by machines it is difficult to be understood by casual users . transforming rdf data into human-comprehensible text would facilitate non-experts to assess this information . in this paper we present a languageindependent method for extracting rdf verbalization templates from a parallel corpus of text and data . our method is based on distant-supervised simultaneous multi-relation learning and frequent maximal subgraph pattern mining . we demonstrate the feasibility of our method on a parallel corpus of wikipedia articles and dbpedia data for english and german .

natural language understanding using temporal action logic
we consider a logicist approach to natural language understanding based on the translation of a quasi-logical form into a temporal logic , explicitly constructed for the representation of action and change , and the subsequent reasoning about this semantic structure in the context of a background knowledge theory using automated theorem proving techniques . the approach is substantiated through a proof-ofconcept question answering system implementation that uses a head-driven phrase structure grammar developed in the linguistic knowledge builder to construct minimal recursion semantics structures which are translated into a temporal action logic where both the snark automated theorem prover and the allegro prolog logic programming environment can be used for reasoning through an interchangeable compilation into first-order logic or logic programs respectively .

balancing conicting factors in argument interpretation
we present a probabilistic approach for the interpretation of arguments that casts the selection of an interpretation as a model selection task . in selecting the best model , our formalism balances conflicting factors : model complexity against data fit , and structure complexity against belief reasonableness . we first describe our basic formalism , which considers interpretations comprising inferential relations , and then show how our formalism is extended to suppositions that account for the beliefs in an argument , and justifications that account for the inferences in an interpretation . our evaluations with users show that the interpretations produced by our system are acceptable , and that there is strong support for the postulated suppositions and justifications .

leveraging lexical cohesion and disruption for topic segmentation
topic segmentation classically relies on one of two criteria , either finding areas with coherent vocabulary use or detecting discontinuities . in this paper , we propose a segmentation criterion combining both lexical cohesion and disruption , enabling a trade-off between the two . we provide the mathematical formulation of the criterion and an efficient graph based decoding algorithm for topic segmentation . experimental results on standard textual data sets and on a more challenging corpus of automatically transcribed broadcast news shows demonstrate the benefit of such a combination . gains were observed in all conditions , with segments of either regular or varying length and abrupt or smooth topic shifts . long segments benefit more than short segments . however the algorithm has proven robust on automatic transcripts with short segments and limited vocabulary reoccurrences .

practical markov logic containing first-order quantifiers with application to identity uncertainty
markov logic is a highly expressive language recently introduced to specify the connectivity of a markov network using first-order logic . while markov logic is capable of constructing arbitrary first-order formulae over the data , the complexity of these formulae is often limited in practice because of the size and connectivity of the resulting network . in this paper , we present approximate inference and estimation methods that incrementally instantiate portions of the network as needed to enable firstorder existential and universal quantifiers in markov logic networks . when applied to the problem of identity uncertainty , this approach results in a conditional probabilistic model that can reason about objects , combining the expressivity of recently introduced blog models with the predictive power of conditional training . we validate our algorithms on the tasks of citation matching and author disambiguation .

multimodal generation in the comic dialogue system
we describe how context-sensitive , usertailored output is specified and produced in the comic multimodal dialogue system . at the conference , we will demonstrate the user-adapted features of the dialogue manager and text planner .

discriminative word alignment by learning the alignment structure and syntactic divergence between a language pair
discriminative approaches for word alignment have gained popularity in recent years because of the flexibility that they offer for using a large variety of features and combining information from various sources . but , the models proposed in the past have not been able to make much use of features that capture the likelihood of an alignment structure ( the set of alignment links ) and the syntactic divergence between sentences in the parallel text . this is primarily because of the limitation of their search techniques . in this paper , we propose a generic discriminative re-ranking approach for word alignment which allows us to make use of structural features effectively . these features are particularly useful for language pairs with high structural divergence ( like english-hindi , englishjapanese ) . we have shown that by using the structural features , we have obtained a decrease of 2.3 % in the absolute value of alignment error rate ( aer ) . when we add the cooccurence probabilities obtained from ibm model-4 to our features , we achieved the best aer ( 50.50 ) for the english-hindi parallel corpus .

using natural language processing , locus link , and the gene ontology to compare omim to medline national library of medicine
researchers in the biomedical and molecular biology fields are faced with a wide variety of information sources . these are presented in the form of images , free text , and structured data files that include medical records , gene and protein sequence data , and whole genome microarray data , all gathered from a variety of experimental organisms and clinical subjects . the need to organize and relate this information , particularly concerning genes , has motivated the development of resources , such as the unified medical language system , gene ontology , locuslink , and the online inheritance in man ( omim ) database . we describe a natural language processing application to extract information on genes from unstructured text and discuss ways to integrate this information with some of the available online resources .

reranking bilingually extracted paraphrases using monolingual distributional similarity tsz ping chan , chris callison-burch and benjamin van durme
this paper improves an existing bilingual paraphrase extraction technique using monolingual distributional similarity to rerank candidate paraphrases . raw monolingual data provides a complementary and orthogonal source of information that lessens the commonly observed errors in bilingual pivotbased methods . our experiments reveal that monolingual scoring of bilingually extracted paraphrases has a significantly stronger correlation with human judgment for grammaticality than the probabilities assigned by the bilingual pivoting method does . the results also show that monolingual distribution similarity can serve as a threshold for high precision paraphrase selection .

probabilistic disambiguation models for wide-coverage hpsg parsing
this paper reports the development of loglinear models for the disambiguation in wide-coverage hpsg parsing . the estimation of log-linear models requires high computational cost , especially with widecoverage grammars . using techniques to reduce the estimation cost , we trained the models using 20 sections of penn treebank . a series of experiments empirically evaluated the estimation techniques , and also examined the performance of the disambiguation models on the parsing of real-world sentences .

telling apart temporal locating adverbials and time-denoting expressions
this paper is concerned with the identification of two semantically close categories temporal locating adverbials and time-denoting expressions . the dividing line between these categories is difficult to draw , inasmuch as there are several phrases that occur with the same surface form in the typical contexts of both of them ( e.g . in adverbial position and as the complement of verbs like to date from ) . these ambivalent phrases include relatively simple expressions like yesterday or last week , but also a fact that has gone practically unnoticed in the literature structurally complex ones , like those headed by before , after , when or ago . in this paper , a uniform semantic categorisation of these phrases as mere time-denoting expressions is advocated and some of its consequences for the grammatical system are assessed . the analysis postulates a null locating preposition ( with a value close to that of in ) in the contexts where the ambivalent forms occur adverbially . a corollary is the partition of the set of particles traditionally classified as temporal locating into two sets : the truly locating ones like in , during , since or until and those that are mere heads of ( structurally complex ) time-denoting expressions like before , after , between , when , ago , or from .

kneser-ney smoothing on expected counts
widely used in speech and language processing , kneser-ney ( kn ) smoothing has consistently been shown to be one of the best-performing smoothing methods . however , kn smoothing assumes integer counts , limiting its potential usesfor example , inside expectation-maximization . in this paper , we propose a generalization of kn smoothing that operates on fractional counts , or , more precisely , on distributions over counts . we rederive all the steps of kn smoothing to operate on count distributions instead of integral counts , and apply it to two tasks where kn smoothing was not applicable before : one in language model adaptation , and the other in word alignment . in both cases , our method improves performance significantly .

daisuke okanohara junichi tsujii
when linear classifiers can not successfully classify data , we often add combination features , which are products of several original features . the searching for effective combination features , namely feature engineering , requires domain-specific knowledge and hard work . we present herein an efficient algorithm for learning an l1 regularized logistic regression model with combination features . we propose to use the grafting algorithm with efficient computation of gradients . this enables us to find optimal weights efficiently without enumerating all combination features . by using l1 regularization , the result we obtain is very compact and achieves very efficient inference . in experiments with nlp tasks , we show that the proposed method can extract effective combination features , and achieve high performance with very few features .

syntactic kernels for natural language learning
in this paper , we use tree kernels to exploit deep syntactic parsing information for natural language applications . we study the properties of different kernels and we provide algorithms for their computation in linear average time . the experiments with svms on the task of predicate argument classification provide empirical data that validates our methods .

sentence compression with joint structural inference
sentence compression techniques often assemble output sentences using fragments of lexical sequences such as ngrams or units of syntactic structure such as edges from a dependency tree representation . we present a novel approach for discriminative sentence compression that unifies these notions and jointly produces sequential and syntactic representations for output text , leveraging a compact integer linear programming formulation to maintain structural integrity . our supervised models permit rich features over heterogeneous linguistic structures and generalize over previous state-of-theart approaches . experiments on corpora featuring human-generated compressions demonstrate a 13-15 % relative gain in 4gram accuracy over a well-studied language model-based compression system .

automatic evaluation of translation quality for distant language pairs
automatic evaluation of machine translation ( mt ) quality is essential to developing highquality mt systems . various evaluation metrics have been proposed , and bleu is now used as the de facto standard metric . however , when we consider translation between distant language pairs such as japanese and english , most popular metrics ( e.g. , bleu , nist , per , and ter ) do not work well . it is well known that japanese and english have completely different word orders , and special care must be paid to word order in translation . otherwise , translations with wrong word order often lead to misunderstanding and incomprehensibility . for instance , smt-based japanese-to-english translators tend to translate a because b as b because a. thus , word order is the most important problem for distant language translation . however , conventional evaluation metrics do not significantly penalize such word order mistakes . therefore , locally optimizing these metrics leads to inadequate translations . in this paper , we propose an automatic evaluation metric based on rank correlation coefficients modified with precision . our meta-evaluation of the ntcir-7 patmt je task data shows that this metric outperforms conventional metrics .

uiuc : a knowledge-rich approach to identifying semantic relations
this paper describes a supervised , knowledge-intensive approach to the automatic identification of semantic relations between nominals in english sentences . the system employs different sets of new and previously used lexical , syntactic , and semantic features extracted from various knowledge sources . at semeval 2007 the system achieved an f-measure of 72.4 % and an accuracy of 76.3 % .

a low-budget tagger for old czech
the paper describes a tagger for old czech ( 1200-1500 ad ) , a fusional language with rich morphology . the practical restrictions ( no native speakers , limited corpora and lexicons , limited funding ) make old czech an ideal candidate for a resource-light crosslingual method that we have been developing ( e.g . hana et al , 2004 ; feldman and hana , 2010 ) . we use a traditional supervised tagger . however , instead of spending years of effort to create a large annotated corpus of old czech , we approximate it by a corpus of modern czech . we perform a series of simple transformations to make a modern text look more like a text in old czech and vice versa . we also use a resource-light morphological analyzer to provide candidate tags . the results are worse than the results of traditional taggers , but the amount of language-specific work needed is minimal .

fast and accurate shift-reduce constituent parsing
shift-reduce dependency parsers give comparable accuracies to their chartbased counterparts , yet the best shiftreduce constituent parsers still lag behind the state-of-the-art . one important reason is the existence of unary nodes in phrase structure trees , which leads to different numbers of shift-reduce actions between different outputs for the same input . this turns out to have a large empirical impact on the framework of global training and beam search . we propose a simple yet effective extension to the shift-reduce process , which eliminates size differences between action sequences in beam-search . our parser gives comparable accuracies to the state-of-the-art chart parsers . with linear run-time complexity , our parser is over an order of magnitude faster than the fastest chart parser .

semi-supervised relation extraction with large-scale word clustering
we present a simple semi-supervised relation extraction system with large-scale word clustering . we focus on systematically exploring the effectiveness of different cluster-based features . we also propose several statistical methods for selecting clusters at an appropriate level of granularity . when training on different sizes of data , our semi-supervised approach consistently outperformed a state-of-the-art supervised baseline system .

extracting bilingual dictionary from comparable corpora with kun yu junichi tsujii
this paper proposes an approach for bilingual dictionary extraction from comparable corpora . the proposed approach is based on the observation that a word and its translation share similar dependency relations . experimental results using 250 randomly selected translation pairs prove that the proposed approach significantly outperforms the traditional contextbased approach that uses bag-of-words around translation candidates .

automatic essay grading with probabilistic latent semantic analysis
probabilistic latent semantic analysis ( plsa ) is an information retrieval technique proposed to improve the problems found in latent semantic analysis ( lsa ) . we have applied both lsa and plsa in our system for grading essays written in finnish , called automatic essay assessor ( aea ) . we report the results comparing plsa and lsa with three essay sets from various subjects . the methods were found to be almost equal in the accuracy measured by spearman correlation between the grades given by the system and a human . furthermore , we propose methods for improving the usage of plsa in essay grading .

unsupervised dependency parsing with transferring distribution via parallel guidance and entropy regularization
we present a novel approach for inducing unsupervised dependency parsers for languages that have no labeled training data , but have translated text in a resourcerich language . we train probabilistic parsing models for resource-poor languages by transferring cross-lingual knowledge from resource-rich language with entropy regularization . our method can be used as a purely monolingual dependency parser , requiring no human translations for the test data , thus making it applicable to a wide range of resource-poor languages . we perform experiments on three data sets version 1.0 and version 2.0 of google universal dependency treebanks and treebanks from conll shared-tasks , across ten languages . we obtain stateof-the art performance of all the three data sets when compared with previously studied unsupervised and projected parsing systems .

automatic identification of english verb particle constructions using linguistic features su nam kim and timothy baldwin
this paper presents a method for identifying token instances of verb particle constructions ( vpcs ) automatically , based on the output of the rasp parser . the proposed method pools together instances of vpcs and verb-pps from the parser output and uses the sentential context of each such instance to differentiate vpcs from verb-pps . we show our technique to perform at an f-score of 97.4 % at identifying vpcs in wall street journal and brown corpus data taken from the penn treebank .

nilc usp : an improved hybrid system for sentiment analysis in sao carlos - sp , brazil
this paper describes the nilc usp system that participated in semeval-2014 task 9 : sentiment analysis in twitter , a re-run of the semeval 2013 task under the same name . our system is an improved version of the system that participated in the 2013 task . this system adopts a hybrid classification process that uses three classification approaches : rule-based , lexiconbased and machine learning . we suggest a pipeline architecture that extracts the best characteristics from each classifier . in this work , we want to verify how this hybrid approach would improve with better classifiers . the improved system achieved an f-score of 65.39 % in the twitter message-level subtask for 2013 dataset ( + 9.08 % of improvement ) and 63.94 % for 2014 dataset .

unsupervised phonemic chinese word segmentation using adaptor
adaptor grammars are a framework for expressing and performing inference over a variety of non-parametric linguistic models . these models currently provide state-of-the-art performance on unsupervised word segmentation from phonemic representations of child-directed unsegmented english utterances . this paper investigates the applicability of these models to unsupervised word segmentation of mandarin . we investigate a wide variety of different segmentation models , and show that the best segmentation accuracy is obtained frommodels that capture interword collocational dependencies . surprisingly , enhancing the models to exploit syllable structure regularities and to capture tone information does improve overall word segmentation accuracy , perhaps because the information these elements convey is redundant when compared to the inter-word dependencies .

inference rules and their application to recognizing textual entailment
in this paper , we explore ways of improving an inference rule collection and its application to the task of recognizing textual entailment . for this purpose , we start with an automatically acquired collection and we propose methods to refine it and obtain more rules using a hand-crafted lexical resource . following this , we derive a dependency-based structure representation from texts , which aims to provide a proper base for the inference rule application . the evaluation of our approach on the recognizing textual entailment data shows promising results on precision and the error analysis suggests possible improvements .

feature decay algorithms for fast deployment of accurate statistical machine translation systems
we use feature decay algorithms ( fda ) for fast deployment of accurate statistical machine translation systems taking only about half a day for each translation direction . we develop parallel fda for solving computational scalability problems caused by the abundance of training data for smt models and lm models and still achieve smt performance that is on par with using all of the training data or better . parallel fda runs separate fda models on randomized subsets of the training data and combines the instance selections later . parallel fda can also be used for selecting the lm corpus based on the training set selected by parallel fda . the high quality of the selected training data allows us to obtain very accurate translation outputs close to the top performing smt systems . the relevancy of the selected lm corpus can reach up to 86 % reduction in the number of oov tokens and up to 74 % reduction in the perplexity . we perform smt experiments in all language pairs in the wmt13 translation task and obtain smt performance close to the top systems using significantly less resources for training and development .

are some speech recognition errors easier to detect than others and electrical engineering
this study investigates whether some speech recognition ( sr ) errors are easier to detect and what patterns can be identified from those errors . specifically , sr errors were examined from both nonlinguistic and linguistic perspectives . the analyses of non-linguistic properties revealed that high error ratios and consecutive errors lowered the ease of error detection . the analyses of linguistic properties showed that ease of error detection was associated with changing parts-of-speech of reference words in sr errors . additionally , syntactic relations themselves and the change of syntactic relations had impact on the ease of error detection .

deterministic statistical mapping of sentences to underspecified semantics
we present a method for training a statistical model for mapping natural language sentences to semantic expressions . the semantics are expressions of an underspecified logical form that has properties making it particularly suitable for statistical mapping from text . an encoding of the semantic expressions into dependency trees with automatically generated labels allows application of existing methods for statistical dependency parsing to the mapping task ( without the need for separate traditional dependency labels or parts of speech ) . the encoding also results in a natural per-word semantic-mapping accuracy measure . we report on the results of training and testing statistical models for mapping sentences of the penn treebank into the semantic expressions , for which per-word semantic mapping accuracy ranges between 79 % and 86 % depending on the experimental conditions . the particular choice of algorithms used also means that our trained mapping is deterministic ( in the sense of deterministic parsing ) , paving the way for large-scale text-to-semantic mapping .

improving cloze test performance of language learners using web n-grams
we study the effectiveness of search engines for common usage , a new category of search engines that exploit n-gram frequencies on the web to measure the commonness of a formulation , and that allow their users to submit wildcard queries about formulation uncertainties often encountered in the process of writing . these search engines help to resolve questions on common prepositions following verbs , common synonyms in given contexts , and word order difficulties , to name only a few . until now , however , it has never been shown that search engines for common usage have a positive impact on writing performance . our contribution is a large-scale user study with 121 participants using the netspeak search engine to shed light on this issue for the first time . via carefully designed cloze tests we show that second language learners who have access to a search engine for common usage significantly and effectively improve their test performance as opposed to not using them .

a description of tunable machine translation evaluation systems in
this paper is to describe our machine translation evaluation systems used for participation in the wmt13 shared metrics task . in the metrics task , we submitted two automatic mt evaluation systems nlepor_baseline and lepor_v3.1 . nlepor_baseline is an n-gram based language independent mt evaluation metric employing the factors of modified sentence length penalty , position difference penalty , n-gram precision and n-gram recall . nlepor_baseline measures the similarity of the system output translations and the reference translations only on word sequences . lepor_v3.1 is a new version of lepor metric using the mathematical harmonic mean to group the factors and employing some linguistic features , such as the part-of-speech information . the evaluation results of wmt13 show lepor_v3.1 yields the highest averagescore 0.86 with human judgments at systemlevel using pearson correlation criterion on english-to-other ( fr , de , es , cs , ru ) language pairs .

asvuniofleipzig : sentiment analysis in twitter using data-driven machine learning techniques
this paper describes university of leipzigs approach to semeval-2013 task 2b on sentiment analysis in twitter : message polarity classification . our system is designed to function as a baseline , to see what we can accomplish with well-understood and purely data-driven lexical features , simple generalizations as well as standard machine learning techniques : we use one-against-one support vector machines with asymmetric cost factors and linear kernels as classifiers , word uni- and bigrams as features and additionally model negation of word uni- and bigrams in word n-gram feature space . we consider generalizations of urls , user names , hash tags , repeated characters and expressions of laughter . our method ranks 23 out of all 48 participating systems , achieving an averaged ( positive , negative ) f-score of 0.5456 and an averaged ( positive , negative , neutral ) f-score of 0.595 , which is above median and average .

emdc : a semi-supervised approach for word alignment
this paper proposes a novel semisupervised word alignment technique called emdc that integrates discriminative and generative methods . a discriminative aligner is used to find high precision partial alignments that serve as constraints for a generative aligner which implements a constrained version of the em algorithm . experiments on small-size chinese and arabic tasks show consistent improvements on aer . we also experimented with moderate-size chinese machine translation tasks and got an average of 0.5 point improvement on bleu scores across five standard nist test sets and four other test sets .

measuring the structural importance through rhetorical structure index
in this paper , we propose a novel rhetorical structure index ( rsi ) to measure the structural importance of a word or a phrase . unlike tf-idf and other content-driven measurements , rsi identifies words or phrases that are structural cues in an unstructured document . we show structurally motivated features with high rsi values are more useful than content-driven features for applications such as segmenting unstructured lecture transcripts into meaningful segments . experiments show that using rsi significantly improves the segmentation accuracy compared to tf-idf , a traditional content-based feature weighting scheme .

unsupervised part-of-speech tagging with bilingual graph-based projections
we describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data , but have translated text in a resource-rich language . our method does not assume any knowledge about the target language ( in particular no tagging dictionary is assumed ) , making it applicable to a wide array of resource-poor languages . we use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model ( bergkirkpatrick et al , 2010 ) . across eight european languages , our approach results in an average absolute improvement of 10.4 % over a state-of-the-art baseline , and 16.7 % over vanilla hidden markov models induced with the expectation maximization algorithm .

deterministic parsing using pcfgs
we propose the design of deterministic constituent parsers that choose parser actions according to the probabilities of parses of a given probabilistic context-free grammar . several variants are presented . one of these deterministically constructs a parse structure while postponing commitment to labels . we investigate theoretical time complexities and report experiments .

match : an architecture for multimodal dialogue systems
mobile interfaces need to allow the user and system to adapt their choice of communication modes according to user preferences , the task at hand , and the physical and social environment . we describe a multimodal application architecture which combines finite-state multimodal language processing , a speech-act based multimodal dialogue manager , dynamic multimodal output generation , and user-tailored text planning to enable rapid prototyping of multimodal interfaces with flexible input and adaptive output . our testbed application match ( multimodal access to city help ) provides a mobile multimodal speech-pen interface to restaurant and subway information for new york city .

edit : a broad-coverage grammar checker using pattern grammar
we introduce a new method for learning to detect grammatical errors in learners writing and provide suggestions . the method involves parsing a reference corpus and inferring grammar patterns in the form of a sequence of content words , function words , and parts-of-speech ( e.g. , play ~ role in ving and look forward to ving ) . at runtime , the given passage submitted by the learner is matched using an extended levenshtein algorithm against the set of pattern rules in order to detect errors and provide suggestions . we present a prototype implementation of the proposed method , edit , that can handle a broad range of errors . promising results are illustrated with three common types of errors in nonnative writing .

data-driven measurement of child language development with simple syntactic templates
when assessing child language development , researchers have traditionally had to choose between easily computable metrics focused on superficial aspects of language , and more expressive metrics that are carefully designed to cover specific syntactic structures and require substantial and tedious labor . recent work has shown that existing expressive metrics for child language development can be automated and produce accurate results . we go a step further and propose that measurement of syntactic development can be performed automatically in a completely data-driven way without the need for definition of language-specific inventories of grammatical structures . as a crucial step in that direction , we show that four simple feature templates are as expressive of language development as a carefully crafted standard inventory of grammatical structures that is commonly used and has been validated empirically .

a web-based interactive computer aided translation tool
we developed caitra , a novel tool that aids human translators by ( a ) making suggestions for sentence completion in an interactive machine translation setting , ( b ) providing alternative word and phrase translations , and ( c ) allowing them to postedit machine translation output . the tool uses the moses decoder , is implemented in ruby on rails and c++ and delivered over the web .

artificial intelligence - dfki , artificial intelligence - dfki ,
we propose an ie based approach to people disambiguation . we assume the mentioning of nes and the relational context of a person in the text to be important discriminating features in order to distinguish different people sharing a name .

implications of pragmatic and cognitive theories on the design of utterance-based aac systems and information sciences
utterance-based aac systems have the potential to significantly speed communication rate for someone who relies on a speech generating device for communication . at the same time , such systems pose interesting challenges including anticipating text needs , remembering what text is stored , and accessing desired text when needed . moreover , using such systems has profound pragmatic implications as a prestored message may or may not capture exactly what the user wishes to say in a particular discourse situation . in this paper we describe a prototype of an utterance-based aac system whose design choices are driven by findings from theoretically driven studies concerning pragmatic choices with which the user of such a system is faced . these findings are coupled with cognitive theories to make choices for system design .

creating a finite-state parser with application semantics
parsli is a finite-state ( fs ) parser which can be tailored to the lexicon , syntax , and semantics of a particular application using a hand-editable declarative lexicon . the lexicon is defined in terms of a lexicalized tree adjoining grammar , which is subsequently mapped to a fs representation . this approach gives the application designer better and easier control over the natural language understanding component than using an off-the-shelf parser . we present results using parsli on an application that creates 3d-images from typed input .

active learning for part-of-speech tagging : accelerating corpus annotation
in the construction of a part-of-speech annotated corpus , we are constrained by a fixed budget . a fully annotated corpus is required , but we can afford to label only a subset . we train a maximum entropy markov model tagger from a labeled subset and automatically tag the remainder . this paper addresses the question of where to focus our manual tagging efforts in order to deliver an annotation of highest quality . in this context , we find that active learning is always helpful . we focus on query by uncertainty ( qbu ) and query by committee ( qbc ) and report on experiments with several baselines and new variations of qbc and qbu , inspired by weaknesses particular to their use in this application . experiments on english prose and poetry test these approaches and evaluate their robustness . the results allow us to make recommendations for both types of text and raise questions that will lead to further inquiry .

build chinese emotion lexicons using a graph-based algorithm and multiple resources
for sentiment analysis , lexicons play an important role in many related tasks . in this paper , aiming to build chinese emotion lexicons for public use , we adopted a graph-based algorithm which ranks words according to a few seed emotion words . the ranking algorithm exploits the similarity between words , and uses multiple similarity metrics which can be derived from dictionaries , unlabeled corpora or heuristic rules . to evaluate the adopted algorithm and resources , two independent judges were asked to label the top words of ranking list . it is observed that noise is almost unavoidable due to imprecise similarity metrics between words . so , to guarantee the quality of emotion lexicons , we use an iterative feedback to combine manual labeling and the automatic ranking algorithm above . we also compared our newly constructed chinese emotion lexicons ( happiness , anger , sadness , fear and surprise ) with existing counterparts , and related analysis is offered .

device-dependent readability for improved text understanding
readability is used to provide users with highquality service in text recommendation or text visualization . with the increasing use of handheld devices , reading device is regarded as an important factor for readability . therefore , this paper investigates the relationship between readability and reading devices such as a smart phone , a tablet , and paper . we suggest readability factors that are strongly related with the readability of a specific device by showing the correlations between various factors in each device and human-rated readability . our experimental results show that each device has its own readability characteristics , and thus different weights should be imposed on readability factors according to the device type . in order to prove the usefulness of the results , we apply the device-dependent readability to news article recommendation .

a corpus of italian web texts
pais ` a is a creative commons licensed , large web corpus of contemporary italian . we describe the design , harvesting , and processing steps involved in its creation .

a hybrid approach to skeleton-based translation
in this paper we explicitly consider sentence skeleton information for machine translation ( mt ) . the basic idea is that we translate the key elements of the input sentence using a skeleton translation model , and then cover the remain segments using a full translation model . we apply our approach to a state-of-the-art phrase-based system and demonstrate very promising bleu improvements and ter reductions on the nist chinese-english mt evaluation data .

sentence completion tests for training and assessment in a computational linguistics curriculum
this paper presents a novel type of test , halfway between multiple-choice and free-form text , used for training and assessment in several courses in a computational linguistics curriculum . we will describe the principles of the test , the different ways in which it can be used by learners , and the tools developed for authoring . use of this type of test is not limited to the field of computational linguistics . wherever text heavy or even picture based topics are taught use of this type of test is possible .

active deep networks for semi-supervised
this paper presents a novel semisupervised learning algorithm called active deep networks ( adn ) , to address the semi-supervised sentiment classification problem with active learning . first , we propose the semi-supervised learning method of adn . adn is constructed by restricted boltzmann machines ( rbm ) with unsupervised learning using labeled data and abundant of unlabeled data . then the constructed structure is finetuned by gradient-descent based supervised learning with an exponential loss function . second , we apply active learning in the semi-supervised learning framework to identify reviews that should be labeled as training data . then adn architecture is trained by the selected labeled data and all unlabeled data . experiments on five sentiment classification datasets show that adn outperforms the semi-supervised learning algorithm and deep learning techniques applied for sentiment classification .

a generative approach to learning from annotator rationales
a human annotator can provide hints to a machine learner by highlighting contextual rationales for each of his or her annotations ( zaidan et al , 2007 ) . how can one exploit this side information to better learn the desired parameters we present a generative model of how a given annotator , knowing the true , stochastically chooses rationales . thus , observing the rationales helps us infer the true . we collect substring rationales for a sentiment classification task and use them to obtain significant accuracy improvements for each annotator . our new generative approach exploits the rationales more effectively than our previous masking svm approach . it is also more principled , and could be adapted to help learn other kinds of probabilistic classifiers for quite different tasks .

blast : a tool for error analysis of machine translation output
we present blast , an open source tool for error analysis of machine translation ( mt ) output . we believe that error analysis , i.e. , to identify and classify mt errors , should be an integral part of mt development , since it gives a qualitative view , which is not obtained by standard evaluation methods . blast can aid mt researchers and users in this process , by providing an easy-to-use graphical user interface . it is designed to be flexible , and can be used with any mt system , language pair , and error typology . the annotation task can be aided by highlighting similarities with a reference translation .

morphological interfaces to dictionaries linguistic data consortium
languages with complex morphologies present difficulties for dictionaries users . one solution to this problem is to use a morphological parser for lookup of morphologically complex words , including fully inflected words , without the user needing to explicitly know the morphology . we discuss the sorts of morphologies which cause the greatest need for such an interface .

word sense disambiguation improves statistical machine translation yee seng chan and hwee tou ng
recent research presents conflicting evidence on whether word sense disambiguation ( wsd ) systems can help to improve the performance of statistical machine translation ( mt ) systems . in this paper , we successfully integrate a state-of-the-art wsd system into a state-of-the-art hierarchical phrase-based mt system , hiero . we show for the first time that integrating a wsd system improves the performance of a state-ofthe-art statistical mt system on an actual translation task . furthermore , the improvement is statistically significant .

controlling gender equality with shallow nlp techniques
this paper introduces the gendercheck editor , a tool to check german texts for gender discriminatory formulations . it relays on shallow rule-based techniques as used in the controlled language authoring technology ( clat ) . the paper outlines major sources of gender imbalances in german texts . it gives a background on the underlying clat technology and describes the marking and annotation strategy to automatically detect and visualize the questionable pieces of text . the paper provides a detailed evaluation of the editor .

steps to excellence : simple inference with refined scoring of
much of the recent work on dependency parsing has been focused on solving inherent combinatorial problems associated with rich scoring functions . in contrast , we demonstrate that highly expressive scoring functions can be used with substantially simpler inference procedures . specifically , we introduce a sampling-based parser that can easily handle arbitrary global features . inspired by samplerank , we learn to take guided stochastic steps towards a high scoring parse . we introduce two samplers for traversing the space of trees , gibbs and metropolis-hastings with random walk . the model outperforms state-of-the-art results when evaluated on 14 languages of non-projective conll datasets . our sampling-based approach naturally extends to joint prediction scenarios , such as joint parsing and pos correction . the resulting method outperforms the best reported results on the catib dataset , approaching performance of parsing with gold tags .

exploring various knowledge in relation extraction
extracting semantic relationships between entities is challenging . this paper investigates the incorporation of diverse lexical , syntactic and semantic knowledge in feature-based relation extraction using svm . our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement . this suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking . we also demonstrate how semantic information such as wordnet and name list , can be used in feature-based relation extraction to further improve the performance . evaluation on the ace corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ace relation subtypes and significantly outperforms tree kernel-based systems by over 20 in f-measure on the 5 ace relation types .

generating artificial errors for grammatical error correction
this paper explores the generation of artificial errors for correcting grammatical mistakes made by learners of english as a second language . artificial errors are injected into a set of error-free sentences in a probabilistic manner using statistics from a corpus . unlike previous approaches , we use linguistic information to derive error generation probabilities and build corpora to correct several error types , including open-class errors . in addition , we also analyse the variables involved in the selection of candidate sentences . experiments using the nucle corpus from the conll 2013 shared task reveal that : 1 ) training on artificially created errors improves precision at the expense of recall and 2 ) different types of linguistic information are better suited for correcting different error types .

a process for predicting mooc attrition
the goal of this shared task was to predict attrition in a mooc through use of the data and logs generated by the course . our approach to the task reinforces the idea that the process of gathering and structuring the data is more important ( and more time consuming ) than the predictive model itself . the result of the analysis was that a subset of 15 different data features did a sufficiently good job at predicting whether or not a student would exhibit any activity in the following week .

hybrid approach to user intention modeling for dialog simulation
this paper proposes a novel user intention simulation method which is a data-driven approach but able to integrate diverse user discourse knowledge together to simulate various type of users . in markov logic framework , logistic regression based data-driven user intention modeling is introduced , and human dialog knowledge are designed into two layers such as domain and discourse knowledge , then it is integrated with the data-driven model in generation time . cooperative , corrective and selfdirecting discourse knowledge are designed and integrated to mimic such type of users . experiments were carried out to investigate the patterns of simulated users , and it turned out that our approach was successful to generate user intention patterns which are not only unseen in the training corpus and but also personalized in the designed direction .

data-driven response generation in social media
we present a data-driven approach to generating responses to twitter status posts , based on phrase-based statistical machine translation . we find that mapping conversational stimuli onto responses is more difficult than translating between languages , due to the wider range of possible responses , the larger fraction of unaligned words/phrases , and the presence of large phrase pairs whose alignment can not be further decomposed . after addressing these challenges , we compare approaches based on smt and information retrieval in a human evaluation . we show that smt outperforms ir on this task , and its output is preferred over actual human responses in 15 % of cases . as far as we are aware , this is the first work to investigate the use of phrase-based smt to directly translate a linguistic stimulus into an appropriate response .

genedisease association extraction by text mining and network
biomedical relations play an important role in biological processes . in this work , we combine information filtering , grammar parsing and network analysis for gene-disease association extraction . the proposed method first extracts sentences potentially containing information about gene-diseases interactions based on maximum entropy classifier with topic features . and then probabilistic contextfree grammars is applied for gene-disease association extraction . the network of genes and the disease is constituted by the extracted interactions , network centrality metrics are used for calculating the importance of each gene . we used breast cancer as testing disease for system evaluation . the 31 top ranked genes and diseases by the weighted degree , betweenness , and closeness centralities have been checked relevance with breast cancer through ncbi database . the evaluation showed 83.9 % accuracy for the testing genes and diseases , 74.2 % accuracy for the testing genes .

learning entailment rules for unary templates
most work on unsupervised entailment rule acquisition focused on rules between templates with two variables , ignoring unary rules - entailment rules between templates with a single variable . in this paper we investigate two approaches for unsupervised learning of such rules and compare the proposed methods with a binary rule learning method . the results show that the learned unary rule-sets outperform the binary rule-set . in addition , a novel directional similarity measure for learning entailment , termed balanced-inclusion , is the best performing measure .

unsupervised analysis for decipherment problems
we study a number of natural language decipherment problems using unsupervised learning . these include letter substitution ciphers , character code conversion , phonetic decipherment , and word-based ciphers with relevance to machine translation . straightforward unsupervised learning techniques most often fail on the first try , so we describe techniques for understanding errors and significantly increasing performance .

developing feature types for classifying clinical notes
this paper proposes a machine learning approach to the task of assigning the international standard on classification of diseases icd-9-cm codes to clinical records . by treating the task as a text categorisation problem , a classification system was built which explores a variety of features including negation , different strategies of measuring gloss overlaps between the content of clinical records and icd-9-cm code descriptions together with expansion of the glosses from the icd-9-cm hierarchy . the best classifier achieved an overall f1 value of 88.2 on a data set of 978 free text clinical records , and was better than the performance of two out of three human annotators .

detecting relations in the gene regulation network
the bionlp shared task 2013 is organised to further advance the field of information extraction in biomedical texts . this paper describes our entry in the gene regulation network in bacteria ( grn ) part , for which our system finished in second place ( out of five ) . to tackle this relation extraction task , we employ a basic support vector machine framework . we discuss our findings in constructing local and contextual features , that augment our precision with as much as 7.5 % . we touch upon the interaction type hierarchy inherent in the problem , and the importance of the evaluation procedure to encourage exploration of that structure .

aug : a combined classification and clustering approach for web people computational web intelligence
this paper presents a combined supervised and unsupervised approach for multidocument person name disambiguation . based on feature vectors reflecting pairwise comparisons between web pages , a classification algorithm provides linking information about document pairs , which leads to initial clusters . in addition , two different clustering algorithms are fed with matrices of weighted keywords . in a final step the seed clusters are combined with the results of the clustering algorithms . results on the validation data show that a combined classification and clustering approach doesnt always compare favorably to those obtained by the different algorithms separately .

sao carlos - sp , brazil
this paper details the system nilc usp that participated in the semeval 2014 : aspect based sentiment analysis task . this system uses a conditional random field ( crf ) algorithm for extracting the aspects mentioned in the text . our work added semantic labels into a basic feature set for measuring the efficiency of those for aspect extraction . we used the semantic roles and the highest verb frame as features for the machine learning . overall , our results demonstrated that the system could not improve with the use of this semantic information , but its precision was increased .

exploring lexicalized features for coreference resolution
in this paper , we describe a coreference solver based on the extensive use of lexical features and features extracted from dependency graphs of the sentences . the solver uses soon et al ( 2001 ) s classical resolution algorithm based on a pairwise classification of the mentions . we applied this solver to the closed track of the conll 2011 shared task ( pradhan et al , 2011 ) . we carried out a systematic optimization of the feature set using cross-validation that led us to retain 24 features . using this set , we reached a muc score of 58.61 on the test set of the shared task . we analyzed the impact of the features on the development set and we show the importance of lexicalization as well as of properties related to dependency links in coreference resolution .

learning to freestyle : hip hop challenge-response induction via transduction rule segmentation
we present a novel model , freestyle , that learns to improvise rhyming and fluent responses upon being challenged with a line of hip hop lyrics , by combining both bottomup token based rule induction and top-down rule segmentation strategies to learn a stochastic transduction grammar that simultaneously learns both phrasing and rhyming associations . in this attack on the woefully under-explored natural language genre of music lyrics , we exploit a strictly unsupervised transduction grammar induction approach . our task is particularly ambitious in that no use of any a priori linguistic or phonetic information is allowed , even though the domain of hip hop lyrics is particularly noisy and unstructured . we evaluate the performance of the learned model against a model learned only using the more conventional bottom-up token based rule induction , and demonstrate the superiority of our combined token based and rule segmentation induction method toward generating higher quality improvised responses , measured on fluency and rhyming criteria as judged by human evaluators . to highlight some of the inherent challenges in adapting other algorithms to this novel task , we also compare the quality of the responses generated by our model to those generated by an out-ofthe-box phrase based smt system . we tackle the challenge of selecting appropriate training data for our task via a dedicated rhyme scheme detection module , which is also acquired via unsupervised learning and report improved quality of the generated responses . finally , we report results with maghrebi french hip hop lyrics indicating that our model performs surprisingly well with no special adaptation to other languages .

learning semantic classes for word sense disambiguation
word sense disambiguation suffers from a long-standing problem of knowledge acquisition bottleneck . although state of the art supervised systems report good accuracies for selected words , they have not been shown to be promising in terms of scalability . in this paper , we present an approach for learning coarser and more general set of concepts from a sense tagged corpus , in order to alleviate the knowledge acquisition bottleneck . we show that these general concepts can be transformed to fine grained word senses using simple heuristics , and applying the technique for recent senseval data sets shows that our approach can yield state of the art performance .

an application of latent semantic analysis to word sense discrimination for words with related and unrelated meanings
we present an application of latent semantic analysis to word sense discrimination within a tutor for english vocabulary learning . we attempt to match the meaning of a word in a document with the meaning of the same word in a fill-in-the-blank question . we compare the performance of the lesk algorithm to latent semantic analysis . we also compare the performance of latent semantic analysis on a set of words with several unrelated meanings and on a set of words having both related and unrelated meanings .

rtm-dcu : referential translation machines for semantic similarity
we use referential translation machines ( rtms ) for predicting the semantic similarity of text . rtms are a computational model for identifying the translation acts between any two data sets with respect to interpretants selected in the same domain , which are effective when making monolingual and bilingual similarity judgments . rtms judge the quality or the semantic similarity of text by using retrieved relevant training data as interpretants for reaching shared semantics . we derive features measuring the closeness of the test sentences to the training data via interpretants , the difficulty of translating them , and the presence of the acts of translation , which may ubiquitously be observed in communication . rtms provide a language independent approach to all similarity tasks and achieve top performance when predicting monolingual cross-level semantic similarity ( task 3 ) and good results in semantic relatedness and entailment ( task 1 ) and multilingual semantic textual similarity ( sts ) ( task 10 ) . rtms remove the need to access any task or domain specific information or resource .

automatic linguistic analysis for language teachers : the case of zeros
this paper presents the natural language processing-based linguistic analysis tool that we have developed for japanese as a second language teachers . this program , zero detector ( zd ) , aims to promote effective instruction of zero anaphora , on the basis of a hypothesis about ideal conditions for second language acquisition , by making invisible zeros visible . zd takes japanese written narrative discourse as input and provides the zero-specified texts and their underlying structures as output . we evaluated zds performance in terms of its zero detecting accuracy . we also present an experimental report of its validity for practical use . as a result , zd has proven to be pedagogically feasible in terms of its accuracy and its impact on effective instruction .

an ontology for accessing transcription systems ( oats )
this paper presents the ontology for accessing transcription systems ( oats ) , a knowledge base that supports interoperation over disparate transcription systems and practical orthographies . the knowledge base includes an ontological description of writing systems and relations for mapping transcription system segments to an interlingua pivot , the ipa . it includes orthographic and phonemic inventories from 203 african languages . oats is motivated by the desire to query data in the knowledge base via ipa or native orthography , and for error checking of digitized data and conversion between transcription systems . the model in this paper implements these goals .

text specificity and impact on quality of news summaries
in our work we use an existing classifier to quantify and analyze the level of specific and general content in news documents and their human and automatic summaries . we discover that while human abstracts contain a more balanced mix of general and specific content , automatic summaries are overwhelmingly specific . we also provide an analysis of summary specificity and the summary quality scores assigned by people . we find that too much specificity could adversely affect the quality of content in the summary . our findings give strong evidence for the need for a new task in abstractive summarization : identification and generation of general sentences .

hierarchy extraction based on inclusion of appearance eiko yamamoto kyoko kanzaki hitoshi isahara
in this paper , we propose a method of automatically extracting word hierarchies based on the inclusion relation of appearance patterns from corpora . we apply a complementary similarity measure to find a hierarchical word structure . this similarity measure was developed for the recognition of degraded machineprinted text in the field and can be applied to estimate one-to-many relations . our purpose is to extract word hierarchies from corpora automatically . as the initial task , we attempt to extract hierarchies of abstract nouns cooccurring with adjectives in japanese and compare with hierarchies in the edr electronic dictionary .

toward hierarchical models for statistical machine translation of lehrstuhl fur informatik vi ,
in statistical machine translation , correspondences between the words in the source and the target language are learned from bilingual corpora on the basis of so called alignment models . existing statistical systems for mt often treat different derivatives of the same lemma as if they were independent of each other . in this paper we argue that a better exploitation of the bilingual training data can be achieved by explicitly taking into account the interdependencies of the different derivatives . we do this along two directions : usage of hierarchical lexicon

stopping criteria for active learning of named entity recognition
active learning is a proven method for reducing the cost of creating the training sets that are necessary for statistical nlp . however , there has been little work on stopping criteria for active learning . an operational stopping criterion is necessary to be able to use active learning in nlp applications . we investigate three different stopping criteria for active learning of named entity recognition ( ner ) and show that one of them , gradient-based stopping , ( i ) reliably stops active learning , ( ii ) achieves nearoptimal ner performance , ( iii ) and needs only about 20 % as much training data as exhaustive labeling .

deciphering foreign language
in this work , we tackle the task of machine translation ( mt ) without parallel training data . we frame the mt problem as a decipherment task , treating the foreign text as a cipher for english and present novel methods for training translation models from nonparallel text .

reordering model using syntactic information of a source tree for statistical machine translation
this paper presents a reordering model using syntactic information of a source tree for phrase-based statistical machine translation . the proposed model is an extension of istitg ( imposing source tree on inversion transduction grammar ) constraints . in the proposed method , the target-side word order is obtained by rotating nodes of the source-side parse-tree . we modeled the node rotation , monotone or swap , using word alignments based on a training parallel corpus and sourceside parse-trees . the model efficiently suppresses erroneous target word orderings , especially global orderings . furthermore , the proposed method conducts a probabilistic evaluation of target word reorderings . in englishto-japanese and english-to-chinese translation experiments , the proposed method resulted in a 0.49-point improvement ( 29.31 to 29.80 ) and a 0.33-point improvement ( 18.60 to 18.93 ) in word bleu-4 compared with ist-itg constraints , respectively . this indicates the validity of the proposed reordering model .

grounding action descriptions in videos michaela regneri , marcus rohrbach , dominikus wetzel , stefan thater , bernt schiele and manfred pinkal
recent work has shown that the integration of visual information into text-based models can substantially improve model predictions , but so far only visual information extracted from static images has been used . in this paper , we consider the problem of grounding sentences describing actions in visual information extracted from videos . we present a general purpose corpus that aligns high quality videos with multiple natural language descriptions of the actions portrayed in the videos , together with an annotation of how similar the action descriptions are to each other . experimental results demonstrate that a text-based model of similarity between actions improves substantially when combined with visual information from videos depicting the described actions .

k-best suffix arrays
suppose we have a large dictionary of strings . each entry starts with a figure of merit ( popularity ) . we wish to find the kbest matches for a substring , s , in a dictinoary , dict . that is , grep s dict | sort n | head k , but we would like to do this in sublinear time . example applications : ( 1 ) web queries with popularities , ( 2 ) products with prices and ( 3 ) ads with click through rates . this paper proposes a novel index , k-best suffix arrays , based on ideas borrowed from suffix arrays and kdtrees . a standard suffix array sorts the suffixes by a single order ( lexicographic ) whereas k-best suffix arrays are sorted by two orders ( lexicographic and popularity ) . lookup time is between log n and sqrt n.

leveraging multiple mt engines for paraphrase generation
this paper proposes a method that leverages multiple machine translation ( mt ) engines for paraphrase generation ( pg ) . the method includes two stages . firstly , we use a multi-pivot approach to acquire a set of candidate paraphrases for a source sentence s. then , we employ two kinds of techniques , namely the selection-based technique and the decoding-based technique , to produce a best paraphrase t for s using the candidates acquired in the first stage . experimental results show that : ( 1 ) the multi-pivot approach is effective for obtaining plenty of valuable candidate paraphrases . ( 2 ) both the selectionbased and decoding-based techniques can make good use of the candidates and produce high-quality paraphrases . moreover , these two techniques are complementary . ( 3 ) the proposed method outperforms a state-of-the-art paraphrase generation approach .

improved automatic keyword extraction given more linguistic knowledge
in this paper , experiments on automatic extraction of keywords from abstracts using a supervised machine learning algorithm are discussed . the main point of this paper is that by adding linguistic knowledge to the representation ( such as syntactic features ) , rather than relying only on statistics ( such as term frequency and ngrams ) , a better result is obtained as measured by keywords previously assigned by professional indexers . in more detail , extracting np-chunks gives a better precision than n-grams , and by adding the pos tag ( s ) assigned to the term as a feature , a dramatic improvement of the results is obtained , independent of the term selection approach applied .

mining inter-entity semantic relations using improved transductive learning
hidden in natural language text . in particular , it approaches the relation classification problem with the strategy of transductive learning . different algorithms are presented and empirically evaluated on the ace corpus . we show that transductive learners exploiting various lexical and syntactic features can achieve promising classification performance . more importantly , transductive learning performance can be significantly improved by using an induced similarity function .

the far reach of multiword expressions in educational technology educational testing service
multiword expressions as they appear as nominal compounds , collocational forms , and idioms are now leveraged in educational technology in assessment and instruction contexts . the talk will focus on how multiword expression identification is used in different kinds of educational applications , including automated essay evaluation , and teacher professional development in curriculum development for english language learners . recent approaches developed to resolve polarity for noun-noun compounds in a sentiment system being designed to handle evaluation of argumentation ( sentiment ) in testtaker writing ( beigman-klebanov , burstein , and madnani , to appear ) will also be described . about the speaker jill burstein is a managing principal research scientist in the research & development division at educational testing service in princeton , new jersey . her background and expertise is in computational linguistics with a focus on educational applications for writing , reading , and teacher professional development . she holds 13 patents for educational technology inventions . jills inventions include e-rater , an automated essay scoring and evaluation system . and , in more recent work , she has leveraged natural language processing to develop language musesm , a teacher professional development application that supports teachers in the development of languagebased instruction that aids english learner content understanding and language skills development . she received her b.a . in linguistics and spanish from new york university , and her m.a .

incremental joint extraction of entity mentions and relations
we present an incremental joint framework to simultaneously extract entity mentions and relations using structured perceptron with efficient beam-search . a segment-based decoder based on the idea of semi-markov chain is adopted to the new framework as opposed to traditional token-based tagging . in addition , by virtue of the inexact search , we developed a number of new and effective global features as soft constraints to capture the interdependency among entity mentions and relations . experiments on automatic content extraction ( ace ) corpora demonstrate that our joint model significantly outperforms a strong pipelined baseline , which attains better performance than the best-reported end-to-end system .

microblog entity linking by leveraging extra posts
linking name mentions in microblog posts to a knowledge base , namely microblog entity linking , is useful for text mining tasks on microblog . entity linking in long text has been well studied in previous works . however few work has focused on short text such as microblog post . microblog posts are short and noisy . previous method can extract few features from the post context . in this paper we propose to use extra posts for the microblog entity linking task . experimental results show that our proposed method significantly improves the linking accuracy over traditional methods by 8.3 % and 7.5 % respectively .

an agreement measure for determining inter-annotator reliability of human judgements on affective text
an affective text may be judged to belong to multiple affect categories as it may evoke different affects with varying degree of intensity . for affect classification of text , it is often required to annotate text corpus with affect categories . this task is often performed by a number of human judges . this paper presents a new agreement measure inspired by kappa coefficient to compute inter-annotator reliability when the annotators have freedom to categorize a text into more than one class . the extended reliability coefficient has been applied to measure the quality of an affective text corpus . an analysis of the factors that influence corpus quality has been provided .

description of the ncu chinese word segmentation and part-of-speech
in chinese , most of the language processing starts from word segmentation and part-of-speech ( pos ) tagging . these two steps tokenize the word from a sequence of characters and predict the syntactic labels for each segmented word . in this paper , we present two distinct sequential tagging models for the above two tasks . the first word segmentation model was basically similar to previous work which made use of conditional random fields ( crf ) and set of predefined dictionaries to recognize word boundaries . second , we revise and modify support vector machine-based chunking model to label the pos tag in the tagging task . our method in the ws task achieves moderately rank among all participants , while in the pos tagging task , it reaches very competitive results .

assigning terms to domains by document classification robert gaizauskas , emma barker , monica lestari paramita and ahmet aker
in this paper we investigate a number of questions relating to the identification of the domain of a term by domain classification of the document in which the term occurs . we propose and evaluate a straightforward method for domain classification of documents in 24 languages that exploits a multilingual thesaurus and wikipedia . we investigate and provide quantitative results about the extent to which humans agree about the domain classification of documents and terms also the extent to which terms are likely to inherit the domain of their parent document .

an incremental model for coreference resolution with restrictive
we introduce an incremental model for coreference resolution that competed in the conll 2011 shared task ( open regular ) . we decided to participate with our baseline model , since it worked well with two other datasets . the benefits of an incremental over a mention-pair architecture are : a drastic reduction of the number of candidate pairs , a means to overcome the problem of underspecified items in pairwise classification and the natural integration of global constraints such as transitivity . we do not apply machine learning , instead the system uses an empirically derived salience measure based on the dependency labels of the true mentions . our experiments seem to indicate that such a system already is on par with machine learning approaches .

learning from collective human behavior to introduce diversity in lexical choice
we analyze collective discourse , a collective human behavior in content generation , and show that it exhibits diversity , a property of general collective systems . using extensive analysis , we propose a novel paradigm for designing summary generation systems that reflect the diversity of perspectives seen in reallife collective summarization . we analyze 50 sets of summaries written by human about the same story or artifact and investigate the diversity of perspectives across these summaries . we show how different summaries use various phrasal information units ( i.e. , nuggets ) to express the same atomic semantic units , called factoids . finally , we present a ranker that employs distributional similarities to build a network of words , and captures the diversity of perspectives by detecting communities in this network . our experiments show how our system outperforms a wide range of other document ranking systems that leverage diversity .

the mitre corporation
this paper describes mitres participation in the native language identification ( nli ) task at bea-8 . our best effort performed at an accuracy of 82.6 % in the eleven-way nli task , placing it in a statistical tie with the best performing systems . we describe the variety of machine learning approaches that we explored , including winnow , language modeling , logistic regression and maximum-entropy models . our primary features were word and character n-grams . we also describe several ensemble methods that we employed for combining these base systems .

multi-domain sentiment classification
this paper addresses a new task in sentiment classification , called multi-domain sentiment classification , that aims to improve performance through fusing training data from multiple domains . to achieve this , we propose two approaches of fusion , feature-level and classifier-level , to use training data from multiple domains simultaneously . experimental studies show that multi-domain sentiment classification using the classifier-level approach performs much better than single domain classification ( using the training data individually ) .

high oov-recall chinese word segmenter
for the competition of chinese word segmentation held in the first cips-sighna joint conference . we applied a subwordbased word segmenter using crfs and extended the segmenter with oov words recognized by accessor variety . moreover , we proposed several post-processing rules to improve the performance . our system achieved promising oov recall among all the participants .

morpho-syntactic lexical generalization for ccg semantic parsing computer science & engineering computer science & engineering
in this paper , we demonstrate that significant performance gains can be achieved in ccg semantic parsing by introducing a linguistically motivated grammar induction scheme . we present a new morpho-syntactic factored lexicon that models systematic variations in morphology , syntax , and semantics across word classes . the grammar uses domain-independent facts about the english language to restrict the number of incorrect parses that must be considered , thereby enabling effective learning from less data . experiments in benchmark domains match previous models with one quarter of the data and provide new state-of-the-art results with all available data , including up to 45 % relative test-error reduction .

a generalized framework for revealing analogous themes across related topics zvika marx ido dagan eli shamir
this work addresses the task of identifying thematic correspondences across subcorpora focused on different topics . we introduce an unsupervised algorithmic framework based on distributional data clustering , which generalizes previous initial works on this task . the empirical results reveal interesting commonalities of different religions . we evaluate the results through measuring the overlap of our clusters with clusters compiled manually by experts . the tested variants of our framework are shown to outperform alternative methods applicable to the task .

effective selection of translation model training data
data selection has been demonstrated to be an effective approach to addressing the lack of high-quality bitext for statistical machine translation in the domain of interest . most current data selection methods solely use language models trained on a small scale in-domain data to select domain-relevant sentence pairs from general-domain parallel corpus . by contrast , we argue that the relevance between a sentence pair and target domain can be better evaluated by the combination of language model and translation model . in this paper , we study and experiment with novel methods that apply translation models into domain-relevant data selection . the results show that our methods outperform previous methods . when the selected sentence pairs are evaluated on an end-to-end mt task , our methods can increase the translation performance by 3 bleu points . *

treebank of chinese bible translations
this paper reports on a treebanking project where eight different modern chinese translations of the bible are syntactically analyzed . the trees are created through dynamic treebanking which uses a parser to produce the trees . the trees have been going through manual checking , but corrections are made not by editing the tree files but by re-generating the trees with an updated grammar and dictionary . the accuracy of the treebank is high due to the fact that the grammar and dictionary are optimized for this specific domain . the tree structures essentially follow the guidelines of the penn chinese treebank . the total number of characters covered by the treebank is 7,872,420 characters . the data has been used in bible translation and bible search . it should also prove useful in the computational study of the chinese language in general .

bootstrapping into filler-gap : an acquisition story marten van schijndel micha elsner
analyses of filler-gap dependencies usually involve complex syntactic rules or heuristics ; however recent results suggest that filler-gap comprehension begins earlier than seemingly simpler constructions such as ditransitives or passives . therefore , this work models filler-gap acquisition as a byproduct of learning word orderings ( e.g . svo vs osv ) , which must be done at a very young age anyway in order to extract meaning from language . specifically , this model , trained on part-of-speech tags , represents the preferred locations of semantic roles relative to a verb as gaussian mixtures over real numbers . this approach learns role assignment in filler-gap constructions in a manner consistent with current developmental findings and is extremely robust to initialization variance . additionally , this model is shown to be able to account for a characteristic error made by learners during this period ( a and b gorped interpreted as a gorped b ) .

active learning with confidence
active learning is a machine learning approach to achieving high-accuracy with a small amount of labels by letting the learning algorithm choose instances to be labeled . most of previous approaches based on discriminative learning use the margin for choosing instances . we present a method for incorporating confidence into the margin by using a newly introduced online learning algorithm and show empirically that confidence improves active learning .

ubiu : a robust system for resolving unrestricted coreference
in this paper , we discuss the application of ubiu to the conll-2011 shared task on modeling unrestricted coreference in ontonotes . the shared task concentrates on the detection of coreference not only in noun phrases but also involving verbs . the information provided for the closed track included wordnet as well as corpus generated number and gender information . our system shows no improvement when using wordnet information , and the number information proved less reliable than the information in the part of speech tags .

self-disclosure topic model for twitter conversations
self-disclosure , the act of revealing oneself to others , is an important social behavior that contributes positively to intimacy and social support from others . it is a natural behavior , and social scientists have carried out numerous quantitative analyses of it through manual tagging and survey questionnaires . recently , the flood of data from online social networks ( osn ) offers a practical way to observe and analyze self-disclosure behavior at an unprecedented scale . the challenge with such analysis is that osn data come with no annotations , and it would be impossible to manually annotate the data for a quantitative analysis of self-disclosure . as a solution , we propose a semi-supervised machine learning approach , using a variant of latent dirichlet allocation for automatically classifying self-disclosure in a massive dataset of twitter conversations . for measuring the accuracy of our model , we manually annotate a small subset of our dataset , and we show that our model shows significantly higher accuracy and f-measure than various other methods . with the results our model , we uncover a positive and significant relationship between self-disclosure and online conversation frequency over time .

composition of word representations
state-of-the-art semantic role labelling systems require large annotated corpora to achieve full performance . unfortunately , such corpora are expensive to produce and often do not generalize well across domains . even in domain , errors are often made where syntactic information does not provide sufficient cues . in this paper , we mitigate both of these problems by employing distributional word representations gathered from unlabelled data . while straight-forward word representations of predicates and arguments improve performance , we show that further gains are achieved by composing representations that model the interaction between predicate and argument , and capture full argument spans .

learning with conditional random fields
there is rich knowledge encoded in online web data . for example , punctuation and entity tags in wikipedia data define some word boundaries in a sentence . in this paper we adopt partial-label learning with conditional random fields to make use of this valuable knowledge for semi-supervised chinese word segmentation . the basic idea of partial-label learning is to optimize a cost function that marginalizes the probability mass in the constrained space that encodes this knowledge . by integrating some domain adaptation techniques , such as easyadapt , our result reaches an f-measure of 95.98 % on the ctb-6 corpus , a significant improvement from both the supervised baseline and a previous proposed approach , namely constrained decode .

graph-based approaches for organization entity resolution in mapreduce
entity resolution is the task of identifying which records in a database refer to the same entity . a standard machine learning pipeline for the entity resolution problem consists of three major components : blocking , pairwise linkage , and clustering . the blocking step groups records by shared properties to determine which pairs of records should be examined by the pairwise linker as potential duplicates . next , the linkage step assigns a probability score to pairs of records inside each block . if a pair scores above a user-defined threshold , the records are presumed to represent the same entity . finally , the clustering step turns the input records into clusters of records ( or profiles ) , where each cluster is uniquely associated with a single real-world entity . this paper describes the blocking and clustering strategies used to deploy a massive database of organization entities to power a major commercial people search engine . we demonstrate the viability of these algorithms for large data sets on a 50-node hadoop cluster .

extracting the native language signal for second language acquisition
we develop a method for effective extraction of linguistic patterns that are differentially expressed based on the native language of the author . this method uses multiple corpora to allow for the removal of data set specific patterns , and addresses both feature relevancy and redundancy . we evaluate different relevancy ranking metrics and show that common measures of relevancy can be inappropriate for data with many rare features . our feature set is a broad class of syntactic patterns , and to better capture the signal we extend the bayesian tree substitution grammar induction algorithm to a supervised mixture of latent grammars . we show that this extension can be used to extract a larger set of relevant features .

encoding tree pair-based graphs in learning algorithms : the textual entailment recognition case fabio massimo zanzotto
in this paper , we provide a statistical machine learning representation of textual entailment via syntactic graphs constituted by tree pairs . we show that the natural way of representing the syntactic relations between text and hypothesis consists in the huge feature space of all possible syntactic tree fragment pairs , which can only be managed using kernel methods . experiments with support vector machines and our new kernels for paired trees show the validity of our interpretation .

anatomy of annotation schemes : mapping to graf
in this paper , we apply the annotation scheme design methodology defined in ( bunt , 2010 ) and demonstrate its use for generating a mapping from an existing annotation scheme to a representation in graf format . the most important features of this methodology are ( 1 ) the distinction of the abstract and concrete syntax of an annotation language ; ( 2 ) the specification of a formal semantics for the abstract syntax ; and ( 3 ) the formalization of the relation between abstract and concrete syntax , which guarantees that any concrete syntax inherits the semantics of the abstract syntax , and thus guarantees meaning-preserving mappings between representation formats . by way of illustration , we apply this mapping strategy to annotations from isotimeml , propbank , and framenet .

extracting aspects and polarity from patents
we describe an approach to terminology extraction from patent corpora that follows from a view of patents as positive reviews of inventions . as in aspect-based sentiment analysis , we focus on identifying not only the components of products but also the attributes and tasks which , in the case of patents , serve to justify an inventions utility . these semantic roles ( component , task , attribute ) can serve as a high level ontology for categorizing domain terminology , within which the positive/negative polarity of attributes serves to identify technical goals and obstacles . we show that bootstrapping using a very small set of domain-independent lexico-syntactic features may be sufficient for constructing domainspecific classifiers capable of assigning semantic roles and polarity to terms in domains as diverse as computer science and health .

data-driven dependency parsing across languages and domains
the conference on computational natural language learning features a shared task , in which participants train and test their learning systems on the same data sets . in 2007 , as in 2006 , the shared task has been devoted to dependency parsing , this year with both a multilingual track and a domain adaptation track . in this paper , i summarize the main findings from the 2007 shared task and try to identify major challenges for the parsing community based on these findings .

unsupervised solution post identification from discussion forums
discussion forums have evolved into a dependable source of knowledge to solve common problems . however , only a minority of the posts in discussion forums are solution posts . identifying solution posts from discussion forums , hence , is an important research problem . in this paper , we present a technique for unsupervised solution post identification leveraging a so far unexplored textual feature , that of lexical correlations between problems and solutions . we use translation models and language models to exploit lexical correlations and solution post character respectively . our technique is designed to not rely much on structural features such as post metadata since such features are often not uniformly available across forums . our clustering-based iterative solution identification approach based on the em-formulation performs favorably in an empirical evaluation , beating the only unsupervised solution identification technique from literature by a very large margin . we also show that our unsupervised technique is competitive against methods that require supervision , outperforming one such technique comfortably .

a neural network approach to selectional preference acquisition irit & cnrs
this paper investigates the use of neural networks for the acquisition of selectional preferences . inspired by recent advances of neural network models for nlp applications , we propose a neural network model that learns to discriminate between felicitous and infelicitous arguments for a particular predicate . the model is entirely unsupervised preferences are learned from unannotated corpus data . we propose two neural network architectures : one that handles standard two-way selectional preferences and one that is able to deal with multi-way selectional preferences . the models performance is evaluated on a pseudo-disambiguation task , on which it is shown to achieve state of the art performance .

parsing croatian and serbian by using croatian dependency treebanks zeljko agic danijela merkler dasa berovic
we investigate statistical dependency parsing of two closely related languages , croatian and serbian . as these two morphologically complex languages of relaxed word order are generally under-resourced with the topic of dependency parsing still largely unaddressed , especially for serbian we make use of the two available dependency treebanks of croatian to produce state-of-the-art parsing models for both languages . we observe parsing accuracy on four test sets from two domains . we give insight into overall parser performance for croatian and serbian , impact of preprocessing for lemmas and morphosyntactic tags and influence of selected morphosyntactic features on parsing accuracy .

rnn-based derivation structure prediction for smt
in this paper , we propose a novel derivation structure prediction ( dsp ) model for smt using recursive neural network ( rnn ) . within the model , two steps are involved : ( 1 ) phrase-pair vector representation , to learn vector representations for phrase pairs ; ( 2 ) derivation structure prediction , to generate a bilingual rnn that aims to distinguish good derivation structures from bad ones . final experimental results show that our dsp model can significantly improve the translation quality .

language model adaptation with map estimation and the perceptron algorithm
in this paper , we contrast two language model adaptation approaches : map estimation and the perceptron algorithm . used in isolation , we show that map estimation outperforms the latter approach , for reasons which argue for combining the two approaches . when combined , the resulting system provides a 0.7 percent absolute reduction in word error rate over map estimation alone . in addition , we demonstrate that , in a multi-pass recognition scenario , it is better to use the perceptron algorithm on early pass word lattices , since the improved error rate improves acoustic model adaptation .

specification in terms of interactional properties as a way to optimize the representation of spatial expressions
the results of the study demonstrate that numerous object-specific restrictions on the use of projective prepositions in english and russian are predicted by their interactional ( functional ) semantic properties . object-independent perceptual properties ( such as distance between objects , direction of their motion , etc ) that seemingly guide the use of the expressions , are also found to be presupposed by their interactional properties . based on these findings , it is suggested that in addition to a basic geometrical specification , the semantic representation should contain functional information . a computational procedure of matching an expression with a spatial scene should thus include detection of the interactional properties of the scene . they can be determined through ( 1 ) retrieval of information about interactional properties of specific objects and ( 2 ) determining functionally relevant objectindependent perceptual properties of the scene .

nonconvex global optimization for latent-variable models
many models in nlp involve latent variables , such as unknown parses , tags , or alignments . finding the optimal model parameters is then usually a difficult nonconvex optimization problem . the usual practice is to settle for local optimization methods such as em or gradient ascent . we explore how one might instead search for a global optimum in parameter space , using branch-and-bound . our method would eventually find the global maximum ( up to a user-specified ) if run for long enough , but at any point can return a suboptimal solution together with an upper bound on the global maximum . as an illustrative case , we study a generative model for dependency parsing . we search for the maximum-likelihood model parameters and corpus parse , subject to posterior constraints . we show how to formulate this as a mixed integer quadratic programming problem with nonlinear constraints . we use the reformulation linearization technique to produce convex relaxations during branch-and-bound . although these techniques do not yet provide a practical solution to our instance of this np-hard problem , they sometimes find better solutions than viterbi em with random restarts , in the same time .

towards semantic validation of a derivational lexicon
derivationally related lemmas like friend n friendly a friendship n are derived from a common stem . frequently , their meanings are also systematically related . however , there are also many examples of derivationally related lemma pairs whose meanings differ substantially , e.g. , object n objective n . most broad-coverage derivational lexicons do not reflect this distinction , mixing up semantically related and unrelated word pairs . in this paper , we investigate strategies to recover the above distinction by recognizing semantically related lemma pairs , a process we call semantic validation . we make two main contributions : first , we perform a detailed data analysis on the basis of a large german derivational lexicon . it reveals two promising sources of information ( distributional semantics and structural information about derivational rules ) , but also systematic problems with these sources . second , we develop a classification model for the task that reflects the noisy nature of the data . it achieves an improvement of 13.6 % in precision and 5.8 % in f1-score over a strong majority class baseline . our experiments confirm that both information sources contribute to semantic validation , and that they are complementary enough that the best results are obtained from a combined model .

graph-based keyword extraction for single-document summarization information system engineering information system engineering
in this paper , we introduce and compare between two novel approaches , supervised and unsupervised , for identifying the keywords to be used in extractive summarization of text documents . both our approaches are based on the graph-based syntactic representation of text and web documents , which enhances the traditional vector-space model by taking into account some structural document features . in the supervised approach , we train classification algorithms on a summarized collection of documents with the purpose of inducing a keyword identification model . in the unsupervised approach , we run the hits algorithm on document graphs under the assumption that the top-ranked nodes should represent the document keywords . our experiments on a collection of benchmark summaries show that given a set of summarized training documents , the supervised classification provides the highest keyword identification accuracy , while the highest f-measure is reached with a simple degree-based ranking . in addition , it is sufficient to perform only the first iteration of hits rather than running it to its convergence .

learning to win by reading manuals in a monte-carlo framework
this paper presents a novel approach for leveraging automatically extracted textual knowledge to improve the performance of control applications such as games . our ultimate goal is to enrich a stochastic player with highlevel guidance expressed in text . our model jointly learns to identify text that is relevant to a given game state in addition to learning game strategies guided by the selected text . our method operates in the monte-carlo search framework , and learns both text analysis and game strategies based only on environment feedback . we apply our approach to the complex strategy game civilization ii using the official game manual as the text guide . our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart , yielding a 27 % absolute improvement and winning over 78 % of games when playing against the builtin ai of civilization ii .

simple effective decipherment via combinatorial optimization
we present a simple objective function that when optimized yields accurate solutions to both decipherment and cognate pair identification problems . the objective simultaneously scores a matching between two alphabets and a matching between two lexicons , each in a different language . we introduce a simple coordinate descent procedure that efficiently finds effective solutions to the resulting combinatorial optimization problem . our system requires only a list of words in both languages as input , yet it competes with and surpasses several state-of-the-art systems that are both substantially more complex and make use of more information .

evaluation of speech dialog strategies for internet applications in the car
due to the mobile internet revolution , people tend to browse the web while driving their car which puts the drivers safety at risk . therefore , an intuitive and nondistractive in-car speech interface to the web needs to be developed . before developing a new speech dialog system in a new domain developers have to examine what the users preferred interaction style is in order to use such a system . this paper reports from a very recent driving simulation study and its preliminary results which are conducted in order to compare different speech dialog strategies . the use of command-based and conversational sds prototypes while driving is evaluated on usability and driving performance . different guis are designed in order to support the respective dialog strategy the most and to evaluate the effect of the gui on usability and driver distraction . the preliminary results show that the conversational speech dialog performs more efficient than the command-based dialog . however , the conversational dialog distracts more from driving than the command-based . furthermore , the results indicate that an sds supported by a gui is more efficient and better accepted by the user than without gui .

evaluate with confidence estimation : machine ranking of translation outputs using grammatical features language technology ( lt ) , berlin , germany
we present a pilot study on an evaluation method which is able to rank translation outputs with no reference translation , given only their source sentence . the system employs a statistical classifier trained upon existing human rankings , using several features derived from analysis of both the source and the target sentences . development experiments on one language pair showed that the method has considerably good correlation with human ranking when using features obtained from a pcfg parser .

fostering digital inclusion and accessibility : the porsimples project for simplification of portuguese texts sandra maria alusio and caroline gasperin
in this paper we present the porsimples project , whose aim is to develop text adaptations tools for brazilian portuguese . the tools developed cater for both people at poor literacy levels and authors that want to produce texts for this audience . here we describe the tools and resources developed over two years of this project and point directions for future work and collaboration . since portuguese and spanish have many aspects in common , we believe our main point for collaboration lies in transferring our knowledge and experience to researches willing to developed simplification and elaboration tools for spanish .

why implementation matters : evaluation of an open-source constraint
in recent years , the problem of finite-state constraint grammar ( cg ) parsing has received renewed attention . several compilers have been proposed to convert cg rules to finite-state transducers . while these formalisms serve their purpose as proofs of the concept , the performance of the generated transducers lags behind other cg implementations and taggers . in this paper , we argue that the fault lies with using generic finite-state libraries , and not with the formalisms themselves . we present an open-source implementation that capitalises on the characteristics of cg rule application to improve execution time . on smaller grammars our implementation achieves performance comparable to the current open-source state of the art .

hindi and marathi to english cross language information
in this paper , we present our hindi - > english and marathi - > english clir systems developed as part of our participation in the clef 2007 ad-hoc bilingual task . we take a query translation based approach using bi-lingual dictionaries . query words not found in the dictionary are transliterated using a simple lookup table based transliteration approach . the resultant transliteration is then compared with the index items of the corpus to return the `k ' closest english index words of the given hindi/marathi word . the resulting multiple translation/transliteration choices for each query word are disambiguated using an iterative page-rank style algorithm , proposed in the literature , which makes use of term-term co-occurrence statistics to produce the final translated query . using the above approach , for hindi , we achieve a mean average precision ( map ) of 0.2366 in title which is 61.36 % of monolingual performance and a map of 0.2952 in title and description which is 67.06 % of monolingual performance . for marathi , we achieve a map of 0.2163 in title which is 56.09 % of monolingual performance .

a robust and extensible exemplar-based model of thematic fit
this paper presents a new , exemplar-based model of thematic fit . in contrast to previous models , it does not approximate thematic fit as argument plausibility or fit with verb selectional preferences , but directly as semantic role plausibility for a verb-argument pair , through similaritybased generalization from previously seen verb-argument pairs . this makes the model very robust for data sparsity . we argue that the model is easily extensible to a model of semantic role ambiguity resolution during online sentence comprehension . the model is evaluated on human semantic role plausibility judgments . its predictions correlate significantly with the human judgments . it rivals two state-of-theart models of thematic fit and exceeds their performance on previously unseen or lowfrequency items .

minimally supervised method for multilingual paraphrase extraction from definition sentences on the web yulan yan chikara hashimoto kentaro torisawa
we propose a minimally supervised method for multilingual paraphrase extraction from definition sentences on the web . hashimoto et al ( 2011 ) extracted paraphrases from japanese definition sentences on the web , assuming that definition sentences defining the same concept tend to contain paraphrases . however , their method requires manually annotated data and is language dependent . we extend their framework and develop a minimally supervised method applicable to multiple languages . our experiments show that our method is comparable to hashimoto et als for japanese and outperforms previous unsupervised methods for english , japanese , and chinese , and that our method extracts 10,000 paraphrases with 92 % precision for english , 82.5 % precision for japanese , and 82 % precision for chinese .

beyond the pipeline : discrete optimization in nlp
we present a discrete optimization model based on a linear programming formulation as an alternative to the cascade of classiers implemented in many language processing systems . since nlp tasks are correlated with one another , sequential processing does not guarantee optimal solutions . we apply our model in an nlg application and show that it performs better than a pipeline-based system .

bagpack : a general framework to represent semantic relations
we introduce a way to represent word pairs instantiating arbitrary semantic relations that keeps track of the contexts in which the words in the pair occur both together and independently . the resulting features are of sufficient generality to allow us , with the help of a standard supervised machine learning algorithm , to tackle a variety of unrelated semantic tasks with good results and almost no task-specific tailoring .

content models with attitude
we present a probabilistic topic model for jointly identifying properties and attributes of social media review snippets . our model simultaneously learns a set of properties of a product and captures aggregate user sentiments towards these properties . this approach directly enables discovery of highly rated or inconsistent properties of a product . our model admits an efficient variational meanfield inference algorithm which can be parallelized and run on large snippet collections . we evaluate our model on a large corpus of snippets from yelp reviews to assess property and attribute prediction . we demonstrate that it outperforms applicable baselines by a considerable margin .

integrating translation memory into phrase-based machine translation during decoding
since statistical machine translation ( smt ) and translation memory ( tm ) complement each other in matched and unmatched regions , integrated models are proposed in this paper to incorporate tm information into phrase-based smt . unlike previous multi-stage pipeline approaches , which directly merge tm result into the final output , the proposed models refer to the corresponding tm information associated with each phrase at smt decoding . on a chineseenglish tm database , our experiments show that the proposed integrated model-iii is significantly better than either the smt or the tm systems when the fuzzy match score is above 0.4. furthermore , integrated model-iii achieves overall 3.48 bleu points improvement and 2.62 ter points reduction in comparison with the pure smt system . besides , the proposed models also outperform previous approaches significantly .

learning reliability of parses for domain adaptation of
the accuracy of parsing has exceeded 90 % recently , but this is not high enough to use parsing results practically in natural language processing ( nlp ) applications such as paraphrase acquisition and relation extraction . we present a method for detecting reliable parses out of the outputs of a single dependency parser . this technique is also applied to domain adaptation of dependency parsing . our goal was to improve the performance of a state-of-the-art dependency parser on the data set of the domain adaptation track of the conll 2007 shared task , a formidable challenge .

unsupervised consonant-vowel prediction over hundreds of languages
in this paper , we present a solution to one aspect of the decipherment task : the prediction of consonants and vowels for an unknown language and alphabet . adopting a classical bayesian perspective , we performs posterior inference over hundreds of languages , leveraging knowledge of known languages and alphabets to uncover general linguistic patterns of typologically coherent language clusters . we achieve average accuracy in the unsupervised consonant/vowel prediction task of 99 % across 503 languages . we further show that our methodology can be used to predict more fine-grained phonetic distinctions . on a three-way classification task between vowels , nasals , and nonnasal consonants , our model yields unsupervised accuracy of 89 % across the same set of languages .

response-based confidence annotation for spoken dialogue systems
spoken and multimodal dialogue systems typically make use of confidence scores to choose among ( or reject ) a speech recognizers nbest hypotheses for a particular utterance . we argue that it is beneficial to instead choose among a list of candidate system responses . we propose a novel method in which a confidence score for each response is derived from a classifier trained on acoustic and lexical features emitted by the recognizer , as well as features culled from the generation of the candidate response itself . our responsebased method yields statistically significant improvements in f-measure over a baseline in which hypotheses are chosen based on recognition confidence scores only .

topic detection based on dialogue history
in this paper , we propose a topic detection method using a dialogue history for selecting a scene in the automatic interpretation system ( ikeda et al , 2002 ) . the method uses a k-nearest neighbor method for the algorithm , automatically clusters target topics into smaller topics grouped by similarity , and incorporates dialogue history weighted in terms of time to detect and track topics on spoken phrases . from the evaluation of detection performance using test corpus comprised of realistic spoken dialogue , the method has shown to perform better with clustering incorporated , and combined with time-weighted dialogue history of three sentences , gives detection accuracy of 77.0 % .

retrieving correct semantic boundaries in dependency structure
this paper describes the retrieval of correct semantic boundaries for predicateargument structures annotated by dependency structure . unlike phrase structure , in which arguments are annotated at the phrase level , dependency structure does not have phrases so the argument labels are associated with head words instead : the subtree of each head word is assumed to include the same set of words as the annotated phrase does in phrase structure . however , at least in english , retrieving such subtrees does not always guarantee retrieval of the correct phrase boundaries . in this paper , we present heuristics that retrieve correct phrase boundaries for semantic arguments , called semantic boundaries , from dependency trees . by applying heuristics , we achieved an f1-score of 99.54 % for correct representation of semantic boundaries . furthermore , error analysis showed that some of the errors could also be considered correct , depending on the interpretation of the annotation .

ontosem methods for processing semantic ellipsis
this paper describes various types of semantic ellipsis and underspecification in natural language , and the ways in which the meaning of semantically elided elements is reconstructed in the ontological semantics ( ontosem ) text processing environment . the description covers phenomena whose treatment in ontosem has reached various levels of advancement : fully implemented , partially implemented , and described algorithmically outside of implementation . we present these research results at this point prior to full implementation and extensive evaluation for two reasons : first , new descriptive material is being reported ; second , some subclasses of the phenomena in question will require a truly long-term effort whose results are best reported in installments .

using the web for language independent spellchecking and casey whitelaw and ben hutchinson and grace y chung and gerard ellis
we have designed , implemented and evaluated an end-to-end system spellchecking and autocorrection system that does not require any manually annotated training data . the world wide web is used as a large noisy corpus from which we infer knowledge about misspellings and word usage . this is used to build an error model and an n-gram language model . a small secondary set of news texts with artificially inserted misspellings are used to tune confidence classifiers . because no manual annotation is required , our system can easily be instantiated for new languages . when evaluated on human typed data with real misspellings in english and german , our web-based systems outperform baselines which use candidate corrections based on hand-curated dictionaries . our system achieves 3.8 % total error rate in english . we show similar improvements in preliminary results on artificial data for russian and arabic .

towards full automation of lexicon construction fair isaac corporation fair isaac corporation
we describe work in progress aimed at developing methods for automatically constructing a lexicon using only statistical data derived from analysis of corpora , a problem we call lexical optimization . specifically , we use statistical methods alone to obtain information equivalent to syntactic categories , and to discover the semantically meaningful units of text , which may be multi-word units or polysemous terms-incontext . our guiding principle is to employ a notion of meaningfulness that can be quantified information-theoretically , so that plausible variants of a lexicon can be judged relative to each other . we describe a technique of this nature called information theoretic co-clustering and give results of a series of experiments built around it that demonstrate the main ingredients of lexical optimization . we conclude by describing our plans for further improvements , and for applying the same mathematical principles to other problems in natural language processing .

automatic generation of information-seeking questions using concept
one of the basic problems of efficiently generating information-seeking dialogue in interactive question answering is to find the topic of an information-seeking question with respect to the answer documents . in this paper we propose an approach to solving this problem using concept clusters . our empirical results on trec collections and our ambiguous question collection shows that this approach can be successfully employed to handle ambiguous and list questions .

parsing with treebank grammars : empirical bounds , theoretical models , and the structure of the penn treebank
this paper presents empirical studies and closely corresponding theoretical models of the performance of a chart parser exhaustively parsing the penn treebank with the treebanks own cfg grammar . we show how performance is dramatically affected by rule representation and tree transformations , but little by top-down vs. bottom-up strategies . we discuss grammatical saturation , including analysis of the strongly connected components of the phrasal nonterminals in the treebank , and model how , as sentence length increases , the effective grammar rule size increases as regions of the grammar are unlocked , yielding super-cubic observed time behavior in some configurations .

evaluating neighbor rank and distance measures as predictors of semantic
this paper summarizes the results of a large-scale evaluation study of bag-ofwords distributional models on behavioral data from three semantic priming experiments . the tasks at issue are ( i ) identification of consistent primes based on their semantic relatedness to the target and ( ii ) correlation of semantic relatedness with latency times . we also provide an evaluation of the impact of specific model parameters on the prediction of priming . to the best of our knowledge , this is the first systematic evaluation of a wide range of dsm parameters in all possible combinations . an important result of the study is that neighbor rank performs better than distance measures in predicting semantic priming .

conll-x shared task on multilingual dependency parsing communication & cognition
each year the conference on computational natural language learning ( conll ) 1 features a shared task , in which participants train and test their systems on exactly the same data sets , in order to better compare systems . the tenth conll ( conll-x ) saw a shared task on multilingual dependency parsing . in this paper , we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured . we also give an overview of the parsing approaches that participants took and the results that they achieved . finally , we try to draw general conclusions about multi-lingual parsing : what makes a particular language , treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser acknowledgement many thanks to amit dubey and yuval krymolowski , the other two organizers of the shared task , for discussions , converting treebanks , writing software and helping with the papers.

inducing information structures for data-driven text analysis
we report ongoing work that is aiming to develop a data-driven approach to text analysis for computational social science . the novel feature is the use of a grammar induction algorithm to identify salient information structures from an unannotated text corpus . the structures provide richer representations of text content than keywords , by capturing patterning related to what is written about key terms . here we show how information structures were induced from texts that record political negotiations , and how the structures were used in analyzing relations between countries and negotiation positions .

relationship between non-projective edges , their level types ,
dependency analysis of natural language gives rise to non-projective structures . the constraint of well-nestedness on dependency trees has been recently shown to give a good fit with empirical linguistic data . we present a reformulation of this constraint using properties of nonprojective edges and show its formal relationship to level types of non-projective edges ; we also derive a simple o ( n2 ) algorithm for checking well-nestedness .

multilingual conceptual access to lexicon based on shared orthography : an ontology-driven study of chinese and japanese
in this paper we propose a model for conceptual access to multilingual lexicon based on shared orthography . our proposal relies crucially on two facts : that both chinese and japanese conventionally use chinese orthography in their respective writing systems , and that the chinese orthography is anchored on a system of radical parts which encodes basic concepts . each orthographic unit , called hanzi and kanji respectively , contains a radical which indicates the broad semantic class of the meaning of that unit . our study utilizes the homomorphism between the chinese hanzi and japanese kanji systems to ide1ntify bilingual word correspondences . we use bilingual dictionaries , including wordnet , to verify semantic relation between the crosslingual pairs . these bilingual pairs are then mapped to an ontology constructed based on relations to the relation between the meaning of each character and thebasic concept of their radical parts . the conceptual structure of the radical ontology is proposed as a model for simultaneous conceptual access to both languages . a study based on words containing characters composed of the ( mouth ) radical is given to illustrate the proposal and the actual model .

linguistic redundancy in twitter fabio massimo zanzotto
in the last few years , the interest of the research community in micro-blogs and social media services , such as twitter , is growing exponentially . yet , so far not much attention has been paid on a key characteristic of microblogs : the high level of information redundancy . the aim of this paper is to systematically approach this problem by providing an operational definition of redundancy . we cast redundancy in the framework of textual entailment recognition . we also provide quantitative evidence on the pervasiveness of redundancy in twitter , and describe a dataset of redundancy-annotated tweets . finally , we present a general purpose system for identifying redundant tweets . an extensive quantitative evaluation shows that our system successfully solves the redundancy detection task , improving over baseline systems with statistical significance .

automatic cluster stopping with criterion functions and the gap statistic
senseclusters is a freely available system that clusters similar contexts . it can be applied to a wide range of problems , although here we focus on word sense and name discrimination . it supports several different measures for automatically determining the number of clusters in which a collection of contexts should be grouped . these can be used to discover the number of senses in which a word is used in a large corpus of text , or the number of entities that share the same name . there are three measures based on clustering criterion functions , and another on the gap statistic .

william gates building
this paper introduces a semi-supervised learning framework for creating training material , namely active annotation . the main intuition is that an unsupervised method is used to initially annotate imperfectly the data and then the errors made are detected automatically and corrected by a human annotator . we applied active annotation to named entity recognition in the biomedical domain and encouraging results were obtained . the main advantages over the popular active learning framework are that no seed annotated data is needed and that the reusability of the data is maintained . in addition to the framework , an efficient uncertainty estimation for hidden markov models is presented .

udel : extending reference generation to multiple entities
we report on an attempt to extend a reference generation system , originally designed only for main subjects , to generate references for multiple entities in a single document . this endeavor yielded three separate systems : one utilizing the original classifier , another with a retrained classifier , and a third taking advantage of new data to improve the identification of interfering antecedents . each subsequent system improved upon the results of the previous iteration .

text categorization using automatically acquired domain ontology
in this paper , we describe ontology-based text categorization in which the domain ontologies are automatically acquired through morphological rules and statistical methods . the ontology-based approach is a promising way for general information retrieval applications such as knowledge management or knowledge discovery . as a way to evaluate the quality of domain ontologies , we test our method through several experiments . automatically acquired domain ontologies , with or without manual editing , have been used for text categorization . the results are quite satisfactory . furthermore , we have developed an automatic method to evaluate the quality of our domain ontology .

localization of difficult-to-translate phrases
this paper studies the impact that difficult-totranslate source-language phrases might have on the machine translation process . we formulate the notion of difficulty as a measurable quantity ; we show that a classifier can be trained to predict whether a phrase might be difficult to translate ; and we develop a framework that makes use of the classifier and external resources ( such as human translators ) to improve the overall translation quality . through experimental work , we verify that by isolating difficult-to-translate phrases and processing them as special cases , their negative impact on the translation of the rest of the sentences can be reduced .

developing an arabic treebank : methods , guidelines , procedures , and tools
in this paper we address the following questions from our experience of the last two and a half years in developing a large-scale corpus of arabic text annotated for morphological information , part-of-speech , english gloss , and syntactic structure : ( a ) how did we leapfrog through the stumbling blocks of both methodology and training in setting up the penn arabic treebank ( atb ) annotation ? ( b ) how did we reconcile the penn treebank annotation principles and practices with the modern standard arabic ( msa ) traditional and more recent grammatical concepts ? ( c ) what are the current issues and nagging problems ? ( d ) what has been achieved and what are our future expectations ?

a probabilistic search for the best solution among partially completed
we consider the problem of identifying among many candidates a single best solution which jointly maximizes several domain-specific target functions . assuming that the candidate solutions can be generated incrementally , we model the error in prediction due to the incompleteness of partial solutions as a normally distributed random variable . using this model , we derive a probabilistic search algorithm that aims at finding the best solution without the necessity to complete and rank all candidate solutions . we do not assume a viterbi-type decoding , allowing a wider range of target functions . we evaluate the proposed algorithm on the problem of best parse identification , combining simple heuristic with more complex machine-learning based target functions . we show that the search algorithm is capable of identifying candidates with a very high score without completing a significant proportion of the candidate solutions .

disambiguating toponyms in news eric garbin inderjeet mani
this research is aimed at the problem of disambiguating toponyms ( place names ) in terms of a classification derived by merging information from two publicly available gazetteers . to establish the difficulty of the problem , we measured the degree of ambiguity , with respect to a gazetteer , for toponyms in news . we found that 67.82 % of the toponyms found in a corpus that were ambiguous in a gazetteer lacked a local discriminator in the text . given the scarcity of humanannotated data , our method used unsupervised machine learning to develop disambiguation rules . toponyms were automatically tagged with information about them found in a gazetteer . a toponym that was ambiguous in the gazetteer was automatically disambiguated based on preference heuristics . this automatically tagged data was used to train a machine learner , which disambiguated toponyms in a human-annotated news corpus at 78.5 % accuracy .

improving word alignment with language model based confidence scores
this paper describes the statistical machine translation systems submitted to the acl-wmt 2008 shared translation task . systems were submitted for two translation directions : englishspanish and spanishenglish . using sentence pair confidence scores estimated with source and target language models , improvements are observed on the newscommentary test sets . genre-dependent sentence pair confidence score and integration of sentence pair confidence score into phrase table are also investigated .

evolving new lexical association measures using genetic programming jan snajder bojana dalbelo basic sasa petrovic ivan sikiric
automatic extraction of collocations from large corpora has been the focus of many research efforts . most approaches concentrate on improving and combining known lexical association measures . in this paper , we describe a genetic programming approach for evolving new association measures , which is not limited to any specific language , corpus , or type of collocation . our preliminary experimental results show that the evolved measures outperform three known association measures .

multilingual dependency learning : a huge feature engineering method to semantic dependency parsing
this paper describes our system about multilingual semantic dependency parsing ( srlonly ) for our participation in the shared task of conll-2009 . we illustrate that semantic dependency parsing can be transformed into a word-pair classification problem and implemented as a single-stage machine learning system . for each input corpus , a large scale feature engineering is conducted to select the best fit feature template set incorporated with a proper argument pruning strategy . the system achieved the top average score in the closed challenge : 80.47 % semantic labeled f1 for the average score .

sentiment analysis on italian tweets
we describe twita , the first corpus of italian tweets , which is created via a completely automatic procedure , portable to any other language . we experiment with sentiment analysis on two datasets from twita : a generic collection and a topic-specific collection . the only resource we use is a polarity lexicon , which we obtain by automatically matching three existing resources thereby creating the first polarity database for italian . we observe that albeit shallow , our simple system captures polarity distinctions matching reasonably well the classification done by human judges , with differences in performance across polarity values and on the two sets .

generating concept map exercises from textbooks
in this paper we present a methodology for creating concept map exercises for students . concept mapping is a common pedagogical exercise in which students generate a graphical model of some domain . our method automatically extracts knowledge representations from a textbook and uses them to generate concept maps . the purpose of the study is to generate and evaluate these concept maps according to their accuracy , completeness , and pedagogy .

cascading xsl filters for content selection in multilingual
content selection is a key factor of any successful document generation system . this paper shows how a content selection algorithm has been implemented using an efficient combination of xml/xsl technology and the framework of rst for discourse modeling . the system generates multilingual documents adapted to user profiles in a learning environment for the web . this courseviewgenerator applies simplified rst schemes to the elaboration of a master document in xml from which content segments are chosen to suit the user 's needs . the personalisation of the document is achieved through the application of a sequence of filtering levels of text selection based on the user aspects given as input . these cascading filters are implemented in xsl .

considerations on the nature of metaphorical meaning arising from a computational treatment of metaphor interpetation
this paper argues that there need not be a full correspondence between source and target domains when interpreting metaphors . instead , inference is performed in the source domain , and conclusions transferred to the target . a description of a computer system , att-meta , that partially implements these ideas is provided .

opinion identification in spanish texts aiala ros dina wonsever
we present our work on the identification of opinions and its components : the source , the topic and the message . we describe a rule-based system for which we achieved a recall of 74 % and a precision of 94 % . experimentation with machine-learning techniques for the same task is currently underway .

automatic acquisition of named entity tagged corpus from world wide gary
in this paper , we present a method that automatically constructs a named entity ( ne ) tagged corpus from the web to be used for learning of named entity recognition systems . we use an ne list and an web search engine to collect web documents which contain the ne instances . the documents are refined through sentence separation and text refinement procedures and ne instances are finally tagged with the appropriate ne categories . our experiments demonstrates that the suggested method can acquire enough ne tagged corpus equally useful to the manually tagged one without any human intervention .

projecting pos tags and syntactic dependencies from english and french to polish in aligned corpora
this paper presents the first step to project pos tags and dependencies from english and french to polish in aligned corpora . both the english and french parts of the corpus are analysed with a pos tagger and a robust parser . the english/polish bi-text and the french/polish bi-text are then aligned at the word level with the giza++ package . the intersection of ibm-4 viterbi alignments for both translation directions is used to project the annotations from english and french to polish . the results show that the precision of direct projection vary according to the type of induced annotations as well as the source language . moreover , the performances are likely to be improved by defining regular conversion rules among pos tags and dependencies .

sentiment propagation via implicature constraints intelligent systems program
opinions may be expressed implicitly via inference over explicit sentiments and events that positively/negatively affect entities ( goodfor/badfor events ) . we investigate how such inferences may be exploited to improve sentiment analysis , given goodfor/badfor event information . we apply loopy belief propagation to propagate sentiments among entities . the graph-based model improves over explicit sentiment classification by 10 points in precision and , in an evaluation of the model itself , we find it has an 89 % chance of propagating sentiments correctly .

neighbors help : bilingual unsupervised wsd using context sudha bhingardive samiulla shaikh pushpak bhattacharyya
word sense disambiguation ( wsd ) is one of the toughest problems in nlp , and in wsd , verb disambiguation has proved to be extremely difficult , because of high degree of polysemy , too fine grained senses , absence of deep verb hierarchy and low inter annotator agreement in verb sense annotation . unsupervised wsd has received widespread attention , but has performed poorly , specially on verbs . recently an unsupervised bilingual em based algorithm has been proposed , which makes use only of the raw counts of the translations in comparable corpora ( marathi and hindi ) . but the performance of this approach is poor on verbs with accuracy level at 25-38 % . we suggest a modification to this mentioned formulation , using context and semantic relatedness of neighboring words . an improvement of 17 % 35 % in the accuracy of verb wsd is obtained compared to the existing em based approach . on a general note , the work can be looked upon as contributing to the framework of unsupervised wsd through context aware expectation maximization .

web text corpus for natural language processing
web text has been successfully used as training data for many nlp applications . while most previous work accesses web text through search engine hit counts , we created a web corpus by downloading web pages to create a topic-diverse collection of 10 billion words of english . we show that for context-sensitive spelling correction the web corpus results are better than using a search engine . for thesaurus extraction , it achieved similar overall results to a corpus of newspaper text . with many more words available on the web , better results can be obtained by collecting much larger web corpora .

learning where to look : modeling eye movements in reading
we propose a novel machine learning task that consists in learning to predict which words in a text are fixated by a reader . in a first pilot experiment , we show that it is possible to outperform a majority baseline using a transitionbased model with a logistic regression classifier and a very limited set of features . we also show that the model is capable of capturing frequency effects on eye movements observed in human readers .

reducing annotation effort on unbalanced corpus based on cost matrix
annotated corpora play a significant role in many nlp applications . however , annotation by humans is time-consuming and costly . in this paper , a high recall predictor based on a cost-sensitive learner is proposed as a method to semi-automate the annotation of unbalanced classes . we demonstrate the effectiveness of our approach in the context of one form of unbalanced task : annotation of transcribed human-human dialogues for presence/absence of uncertainty . in two data sets , our cost-matrix based method of uncertainty annotation achieved high levels of recall while maintaining acceptable levels of accuracy . the method is able to reduce human annotation effort by about 80 % without a significant loss in data quality , as demonstrated by an extrinsic evaluation showing that results originally achieved using manually-obtained uncertainty annotations can be replicated using semi-automatically obtained uncertainty annotations .

stochastic iterative alignment for machine translation evaluation
a number of metrics for automatic evaluation of machine translation have been proposed in recent years , with some metrics focusing on measuring the adequacy of mt output , and other metrics focusing on fluency . adequacy-oriented metrics such as bleu measure n-gram overlap of mt outputs and their references , but do not represent sentence-level information . in contrast , fluency-oriented metrics such as rouge-w compute longest common subsequences , but ignore words not aligned by the lcs . we propose a metric based on stochastic iterative string alignment ( sia ) , which aims to combine the strengths of both approaches . we compare sia with existing metrics , and find that it outperforms them in overall evaluation , and works specially well in fluency evaluation .

phrase based decoding using a discriminative model
in this paper , we present an approach to statistical machine translation that combines the power of a discriminative model ( for training a model for machine translation ) , and the standard beam-search based decoding technique ( for the translation of an input sentence ) . a discriminative approach for learning lexical selection and reordering utilizes a large set of feature functions ( thereby providing the power to incorporate greater contextual and linguistic information ) , which leads to an effective training of these models . this model is then used by the standard state-of-art moses decoder ( koehn et al , 2007 ) for the translation of an input sentence . we conducted our experiments on spanish-english language pair . we used maximum entropy model in our experiments . we show that the performance of our approach ( using simple lexical features ) is comparable to that of the state-of-art statistical mt system ( koehn et al , 2007 ) . when additional syntactic features ( pos tags in this paper ) are used , there is a boost in the performance which is likely to improve when richer syntactic features are incorporated in the model .

the viability of web-derived polarity lexicons leonid velikovich sasha blair-goldensohn kerry hannan ryan mcdonald
we examine the viability of building large polarity lexicons semi-automatically from the web . we begin by describing a graph propagation framework inspired by previous work on constructing polarity lexicons from lexical graphs . we then apply this technique to build an english lexicon that is significantly larger than those previously studied . crucially , this web-derived lexicon does not require wordnet , part-of-speech taggers , or other language-dependent resources typical of sentiment analysis systems . as a result , the lexicon is not limited to specific word classes e.g. , adjectives that occur in wordnet and in fact contains slang , misspellings , multiword expressions , etc . we evaluate a lexicon derived from english documents , both qualitatively and quantitatively , and show that it provides superior performance to previously studied lexicons , including one derived from wordnet .

scaling semantic parsers with on-the-fly ontology matching
we consider the challenge of learning semantic parsers that scale to large , open-domain problems , such as question answering with freebase . in such settings , the sentences cover a wide variety of topics and include many phrases whose meaning is difficult to represent in a fixed target ontology . for example , even simple phrases such as daughter and number of people living in can not be directly represented in freebase , whose ontology instead encodes facts about gender , parenthood , and population . in this paper , we introduce a new semantic parsing approach that learns to resolve such ontological mismatches . the parser is learned from question-answer pairs , uses a probabilistic ccg to build linguistically motivated logicalform meaning representations , and includes an ontology matching model that adapts the output logical forms for each target ontology . experiments demonstrate state-of-the-art performance on two benchmark semantic parsing datasets , including a nine point accuracy improvement on a recent freebase qa corpus .

a systematic exploration of diversity in machine translation
this paper addresses the problem of producing a diverse set of plausible translations . we present a simple procedure that can be used with any statistical machine translation ( mt ) system . we explore three ways of using diverse translations : ( 1 ) system combination , ( 2 ) discriminative reranking with rich features , and ( 3 ) a novel post-editing scenario in which multiple translations are presented to users . we find that diversity can improve performance on these tasks , especially for sentences that are difficult for mt .

an incremental bayesian model for learning syntactic categories
we present an incremental bayesian model for the unsupervised learning of syntactic categories from raw text . the model draws information from the distributional cues of words within an utterance , while explicitly bootstrapping its development on its own partiallylearned knowledge of syntactic categories . testing our model on actual child-directed data , we demonstrate that it is robust to noise , learns reasonable categories , manages lexical ambiguity , and in general shows learning behaviours similar to those observed in children .

machine learning of temporal relations
this paper investigates a machine learning approach for temporally ordering and anchoring events in natural language texts . to address data sparseness , we used temporal reasoning as an oversampling method to dramatically expand the amount of training data , resulting in predictive accuracy on link labeling as high as 93 % using a maximum entropy classifier on human annotated data . this method compared favorably against a series of increasingly sophisticated baselines involving expansion of rules derived from human intuitions .

name matching between chinese and roman scripts : machine complements human
there are generally many ways to transliterate a name from one language script into another . the resulting ambiguity can make it very difficult to untransliterate a name by reverse engineering the process . in this paper , we present a highly successful cross-script name matching system that we developed by combining the creativity of human intuition with the power of machine learning . our system determines whether a name in roman script and a name in chinese script match each other with an f-score of 96 % . in addition , for name pairs that satisfy a computational test , the f-score is 98 % .

third-order variational reranking on packed-shared dependency forests
we propose a novel forest reranking algorithm for discriminative dependency parsing based on a variant of eisners generative model . in our framework , we define two kinds of generative model for reranking . one is learned from training data offline and the other from a forest generated by a baseline parser on the fly . the final prediction in the reranking stage is performed using linear interpolation of these models and discriminative model . in order to efficiently train the model from and decode on a hypergraph data structure representing a forest , we apply extended inside/outside and viterbi algorithms . experimental results show that our proposed forest reranking algorithm achieves significant improvement when compared with conventional approaches .

example-based machine translation using dp-matching between
we propose a new approach under the example-based machine translation paradigm . first , the proposed approach retrieves the most similar example by carrying out dp-matching of the input sentence and example sentences while measuring the semantic distance of the words . second , the approach adjusts the gap between the input and the most similar example by using a bilingual dictionary . we show the results of a computational experiment .

explorations in sentence fusion
sentence fusion is a text-to-text ( revision-like ) generation task which takes related sentences as input and merges these into a single output sentence . in this paper we describe our ongoing work on developing a sentence fusion module for dutch . we propose a generalized version of alignment which not only indicates which words and phrases should be aligned but also labels these in terms of a small set of primitive semantic relations , indicating how words and phrases from the two input sentences relate to each other . it is shown that human labelers can perform this task with a high agreement ( fscore of .95 ) . we then describe and evaluate our adaptation of an existing automatic alignment algorithm , and use the resulting alignments , plus the semantic labels , in a generalized fusion and generation algorithm . a small-scale evaluation study reveals that most of the resulting sentences are adequate to good .

soft-supervised learning for text classification
we propose a new graph-based semisupervised learning ( ssl ) algorithm and demonstrate its application to document categorization . each document is represented by a vertex within a weighted undirected graph and our proposed framework minimizes the weighted kullback-leibler divergence between distributions that encode the class membership probabilities of each vertex . the proposed objective is convex with guaranteed convergence using an alternating minimization procedure . further , it generalizes in a straightforward manner to multi-class problems . we present results on two standard tasks , namely reuters-21578 and webkb , showing that the proposed algorithm significantly outperforms the state-of-the-art .

unsupervised recognition of literal and non-literal use of idiomatic expressions
we propose an unsupervised method for distinguishing literal and non-literal usages of idiomatic expressions . our method determines how well a literal interpretation is linked to the overall cohesive structure of the discourse . if strong links can be found , the expression is classified as literal , otherwise as idiomatic . we show that this method can help to tell apart literal and non-literal usages , even for idioms which occur in canonical form .

fully unsupervised word segmentation with bve and mdl
several results in the word segmentation literature suggest that description length provides a useful estimate of segmentation quality in fully unsupervised settings . however , since the space of potential segmentations grows exponentially with the length of the corpus , no tractable algorithm follows directly from the minimum description length ( mdl ) principle . therefore , it is necessary to generate a set of candidate segmentations and select between them according to the mdl principle . we evaluate several algorithms for generating these candidate segmentations on a range of natural language corpora , and show that the bootstrapped voting experts algorithm consistently outperforms other methods when paired with mdl .

correcting esl errors using phrasal smt techniques
this paper presents a pilot study of the use of phrasal statistical machine translation ( smt ) techniques to identify and correct writing errors made by learners of english as a second language ( esl ) . using examples of mass noun errors found in the chinese learner error corpus ( clec ) to guide creation of an engineered training set , we show that application of the smt paradigm can capture errors not well addressed by widely-used proofing tools designed for native speakers . our system was able to correct 61.81 % of mistakes in a set of naturallyoccurring examples of mass noun errors found on the world wide web , suggesting that efforts to collect alignable corpora of pre- and post-editing esl writing samples offer can enable the development of smt-based writing assistance tools capable of repairing many of the complex syntactic and lexical problems found in the writing of esl learners .

sentiment analysis using support vector machines with diverse information
this paper introduces an approach to sentiment analysis which uses support vector machines ( svms ) to bring together diverse sources of potentially pertinent information , including several favorability measures for phrases and adjectives and , where available , knowledge of the topic of the text . models using the features introduced are further combined with unigram models which have been shown to be effective in the past ( pang et al. , 2002 ) and lemmatized versions of the unigram models . experiments on movie review data from epinions.com demonstrate that hybrid svms which combine unigram-style feature-based svms with those based on real-valued favorability measures obtain superior performance , producing the best results yet published using this data . further experiments using a feature set enriched with topic information on a smaller dataset of music reviews handannotated for topic are also reported , the results of which suggest that incorporating topic information into such models may also yield improvement .

discourse processing for explanatory essays in tutorial applications
the why-atlas tutoring system presents students with qualitative physics questions and encourages them to explain their answers via natural language . although there are inexpensive techniques for analyzing explanations , we claim that better understanding is necessary for use within tutoring systems . in this paper we describe how why-atlas creates and utilizes a proof-based representation of student essays . we describe how it creates the proof given the output of sentence-level understanding , how it uses the proofs to give students feedback , some preliminary runtime measures , and the work we are currently doing to derive additional benefits from a proof-based approach for tutoring applications .

effective morphological feature selection
the inclusion of morphological features provides very useful information that helps to enhance the results when parsing morphologically rich languages . maltoptimizer is a tool , that given a data set , searches for the optimal parameters , parsing algorithm and optimal feature set achieving the best results that it can find for parsers trained with maltparser . in this paper , we present an extension of maltoptimizer that explores , one by one and in combination , the features that are geared towards morphology . from our experiments in the context of the shared task on parsing morphologically rich languages , we extract an in-depth study that shows which features are actually useful for transition-based parsing and we provide competitive results , in a fast and simple way .

eeg responds to conceptual stimuli and corpus semantics
mitchell et al ( 2008 ) demonstrated that corpus-extracted models of semantic knowledge can predict neural activation patterns recorded using fmri . this could be a very powerful technique for evaluating conceptual models extracted from corpora ; however , fmri is expensive and imposes strong constraints on data collection . following on experiments that demonstrated that eeg activation patterns encode enough information to discriminate broad conceptual categories , we show that corpus-based semantic representations can predict eeg activation patterns with significant accuracy , and we evaluate the relative performance of different corpus-models on this task .

english-japanese example-based machine translation using abstract
linguistic representations chris brockett , takako aikawa , anthony aue , arul menezes , chris quirk and hisami suzuki natural language processing group , microsoft research one microsoft way redmond , wa 98052 , usa { chrisbkt , takakoa , anthaue , arulm , chrisq , hisamis } @ microsoft.com abstract this presentation describes an example- based english-japanese machine trans- lation system in which an abstract linguistic representation layer is used to extract and store bilingual translation knowledge , transfer patterns between languages , and generate output strings . abstraction permits structural neutralizations that facilitate learning of translation examples across languages with radically different surface structure characteristics , and allows mt development to proceed within a largely language- independent nlp architecture . comparative evaluation indicates that after training in a domain the english-japanese system is statistically indistinguishable from a non-customized commercially available mt system in the same domain .

overcoming the customization bottleneck using example-based mt
we describe msr-mt , a large-scale hybrid machine translation system under development for several language pairs . this systems ability to acquire its primary translation knowledge automatically by parsing a bilingual corpus of hundreds of thousands of sentence pairs and aligning resulting logical forms demonstrates true promise for overcoming the so-called mt customization bottleneck . trained on english and spanish technical prose , a blind evaluation shows that msr-mts integration of rule-based parsers , example based processing , and statistical techniques produces translations whose quality exceeds that of uncustomized commercial mt systems in this domain .

a system for semantic analysis of chemical compound names
mapping and classification of chemical compound names are important aspects of the tasks of bionlp . this paper introduces the architecture of a system for the syntactic and semantic analysis of such names . our system aims at yielding both the denoted chemical structure and a classification of a given name . we employ a novel approach to the task which promises an elegant and efficient way of solving the problem . the proposed system differs significantly from existing systems , in that it is also able to deal with underspecifying names and class names .

instance-based ontology population exploiting named-entity substitution
we present an approach to ontology population based on a lexical substitution technique . it consists in estimating the plausibility of sentences where the named entity to be classified is substituted with the ones contained in the training data , in our case , a partially populated ontology . plausibility is estimated by using web data , while the classification algorithm is instance-based . we evaluated our method on two different ontology population tasks . experiments show that our solution is effective , outperforming existing methods , and it can be applied to practical ontology population problems .

dialogue systems that can handle face-to-face joint reference to actions in space
this talk introduces new research that works towards an overarching model of natural face-to-face conversation about spatially-located actions in the world , and then uses that model to implement a trustworthy embodied conversational agent to guide users ' ongoing , real-world activities away from the desktop . past research has demonstrated that the relationship between verbal and nonverbal behavior exists at the level of intonational phrases , conversational turns , discourse units , and the negotiation of reference to objects and actions , that mental representations of shared space are structured in such a way as to allow participants in a dialogue to draw on them , that dialogue systems must be based on models of coordination and collaboration , and that users are willing to engage in persistent , natural , trusting conversation with embodied conversational systems . in this talk , these diverse strands of research are brought together in the service of a single underlying modality-independent model of action and language , non-verbal behaviors and words , production and comprehension that can lead to a physically-located , spatially-aware , collaborative embodied conversational agent .

a generalized-zero-preserving method for compact encoding of
constructing an encoding of a concept lattice using short bit vectors allows for efficient computation of join operations on the lattice . join is the central operation any unification-based parser must support . we extend the traditional bit vector encoding , which represents join failure using the zero vector , to count any vector with less than a fixed number of one bits as failure . this allows non-joinable elements to share bits , resulting in a smaller vector size . a constraint solver is used to construct the encoding , and a variety of techniques are employed to find near-optimal solutions and handle timeouts . an evaluation is provided comparing the extended representation of failure with traditional bit vector techniques .

an integrated approach to robust processing of situated spoken dialogue
spoken dialogue is notoriously hard to process with standard nlp technologies . natural spoken dialogue is replete with disfluent , partial , elided or ungrammatical utterances , all of which are very hard to accommodate in a dialogue system . furthermore , speech recognition is known to be a highly error-prone task , especially for complex , open-ended discourse domains . the combination of these two problems ill-formed and/or misrecognised speech inputs raises a major challenge to the development of robust dialogue systems . we present an integrated approach for addressing these two issues , based on a incremental parser for combinatory categorial grammar . the parser takes word lattices as input and is able to handle illformed and misrecognised utterances by selectively relaxing its set of grammatical rules . the choice of the most relevant interpretation is then realised via a discriminative model augmented with contextual information . the approach is fully implemented in a dialogue system for autonomous robots . evaluation results on a wizard of oz test suite demonstrate very significant improvements in accuracy and robustness compared to the baseline .

learning to generate naturalistic utterances using reviews in spoken
spoken language generation for dialogue systems requires a dictionary of mappings between semantic representations of concepts the system wants to express and realizations of those concepts . dictionary creation is a costly process ; it is currently done by hand for each dialogue domain . we propose a novel unsupervised method for learning such mappings from user reviews in the target domain , and test it on restaurant reviews . we test the hypothesis that user reviews that provide individual ratings for distinguished attributes of the domain entity make it possible to map review sentences to their semantic representation with high precision . experimental analyses show that the mappings learned cover most of the domain ontology , and provide good linguistic variation . a subjective user evaluation shows that the consistency between the semantic representations and the learned realizations is high and that the naturalness of the realizations is higher than a hand-crafted baseline .

discourse annotation and semantic annotation in the gnome corpus
the gnome corpus was created to study the discourse and semantic properties of discourse entities that affect their realization and interpretation , and particularly salience . we discuss what information was annotated and the methods we followed .

extracting data records from unstructured biomedical full text donghui feng gully burns eduard hovy
in this paper , we address the problem of extracting data records and their attributes from unstructured biomedical full text . there has been little effort reported on this in the research community . we argue that semantics is important for record extraction or finer-grained language processing tasks . we derive a data record template including semantic language models from unstructured text and represent them with a discourse level conditional random fields ( crf ) model . we evaluate the approach from the perspective of information extraction and achieve significant improvements on system performance compared with other baseline systems .

for low resource machine translation
statistical machine translation ( smt ) performance suffers when models are trained on only small amounts of parallel data . the learned models typically have both low accuracy ( incorrect translations and feature scores ) and low coverage ( high out-of-vocabulary rates ) . in this work , we use an additional data resource , comparable corpora , to improve both . beginning with a small bitext and corresponding phrase-based smt model , we improve coverage by using bilingual lexicon induction techniques to learn new translations from comparable corpora . then , we supplement the models feature space with translation scores estimated over comparable corpora in order to improve accuracy . we observe improvements between 0.5 and 1.7 bleu translating tamil , telugu , bengali , malayalam , hindi , and urdu into english .

literal and metaphorical sense identification
metaphor is ubiquitous in text , even in highly technical text . correct inference about textual entailment requires computers to distinguish the literal and metaphorical senses of a word . past work has treated this problem as a classical word sense disambiguation task .

error handling in the ravenclaw dialog management framework
we describe the error handling architectture underlying the ravenclaw dialog management framework . the architecture provides a robust basis for current and future research in error detection and recovery . several objectives were pursued in its development : task-independence , ease-ofuse , adaptability and scalability . we describe the key aspects of architectural design which confer these properties , and discuss the deployment of this architectture in a number of spoken dialog systems spanning several domains and interaction types . finally , we outline current research projects supported by this architecture .

v-measure : a conditional entropy-based external cluster evaluation
we present v-measure , an external entropybased cluster evaluation measure . vmeasure provides an elegant solution to many problems that affect previously defined cluster evaluation measures including 1 ) dependence on clustering algorithm or data set , 2 ) the problem of matching , where the clustering of only a portion of data points are evaluated and 3 ) accurate evaluation and combination of two desirable aspects of clustering , homogeneity and completeness . we compare v-measure to a number of popular cluster evaluation measures and demonstrate that it satisfies several desirable properties of clustering solutions , using simulated clustering results . finally , we use v-measure to evaluate two clustering tasks : document clustering and pitch accent type clustering .

towards the detection of reliable food-health relationships spoken language systems
we investigate the task of detecting reliable statements about food-health relationships from natural language texts . for that purpose , we created a specially annotated web corpus from forum entries discussing the healthiness of certain food items . we examine a set of task-specific features ( mostly ) based on linguistic insights that are instrumental in finding utterances that are commonly perceived as reliable . these features are incorporated in a supervised classifier and compared against standard features that are widely used for various tasks in natural language processing , such as bag of words , part-of speech and syntactic parse information .

lrscore for evaluating lexical and reordering quality in mt
the ability to measure the quality of word order in translations is an important goal for research in machine translation . current machine translation metrics do not adequately measure the reordering performance of translation systems . we present a novel metric , the lrscore , which directly measures reordering success . the reordering component is balanced by a lexical metric . capturing the two most important elements of translation success in a simple combined metric with only one parameter results in an intuitive , shallow , language independent metric .

bigram hmm with context distribution clustering for unsupervised chinese part-of-speech tagging
this paper presents an unsupervised chinese part-of-speech ( pos ) tagging model based on the first-order hmm . unlike the conventional hmm , the number of hidden states is not fixed and will be increased to fit the training data . in favor of sparse distribution , the dirichlet priors are introduced with variational inference method . to reduce the emission variables , words are represented by their contexts and clustered based on the distributional similarities between contexts . experiment results show the output state sequence of hmm are highly correlated to the latent annotations of gold pos tags , in context of clustering similarity measures . the other experiments on a real application , unsupervised dependency parsing , reveal that the output sequence can replace the manually annotated tags without loss of accuracies .

hmm word-to-phrase alignment with dependency constraints yanjun ma andy way
in this paper , we extend the hmm wordto-phrase alignment model with syntactic dependency constraints . the syntactic dependencies between multiple words in one language are introduced into the model in a bid to produce coherent alignments . our experimental results on a variety of chineseenglish data show that our syntactically constrained model can lead to as much as a 3.24 % relative improvement in bleu score over current hmm word-to-phrase alignment models on a phrase-based statistical machine translation system when the training data is small , and a comparable performance compared to ibm model 4 on a hiero-style system with larger training data . an intrinsic alignment quality evaluation shows that our alignment model with dependency constraints leads to improvements in both precision ( by 1.74 % relative ) and recall ( by 1.75 % relative ) over the model without dependency information .

automated team discourse annotation and performance prediction
we describe two approaches to analyzing and tagging team discourse using latent semantic analysis ( lsa ) to predict team performance . the first approach automatically categorizes the contents of each statement made by each of the three team members using an established set of tags . performance predicting the tags automatically was 15 % below human agreement . these tagged statements are then used to predict team performance . the second approach measures the semantic content of the dialogue of the team as a whole and accurately predicts the teams performance on a simulated military mission .

cheap and easy entity evaluation ben hachey joel nothman will radford
the aida-yago dataset is a popular target for whole-document entity recognition and disambiguation , despite lacking a shared evaluation tool . we review evaluation regimens in the literature while comparing the output of three approaches , and identify research opportunities . this utilises our open , accessible evaluation tool . we exemplify a new paradigm of distributed , shared evaluation , in which evaluation software and standardised , versioned system outputs are provided online .

utterance-level multimodal sentiment analysis
during real-life interactions , people are naturally gesturing and modulating their voice to emphasize specific points or to express their emotions . with the recent growth of social websites such as youtube , facebook , and amazon , video reviews are emerging as a new source of multimodal and natural opinions that has been left almost untapped by automatic opinion analysis techniques . this paper presents a method for multimodal sentiment classification , which can identify the sentiment expressed in utterance-level visual datastreams . using a new multimodal dataset consisting of sentiment annotated utterances extracted from video reviews , we show that multimodal sentiment analysis can be effectively performed , and that the joint use of visual , acoustic , and linguistic modalities can lead to error rate reductions of up to 10.5 % as compared to the best performing individual modality .

on named entity recognition in targeted twitter streams in polish
this paper reports on some experiments aiming at tuning a rule-based ner system designed for detecting names in polish online news to the processing of targeted twitter streams . in particular , one explores whether the performance of the baseline ner system can be improved through the incremental application of knowledge-poor methods for name matching and guessing . we study various settings and combinations of the methods and present evaluation results on five corpora gathered from twitter , centred around major events and known individuals .

re-ranking models for spoken language understanding
spoken language understanding aims at mapping a natural language spoken sentence into a semantic representation . in the last decade two main approaches have been pursued : generative and discriminative models . the former is more robust to overfitting whereas the latter is more robust to many irrelevant features . additionally , the way in which these approaches encode prior knowledge is very different and their relative performance changes based on the task . in this paper we describe a machine learning framework where both models are used : a generative model produces a list of ranked hypotheses whereas a discriminative model based on structure kernels and support vector machines , re-ranks such list . we tested our approach on the media corpus ( human-machine dialogs ) and on a new corpus ( human-machine and humanhuman dialogs ) produced in the european luna project . the results show a large improvement on the state-of-the-art in concept segmentation and labeling .

pos induction with distributional and morphological information using a distance-dependent chinese restaurant process
we present a new approach to inducing the syntactic categories of words , combining their distributional and morphological properties in a joint nonparametric bayesian model based on the distance-dependent chinese restaurant process . the prior distribution over word clusterings uses a log-linear model of morphological similarity ; the likelihood function is the probability of generating vector word embeddings . the weights of the morphology model are learned jointly while inducing part-ofspeech clusters , encouraging them to cohere with the distributional features . the resulting algorithm outperforms competitive alternatives on english pos induction .

anchors regularized : adding robustness and extensibility and national library of medicine ,
spectral methods offer scalable alternatives to markov chain monte carlo and expectation maximization . however , these new methods lack the rich priors associated with probabilistic models . we examine arora et al.s anchor words algorithm for topic modeling and develop new , regularized algorithms that not only mathematically resemble gaussian and dirichlet priors but also improve the interpretability of topic models . our new regularization approaches make these efficient algorithms more flexible ; we also show that these methods can be combined with informed priors .

srcb-wsd : supervised chinese word sense disambiguation with key features
this article describes the implementation of word sense disambiguation system that participated in the semeval-2007 multilingual chinese-english lexical sample task . we adopted a supervised learning approach with maximum entropy classifier . the features used were neighboring words and their part-of-speech , as well as single words in the context , and other syntactic features based on shallow parsing . in addition , we used word category information of a chinese thesaurus as features for verb disambiguation . for the task we participated in , we obtained precision of 0.716 in micro-average , which is the best among all participated systems .

automatic construction of polarity-tagged corpus from html
this paper proposes a novel method of building polarity-tagged corpus from html documents . the characteristics of this method is that it is fully automatic and can be applied to arbitrary html documents . the idea behind our method is to utilize certain layout structures and linguistic pattern . by using them , we can automatically extract such sentences that express opinion . in our experiment , the method could construct a corpus consisting of 126,610 sentences .

conceptual coherence in the generation of referring expressions
one of the challenges in the automatic generation of referring expressions is to identify a set of domain entities coherently , that is , from the same conceptual perspective . we describe and evaluate an algorithm that generates a conceptually coherent description of a target set . the design of the algorithm is motivated by the results of psycholinguistic experiments .

domain specific speech acts for spoken language translation
we describe a coding scheme for machine translation of spoken taskoriented dialogue . the coding scheme covers two levels of speaker intention domain independent speech acts and domain dependent domain actions . our database contains over 14,000 tagged sentences in english , italian , and german . we argue that domain actions , and not speech acts , are the relevant discourse unit for improving translation quality . we also show that , although domain actions are domain specific , the approach scales up to large domains without an explosion of domain actions and can be coded with high inter-coder reliability across research sites . furthermore , although the number of domain actions is on the order of ten times the number of speech acts , sparseness is not a problem for the training of classifiers for identifying the domain action . we describe our work on developing high accuracy speech act and domain action classifiers , which is the core of the source language analysis module of our nespole machine translation system .

unsupervised discovery of a statistical verb lexicon
this paper demonstrates how unsupervised techniques can be used to learn models of deep linguistic structure . determining the semantic roles of a verbs dependents is an important step in natural language understanding . we present a method for learning models of verb argument patterns directly from unannotated text . the learned models are similar to existing verb lexicons such as verbnet and propbank , but additionally include statistics about the linkings used by each verb . the method is based on a structured probabilistic model of the domain , and unsupervised learning is performed with the em algorithm . the learned models can also be used discriminatively as semantic role labelers , and when evaluated relative to the propbank annotation , the best learned model reduces 28 % of the error between an informed baseline and an oracle upper bound .

dependency-based discourse parser for single-document summarization
the current state-of-the-art singledocument summarization method generates a summary by solving a tree knapsack problem ( tkp ) , which is the problem of finding the optimal rooted subtree of the dependency-based discourse tree ( dep-dt ) of a document . we can obtain a gold dep-dt by transforming a gold rhetorical structure theory-based discourse tree ( rst-dt ) . however , there is still a large difference between the rouge scores of a system with a gold dep-dt and a system with a dep-dt obtained from an automatically parsed rst-dt . to improve the rouge score , we propose a novel discourse parser that directly generates the dep-dt . the evaluation results showed that the tkp with our parser outperformed that with the state-of-the-art rst-dt parser , and achieved almost equivalent rouge scores to the tkp with the gold dep-dt .

a language model approach to keyphrase extraction
we present a new approach to extracting keyphrases based on statistical language models . our approach is to use pointwise kl-divergence between multiple language models for scoring both phraseness and informativeness , which can be unified into a single score to rank extracted phrases .

fast duplicate document detection using multi-level prefix-filter
duplicate document detection is the problem of finding all document-pairs rapidly whose similarities are equal to or greater than a given threshold . there is a method proposed recently called prefix-filter that finds document-pairs whose similarities never reach the threshold based on the number of uncommon terms ( words/characters ) in a document-pair and removes them before similarity calculation . however , prefix-filter can not decrease the number of similarity calculations sufficiently because it leaves many document-pairs whose similarities are less than the threshold . in this paper , we propose multi-level prefix-filter , which reduces the number of similarity calculations more efficiently and maintains the advantage of prefix-filter ( no detection loss , no extra parameter ) by applying multiple different prefix-filters .

minimum bayes risk combination of translation hypotheses from alternative morphological decompositions
we describe a simple strategy to achieve translation performance improvements by combining output from identical statistical machine translation systems trained on alternative morphological decompositions of the source language . combination is done by means of minimum bayes risk decoding over a shared nbest list . when translating into english from two highly inflected languages such as arabic and finnish we obtain significant improvements over simply selecting the best morphological decomposition .

learning phrasal categories
in this work we learn clusters of contextual annotations for non-terminals in the penn treebank . perhaps the best way to think about this problem is to contrast our work with that of klein and manning ( 2003 ) . that research used treetransformations to create various grammars with different contextual annotations on the non-terminals . these grammars were then used in conjunction with a cky parser . the authors explored the space of different annotation combinations by hand . here we try to automate the process to learn the right combination automatically . our results are not quite as good as those carefully created by hand , but they are close ( 84.8 vs 85.7 ) .

bilingual random walk models for automated grammar correction of esl author-produced text
we present a novel noisy channel model for correcting text produced by english as a second language ( esl ) authors . we model the english word choices made by esl authors as a random walk across an undirected bipartite dictionary graph composed of edges between english words and associated words in an authors native language . we present two such models , using cascades of weighted finitestate transducers ( wfsts ) to model language model priors , random walk-induced noise , and observed sentences , and expectation maximization ( em ) to learn model parameters after park and levy ( 2011 ) . we show that such models can make intelligent word substitutions to improve grammaticality in an unsupervised setting .

mttk : an alignment toolkit for statistical machine translation
the mttk alignment toolkit for statistical machine translation can be used for word , phrase , and sentence alignment of parallel documents . it is designed mainly for building statistical machine translation systems , but can be exploited in other multi-lingual applications . it provides computationally efficient alignment and estimation procedures that can be used for the unsupervised alignment of parallel text collections in a language independent fashion . mttk version 1.0 is available under the open source educational community license .

syntactic and semantic factors in processing difficulty : an integrated measure
the analysis of reading times can provide insights into the processes that underlie language comprehension , with longer reading times indicating greater cognitive load . there is evidence that the language processor is highly predictive , such that prior context allows upcoming linguistic material to be anticipated . previous work has investigated the contributions of semantic and syntactic contexts in isolation , essentially treating them as independent factors . in this paper we analyze reading times in terms of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model .

using machine learning techniques to interpret wh-questions
we describe a set of supervised machine learning experiments centering on the construction of statistical models of wh-questions . these models , which are built from shallow linguistic features of questions , are employed to predict target variables which represent a users informational goals . we report on different aspects of the predictive performance of our models , including the influence of various training and testing factors on predictive performance , and examine the relationships among the target variables .

probabilistic type theory for incremental dialogue processing
we present an adaptation of recent work on probabilistic type theory with records ( cooper et al. , 2014 ) for the purposes of modelling the incremental semantic processing of dialogue participants . after presenting the formalism and dialogue framework , we show how probabilistic ttr type judgements can be integrated into the inference system of an incremental dialogue system , and discuss how this could be used to guide parsing and dialogue management decisions .

improving word segmentation by simultaneously learning phonotactics computer & information sciences linguistics & cognitive science
the most accurate unsupervised word segmentation systems that are currently available ( brent , 1999 ; venkataraman , 2001 ; goldwater , 2007 ) use a simple unigram model of phonotactics . while this simplifies some of the calculations , it overlooks cues that infant language acquisition researchers have shown to be useful for segmentation ( mattys et al , 1999 ; mattys and jusczyk , 2001 ) . here we explore the utility of using bigram and trigram phonotactic models by enhancing brents ( 1999 ) mbdp-1 algorithm . the results show the improved mbdp-phon model outperforms other unsupervised word segmentation systems ( e.g. , brent , 1999 ; venkataraman , 2001 ; goldwater , 2007 ) .

use and acquisition of semantic language model
semantic language model is a technique that utilizes the semantic structure of an utterance to better rank the likelihood of words composing the sentence . when used in a conversational system , one can dynamically integrate the dialog state and domain semantics into the semantic language model to better guide the speech recognizer executing the decoding process . we describe one such application that employs semantic language model to cope with spontaneous speech in a robust manner . the semantic language model , though can be manually crafted without data , can benefit significantly from data driven machine learning techniques . an example based approach is also described here to demonstrate a viable approach .

topic identification in natural language dialogues using
in humancomputer interaction systems using natural language , the recognition of the topic from users utterances is an important task . we examine two different perspectives to the problem of topic analysis needed for carrying out a successful dialogue . first , we apply selforganized document maps for modeling the broader subject of discourse based on the occurrence of content words in the dialogue context . on a finnish corpus of 57 dialogues the method is shown to work well for recognizing subjects of longer dialogue segments , whereas for individual utterances the subject recognition history should perhaps be taken into account . second , we attempt to identify topically relevant words in the utterances and thus locate the old information ( topic words ) and new information ( focus words ) . for this we define a probabilistic model and compare different methods for model parameter estimation on a corpus of 189 dialogues . moreover , the utilization of information regarding the position of the word in the utterance is found to improve the results .

semi-supervised semantic tagging of conversational understanding using markov topic regression
finding concepts in natural language utterances is a challenging task , especially given the scarcity of labeled data for learning semantic ambiguity . furthermore , data mismatch issues , which arise when the expected test ( target ) data does not exactly match the training data , aggravate this scarcity problem . to deal with these issues , we describe an efficient semisupervised learning ( ssl ) approach which has two components : ( i ) markov topic regression is a new probabilistic model to cluster words into semantic tags ( concepts ) . it can efficiently handle semantic ambiguity by extending standard topic models with two new features . first , it encodes word n-gram features from labeled source and unlabeled target data . second , by going beyond a bag-of-words approach , it takes into account the inherent sequential nature of utterances to learn semantic classes based on context . ( ii ) retrospective learner is a new learning technique that adapts to the unlabeled target data . our new ssl approach improves semantic tagging performance by 3 % absolute over the baseline models , and also compares favorably on semi-supervised syntactic tagging .

learning to distinguish hypernyms and co-hyponyms
this work is concerned with distinguishing different semantic relations which exist between distributionally similar words . we compare a novel approach based on training a linear support vector machine on pairs of feature vectors with state-of-the-art methods based on distributional similarity . we show that the new supervised approach does better even when there is minimal information about the target words in the training data , giving a 15 % reduction in error rate over unsupervised approaches .

developing online icall exercises for russian
we outline a new icall system for learners of russian , focusing on the processing needed for basic morphological errors . by setting out an appropriate design for a lexicon and distinguishing the types of morphological errors to be detected , we establish a foundation for error detection across exercises .

a comparative evaluation of data-driven models in translation selection of machine translation
we present a comparative evaluation of two data-driven models used in translation selection of english-korean machine translation . latent semantic analysis ( lsa ) and probabilistic latent semantic analysis ( plsa ) are applied for the purpose of implementation of data-driven models in particular . these models are able to represent complex semantic structures of given contexts , like text passages . grammatical relationships , stored in dictionaries , are utilized in translation selection essentially . we have used k-nearest neighbor ( k-nn ) learning to select an appropriate translation of the unseen instances in the dictionary . the distance of instances in k-nn is computed by estimating the similarity measured by lsa and plsa . for experiments , we used trec data ( ap news in 1988 ) for constructing latent semantic spaces of two models and wall street journal corpus for evaluating the translation accuracy in each model . plsa selected relatively more accurate translations than lsa in the experiment , irrespective of the value of k and the types of grammatical relationship .

donghui feng gully burns jingbo zhu eduard hovy
in this paper , we present an empirical study on adapting conditional random fields ( crf ) models to conduct semantic analysis on biomedical articles using active learning . we explore uncertaintybased active learning with the crf model to dynamically select the most informative training examples . this abridges the power of the supervised methods and expensive human annotation cost .

search result re-ranking by feedback control adjustment for
we propose a new method to rank a special category of time-sensitive queries that are year qualified . the method adjusts the retrieval scores of a base ranking function according to time-stamps of web documents so that the freshest documents are ranked higher . our method , which is based on feedback control theory , uses ranking errors to adjust the search engine behavior . for this purpose , we use a simple but effective method to extract year qualified queries by mining query logs and a time-stamp recognition method that considers titles and urls of web documents . our method was tested on a commercial search engine . the experiments show that our approach can significantly improve relevance ranking for year qualified queries even if all the existing methods for comparison failed .

learning to distinguish pp arguments from adjuncts
words differ in the subcategorisation frames in which they occur , and there is a strong correlation between the semantic arguments of a given word and its subcategorisation frame , so that all its arguments should be included in its subcategorisation frame . one problem is posed by the ambiguity between locative prepositional phrases as arguments of a verb or adjuncts . as the semantics for the verb is the same in both cases , it is difficult to differentiate them , and to learn the appropriate subcategorisation frame . we propose an approach that uses semantically motivated preposition selection and frequency information to determine if a locative pp is an argument or an adjunct . in order to test this approach , we perform an experiment using a computational learning system that receives as input utterances annotated with logical forms . the results obtained indicate that the learner successfully distinguishes between arguments ( obligatory and optional ) and adjuncts .

modeling newswire events using neural networks for anomaly detection
automatically identifying anomalous newswire events is a hard problem . we discuss the complexity of the problem and introduce a novel technique to model events based on recursive neural networks to represent events as composition of their semantic arguments . our model learns to differentiate between normal and anomalous events . we model anomaly detection as a binary classification problem and show that the model learns useful features to classify anomaly . we use headlines from the weird news category publicly available on newswire websites to extract anomalous training examples and those from gigaword as normal examples . we evaluate the classifier on human annotated data and obtain an accuracy of 65.44 % . we also show that our model is at least as competent as the least competent human annotator in anomaly detection .

named entities translation based on comparable corpora
in this paper we present a system for translating named entities from basque to spanish based on comparable corpora . for that purpose we have tried two approaches : one based on basque linguistic features , and a language-independent tool . for both tools we have used basquespanish comparable corpora , a bilingual dictionary and the web as resources .

discriminative feature-tied mixture modeling for statistical machine
in this paper we present a novel discriminative mixture model for statistical machine translation ( smt ) . we model the feature space with a log-linear combination of multiple mixture components . each component contains a large set of features trained in a maximumentropy framework . all features within the same mixture component are tied and share the same mixture weights , where the mixture weights are trained discriminatively to maximize the translation performance . this approach aims at bridging the gap between the maximum-likelihood training and the discriminative training for smt . it is shown that the feature space can be partitioned in a variety of ways , such as based on feature types , word alignments , or domains , for various applications . the proposed approach improves the translation performance significantly on a large-scale arabic-to-english mt task .

a syntactified direct translation model with linear-time decoding
recent syntactic extensions of statistical translation models work with a synchronous context-free or tree-substitution grammar extracted from an automatically parsed parallel corpus . the decoders accompanying these extensions typically exceed quadratic time complexity . this paper extends the direct translation model 2 ( dtm2 ) with syntax while maintaining linear-time decoding . we employ a linear-time parsing algorithm based on an eager , incremental interpretation of combinatory categorial grammar ( ccg ) . as every input word is processed , the local parsing decisions resolve ambiguity eagerly , by selecting a single supertagoperator pair for extending the dependency parse incrementally . alongside translation features extracted from the derived parse tree , we explore syntactic features extracted from the incremental derivation process . our empirical experiments show that our model significantly outperforms the state-of-the art dtm2 system .

the talp-upc ngram-based statistical machine translation system for
this paper reports on the participation of the talp research center of the upc ( universitat politcnica de catalunya ) to the acl wmt 2008 evaluation campaign . this years system is the evolution of the one we employed for the 2007 campaign . main updates and extensions involve linguistically motivated word reordering based on the reordering patterns technique . in addition , this system introduces a target language model , based on linguistic classes ( part-of-speech ) , morphology reduction for an inflectional language ( spanish ) and an improved optimization procedure . results obtained over the development and test sets on spanish to english ( and the other way round ) translations for both the traditional europarl and a challenging news stories tasks are analyzed and commented .

tsubaki : an open search engine infrastructure for developing new information access methodology
as the amount of information created by human beings is explosively grown in the last decade , it is getting extremely harder to obtain necessary information by conventional information access methods . hence , creation of drastically new technology is needed . for developing such new technology , search engine infrastructures are required . although the existing search engine apis can be regarded as such infrastructures , these apis have several restrictions such as a limit on the number of api calls . to help the development of new technology , we are running an open search engine infrastructure , tsubaki , on a high-performance computing environment . in this paper , we describe tsubaki infrastructure .

wysiwym with wider coverage
we describe an extension of the wysiwym technology for knowledge editing through natural language feedback . previous applications have addressed relatively simple tasks requiring a very limited range of nominal and clause patterns . we show that by adding a further editing operation called reconfiguration , the technology can achieve a far wider coverage more in line with other general-purpose generators . the extension will be included in a java-based library package for producing wysiwym applications .

multi-document summarization of evaluative text deptartment of computer science
we present and compare two approaches to the task of summarizing evaluative arguments . the first is a sentence extractionbased approach while the second is a language generation-based approach . we evaluate these approaches in a user study and find that they quantitatively perform equally well . qualitatively , however , we find that they perform well for different but complementary reasons . we conclude that an effective method for summarizing evaluative arguments must effectively synthesize the two approaches .

a gateway to the computational linguistics curriculum william gregory sakas
computational modeling of human language processes is a small but growing subfield of computational linguistics . this paper describes a course that makes use of recent research in psychocomputational modeling as a framework to introduce a number of mainstream computational linguistics concepts to an audience of linguistics , cognitive science and computer science doctoral students . the emphasis on what i take to be the largely interdisciplinary nature of computational linguistics is particularly germane for the computer science students . since 2002 the course has been taught three times under the auspices of the ma/phd program in linguistics at the city university of new yorks graduate center . a brief description of some of the students experiences after having taken the course is also provided .

building english-vietnamese named entity corpus with aligned bilingual news articles
named entity recognition aims to classify words in a document into pre-defined target entity classes . it is now considered to be fundamental for many natural language processing tasks such as information retrieval , machine translation , information extraction and question answering . this paper presents a workflow to build an english-vietnamese named entity corpus from an aligned bilingual corpus . the workflow is based on a state of the art named entity recognition tool to identify english named entities and map them into vietnamese text . the paper also presents a detailed discussion about several mapping errors and differences between english and vietnamese sentences that affect this task .

an efficient two-pass approach to synchronous-cfg driven statistical mt
we present an efficient , novel two-pass approach to mitigate the computational impact resulting from online intersection of an n-gram language model ( lm ) and a probabilistic synchronous context-free grammar ( pscfg ) for statistical machine translation . in first pass cyk-style decoding , we consider first-best chart item approximations , generating a hypergraph of sentence spanning target language derivations . in the second stage , we instantiate specific alternative derivations from this hypergraph , using the lm to drive this search process , recovering from search errors made in the first pass . model search errors in our approach are comparable to those made by the state-of-the-art cube pruning approach in ( chiang , 2007 ) under comparable pruning conditions evaluated on both hierarchical and syntax-based grammars .

significance of bridging real-world documents and nlp technologies tadayoshi hara goran topic yusuke miyao akiko aizawa
most conventional natural language processing ( nlp ) tools assume plain text as their input , whereas real-world documents display text more expressively , using a variety of layouts , sentence structures , and inline objects , among others . when nlp tools are applied to such text , users must first convert the text into the input/output formats of the tools . moreover , this awkwardly obtained input typically does not allow the expected maximum performance of the nlp tools to be achieved . this work attempts to raise awareness of this issue using xml documents , where textual composition beyond plain text is given by tags . we propose a general framework for data conversion between xml-tagged text and plain text used as input/output for nlp tools and show that text sequences obtained by our framework can be much more thoroughly and efficiently processed by parsers than naively tag-removed text . these results highlight the significance of bridging real-world documents and nlp technologies .

quasi-synchronous phrase dependency grammars for machine translation carnegie mellon univeristy
we present a quasi-synchronous dependency grammar ( smith and eisner , 2006 ) for machine translation in which the leaves of the tree are phrases rather than words as in previous work ( gimpel and smith , 2009 ) . this formulation allows us to combine structural components of phrase-based and syntax-based mt in a single model . we describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser . for decoding , we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices . we demonstrate performance improvements for chinese-english and urduenglish translation over a phrase-based baseline . we also investigate the use of unsupervised dependency parsers , reporting encouraging preliminary results .

shallow semantic parsing for spoken language understanding
most spoken dialog systems are based on speech grammars and frame/slot semantics . the semantic descriptions of input utterances are usually defined ad-hoc with no ability to generalize beyond the target application domain or to learn from annotated corpora . the approach we propose in this paper exploits machine learning of frame semantics , borrowing its theoretical model from computational linguistics . while traditional automatic semantic role labeling approaches on written texts may not perform as well on spoken dialogs , we show successful experiments on such porting . hence , we design and evaluate automatic framenet-based parsers both for english written texts and for italian dialog utterances . the results show that disfluencies of dialog data do not severely hurt performance . also , a small set of framenet-like manual annotations is enough for realizing accurate semantic role labeling on the target domains of typical dialog systems .

modeltalker voice recorder an interface system for recording a corpus of speech for synthesis
we will demonstrate the modeltalker voice recorder ( mt voice recorder ) an interface system that lets individuals record and bank a speech database for the creation of a synthetic voice . the system guides users through an automatic calibration process that sets pitch , amplitude , and silence . the system then prompts users with both visual ( text-based ) and auditory prompts . each recording is screened for pitch , amplitude and pronunciation and users are given immediate feedback on the acceptability of each recording . users can then rerecord an unacceptable utterance . recordings are automatically labeled and saved and a speech database is created from these recordings . the systems intention is to make the process of recording a corpus of utterances relatively easy for those inexperienced in linguistic analysis . ultimately , the recorded corpus and the resulting speech database is used for concatenative synthetic speech , thus allowing individuals at home or in clinics to create a synthetic voice in their own voice . the interface may prove useful for other purposes as well . the system facilitates the recording and labeling of large corpora of speech , making it useful for speech and linguistic research , and it provides immediate feedback on pronunciation , thus making it useful as a clinical learning tool .

question answering as question-biased term extraction : a new approach toward multilingual qa
this paper regards question answering ( qa ) as question-biased term extraction ( qbte ) . this new qbte approach liberates qa systems from the heavy burden imposed by question types ( or answer types ) . in conventional approaches , a qa system analyzes a given question and determines the question type , and then it selects answers from among answer candidates that match the question type . consequently , the output of a qa system is restricted by the design of the question types . the qbte directly extracts answers as terms biased by the question . to confirm the feasibility of our qbte approach , we conducted experiments on the crl qa data based on 10-fold cross validation , using maximum entropy models ( mems ) as an ml technique . experimental results showed that the trained system achieved 0.36 in mrr and 0.47 in top5 accuracy .

retrieval of reading materials for vocabulary and reading practice
finding appropriate , authentic reading materials is a challenge for language instructors . the web is a vast resource of texts , but most pages are not suitable for reading practice , and commercial search engines are not well suited to finding texts that satisfy pedagogical constraints such as reading level , length , text quality , and presence of target vocabulary . we present a system that uses various language technologies to facilitate the retrieval and presentation of authentic reading materials gathered from the web . it is currently deployed in two english as a second language courses at the university of pittsburgh .

probabilistic reasoning for entity & relation recognition
this paper develops a method for recognizing relations and entities in sentences , while taking mutual dependencies among them into account . e.g. , the kill ( johns , oswald ) relation in : j. v. oswald was murdered at jfk after his assassin , k. f. johns ... depends on identifying oswald and johns as people , jfk being identified as a location , and the kill relation between oswald and johns ; this , in turn , enforces that oswald and johns are people . in our framework , classifiers that identify entities and relations among them are first learned from local information in the sentence ; this information , along with constraints induced among entity types and relations , is used to perform global inference that accounts for the mutual dependencies among the entities . our preliminary experimental results are promising and show that our global inference approach improves over learning relations and entities separately .

toward plot units : automatic affect state analysis amit goyal and ellen riloff and hal daume iii and nathan gilbert
we present a system called aesop that automatically produces affect states associated with characters in a story . this research represents a first step toward the automatic generation of plot unit structures from text . aesop incorporates several existing sentiment analysis tools and lexicons to evaluate the effectiveness of current sentiment technology on this task . aesop also includes two novel components : a method for acquiring patient polarity verbs , which impart negative affect on their patients , and affect projection rules to propagate affect tags from surrounding words onto the characters in the story . we evaluate aesop on a small collection of fables .

the acl anthology searchbench
we describe a novel application for structured search in scientific digital libraries . the acl anthology searchbench is meant to become a publicly available research tool to query the content of the acl anthology . the application provides search in both its bibliographic metadata and semantically analyzed full textual content . by combining these two features , very efficient and focused queries are possible . at the same time , the application serves as a showcase for the recent progress in natural language processing ( nlp ) research and language technology . the system currently indexes the textual content of 7,500 anthology papers from 20022009 with predicateargument-like semantic structures . it also provides useful search filters based on bibliographic metadata . it will be extended to provide the full anthology content and enhanced functionality based on further nlp techniques .

automatic single-document key fact extraction from newswire articles
this paper addresses the problem of extracting the most important facts from a news article . our approach uses syntactic , semantic , and general statistical features to identify the most important sentences in a document . the importance of the individual features is estimated using generalized iterative scaling methods trained on an annotated newswire corpus . the performance of our approach is evaluated against 300 unseen news articles and shows that use of these features results in statistically significant improvements over a provenly robust baseline , as measured using metrics such as precision , recall and rouge .

hybrid strategies for better products and shorter time-to-market
the main lingenio mt products are based on rule-based architectures . in the presentation we show how knowledge from corpora is integrated into the systems using the language analysis- and translation-components in a bootstrapping approach . this relates to the bilingual dictionaries , but also to learning decisions concerning the selection of syntactic rules and semantic readings in parsing and semantic evaluation . these strategies contribute both to improve the quality of the systems and to shorten go-to-market of new products significantly . also a number of attractive spinoff functions can be generated from them which , in addition , can be used for designing new types of products and as preparatory and postediting features in mt systems whose core is of type smt . 97

deepak ravichandran , eduard hovy , and franz josef och
in this paper , we show that we can obtain a good baseline performance for question answering ( qa ) by using only 4 simple features . using these features , we contrast two approaches used for a maximum entropy based qa system . we view the qa problem as a classification problem and as a reranking problem . our results indicate that the qa system viewed as a reranker clearly outperforms the qa system used as a classifier . both systems are trained using the same data .

reactive redundancy and listener comprehension in direction-giving
we explore the role of redundancy , both in anticipation of and in response to listener confusion , in task-oriented dialogue . we find that direction-givers provide redundant utterances in response to both verbal and non-verbal signals of listener confusion . we also examine the effects of prior acquaintance and visibility upon redundancy . as expected , givers use more redundant utterances overall , and more redundant utterances in response to listener questions , when communicating with strangers . we discuss our findings in relation to theories of redundancy , the balance of speaker and listener effort , and potential applications .

annotation of chemical named entities thomas graham house natural language and
we describe the annotation of chemical named entities in scientific text . a set of annotation guidelines defines 5 types of named entities , and provides instructions for the resolution of special cases . a corpus of fulltext chemistry papers was annotated , with an inter-annotator agreement score of 93 % . an investigation of named entity recognition using lingpipe suggests that scores of 63 % are possible without customisation , and scores of 74 % are possible with the addition of custom tokenisation and the use of dictionaries .

pairwise document similarity in large collections with mapreduce
this paper presents a mapreduce algorithm for computing pairwise document similarity in large document collections . mapreduce is an attractive framework because it allows us to decompose the inner products involved in computing document similarity into separate multiplication and summation stages in a way that is well matched to efficient disk access patterns across several machines . on a collection consisting of approximately 900,000 newswire articles , our algorithm exhibits linear growth in running time and space in terms of the number of documents .

automatically learning qualia structures from the web philipp cimiano & johanna wenderoth
qualia structures have many applications within computational linguistics , but currently there are no corresponding lexical resources such as wordnet or framenet . this paper presents an approach to automatically learn qualia structures for nominals from the world wide web and thus opens the possibility to explore the impact of qualia structures for natural language processing at a larger scale . furthermore , our approach can be also used support a lexicographer in the task of manually creating a lexicon of qualia structures . the approach is based on the idea of matching certain lexicosyntactic patterns conveying a certain semantic relation on the world wide web using standard search engines . we evaluate our approach qualitatively by comparing our automatically learned qualia structures with the ones from the literature , but also quantitatively by presenting results of a human evaluation .

dialogue management based on entities and constraints
this paper introduces a new dialogue management framework for goal-directed conversations . a declarative specification defines the domain-specific elements and guides the dialogue manager , which communicates with the knowledge sources to complete the specified goal . the user is viewed as another knowledge source . the dialogue manager finds the next action by a mixture of rule-based reasoning and a simple statistical model . implementation in the flight-reservation domain demonstrates that the framework enables the developer to easily build a conversational dialogue system .

gaf : a grounded annotation framework for events
this paper introduces gaf , a grounded annotation framework to represent events in a formal context that can represent information from both textual and extra-textual sources . gaf makes a clear distinction between mentions of events in text and their formal representation as instances in a semantic layer . instances are represented by rdf compliant uris that are shared across different research disciplines . this allows us to complete textual information with external sources and facilitates reasoning . the semantic layer can integrate any linguistic information and is compatible with previous event representations in nlp . through a use case on earthquakes in southeast asia , we demonstrate gaf flexibility and ability to reason over events with the aid of extra-linguistic resources .

towards an empirical subcategorization of multiword expressions dipartimento di scienze documentarie , linguistico-filologiche e geografiche
the subcategorization of multiword expressions ( mwes ) is still problematic because of the great variability of their phenomenology . this article presents an attempt to categorize italian nominal mwes on the basis of their syntactic and semantic behaviour by considering features that can be tested on corpora . our analysis shows how these features can lead to a differentiation of the expressions in two groups which correspond to the intuitive notions of multiword units and lexical collocations .

automating xml markup of text documents
we present a novel system for automatically marking up text documents into xml and discuss the benefits of xml markup for intelligent information retrieval . the system uses the self-organizing map ( som ) algorithm to arrange xml marked-up documents on a twodimensional map so that similar documents appear closer to each other . it then employs an inductive learning algorithm c5 to automatically extract and apply markup rules from the nearest som neighbours of an unmarked document . the system is designed to be adaptive , so that once a document is marked-up ; its behaviour is modified to improve accuracy . the automatically marked-up documents are again categorized on the self-organizing map .

seeking informativeness in literature based discovery
the continuously increasing number of publications within the biomedical domain has fuelled the creation of literature based discovery ( lbd ) systems which identify unconnected pieces of knowledge appearing in separate literatures which can be combined to make new discoveries . without filtering , the amount of hidden knowledge found is vast due to noise , making it impractical for a researcher to examine , or clinically evaluate , the potential discoveries . we present a number of filtering techniques , including two which exploit the lbd system itself rather than being based on a statistical or manual examination of document collections , and we demonstrate usefulness via replication of known discoveries .

zone identification in biology articles as a basis for information extraction
information extraction ( ie ) in the biomedical domain is now regarded as an essential technique for the dynamic management of factual information contained in archived journal articles and abstract collections . we aim to provide a technique serving as a basis for pinpointing and organizing factual information related to experimental results . in this paper , we enhance the idea proposed in ( mizuta and collier , 2004 ) ; annotating articles in terms of rhetorical zones with shallow nesting . we give a qualitative analysis of the zone identification ( zi ) process in biology articles . specifically , we illustrate the linguistic and other features of each zone based on our investigation of articles selected from four major online journals . we also discuss controversial cases and nested zones , and zi using multiple features . in doing so , we provide a stronger theoretical and practical support for our framework toward automatic zi .

smatch : an evaluation metric for semantic feature structures
the evaluation of whole-sentence semantic structures plays an important role in semantic parsing and large-scale semantic structure annotation . however , there is no widely-used metric to evaluate wholesentence semantic structures . in this paper , we present smatch , a metric that calculates the degree of overlap between two semantic feature structures . we give an efficient algorithm to compute the metric and show the results of an inter-annotator agreement study .

sparse bayesian classification of predicate arguments
we present an application of sparse bayesian learning to the task of semantic role labeling , and we demonstrate that this method produces smaller classifiers than the popular support vector approach . we describe the classification strategy and the features used by the classifier . in particular , the contribution of six parse tree path features is investigated .

identification of event mentions and their semantic class
complex tasks like question answering need to be able to identify events in text and the relations among those events . we show that this event identification task and a related task , identifying the semantic class of these events , can both be formulated as classification problems in a word-chunking paradigm . we introduce a variety of linguistically motivated features for this task and then train a system that is able to identify events with a precision of 82 % and a recall of 71 % . we then show a variety of analyses of this model , and their implications for the event identification task .

improving bitext word alignments via syntax-based reordering of english
we present an improved method for automated word alignment of parallel texts which takes advantage of knowledge of syntactic divergences , while avoiding the need for syntactic analysis of the less resource rich language , and retaining the robustness of syntactically agnostic approaches such as the ibm word alignment models . we achieve this by using simple , easily-elicited knowledge to produce syntaxbased heuristics which transform the target language ( e.g . english ) into a form more closely resembling the source language , and then by using standard alignment methods to align the transformed bitext . we present experimental results under variable resource conditions . the method improves word alignment performance for language pairs such as english-korean and english-hindi , which exhibit longer-distance syntactic divergences .

joint coreference resolution and named-entity linking with multi-pass sieves
many errors in coreference resolution come from semantic mismatches due to inadequate world knowledge . errors in named-entity linking ( nel ) , on the other hand , are often caused by superficial modeling of entity context . this paper demonstrates that these two tasks are complementary . we introduce neco , a new model for named entity linking and coreference resolution , which solves both problems jointly , reducing the errors made on each . neco extends the stanford deterministic coreference system by automatically linking mentions to wikipedia and introducing new nel-informed mention-merging sieves . linking improves mention-detection and enables new semantic attributes to be incorporated from freebase , while coreference provides better context modeling by propagating named-entity links within mention clusters . experiments show consistent improvements across a number of datasets and experimental conditions , including over 11 % reduction in muc coreference error and nearly 21 % reduction in f1 nel error on ace 2004 newswire data .

recognizing implied predicate-argument relationships in textual inference
we investigate recognizing implied predicate-argument relationships which are not explicitly expressed in syntactic structure . while prior works addressed such relationships as an extension to semantic role labeling , our work investigates them in the context of textual inference scenarios . such scenarios provide prior information , which substantially eases the task . we provide a large and freely available evaluation dataset for our task setting , and propose methods to cope with it , while obtaining promising results in empirical evaluations .

initial explorations in english to turkish statistical machine translation ilknur durgar el-kahlout
this paper presents some very preliminary results for and problems in developing a statistical machine translation system from english to turkish . starting with a baseline word model trained from about 20k aligned sentences , we explore various ways of exploiting morphological structure to improve upon the baseline system . as turkish is a language with complex agglutinative word structures , we experiment with morphologically segmented and disambiguated versions of the parallel texts in order to also uncover relations between morphemes and function words in one language with morphemes and functions words in the other , in addition to relations between open class content words . morphological segmentation on the turkish side also conflates the statistics from allomorphs so that sparseness can be alleviated to a certain extent . we find that this approach coupled with a simple grouping of most frequent morphemes and function words on both sides improve the bleu score from the baseline of 0.0752 to 0.0913 with the small training data . we close with a discussion on why one should not expect distortion parameters to model word-local morpheme ordering and that a new approach to handling complex morphotactics is needed .

parsing the syntagrus treebank of russian
we present the first results on parsing the syntagrus treebank of russian with a data-driven dependency parser , achieving a labeled attachment score of over 82 % and an unlabeled attachment score of 89 % . a feature analysis shows that high parsing accuracy is crucially dependent on the use of both lexical and morphological features . we conjecture that the latter result can be generalized to richly inflected languages in general , provided that sufficient amounts of training data are available .

evaluating contextual dependency of paraphrases using a latent variable model
this paper presents an evaluation method employing a latent variable model for paraphrases with their contexts . we assume that the context of a sentence is indicated by a latent variable of the model as a topic and that the likelihood of each variable can be inferred . a paraphrase is evaluated for whether its sentences are used in the same context . experimental results showed that the proposed method achieves almost 60 % accuracy and that there is not a large performance difference between the two models . the results also revealed an upper bound of accuracy of 77 % with the method when using only topic information .

acquisition of unknown word paradigms for large-scale grammars
unknown words are a major issue for large-scale grammars of natural language . we propose a machine learning based algorithm for acquiring lexical entries for all forms in the paradigm of a given unknown word . the main advantages of our method are the usage of word paradigms to obtain valuable morphological knowledge , the consideration of different contexts which the unknown word and all members of its paradigm occur in and the employment of a full-blown syntactic parser and the grammar we want to improve to analyse these contexts and provide elaborate syntactic constraints . we test our algorithm on a large-scale grammar of dutch and show that its application leads to an improved parsing accuracy .

detecting complex predicates in hindi using pos projection across parallel corpora achla m raina
complex predicates or cps are multiword complexes functioning as single verbal units . cps are particularly pervasive in hindi and other indoaryan languages , but an usage account driven by corpus-based identification of these constructs has not been possible since single-language systems based on rules and statistical approaches require reliable tools ( pos taggers , parsers , etc . ) that are unavailable for hindi . this paper highlights the development of first such database based on the simple idea of projecting pos tags across an english-hindi parallel corpus . the cp types considered include adjective-verb ( av ) , noun-verb ( nv ) , adverb-verb ( adv-v ) , and verb-verb ( vv ) composites . cps are hypothesized where a verb in english is projected onto a multi-word sequence in hindi . while this process misses some cps , those that are detected appear to be more reliable ( 83 % precision , 46 % recall ) . the resulting database lists usage instances of 1439 cps in 4400 sentences .

learning word sense distributions , detecting unattested senses and identifying novel senses using topic models jey han lau , and timothy baldwin
unsupervised word sense disambiguation ( wsd ) methods are an attractive approach to all-words wsd due to their non-reliance on expensive annotated data . unsupervised estimates of sense frequency have been shown to be very useful for wsd due to the skewed nature of word sense distributions . this paper presents a fully unsupervised topic modelling-based approach to sense frequency estimation , which is highly portable to different corpora and sense inventories , in being applicable to any part of speech , and not requiring a hierarchical sense inventory , parsing or parallel text . we demonstrate the effectiveness of the method over the tasks of predominant sense learning and sense distribution acquisition , and also the novel tasks of detecting senses which arent attested in the corpus , and identifying novel senses in the corpus which arent captured in the sense inventory .

model adaptation via model interpolation and boosting for web search ranking
this paper explores two classes of model adaptation methods for web search ranking : model interpolation and error-driven learning approaches based on a boosting algorithm . the results show that model interpolation , though simple , achieves the best results on all the open test sets where the test data is very different from the training data . the tree-based boosting algorithm achieves the best performance on most of the closed test sets where the test data and the training data are similar , but its performance drops significantly on the open test sets due to the instability of trees . several methods are explored to improve the robustness of the algorithm , with limited success .

bilingual lexicon extraction from comparable corpora using
many existing methods for bilingual lexicon learning from comparable corpora are based on similarity of context vectors . these methods suffer from noisy vectors that greatly affect their accuracy . we introduce a method for filtering this noise allowing highly accurate learning of bilingual lexicons . our method is based on the notion of in-domain terms which can be thought of as the most important contextually relevant words . we provide a method for identifying such terms . our evaluation shows that the proposed method can learn highly accurate bilingual lexicons without using orthographic features or a large initial seed dictionary . in addition , we also introduce a method for measuring the similarity between two words in different languages without requiring any initial dictionary .

ontology-based linguistic annotation
we propose an ontology-based framework for linguistic annotation of written texts . we argue that linguistic annotation can be actually considered a special case of semantic annotation with regard to an ontology such as pursued within the context of the semantic web . furthermore , we present cream , a semantic annotation framework , as well as its concrete implementation ontomat and show how they can be used for the purpose of linguistic annotation . we demonstrate the value of our framework by applying it to the annotation of anaphoric relations in written texts .

learning nonstructural distance metric by minimum cluster distortions atr spoken language translation
much natural language processing still depends on the euclidean ( cosine ) distance function between two feature vectors , but this has severe problems with regard to feature weightings and feature correlations . to answer these problems , we propose an optimal metric distance that can be used as an alternative to the cosine distance , thus accommodating the two problems at the same time . this metric is optimal in the sense of global quadratic minimization , and can be obtained from the clusters in the training data in a supervised fashion . we confirmed the effect of the proposed metric distance by a synonymous sentence retrieval task , document retrieval task and the k-means clustering of general vectorial data . the results showed constant improvement over the baseline method of euclid and tf.idf , and were especially prominent for the sentence retrieval task , showing a 33 % increase in the 11-point average precision .

character-level chinese dependency parsing
recent work on chinese analysis has led to large-scale annotations of the internal structures of words , enabling characterlevel analysis of chinese syntactic structures . in this paper , we investigate the problem of character-level chinese dependency parsing , building dependency trees over characters . character-level information can benefit downstream applications by offering flexible granularities for word segmentation while improving wordlevel dependency parsing accuracies . we present novel adaptations of two major shift-reduce dependency parsing algorithms to character-level parsing . experimental results on the chinese treebank demonstrate improved performances over word-based parsing methods .

a comparative study on translation units for bilingual lexicon extraction
this paper presents on-going research on automatic extraction of bilingual lexicon from english-japanese parallel corpora . the main objective of this paper is to examine various ngram models of generating translation units for bilingual lexicon extraction . three n-gram models , a baseline model ( bound-length n-gram ) and two new models ( chunk-bound ngram and dependency-linked n-gram ) are compared . an experiment with 10000 english-japanese parallel sentences shows that chunk-bound ngram produces the best result in terms of accuracy ( 83 % ) as well as coverage ( 60 % ) and it improves approximately by 13 % in accuracy and by 5-9 % in coverage from the previously proposed baseline model .

distributed parse mining
we describe the design and implementation of a system for data exploration over dependency parses and derived semantic representations in a large-scale nlp-based search system at powerset.com . because of the distributed nature of the document repository and the processing infrastructure , and also the complex representations of the corpus data , standard text analysis tools such as grep or awk or language modeling toolkits are not applicable . this paper explores the challenges of extracting statistical information and of building language models in such a distributed nlp environment , and introduces a corpus analysis system , oceanography , that simplifies the writing of analysis code and transparently takes advantage of existing distributed processing infrastructure .

confidence-based active learning methods for machine translation
the paper presents experiments with active learning methods for the acquisition of training data in the context of machine translation . we propose a confidencebased method which is superior to the state-of-the-art method both in terms of quality and complexity . additionally , we discovered that oracle selection techniques that use real quality scores lead to poor results , making the effectiveness of confidence-driven methods of active learning for machine translation questionable .

statistical phrase-based translation
we propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several , previously proposed phrase-based translation models . within our framework , we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models . our empirical results , which hold for all examined language pairs , suggest that the highest levels of performance can be obtained through relatively simple means : heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations . surprisingly , learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance . learning only syntactically motivated phrases degrades the performance of our systems .

a corpus-based approach for the prediction of language impairment in monolingual english and spanish-english bilingual children
in this paper we explore a learning-based approach to the problem of predicting language impairment in children . we analyzed spontaneous narratives of children and extracted features measuring different aspects of language including morphology , speech fluency , language productivity and vocabulary . then , we evaluated a learning-based approach and compared its predictive accuracy against a method based on language models . empirical results on monolingual english-speaking children and bilingual spanish-english speaking children show the learning-based approach is a promising direction for automatic language assessment .

a fast and accurate dependency parser using neural networks
almost all current dependency parsers classify based on millions of sparse indicator features . not only do these features generalize poorly , but the cost of feature computation restricts parsing speed significantly . in this work , we propose a novel way of learning a neural network classifier for use in a greedy , transition-based dependency parser . because this classifier learns and uses just a small number of dense features , it can work very fast , while achieving an about 2 % improvement in unlabeled and labeled attachment scores on both english and chinese datasets . concretely , our parser is able to parse more than 1000 sentences per second at 92.2 % unlabeled attachment score on the english penn treebank .

a viable method for rapid discovery of arabic nicknames chiara higgins elizabeth mcgrath lailla moretto
this paper presents findings on using crowdsourcing via amazon mechanical turk ( mturk ) to obtain arabic nicknames as a contribution to exiting named entity ( ne ) lexicons . it demonstrates a strategy for increasing mturk participation from arab countries . the researchers validate the nicknames using experts , mturk workers , and google search and then compare them against the database of arabic names ( dan ) . additionally , the experiment looks at the effect of pay rate on speed of nickname collection and documents an advertising effect where mturk workers respond to existing work batches , called human intelligence tasks ( hits ) , more quickly once similar higher paying hits are posted .

evaluation of utility of lsa for word sense discrimination
the goal of the on-going project described in this paper is evaluation of the utility of latent semantic analysis ( lsa ) for unsupervised word sense discrimination . the hypothesis is that lsa can be used to compute context vectors for ambiguous words that can be clustered together with each cluster corresponding to a different sense of the word . in this paper we report first experimental result on tightness , separation and purity of sense-based clusters as a function of vector space dimensionality and using different distance metrics .

what do computational linguists need to know about linguistics
in this position paper , we argue that although the data-driven , empirical paradigm for computational linguistics seems to be the best way forward at the moment , a thorough grounding in descriptive linguistics is still needed to do competent work in the field . examples are given of how knowledge of linguistic phenomena leads to understanding the limitations of particular statistical models and to better feature selection for such models .

exploiting shallow linguistic information for relation extraction from biomedical literature
we propose an approach for extracting relations between entities from biomedical literature based solely on shallow linguistic information . we use a combination of kernel functions to integrate two different information sources : ( i ) the whole sentence where the relation appears , and ( ii ) the local contexts around the interacting entities . we performed experiments on extracting gene and protein interactions from two different data sets . the results show that our approach outperforms most of the previous methods based on syntactic and semantic information .

can semantic roles generalize across genres
propbank has been widely used as training data for semantic role labeling . however , because this training data is taken from the wsj , the resulting machine learning models tend to overfit on idiosyncrasies of that texts style , and do not port well to other genres . in addition , since propbank was designed on a verb-by-verb basis , the argument labels arg2 - arg5 get used for very diverse argument roles with inconsistent training instances . for example , the verb make uses arg2 for the material argument ; but the verb multiply uses arg2 for the extent argument . as a result , it can be difficult for automatic classifiers to learn to distinguish arguments arg2-arg5 . we have created a mapping between propbank and verbnet that provides a verbnet thematic role label for each verb-specific propbank label . since verbnet uses argument labels that are more consistent across verbs , we are able to demonstrate that these new labels are easier to learn .

text classication in asian languages without word segmentation
we present a simple approach for asian language text classification without word segmentation , based on statistical -gram language modeling . in particular , we examine chinese and japanese text classification . with character -gram models , our approach avoids word segmentation . however , unlike traditional ad hoc -gram models , the statistical language modeling based approach has strong information theoretic basis and avoids explicit feature selection procedure which potentially loses significantly amount of useful information . we systematically study the key factors in language modeling and their influence on classification . experiments on chinese trec and japanese ntcir topic detection show that the simple approach can achieve better performance compared to traditional approaches while avoiding word segmentation , which demonstrates its superiority in asian language text classification .

a clustering approach for the nearly unsupervised recognition of
in this paper we present trofi ( trope finder ) , a system for automatically classifying literal and nonliteral usages of verbs through nearly unsupervised word-sense disambiguation and clustering techniques . trofi uses sentential context instead of selectional constraint violations or paths in semantic hierarchies . it also uses literal and nonliteral seed sets acquired and cleaned without human supervision in order to bootstrap learning . we adapt a word-sense disambiguation algorithm to our task and augment it with multiple seed set learners , a voting schema , and additional features like supertags and extrasentential context . detailed experiments on hand-annotated data show that our enhanced algorithm outperforms the baseline by 24.4 % . using the trofi algorithm , we also build the trofi example base , an extensible resource of annotated literal/nonliteral examples which is freely available to the nlp research community .

direct parsing of discontinuous constituents in german
discontinuities occur especially frequently in languages with a relatively free word order , such as german . generally , due to the longdistance dependencies they induce , they lie beyond the expressivity of probabilistic cfg , i.e. , they can not be directly reconstructed by a pcfg parser . in this paper , we use a parser for probabilistic linear context-free rewriting systems ( plcfrs ) , a formalism with high expressivity , to directly parse the german negra and tiger treebanks . in both treebanks , discontinuities are annotated with crossing branches . based on an evaluation using different metrics , we show that an output quality can be achieved which is comparable to the output quality of pcfg-based systems .

user requirements analysis for meeting information retrieval based on query elicitation ecole polytechnique fdrale
we present a user requirements study for question answering on meeting records that assesses the difficulty of users questions in terms of what type of knowledge is required in order to provide the correct answer . we grounded our work on the empirical analysis of elicited user queries . we found that the majority of elicited queries ( around 60 % ) pertain to argumentative processes and outcomes . our analysis also suggests that standard keyword-based information retrieval can only deal successfully with less than 20 % of the queries , and that it must be complemented with other types of metadata and inference .

how gender and gender environment affect manifestations of power
we investigate the interaction of power , gender , and language use in the enron email corpus . we present a freely available extension to the enron corpus , with the gender of senders of 87 % messages reliably identified . using this data , we test two specific hypotheses drawn from the sociolinguistic literature pertaining to gender and power : women managers use face-saving communicative strategies , and women use language more explicitly than men to create and maintain social relations . we introduce the notion of gender environment to the computational study of written conversations ; we interpret this notion as the gender makeup of an email thread , and show that some manifestations of power differ significantly between gender environments . finally , we show the utility of gender information in the problem of automatically predicting the direction of power between pairs of participants in email interactions .

keyword translation accuracy and cross-lingual question answering in chinese and japanese
in this paper , we describe the extension of an existing monolingual qa system for english-to-chinese and english-tojapanese cross-lingual question answering ( clqa ) . we also attempt to characterize the influence of translation on clqa performance through experimental evaluation and analysis . the paper also describes some language-specific issues for keyword translation in clqa .

the edinburgh twitter corpus
we describe the first release of our corpus of 97 million twitter posts . we believe that this data will prove valuable to researchers working in social media , natural language processing , large-scale data processing , and similar areas .

irst-bp : preposition disambiguation chain clarifying relationships contexts
we are going to present a technique of preposition disambiguation based on sense discriminative patterns , which are acquired using a variant of angluins algorithm . they represent the essential information extracted from a particular type of local contexts we call chain clarifying relationship contexts . the data set and the results we present are from the semeval task , wsd of preposition ( litkowski 2007 ) .

the ' nays ' have it : exploring effects of sentiment in collaborative knowledge sharing
in this paper we study what effects sentiment have on the temporal dynamics of user interaction and content generation in a knowledge sharing setting . we try to identify how sentiment influences interaction dynamics in terms of answer arrival , user ratings arrival , community agreement and content popularity . our study suggests that negativity bias triggers more community attention and consequently more content contribution . our findings provide insight into how users interact in online knowledge sharing communities , and helpful for improving existing systems .

two methods to incorporate local morphosyntactic features in hindi dependency parsing
in this paper we explore two strategies to incorporate local morphosyntactic features in hindi dependency parsing . these features are obtained using a shallow parser . we first explore which information provided by the shallow parser is most beneficial and show that local morphosyntactic features in the form of chunk type , head/non-head information , chunk boundary information , distance to the end of the chunk and suffix concatenation are very crucial in hindi dependency parsing . we then investigate the best way to incorporate this information during dependency parsing . further , we compare the results of various experiments based on various criterions and do some error analysis . all the experiments were done with two data-driven parsers , maltparser and mstparser , on a part of multi-layered and multi-representational hindi treebank which is under development . this paper is also the first attempt at complete sentence level parsing for hindi .

generalizing word lattice translation
word lattice decoding has proven useful in spoken language translation ; we argue that it provides a compelling model for translation of text genres , as well . we show that prior work in translating lattices using finite state techniques can be naturally extended to more expressive synchronous context-free grammarbased models . additionally , we resolve a significant complication that non-linear word lattice inputs introduce in reordering models . our experiments evaluating the approach demonstrate substantial gains for chineseenglish and arabic-english translation .

towards conversational qa : automatic identification of problematic
to enable conversational qa , it is important to examine key issues addressed in conversational systems in the context of question answering . in conversational systems , understanding user intent is critical to the success of interaction . recent studies have also shown that the capability to automatically identify problematic situations during interaction can significantly improve the system performance . therefore , this paper investigates the new implications of user intent and problematic situations in the context of question answering . our studies indicate that , in basic interactive qa , there are different types of user intent that are tied to different kinds of system performance ( e.g. , problematic/error free situations ) . once users are motivated to find specific information related to their information goals , the interaction context can provide useful cues for the system to automatically identify problematic situations and user intent .

european language translation with weighted finite state transducers :
we describe the cambridge university engineering department phrase-based statistical machine translation system for spanishenglish and french-english translation in the acl 2008 third workshop on statistical machine translation shared task . the cued system follows a generative model of translation and is implemented by composition of component models realised as weighted finite state transducers , without the use of a special-purpose decoder . details of system tuning for both europarl and news translation tasks are provided .

fast tweet retrieval with compact binary codes
the most widely used similarity measure in the field of natural language processing may be cosine similarity . however , in the context of twitter , the large scale of massive tweet data inevitably makes it expensive to perform cosine similarity computations among tremendous data samples . in this paper , we exploit binary coding to tackle the scalability issue , which compresses each data sample into a compact binary code and hence enables highly efficient similarity computations via hamming distances between the generated codes . in order to yield semantics sensitive binary codes for tweet data , we design a binarized matrix factorization model and further improve it in two aspects . first , we force the projection directions employed by the model nearly orthogonal to reduce the redundant information in their resulting binary bits . second , we leverage the tweets neighborhood information to encourage similar tweets to have adjacent binary codes . evaluated on a tweet dataset using hashtags to create gold labels in an information retrieval scenario , our proposed model shows significant performance gains over competing methods .

transformation based chinese entity detection and tracking
this paper proposes a unified transformation based learning ( tbl , brill , 1995 ) framework for chinese entity detection and tracking ( edt ) . it consists of two sub models : a mention detection model and an entity tracking/coreference model . the first sub-model is used to adapt existing chinese word segmentation and named entity ( ne ) recognition results to a specific edt standard to find all the mentions . the second sub-model is used to find the coreference relation between the mentions . in addition , a feedback technique is proposed to further improve the performance of the system . we evaluated our methods on the automatic content extraction ( ace , nist , 2003 ) chinese edt corpus . results show that it outperforms the baseline , and achieves comparable performance with the stateof-the-art methods .

towards agile and test-driven development in nlp applications educational testing service educational testing service rosedale road rosedale road
c-rater is the educational testing service technology for automatic content scoring for short free-text responses . in this paper , we contend that an agile and test-driven development environment optimizes the development of an nlp-based technology .

lm studies on filled pauses in spontaneous medical dictation
we investigate the optimal lm treatment of abundant filled pauses ( fp ) in spontaneous monologues of a professional dictation task . questions addressed here are ( 1 ) how to deal with fp in the lm history and ( 2 ) to which extent can the lm distinguish between positions with high and low fp likelihood . our results differ partly from observations reported on dialogues . discarding fp from all lm histories clearly improves the performance . local perplexities , entropies and word rankings at positions following fp suggest that most fp indicate hesitations rather than restarts . proper prediction of fp allows to distinguish fp from word positions by a doubled fp probability . recognition experiments confirm the improvements found in our perplexity studies .

( linear ) maps of the impossible : capturing semantic anomalies in distributional space eva maria vecchi and marco baroni and roberto zamparelli
in this paper , we present a first attempt to characterize the semantic deviance of composite expressions in distributional semantics . specifically , we look for properties of adjective-noun combinations within a vectorbased semantic space that might cue their lack of meaning . we evaluate four different compositionality models shown to have various levels of success in representing the meaning of an pairs : the simple additive and multiplicative models of mitchell and lapata ( 2008 ) , and the linear-map-based models of guevara ( 2010 ) and baroni and zamparelli ( 2010 ) . for each model , we generate composite vectors for a set of an combinations unattested in the source corpus and which have been deemed either acceptable or semantically deviant . we then compute measures that might cue semantic anomaly , and compare each models results for the two classes of ans . our study shows that simple , unsupervised cues can indeed significantly tell unattested but acceptable ans apart from impossible , or deviant , ans , and that the simple additive and multiplicative models are the most effective in this task .

translating compounds by learning component gloss translation models via multiple languages
this paper presents an approach to the translation of compound words without the need for bilingual training text , by modeling the mapping of literal component word glosses ( e.g . iron-path ) into fluent english ( e.g . railway ) across multiple languages . performance is improved by adding component-sequence and learnedmorphology models along with context similarity from monolingual text and optional combination with traditional bilingual-textbased translation discovery .

resolving ambiguities of chinese conjunctive structures by divideand-conquer approaches
this paper presents a method to enhance a chinese parser in parsing conjunctive structures . long conjunctive structures cause long-distance dependencies and tremendous syntactic ambiguities . pure syntactic approaches hardly can determine boundaries of conjunctive phrases properly . in this paper , we propose a divide-andconquer approach which overcomes the difficulty of data-sparseness of the training data and uses both syntactic symmetry and semantic reasonableness to evaluate ambiguous conjunctive structures . in comparing with the performances of the pcfg parser without using the divide-andconquer approach , the precision of the conjunctive boundary detection is improved from 53.47 % to 83.17 % , and the bracketing f-score of sentences with conjunctive structures is raised up about 11 % .

comparison of similarity models for the relation discovery task
we present results on the relation discovery task , which addresses some of the shortcomings of supervised relation extraction by applying minimally supervised methods . we describe a detailed experimental design that compares various configurations of conceptual representations and similarity measures across six different subsets of the ace relation extraction data . previous work on relation discovery used a semantic space based on a term-bydocument matrix . we find that representations based on term co-occurrence perform significantly better . we also observe further improvements when reducing the dimensionality of the term co-occurrence matrix using probabilistic topic models , though these are not significant .

sentiment translation through multi-edge graphs
sentiment analysis systems can benefit from the translation of sentiment information . we present a novel , graph-based approach using simrank , a well-established graph-theoretic algorithm , to transfer sentiment information from a source language to a target language . we evaluate this method in comparison with semantic orientation using pointwise mutual information ( so-pmi ) , an established unsupervised method for learning the sentiment of phrases .

open book : a tool for helping asd users semantic comprehension
persons affected by autism spectrum disorders ( asd ) present impairments in social interaction . a significant percentile of them have inadequate reading comprehension skills . in the ongoing first project we build a multilingual tool called open book that helps the asd people to better understand the texts . the tool applies a series of automatic transformations to user documents to identify and remove the reading obstacles to comprehension . we focus on three semantic components : an image component that retrieves images for the concepts in the text , an idiom detection component and a topic model component . moreover , we present the personalization component that adapts the system output to user preferences .

i will shoot your shopping down and you can shoot all my tins automatic lexical acquisition from the childes database
empirical data regarding the syntactic complexity of childrens speech is important for theories of language acquisition . currently much of this data is absent in the annotated versions of the childes database . in this perliminary study , we show that a state-ofthe-art subcategorization acquisition system of preiss et al ( 2007 ) can be used to extract largescale subcategorization ( frequency ) information from the ( i ) child and ( ii ) child-directed speech within the childes database without any domain-specific tuning . we demonstrate that the acquired information is sufficiently accurate to confirm and extend previously reported research findings . we also report qualitative results which can be used to further improve parsing and lexical acquisition technology for child language data in the future .

using the web in machine learning for other-anaphora resolution
we present a machine learning framework for resolving other-anaphora . besides morpho-syntactic , recency , and semantic features based on existing lexical knowledge resources , our algorithm obtains additional semantic knowledge from the web . we search the web via lexico-syntactic patterns that are specific to other-anaphors . incorporating this innovative feature leads to an 11.4 percentage point improvement in the classifiers f -measure ( 25 % improvement relative to results without this feature ) .

semitic morphological analysis and generation using finite state transducers with feature structures
this paper presents an application of finite state transducers weighted with feature structure descriptions , following amtrup ( 2003 ) , to the morphology of the semitic language tigrinya . it is shown that feature-structure weights provide an efficient way of handling the templatic morphology that characterizes semitic verb stems as well as the long-distance dependencies characterizing the complex tigrinya verb morphotactics . a relatively complete computational implementation of tigrinya verb morphology is described .

transition-based dependency parsing with rich non-local features
transition-based dependency parsers generally use heuristic decoding algorithms but can accommodate arbitrarily rich feature representations . in this paper , we show that we can improve the accuracy of such parsers by considering even richer feature sets than those employed in previous systems . in the standard penn treebank setup , our novel features improve attachment score form 91.4 % to 92.9 % , giving the best results so far for transitionbased parsing and rivaling the best results overall . for the chinese treebank , they give a signficant improvement of the state of the art . an open source release of our parser is freely available .

distributional semantic models for the evaluation of disordered language
atypical semantic and pragmatic expression is frequently reported in the language of children with autism . although this atypicality often manifests itself in the use of unusual or unexpected words and phrases , the rate of use of such unexpected words is rarely directly measured or quantified . in this paper , we use distributional semantic models to automatically identify unexpected words in narrative retellings by children with autism . the classification of unexpected words is sufficiently accurate to distinguish the retellings of children with autism from those with typical development . these techniques demonstrate the potential of applying automated language analysis techniques to clinically elicited language data for diagnostic purposes .

alfara del patriarca ( valencia ) , spain
this paper describes the system developed in collabaration between uch and upv for the 2010 wmt . for this years workshop , we present a system for englishspanish translation . output n -best lists were rescored via a target neural network language model , yielding improvements in the final translation quality as measured by bleu and ter .

multi-word expression identification using sentence surface features
much nlp research on multi-word expressions ( mwes ) focuses on the discovery of new expressions , as opposed to the identification in texts of known expressions . however , mwe identification is not trivial because many expressions allow variation in form and differ in the range of variations they allow . we show that simple rule-based baselines do not perform identification satisfactorily , and present a supervised learning method for identification that uses sentence surface features based on expressions canonical form . to evaluate the method , we have annotated 3350 sentences from the british national corpus , containing potential uses of 24 verbal mwes . the method achieves an f-score of 94.86 % , compared with 80.70 % for the leading rule-based baseline . our method is easily applicable to any expression type . experiments in previous research have been limited to the compositional/non-compositional distinction , while we also test on sentences in which the words comprising the mwe appear but not as an expression .

training dependency parsers by jointly optimizing multiple objectives
we present an online learning algorithm for training parsers which allows for the inclusion of multiple objective functions . the primary example is the extension of a standard supervised parsing objective function with additional loss-functions , either based on intrinsic parsing quality or task-specific extrinsic measures of quality . our empirical results show how this approach performs for two dependency parsing algorithms ( graph-based and transition-based parsing ) and how it achieves increased performance on multiple target tasks including reordering for machine translation and parser adaptation .

kernels on linguistic structures for answer extraction
natural language processing ( nlp ) for information retrieval has always been an interesting and challenging research area . despite the high expectations , most of the results indicate that successfully using nlp is very complex . in this paper , we show how support vector machines along with kernel functions can effectively represent syntax and semantics . our experiments on question/answer classification show that the above models highly improve on bag-of-words on a trec dataset .

comparing representations of semantic roles for
we introduce new features for incorporating semantic predicate-argument structures in machine translation ( mt ) . the methods focus on the completeness of the semantic structures of the translations , as well as the order of the translated semantic roles . we experiment with translation rules which contain the core arguments for the predicates in the source side of a mt system , and observe that using these rules significantly improves the translation quality . we also present a new semantic feature that resembles a language model . our results show that the language model feature can also significantly improve mt results .

broadening the scope of the eagles / isle lexical standardization initiative
isle is a continuation of the long standing eagles initiative and it is supported by ec and nsf under the human language technology ( hlt ) programme . its objective is to develop widely agreed and urgently demanded standards and guidelines for infrastructural language resources , tools , and hlt products . eagles itself is a well-known trademark and point of reference for hlt projects and products and its previous results have already become de facto widely adopted standards . multilingual computational lexicons , natural interaction and multimodality , and evaluation are the three areas targeted by isle . in the first section of the paper we describe the overall goals and methodology of eagles/isle , in the second section we focus on the work of the computational lexicon working group , introducing its work strategy and the preliminary guidelines of a standard framework for multilingual computational lexicons , based on a general schema for the multilingual isle lexical entry ( mile ) .

convolution kernels on constituent , dependency and sequential structures for relation extraction
this paper explores the use of innovative kernels based on syntactic and semantic structures for a target relation extraction task . syntax is derived from constituent and dependency parse trees whereas semantics concerns to entity types and lexical sequences . we investigate the effectiveness of such representations in the automated relation extraction from texts . we process the above data by means of support vector machines along with the syntactic tree , the partial tree and the word sequence kernels . our study on the ace 2004 corpus illustrates that the combination of the above kernels achieves high effectiveness and significantly improves the current state-of-the-art .

part-of-speech induction in dependency trees for statistical machine
this paper proposes a nonparametric bayesian method for inducing part-ofspeech ( pos ) tags in dependency trees to improve the performance of statistical machine translation ( smt ) . in particular , we extend the monolingual infinite tree model ( finkel et al , 2007 ) to a bilingual scenario : each hidden state ( pos tag ) of a source-side dependency tree emits a source word together with its aligned target word , either jointly ( joint model ) , or independently ( independent model ) . evaluations of japanese-to-english translation on the ntcir-9 data show that our induced japanese pos tags for dependency trees improve the performance of a forestto-string smt system . our independent model gains over 1 point in bleu by resolving the sparseness problem introduced in the joint model .

named entity recognition for south asian languages salt lake city , utah
much work has already been done on building named entity recognition systems . however most of this work has been concentrated on english and other european languages . hence , building a named entity recognition ( ner ) system for south asian languages ( sal ) is still an open problem because they exhibit characteristics different from english . this paper builds a named entity recognizer which also identifies nested name entities for the hindi language using machine learning algorithm , trained on an annotated corpus . however , the algorithm is designed in such a manner that it can easily be ported to other south asian languages provided the necessary nlp tools like pos tagger and chunker are available for that language . i compare results of hindi data with english data of conll shared task of 2003 .

empirical study on the performance stability of named entity recognition model across domains
when a machine learning-based named entity recognition system is employed in a new domain , its performance usually degrades . in this paper , we provide an empirical study on the impact of training data size and domain information on the performance stability of named entity recognition models . we present an informative sample selection method for building high quality and stable named entity recognition models across domains . experimental results show that the performance of the named entity recognition model is enhanced significantly after being trained with these informative samples .

conundrums in unsupervised keyphrase extraction : making sense of the state-of-the-art kazi saidul hasan and vincent ng
state-of-the-art approaches for unsupervised keyphrase extraction are typically evaluated on a single dataset with a single parameter setting . consequently , it is unclear how effective these approaches are on a new dataset from a different domain , and how sensitive they are to changes in parameter settings . to gain a better understanding of state-of-the-art unsupervised keyphrase extraction algorithms , we conduct a systematic evaluation and analysis of these algorithms on a variety of standard evaluation datasets .

open entity extraction from web search query logs
in this paper we propose a completely unsupervised method for open-domain entity extraction and clustering over query logs . the underlying hypothesis is that classes defined by mining search user activity may significantly differ from those typically considered over web documents , in that they better model the user space , i.e . users perception and interests . we show that our method outperforms state of the art ( semi- ) supervised systems based either on web documents or on query logs ( 16 % gain on the clustering task ) . we also report evidence that our method successfully supports a real world application , namely keyword generation for sponsored search .

clac-core : exhaustive feature combination for measuring textual
clac-core , an exhaustive feature combination system ranked 4th among 34 teams in the semantic textual similarity shared task sts 2013. using a core set of 11 lexical features of the most basic kind , it uses a support vector regressor which uses a combination of these lexical features to train a model for predicting similarity between sentences in a two phase method , which in turn uses all combinations of the features in the feature space and trains separate models based on each combination . then it creates a meta-feature space and trains a final model based on that . this two step process improves the results achieved by singlelayer standard learning methodology over the same simple features . we analyze the correlation of feature combinations with the data sets over which they are effective .

towards an iterative reinforcement approach for simultaneous
though both document summarization and keyword extraction aim to extract concise representations from documents , these two tasks have usually been investigated independently . this paper proposes a novel iterative reinforcement approach to simultaneously extracting summary and keywords from single document under the assumption that the summary and keywords of a document can be mutually boosted . the approach can naturally make full use of the reinforcement between sentences and keywords by fusing three kinds of relationships between sentences and words , either homogeneous or heterogeneous . experimental results show the effectiveness of the proposed approach for both tasks . the corpus-based approach is validated to work almost as well as the knowledge-based approach for computing word semantics .

hit-cir : an unsupervised wsd system based on domain most frequent sense estimation
this paper presents an unsupervised system for all-word domain specific word sense disambiguation task . this system tags target word with the most frequent sense which is estimated using a thesaurus and the word distribution information in the domain . the thesaurus is automatically constructed from bilingual parallel corpus using paraphrase technique . the recall of this system is 43.5 % on semeval2 task 17 english data set .

minimalist parsing of subjects displaced from embedded clauses in free word order languages
in sayeed and szpakowicz ( 2004 ) , we proposed a parser inspired by some aspects of the minimalist program . this incremental parser was designed specifically to handle discontinuous constituency phenomena for nps in latin . we take a look at the application of this parser to a specific kind of apparent island violation in latin involving the extraction of constituents , including subjects , from tensed embedded clauses . we make use of ideas about the left periphery from rizzi ( 1997 ) to modify our parser in order to handle apparently violated subject islands and similar phenomena .

automatic generation of large-scale paraphrases
research on paraphrase has mostly focussed on lexical or syntactic variation within individual sentences . our concern is with larger-scale paraphrases , from multiple sentences or paragraphs to entire documents . in this paper we address the problem of generating paraphrases of large chunks of texts . we ground our discussion through a worked example of extending an existing nlg system to accept as input a source text , and to generate a range of fluent semantically-equivalent alternatives , varying not only at the lexical and syntactic levels , but also in document structure and layout .

enunciative and modal variations in newswire texts in french : from guideline to automatic annotation
in this paper we present the development of a corpus of french newswire texts annotated with enunciative and modal commitment information . the annotation scheme we propose is based on the detection of predicative cues - referring to an enunciative and/or modal variation - and their scope at a sentence level . we describe how we have improved our annotation guideline by using the evaluation ( in terms of precision , recall and f-measure ) of a first round of annotation produced by two expert annotators and by our automatic annotation system .

statistical script learning with multi-argument events
scripts represent knowledge of stereotypical event sequences that can aid text understanding . initial statistical methods have been developed to learn probabilistic scripts from raw text corpora ; however , they utilize a very impoverished representation of events , consisting of a verb and one dependent argument . we present a script learning approach that employs events with multiple arguments . unlike previous work , we model the interactions between multiple entities in a script . experiments on a large corpus using the task of inferring held-out events ( the narrative cloze evaluation ) demonstrate that modeling multi-argument events improves predictive accuracy .

demonstration of the parlance system : a data-driven , incremental , spoken dialogue system for interactive search
the parlance system for interactive search processes dialogue at a microturn level , displaying dialogue phenomena that play a vital role in human spoken conversation . these dialogue phenomena include more natural turn-taking through rapid system responses , generation of backchannels , and user barge-ins . the parlance demonstration system differentiates from other incremental systems in that it is data-driven with an infrastructure that scales well .

sampling alignment structure under a bayesian translation model
we describe the first tractable gibbs sampling procedure for estimating phrase pair frequencies under a probabilistic model of phrase alignment . we propose and evaluate two nonparametric priors that successfully avoid the degenerate behavior noted in previous work , where overly large phrases memorize the training data . phrase table weights learned under our model yield an increase in bleu score over the word-alignment based heuristic estimates used regularly in phrasebased translation systems .

learning of graph-based question answering rules
in this paper we present a graph-based approach to question answering . the method assumes a graph representation of question sentences and text sentences . question answering rules are automatically learnt from a training corpus of questions and answer sentences with the answer annotated . the method is independent from the graph representation formalism chosen . a particular example is presented that uses a specific graph representation of the logical contents of sentences .

coping with the subjectivity of human judgements in mt quality estimation
supervised approaches to nlp tasks rely on high-quality data annotations , which typically result from expensive manual labelling procedures . for some tasks , however , the subjectivity of human judgements might reduce the usefulness of the annotation for real-world applications . in machine translation ( mt ) quality estimation ( qe ) , for instance , using humanannotated data to train a binary classifier that discriminates between good ( useful for a post-editor ) and bad translations is not trivial . focusing on this binary task , we show that subjective human judgements can be effectively replaced with an automatic annotation procedure . to this aim , we compare binary classifiers trained on different data : the human-annotated dataset from the 7th workshop on statistical machine translation ( wmt-12 ) , and an automatically labelled version of the same corpus . our results show that human labels are less suitable for the task .

learning from a neighbor : adapting a japanese parser for korean through feature transfer learning
we present a new dependency parsing method for korean applying cross-lingual transfer learning and domain adaptation techniques . unlike existing transfer learning methods relying on aligned corpora or bilingual lexicons , we propose a feature transfer learning method with minimal supervision , which adapts an existing parser to the target language by transferring the features for the source language to the target language . specifically , we utilize the triplet/quadruplet model , a hybrid parsing algorithm for japanese , and apply a delexicalized feature transfer for korean . experiments with penn korean treebank show that even using only the transferred features from japanese achieves a high accuracy ( 81.6 % ) for korean dependency parsing . further improvements were obtained when a small annotated korean corpus was combined with the japanese training corpus , confirming that efficient crosslingual transfer learning can be achieved without expensive linguistic resources .

a probabilistic co-bootstrapping method for entity set expansion
entity set expansion ( ese ) aims at automatically acquiring instances of a specific target category . unfortunately , traditional ese methods usually have the expansion boundary problem and the semantic drift problem . to resolve the above two problems , this paper proposes a probabilistic co-bootstrapping method , which can accurately determine the expansion boundary using both the positive and the discriminant negative instances , and resolve the semantic drift problem by effectively maintaining and refining the expansion boundary during bootstrapping iterations . experimental results show that our method can achieve a competitive performance .

n-gram based two-step algorithm for word segmentation
this paper describes an n-gram based reinforcement approach to the closed track of word segmentation in the third chinese word segmentation bakeoff . character n-gram features of unigram , bigram , and trigram are extracted from the training corpus and its frequencies are counted . we investigated a step-by-step methodology by using the n-gram statistics . in the first step , relatively definite segmentations are fixed by the tight threshold value . the remaining tags are decided by considering the left or right space tags that are already fixed in the first step . definite and loose segmentation are performed simply based on the bigram and trigram statistics . in order to overcome the data sparseness problem of bigram data , unigram is used for the smoothing .

speech translation with grammatical framework
grammatical framework ( gf ) is a grammar formalism which supports interlinguabased translation , library-based grammar engineering , and compilation to speech recognition grammars . we show how these features can be used in the construction of portable high-precision domain-specific speech translators .

analyzing the errors of unsupervised learning
we identify four types of errors that unsupervised induction systems make and study each one in turn . our contributions include ( 1 ) using a meta-model to analyze the incorrect biases of a model in a systematic way , ( 2 ) providing an efficient and robust method of measuring distance between two parameter settings of a model , and ( 3 ) showing that local optima issues which typically plague em can be somewhat alleviated by increasing the number of training examples . we conduct our analyses on three models : the hmm , the pcfg , and a simple dependency model .

positional language models for clinical information retrieval
the peco framework is a knowledge representation for formulating clinical questions . queries are decomposed into four aspects , which are patient-problem ( p ) , exposure ( e ) , comparison ( c ) and outcome ( o ) . however , no test collection is available to evaluate such framework in information retrieval . in this work , we first present the construction of a large test collection extracted from systematic literature reviews . we then describe an analysis of the distribution of peco elements throughout the relevant documents and propose a language modeling approach that uses these distributions as a weighting strategy . in our experiments carried out on a collection of 1.5 million documents and 423 queries , our method was found to lead to an improvement of 28 % in map and 50 % in p @ 5 , as compared to the state-of-the-art method .

automatic knowledge representation using a graph-based algorithm for language-independent lexical chaining
lexical chains are powerful representations of documents . in particular , they have successfully been used in the field of automatic text summarization . however , until now , lexical chaining algorithms have only been proposed for english . in this paper , we propose a greedy language-independent algorithm that automatically extracts lexical chains from texts . for that purpose , we build a hierarchical lexico-semantic knowledge base from a collection of texts by using the pole-based overlapping clustering algorithm . as a consequence , our methodology can be applied to any language and proposes a solution to languagedependent lexical chainers .

investigating the usefulness of generalized word representations in smt
we investigate the use of generalized representations ( pos , morphological analysis and word clusters ) in phrase-based models and the n-gram-based operation sequence model ( osm ) . our integration enables these models to learn richer lexical and reordering patterns , consider wider contextual information and generalize better in sparse data conditions . when interpolating generalized osm models on the standard iwslt and wmt tasks we observed improvements of up to +1.35 on the english-to-german task and +0.63 for the german-to-english task . using automatically generated word classes in standard phrase-based models and the osm models yields an average improvement of +0.80 across 8 language pairs on the iwslt shared task .

semtag , the loria toolbox for tag-based parsing and generation
in this paper , we introduce semtag , a toolbox for tag-based parsing and generation . this environment supports the development of wide-coverage grammars and differs from existing environments for tag such as xtag , ( xtag-researchgroup , 2001 ) in that it includes a semantic dimension . semtag is open-source and freely available .

toward completeness in concept extraction and classification
many algorithms extract terms from text together with some kind of taxonomic classification ( is-a ) link . however , the general approaches used today , and specifically the methods of evaluating results , exhibit serious shortcomings . harvesting without focusing on a specific conceptual area may deliver large numbers of terms , but they are scattered over an immense concept space , making recall judgments impossible . regarding precision , simply judging the correctness of terms and their individual classification links may provide high scores , but this doesnt help with the eventual assembly of terms into a single coherent taxonomy . furthermore , since there is no correct and complete gold standard to measure against , most work invents some ad hoc evaluation measure . we present an algorithm that is more precise and complete than previous ones for identifying from web text just those concepts below a given seed term . comparing the results to wordnet , we find that the algorithm misses terms , but also that it learns many new terms not in wordnet , and that it classifies them in ways acceptable to humans but different from wordnet .

recursive autoencoders for itg-based translation
while inversion transduction grammar ( itg ) is well suited for modeling ordering shifts between languages , how to make applying the two reordering rules ( i.e. , straight and inverted ) dependent on actual blocks being merged remains a challenge . unlike previous work that only uses boundary words , we propose to use recursive autoencoders to make full use of the entire merging blocks alternatively . the recursive autoencoders are capable of generating vector space representations for variable-sized phrases , which enable predicting orders to exploit syntactic and semantic information from a neural language modelings perspective . experiments on the nist 2008 dataset show that our system significantly improves over the maxent classifier by 1.07 bleu points .

truecasing
truecasing is the process of restoring case information to badly-cased or noncased text . this paper explores truecasing issues and proposes a statistical , language modeling based truecaser which achieves an accuracy of 98 % on news articles . task based evaluation shows a 26 % f-measure improvement in named entity recognition when using truecasing . in the context of automatic content extraction , mention detection on automatic speech recognition text is also improved by a factor of 8. truecasing also enhances machine translation output legibility and yields a bleu score improvement of 80.2 % . this paper argues for the use of truecasing as a valuable component in text processing applications .

joint versus independent phonological feature models within crf phone recognition
we compare the effect of joint modeling of phonological features to independent feature detectors in a conditional random fields framework . joint modeling of features is achieved by deriving phonological feature posteriors from the posterior probabilities of the phonemes . we find that joint modeling provides superior performance to the independent models on the timit phone recognition task . we explore the effects of varying relationships between phonological features , and suggest that in an asr system , phonological features should be handled as correlated , rather than independent .

learning discriminative projections for text similarity measures
traditional text similarity measures consider each term similar only to itself and do not model semantic relatedness of terms . we propose a novel discriminative training method that projects the raw term vectors into a common , low-dimensional vector space . our approach operates by finding the optimal matrix to minimize the loss of the pre-selected similarity function ( e.g. , cosine ) of the projected vectors , and is able to efficiently handle a large number of training examples in the highdimensional space . evaluated on two very different tasks , cross-lingual document retrieval and ad relevance measure , our method not only outperforms existing state-of-the-art approaches , but also achieves high accuracy at low dimensions and is thus more efficient .

automatic selection of high quality parses created by a fully
the average results obtained by unsupervised statistical parsers have greatly improved in the last few years , but on many specific sentences they are of rather low quality . the output of such parsers is becoming valuable for various applications , and it is radically less expensive to create than manually annotated training data . hence , automatic selection of high quality parses created by unsupervised parsers is an important problem . in this paper we present pupa , a pos-based unsupervised parse assessment algorithm . the algorithm assesses the quality of a parse tree using pos sequence statistics collected from a batch of parsed sentences . we evaluate the algorithm by using an unsupervised pos tagger and an unsupervised parser , selecting high quality parsed sentences from english ( wsj ) and german ( negra ) corpora . we show that pupa outperforms the leading previous parse assessment algorithm for supervised parsers , as well as a strong unsupervised baseline . consequently , pupa allows obtaining high quality parses without any human involvement .

word selection for ebmt based on monolingual similarity and translation
we propose a method of constructing an example-based machine translation ( ebmt ) system that exploits a content-aligned bilingual corpus . first , the sentences and phrases in the corpus are aligned across the two languages , and the pairs with high translation confidence are selected and stored in the translation memory . then , for a given input sentences , the system searches for fitting examples based on both the monolingual similarity and the translation confidence of the pair , and the obtained results are then combined to generate the translation . our experiments on translation selection showed the accuracy of 85 % demonstrating the basic feasibility of our approach .

handling phrase reorderings for machine translation
we propose a distance phrase reordering model ( dpr ) for statistical machine translation ( smt ) , where the aim is to capture phrase reorderings using a structure learning framework . on both the reordering classification and a chinese-to-english translation task , we show improved performance over a baseline smt system .

taiwan child language corpus : data collection and annotation
taiwan child language corpus contains scripts transcribed from about 330 hours of recordings of fourteen young children from southern min chinese speaking families in taiwan . the format of the corpus adopts the child language data exchange system ( childes ) . the size of the corpus is about 1.6 million words . in this paper , we describe data collection , transcription , word segmentation , and part-of-speech annotation of this corpus . applications of the corpus are also discussed .

application of different techniques to dependency parsing
we present a set of experiments on dependency parsing of the basque dependency treebank ( bdt ) . the present work has examined several directions that try to explore the rich set of morphosyntactic features in the bdt : i ) experimenting the impact of morphological features , ii ) application of dependency tree transformations , iii ) application of a two-stage parsing scheme ( stacking ) , and iv ) combinations of the individual experiments . all the tests were conducted using maltparser ( nivre et al , 2007a ) , a freely available and state of the art dependency parser generator .

stacking for statistical machine translation
we propose the use of stacking , an ensemble learning technique , to the statistical machine translation ( smt ) models . a diverse ensemble of weak learners is created using the same smt engine ( a hierarchical phrase-based system ) by manipulating the training data and a strong model is created by combining the weak models on-the-fly . experimental results on two language pairs and three different sizes of training data show significant improvements of up to 4 bleu points over a conventionally trained smt model .

towards tracking political sentiment through microblog data
people express and amplify political opinions in microblogs such as twitter , especially when major political decisions are made . twitter provides a useful vehicle for capturing and tracking popular opinion on burning issues of the day . in this paper , we focus on tracking the changes in political sentiment related to the u.s. supreme court ( scotus ) and its decisions , focusing on the key dimensions on support , emotional intensity , and polarity . measuring changes in these sentiment dimensions could be useful for social and political scientists , policy makers , and the public . this preliminary work adapts existing sentiment analysis techniques to these new dimensions and the specifics of the corpus ( twitter ) . we illustrate the promise of our work with an important case study of tracking sentiment change building up to , and immediately following one recent landmark supreme court decision . this example illustrates how our work could help answer fundamental research questions in political science about the nature of supreme court power and its capacity to influence public discourse .

a joint information model for n-best ranking marina del rey , ca
in this paper , we present a method for modeling joint information when generating n-best lists . we apply the method to a novel task of characterizing the similarity of a group of terms where only a small set of many possible semantic properties may be displayed to a user . we demonstrate that considering the results jointly , by accounting for the information overlap between results , generates better n-best lists than considering them independently . we propose an information theoretic objective function for modeling the joint information in an n-best list and show empirical evidence that humans prefer the result sets produced by our joint model . our results show with 95 % confidence that the n-best lists generated by our joint ranking model are significantly different from a baseline independent model 50.0 % 3.1 % of the time , out of which they are preferred 76.6 % 5.2 % of the time .

a statistical language modeling approach to lattice-based spoken document retrieval
speech recognition transcripts are far from perfect ; they are not of sufficient quality to be useful on their own for spoken document retrieval . this is especially the case for conversational speech . recent efforts have tried to overcome this issue by using statistics from speech lattices instead of only the 1best transcripts ; however , these efforts have invariably used the classical vector space retrieval model . this paper presents a novel approach to lattice-based spoken document retrieval using statistical language models : a statistical model is estimated for each document , and probabilities derived from the document models are directly used to measure relevance . experimental results show that the lattice-based language modeling method outperforms both the language modeling retrieval method using only the 1-best transcripts , as well as a recently proposed lattice-based vector space retrieval method .

biomedical term recognition with the perceptron hmm algorithm
we propose a novel approach to the identification of biomedical terms in research publications using the perceptron hmm algorithm . each important term is identified and classified into a biomedical concept class . our proposed system achieves a 68.6 % f-measure based on 2,000 training medline abstracts and 404 unseen testing medline abstracts . the system achieves performance that is close to the state-of-the-art using only a small feature set . the perceptron hmm algorithm provides an easy way to incorporate many potentially interdependent features .

on robustness and domain adaptation using svd for word sense disambiguation
in this paper we explore robustness and domain adaptation issues for word sense disambiguation ( wsd ) using singular value decomposition ( svd ) and unlabeled data . we focus on the semi-supervised domain adaptation scenario , where we train on the source corpus and test on the target corpus , and try to improve results using unlabeled data . our method yields up to 16.3 % error reduction compared to state-of-the-art systems , being the first to report successful semi-supervised domain adaptation . surprisingly the improvement comes from the use of unlabeled data from the source corpus , and not from the target corpora , meaning that we get robustness rather than domain adaptation . in addition , we study the behavior of our system on the target domain .

marta recasens , toni mart , mariona taul llus mrquez , emili sapena
this paper presents the task coreference resolution in multiple languages to be run in semeval-2010 ( 5th international workshop on semantic evaluations ) . this task aims to evaluate and compare automatic coreference resolution systems for three different languages ( catalan , english , and spanish ) by means of two alternative evaluation metrics , thus providing an insight into ( i ) the portability of coreference resolution systems across languages , and ( ii ) the effect of different scoring metrics on ranking the output of the participant systems .

fully unsupervised graph-based discovery of general-specific noun relationships from web corpora frequency counts
in this paper , we propose a new methodology based on directed graphs and the textrank algorithm to automatically induce general-specific noun relations from web corpora frequency counts . different asymmetric association measures are implemented to build the graphs upon which the textrank algorithm is applied and produces an ordered list of nouns from the most general to the most specific . experiments are conducted based on the wordnet noun hierarchy and assess 65.69 % of correct word ordering .

use of coreference in automatic searching for multiword discourse markers in the prague dependency treebank magdalna rysov ji mrovsk
the paper introduces a possibility of new research offered by a multi-dimensional annotation of the prague dependency treebank . it focuses on exploitation of the annotation of coreference for the annotation of discourse relations expressed by multiword expressions . it tries to find which aspect interlinks these linguistic areas and how we can use this interplay in automatic searching for czech expressions like despite this ( navzdory tomu ) , because of this fact ( dky tto skutenosti ) functioning as multiword discourse markers .

from neighborhood to parenthood : the advantages of dependency representation over bigrams in brown clustering
we present an effective modification of the popular brown et al . 1992 word clustering algorithm , using a dependency language model . by leveraging syntax-based context , resulting clusters are better when evaluated against a wordnet for dutch . the improvements are stable across parameters such as number of clusters , minimum frequency and granularity . further refinement is possible through dependency relation selection . our approach achieves a desired clustering quality with less data , resulting in a decrease in cluster creation times .

leila : learning to extract information by linguistic analysis for computer science for computer science for computer science
one of the challenging tasks in the context of the semantic web is to automatically extract instances of binary relations from web documents for example all pairs of a person and the corresponding birthdate . in this paper , we present leila , a system that can extract instances of arbitrary given binary relations from natural language web documents without human interaction . different from previous approaches , leila uses a deep syntactic analysis . this results in consistent improvements over comparable systems ( such as e.g . snowball or texttoonto ) .

abstract for the invited talk
unsupervised nlp and human language acquisition : making connections to make progress sharon goldwater natural language processing and cognitive science are two fields in which unsupervised language learning is an important area of research . yet there is often little crosstalk between the two fields . in this talk , i will argue that considering the problem of unsupervised language learning from a cognitive perspective can lead to useful insights for the nlp researcher , while also showing how tools and methods from nlp and machine learning can shed light on human language acquisition . i will present two case examples , both of them models inspired by cognitive questions . the first is a model of word segmentation , which introduced new modeling and inference techniques into nlp while also yielding a better fit than previous models to human behavioral data on word segmentation . the second is more recent work on unsupervised grammar induction , in which prosodic cues are used to help identify syntactic boundaries . preliminary results indicate that such cues can be helpful , but also reveal weaknesses in existing unsupervised grammar induction methods from nlp , suggesting possible directions for future research .

experiments with french
this paper describes a series of french semantic role labelling experiments which show that a small set of manually annotated training data is superior to a much larger set containing semantic role labels which have been projected from a source language via word alignment . using universal part-of-speech tags and dependencies makes little difference over the original fine-grained tagset and dependency scheme . moreover , there seems to be no improvement gained from projecting semantic roles between direct translations than between indirect translations .

unsupervised translation induction for chinese abbreviations using monolingual corpora
chinese abbreviations are widely used in modern chinese texts . compared with english abbreviations ( which are mostly acronyms and truncations ) , the formation of chinese abbreviations is much more complex . due to the richness of chinese abbreviations , many of them may not appear in available parallel corpora , in which case current machine translation systems simply treat them as unknown words and leave them untranslated . in this paper , we present a novel unsupervised method that automatically extracts the relation between a full-form phrase and its abbreviation from monolingual corpora , and induces translation entries for the abbreviation by using its full-form as a bridge . our method does not require any additional annotated data other than the data that a regular translation system uses . we integrate our method into a state-ofthe-art baseline translation system and show that it consistently improves the performance of the baseline system on various nist mt test sets .

shallow language processing architecture for bulgarian centro per la ricerca scientifica e tecnologica
this paper describes lingua - an architecture for text processing in bulgarian . first , the pre-processing modules for tokenisation , sentence splitting , paragraph segmentation , partof-speech tagging , clause chunking and noun phrase extraction are outlined . next , the paper proceeds to describe in more detail the anaphora resolution module . evaluation results are reported for each processing task .

data-driven graph construction for semi-supervised graph-based learning in nlp
graph-based semi-supervised learning has recently emerged as a promising approach to data-sparse learning problems in natural language processing . all graph-based algorithms rely on a graph that jointly represents labeled and unlabeled data points . the problem of how to best construct this graph remains largely unsolved . in this paper we introduce a data-driven method that optimizes the representation of the initial feature space for graph construction by means of a supervised classifier . we apply this technique in the framework of label propagation and evaluate it on two different classification tasks , a multi-class lexicon acquisition task and a word sense disambiguation task . significant improvements are demonstrated over both label propagation using conventional graph construction and state-of-the-art supervised classifiers .

linguistically nave ! = language independent : why nlp needs linguistic typology
in this position paper , i argue that in order to create truly language-independent nlp systems , we need to incorporate linguistic knowledge . the linguistic knowledge in question is not intricate rule systems , but generalizations from linguistic typology about the range of variation in linguistic structures across languages .

dialog input ranking in a multi-domain environment using transferable belief model
this paper presents results of using belief functions to rank the list of candidate information provided in a noisy dialogue input . the information under consideration is the intended task to be performed and the information provided for the completion of the task . as an example , we use the task of information access in a multi-domain dialogue system . currently , the system contains knowledge of ten different domains . callers calling in are greeted with an open-ended how may i help you prompt ( thomson and wisowaty , 1999 ; chu-carroll and carpenter , 1999 ; gorin et al , 1997 ) . after receiving a reply from the caller , we extract word evidences from the recognized utterances . by using transferable belief model ( tbm ) , we in turn determine the task that the caller intends to perform as well as any information provided .

conversational robots : building blocks for grounding word meaning
how can we build robots that engage in fluid spoken conversations with people , moving beyond canned responses to words and towards actually understanding as a step towards addressing this question , we introduce a robotic architecture that provides a basis for grounding word meanings . the architecture provides perceptual , procedural , and affordance representations for grounding words . a perceptuallycoupled on-line simulator enables sensorymotor representations that can shift points of view . held together , we show that this architecture provides a rich set of data structures and procedures that provide the foundations for grounding the meaning of certain classes of words .

cross-lingual distributional profiles of concepts for measuring semantic distance
we present the idea of estimating semantic distance in one , possibly resource-poor , language using a knowledge source in another , possibly resource-rich , language . we do so by creating cross-lingual distributional profiles of concepts , using a bilingual lexicon and a bootstrapping algorithm , but without the use of any sense-annotated data or word-aligned corpora . the cross-lingual measures of semantic distance are evaluated on two tasks : ( 1 ) estimating semantic distance between words and ranking the word pairs according to semantic distance , and ( 2 ) solving readers digest word power problems . in task ( 1 ) , cross-lingual measures are superior to conventional monolingual measures based on a wordnet . in task ( 2 ) , cross-lingual measures are able to solve more problems correctly , and despite scores being affected by many tied answers , their overall performance is again better than the best monolingual measures .

clinical information retrieval using document and pico structure
in evidence-based medicine , clinical questions involve four aspects : patient/problem ( p ) , intervention ( i ) , comparison ( c ) and outcome ( o ) , known as pico elements . in this paper we present a method that extends the language modeling approach to incorporate both document structure and pico query formulation . we present an analysis of the distribution of pico elements in medical abstracts that motivates the use of a location-based weighting strategy . in experiments carried out on a collection of 1.5 million abstracts , the method was found to lead to an improvement of roughly 60 % in map and 70 % in p @ 10 as compared to state-of-the-art methods .

concepts across categories
verbs or adjectives and their nominalizations and certain adverb adjective pairs can be argued to introduce the same concept . this can be shown through inference patterns , which can be explained if we assume davidsonian eventualities underlying all predicates . we make a contribution to the underlying state discussion by investigating the advantages and disadvantages of davidsonian versus kimian states for statives such as copular predicates . findings are implemented in our parser delilah .

joint syntactic and semantic parsing with combinatory categorial grammar
we present an approach to training a joint syntactic and semantic parser that combines syntactic training information from ccgbank with semantic training information from a knowledge base via distant supervision . the trained parser produces a full syntactic parse of any sentence , while simultaneously producing logical forms for portions of the sentence that have a semantic representation within the parsers predicate vocabulary . we demonstrate our approach by training a parser whose semantic representation contains 130 predicates from the nell ontology . a semantic evaluation demonstrates that this parser produces logical forms better than both comparable prior work and a pipelined syntax-then-semantics approach . a syntactic evaluation on ccgbank demonstrates that the parsers dependency fscore is within 2.5 % of state-of-the-art .

optimizing semantic coherence in topic models
latent variable models have the potential to add value to large document collections by discovering interpretable , low-dimensional subspaces . in order for people to use such models , however , they must trust them . unfortunately , typical dimensionality reduction methods for text , such as latent dirichlet al location , often produce low-dimensional subspaces ( topics ) that are obviously flawed to human domain experts . the contributions of this paper are threefold : ( 1 ) an analysis of the ways in which topics can be flawed ; ( 2 ) an automated evaluation metric for identifying such topics that does not rely on human annotators or reference collections outside the training data ; ( 3 ) a novel statistical topic model based on this metric that significantly improves topic quality in a large-scale document collection from the national institutes of health ( nih ) .

online learning in tensor space yuan cao sanjeev khudanpur
we propose an online learning algorithm based on tensor-space models . a tensorspace model represents data in a compact way , and via rank-1 approximation the weight tensor can be made highly structured , resulting in a significantly smaller number of free parameters to be estimated than in comparable vector-space models . this regularizes the model complexity and makes the tensor model highly effective in situations where a large feature set is defined but very limited resources are available for training . we apply with the proposed algorithm to a parsing task , and show that even with very little training data the learning algorithm based on a tensor model performs well , and gives significantly better results than standard learning algorithms based on traditional vectorspace models .

unsupervised learning of morphology using a novel directed search algorithm : taking the first step
this paper describes a system for the unsupervised learning of morphological suffixes and stems from word lists . the system is composed of a generative probability model and a novel search algorithm . by examining morphologically rich subsets of an input lexicon , the search identifies highly productive paradigms . quantitative results are shown by measuring the accuracy of the morphological relations identified . experiments in english and polish , as well as comparisons with other recent unsupervised morphology learning algorithms demonstrate the effectiveness of this technique .

polarization and abstraction of grammatical formalisms as
in the context of lexicalized grammars , we propose general methods for lexical disambiguation based on polarization and abstraction of grammatical formalisms . polarization makes their resource sensitivity explicit and abstraction aims at keeping essentially the mechanism of neutralization between polarities . parsing with the simplified grammar in the abstract formalism can be used efficiently for filtering lexical selections .

document structuring la sdrt lattice loria
in this paper , the issue of document structuring is addressed . to achieve this task , we advocate that segmented discourse representation theory ( sdrt ) is a most expressive discourse framework . then we sketch a discourse planning mechanism which aims at producing as many paraphrastic document structures as possible from a set of factual data encoded into a logical form .

learning question classifiers
in order to respond correctly to a free form factual question given a large collection of texts , one needs to understand the question to a level that allows determining some of the constraints the question imposes on a possible answer . these constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer . this paper presents a machine learning approach to question classification . we learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types , and eventually classifies questions into finegrained classes . we show accurate results on a large collection of free-form questions used in trec 10 .

cross-lingual variation of light verb constructions : using parallel corpora
cross-lingual parallelism and small-scale language variation have recently become subject of research in both computational and theoretical linguistics . in this article , we use a parallel corpus and an automatic aligner to study english light verb constructions and their german translations . we show that parallel corpus data can provide new empirical evidence for better understanding the properties of light verbs . we also study the influence that the identified properties of light verb constructions have on the quality of their automatic alignment in a parallel corpus . we show that , even though characterised by limited compositionality , these constructions can be aligned better than fully compositional phrases , due to an interaction between the type of light verb construction and its frequency .

a graph model for unsupervised lexical acquisition
this paper presents an unsupervised method for assembling semantic knowledge from a part-ofspeech tagged corpus using graph algorithms . the graph model is built by linking pairs of words which participate in particular syntactic relationships . we focus on the symmetric relationship between pairs of nouns which occur together in lists . an incremental cluster-building algorithm using this part of the graph achieves 82 % accuracy at a lexical acquisition task , evaluated against wordnet classes . the model naturally realises domain and corpus specific ambiguities as distinct components in the graph surrounding an ambiguous word .

exceptionality and natural language learning
previous work has argued that memory-based learning is better than abstraction-based learning for a set of language learning tasks . in this paper , we first attempt to generalize these results to a new set of language learning tasks from the area of spoken dialog systems and to a different abstraction-based learner . we then examine the utility of various exceptionality measures for predicting where one learner is better than the other . our results show that generalization of previous results to our tasks is not so obvious and some of the exceptionality measures may be used to characterize the performance of our learners .

determining the syntactic structure of medical terms in clinical notes for health informatics
this paper demonstrates a method for determining the syntactic structure of medical terms . we use a model-fitting method based on the log likelihood ratio to classify three-word medical terms as right or left-branching . we validate this method by computing the agreement between the classification produced by the method and manually annotated classifications . the results show an agreement of 75 % - 83 % . this method may be used effectively to enable a wide range of applications that depend on the semantic interpretation of medical terms including automatic mapping of terms to standardized vocabularies and induction of terminologies from unstructured medical text .

using synonym relations in chinese collocation extraction
a challenging task in chinese collocation extraction is to improve both the precision and recall rate . most lexical statistical methods including xtract face the problem of unable to extract collocations with lower frequencies than a given threshold . this paper presents a method where hownet is used to find synonyms using a similarity function . based on such synonym information , we have successfully extracted synonymous collocations which normally can not be extracted using the lexical statistical approach . we applied synonyms mapping to each headword to extract more synonymous word bi-grams . our evaluation over 60mb tagged corpus shows that we can extract synonymous collocations that occur with very low frequency , sometimes even for collocations that occur only once in the training set . comparing to a collocation extraction system based on xtract , we have reached the precision rate of 43 % on word bi-grams for a set of 9 headwords , almost 50 % improvement from precision rate of 30 % in the xtract system . furthermore , it improves the recall rate of word bi-gram collocation extraction by 30 % .

hierarchical phrase-based machine translation with word-based
hierarchical phrase-based machine translation can capture global reordering with synchronous context-free grammar , but has little ability to evaluate the correctness of word orderings during decoding . we propose a method to integrate word-based reordering model into hierarchical phrasebased machine translation to overcome this weakness . our approach extends the synchronous context-free grammar rules of hierarchical phrase-based model to include reordered source strings , allowing efficient calculation of reordering model scores during decoding . our experimental results on japanese-to-english basic travel expression corpus showed that the bleu scores obtained by our proposed system were better than those obtained by a standard hierarchical phrase-based machine translation system .

recursive deep models for semantic compositionality over a sentiment treebank
semantic word spaces have been very useful but can not express the meaning of longer phrases in a principled way . further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition . to remedy this , we introduce a sentiment treebank . it includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality . to address them , we introduce the recursive neural tensor network . when trained on the new treebank , this model outperforms all previous methods on several metrics . it pushes the state of the art in single sentence positive/negative classification from 80 % up to 85.4 % . the accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7 % , an improvement of 9.7 % over bag of features baselines . lastly , it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases .

inference and computational semantics
this paper discusses inference in computational semantics. we argue that state of - the - art methods in rst-order theorem proving and model building are of direct relevance to inference for natural language processing. we support our claim by discussing our implementation of van der sandt ' s presupposition pro jection algorithm in discourse representation theory, an algorithm which demands sustained use of powerful inference mechanisms .

automated measures of specific vocabulary knowledge from constructed responses ( use these words to write a sentence based on this picture ) educational testing services
we describe a system for automatically scoring a vocabulary item type that asks test-takers to use two specific words in writing a sentence based on a picture . the system consists of a rule-based component and a machine learned statistical model which uses a variety of construct-relevant features . specifically , in constructing the statistical model , we investigate if grammar , usage , and mechanics features developed for scoring essays can be applied to short answers , as in our task . we also explore new features reflecting the quality of the collocations in the response , as well as features measuring the consistency of the response to the picture . system accuracy in scoring is 15 percentage points greater than the majority class baseline and 10 percentage points less than human performance .

exploiting wikipedia as external knowledge for named entity recognition
we explore the use of wikipedia as external knowledge to improve named entity recognition ( ner ) . our method retrieves the corresponding wikipedia entry for each candidate word sequence and extracts a category label from the first sentence of the entry , which can be thought of as a definition part . these category labels are used as features in a crf-based ne tagger . we demonstrate using the conll 2003 dataset that the wikipedia category labels extracted by such a simple method actually improve the accuracy of ner .

but what do they mean an exploration into the range of cross-turn expectations denied by but
in this paper we hypothesise that denial of expectation ( dofe ) across turns in dialogue signalled by but can involve a range of different expectations , i.e. , not just causal expectations , as argued in the literature . we will argue for this hypothesis and outline a methodology to distinguish the relations these denied expectations convey . finally we will demonstrate the practical utility of this hypothesis by showing how it can improve generation of appropriate responses to dofe and decrease the likelihood of misunderstandings based on incorrectly interpreting these underlying cross-speaker relations .

parsing noun phrase structure with ccg
statistical parsing of noun phrase ( np ) structure has been hampered by a lack of goldstandard data . this is a significant problem for ccgbank , where binary branching np derivations are often incorrect , a result of the automatic conversion from the penn treebank . we correct these errors in ccgbank using a gold-standard corpus of np structure , resulting in a much more accurate corpus . we also implement novel ner features that generalise the lexical information needed to parse nps and provide important semantic information . finally , evaluating against depbank demonstrates the effectiveness of our modified corpus and novel features , with an increase in parser performance of 1.51 % .

a best-first alignment algorithm for automatic extraction of transfer mappings from bilingual corpora
translation systems that automatically extract transfer mappings ( rules or examples ) from bilingual corpora have been hampered by the difficulty of achieving accurate alignment and acquiring high quality mappings . we describe an algorithm that uses a bestfirst strategy and a small alignment grammar to significantly improve the quality of the transfer mappings extracted . for each mapping , frequencies are computed and sufficient context is retained to distinguish competing mappings during translation . variants of the algorithm are run against a corpus containing 200k sentence pairs and evaluated based on the quality of resulting translations .

a flexible pragmatics-driven language generator for animated agents
this paper describes the neca mnlg ; a fully implemented multimodal natural language generation module . the mnlg is deployed as part of the neca system which generates dialogues between animated agents . the generation module supports the seamless integration of full grammar rules , templates and canned text . the generator takes input which allows for the specification of syntactic , semantic and pragmatic constraints on the output .

an examination of regret in bullying tweets
social media users who post bullying related tweets may later experience regret , potentially causing them to delete their posts . in this paper , we construct a corpus of bullying tweets and periodically check the existence of each tweet in order to infer if and when it becomes deleted . we then conduct exploratory analysis in order to isolate factors associated with deleted posts . finally , we propose the construction of a regrettable posts predictor to warn users if a tweet might cause regret .

acquiring domain-specific dialog information from task-oriented human-human interaction through an unsupervised learning
we describe an approach for acquiring the domain-specific dialog knowledge required to configure a task-oriented dialog system that uses human-human interaction data . the key aspects of this problem are the design of a dialog information representation and a learning approach that supports capture of domain information from in-domain dialogs . to represent a dialog for a learning purpose , we based our representation , the form-based dialog structure representation , on an observable structure . we show that this representation is sufficient for modeling phenomena that occur regularly in several dissimilar taskoriented domains , including informationaccess and problem-solving . with the goal of ultimately reducing human annotation effort , we examine the use of unsupervised learning techniques in acquiring the components of the form-based representation ( i.e . task , subtask , and concept ) . these techniques include statistical word clustering based on mutual information and kullback-liebler distance , texttiling , hmm-based segmentation , and bisecting k-mean document clustering . with some modifications to make these algorithms more suitable for inferring the structure of a spoken dialog , the unsupervised learning algorithms show promise .

cfilt-core : semantic textual similarity using universal networking
this paper describes the system that was submitted in the *sem 2013 semantic textual similarity shared task . the task aims to find the similarity score between a pair of sentences . we describe a universal networking language ( unl ) based semantic extraction system for measuring the semantic similarity . our approach combines syntactic and word level similarity measures along with the unl based semantic similarity measures for finding similarity scores between sentences .

computational analysis of move structures in academic abstracts
this paper introduces a method for computational analysis of move structures in abstracts of research articles . in our approach , sentences in a given abstract are analyzed and labeled with a specific move in light of various rhetorical functions . the method involves automatically gathering a large number of abstracts from the web and building a language model of abstract moves . we also present a prototype concordancer , care , which exploits the move-tagged abstracts for digital learning . this system provides a promising approach to webbased computer-assisted academic writing .

web-based frequency dictionaries for medium density languages u of edinburgh
frequency dictionaries play an important role both in psycholinguistic experiment design and in language technology . the paper describes a new , freely available , web-based frequency dictionary of hungarian that is being used for both purposes , and the language-independent techniques used for creating it .

an orthonormal basis for topic segmentation in tutorial dialogue andrew olney zhiqiang cai
this paper explores the segmentation of tutorial dialogue into cohesive topics . a latent semantic space was created using conversations from human to human tutoring transcripts , allowing cohesion between utterances to be measured using vector similarity . previous cohesionbased segmentation methods that focus on expository monologue are reapplied to these dialogues to create benchmarks for performance . a novel moving window technique using orthonormal bases of semantic vectors significantly outperforms these benchmarks on this dialogue segmentation task .

scaling semi-supervised naive bayes with feature marginals
semi-supervised learning ( ssl ) methods augment standard machine learning ( ml ) techniques to leverage unlabeled data . ssl techniques are often effective in text classification , where labeled data is scarce but large unlabeled corpora are readily available . however , existing ssl techniques typically require multiple passes over the entirety of the unlabeled data , meaning the techniques are not applicable to large corpora being produced today . in this paper , we show that improving marginal word frequency estimates using unlabeled data can enable semi-supervised text classification that scales to massive unlabeled data sets . we present a novel learning algorithm , which optimizes a naive bayes model to accord with statistics calculated from the unlabeled corpus . in experiments with text topic classification and sentiment analysis , we show that our method is both more scalable and more accurate than ssl techniques from previous work .

shared task on prediction of dropout over time in massively open
the shared task on prediction of dropout over time in moocs involves analysis of data from 6 moocs offered through coursera . data from one mooc with approximately 30k students was distributed as training data and consisted of discussion forum data ( in sql ) and clickstream data ( in json format ) . the prediction task was predicting attrition over time . based on behavioral data from a weeks worth of activity in a mooc for a student , predict whether the student will cease to actively participate after that week . this paper describes the task . a full write up of the results is published separately ( ros & siemens , 2014 ) .

an automated english scoring system
this paper explores an issue of redundant errors reported while automatically scoring english learners sentences . we use a human-computer collaboration approach to eliminate redundant errors . the first step is to automatically select candidate redundant errors using pmi and rfc . since those errors are detected with different ids although they represent the same error , the candidacy can not be confirmed automatically . the errors are then handed over to human experts to determine the candidacy . the final candidates are provided to the system and trained with a decision tree . with those redundant errors eliminated , the system accuracy has been improved .

experiments using ostia for a language production task
the phenomenon of meaning-preserving corrections given by an adult to a child involves several aspects : ( 1 ) the child produces an incorrect utterance , which the adult nevertheless understands , ( 2 ) the adult produces a correct utterance with the same meaning and ( 3 ) the child recognizes the adult utterance as having the same meaning as its previous utterance , and takes that as a signal that its previous utterance is not correct according to the adult grammar . an adequate model of this phenomenon must incorporate utterances and meanings , account for how the child and adult can understand each others meanings , and model how meaning-preserving corrections interact with the childs increasing mastery of language production . in this paper we are concerned with how a learner who has learned to comprehend utterances might go about learning to produce them .

towards emotion prediction in spoken tutoring dialogues
human tutors detect and respond to student emotional states , but current machine tutors do not . our preliminary machine learning experiments involving transcription , emotion annotation and automatic feature extraction from our human-human spoken tutoring corpus indicate that the spoken tutoring system we are developing can be enhanced to automatically predict and adapt to student emotional states .

chinese syntactic reordering for adequate generation of korean verbal phrases in chinese-to-korean smt electrical and computer engineering division ,
chinese and korean belong to different language families in terms of word-order and morphological typology . chinese is an svo and morphologically poor language while korean is an sov and morphologically rich one . in chinese-to-korean smt systems , systematic differences between the verbal systems of the two languages make the generation of korean verbal phrases difficult . to resolve the difficulties , we address two issues in this paper . the first issue is that the verb position is different from the viewpoint of word-order typology . the second is the difficulty of complex morphology generation of korean verbs from the viewpoint of morphological typology . we propose a chinese syntactic reordering that is better at generating korean verbal phrases in chinese-to-korean smt . specifically , we consider reordering rules targeting chinese verb phrases ( vps ) , preposition phrases ( pps ) , and modality-bearing words that are closely related to korean verbal phrases . we verify our system with two corpora of different domains . our proposed approach significantly improves the performance of our system over a baseline phrased-based smt system .

detecting change and emergence for multiword expressions
this work looks at a temporal aspect of multiword expressions ( mwes ) , namely that the behaviour of a given n-gram and its status as a mwe change over time . we propose a model in which context words have particular probabilities given a usage choice for an n-gram , and those usage choices have time dependent probabilities , and we put forward an expectationmaximisation technique for estimating the parameters from data with no annotation of usage choice . for a range of mwe usages of recent coinage , we evaluate whether the technique is able to detect the emerging usage .

syntactic features for high precision word sense disambiguation
this paper explores the contribution of a broad range of syntactic features to wsd : grammatical relations coded as the presence of adjuncts/arguments in isolation or as subcategorization frames , and instantiated grammatical relations between words . we have tested the performance of syntactic features using two different ml algorithms ( decision lists and adaboost ) on the senseval-2 data . adding syntactic features to a basic set of traditional features improves performance , especially for adaboost . in addition , several methods to build arbitrarily high accuracy wsd systems are also tried , showing that syntactic features allow for a precision of 86 % and a coverage of 26 % or 95 % precision and 8 % coverage .

what decisions have you made : automatic decision detection in conversational speech
this study addresses the problem of automatically detecting decisions in conversational speech . we formulate the problem as classifying decision-making units at two levels of granularity : dialogue acts and topic segments . we conduct an empirical analysis to determine the characteristic features of decision-making dialogue acts , and train maxent models using these features for the classification tasks . we find that models that combine lexical , prosodic , contextual and topical features yield the best results on both tasks , achieving 72 % and 86 % precision , respectively . the study also provides a quantitative analysis of the relative importance of the feature types .

hyena-live : fine-grained online entity type classification from
recent research has shown progress in achieving high-quality , very fine-grained type classification in hierarchical taxonomies . within such a multi-level type hierarchy with several hundreds of types at different levels , many entities naturally belong to multiple types . in order to achieve high-precision in type classification , current approaches are either limited to certain domains or require time consuming multistage computations . as a consequence , existing systems are incapable of performing ad-hoc type classification on arbitrary input texts . in this demo , we present a novel webbased tool that is able to perform domain independent entity type classification under real time conditions . thanks to its efficient implementation and compacted feature representation , the system is able to process text inputs on-the-fly while still achieving equally high precision as leading state-ofthe-art implementations . our system offers an online interface where natural-language text can be inserted , which returns semantic type labels for entity mentions . further more , the user interface allows users to explore the assigned types by visualizing and navigating along the type-hierarchy .

generating synthetic comparable questions for news articles
we introduce the novel task of automatically generating questions that are relevant to a text but do not appear in it . one motivating example of its application is for increasing user engagement around news articles by suggesting relevant comparable questions , such as is beyonce a better singer than madonna , for the user to answer . we present the first algorithm for the task , which consists of : ( a ) offline construction of a comparable question template database ; ( b ) ranking of relevant templates to a given article ; and ( c ) instantiation of templates only with entities in the article whose comparison under the templates relation makes sense . we tested the suggestions generated by our algorithm via a mechanical turk experiment , which showed a significant improvement over the strongest baseline of more than 45 % in all metrics .

annotating events , temporal expressions and relations in italian : the it-timeml experience for the ita-timebank tommaso caselli valentina bartalesi lenzi rachele sprugnoli emanuele pianta irina prodanof
this paper presents the annotation guidelines and specifications which have been developed for the creation of the italian timebank , a language resource composed of two corpora manually annotated with temporal and event information . in particular , the adaptation of the timeml scheme to italian is described , and a special attention is given to the methodology used for the realization of the annotation specifications , which are strategic in order to create good quality annotated resources and to justify the annotated items . the reliability of the it-timeml guidelines and specifications is evaluated on the basis of the results of the inter-coder agreement performed during the annotation of the two corpora .

applying morphological decomposition to statistical machine translation
this paper describes the aalto submission for the german-to-english and the czechto-english translation tasks of the acl 2010 joint fifth workshop on statistical machine translation and metricsmatr . statistical machine translation has focused on using words , and longer phrases constructed from words , as tokens in the system . in contrast , we apply different morphological decompositions of words using the unsupervised morfessor algorithms . while translation models trained using the morphological decompositions did not improve the bleu scores , we show that the minimum bayes risk combination with a word-based translation model produces significant improvements for the germanto-english translation . however , we did not see improvements for the czech-toenglish translations .

statistical transliteration for cross langauge information retrieval using
in this paper we present a statistical transliteration technique that is language independent . this technique uses hidden markov model ( hmm ) alignment and conditional random fields ( crf ) , a discriminative model . hmm alignment maximizes the probability of the observed ( source , target ) word pairs using the expectation maximization algorithm and then the character level alignments ( n-gram ) are set to maximum posterior predictions of the model . crf has efficient training and decoding processes which is conditioned on both source and target languages and produces globally optimal solutions . we apply this technique for hindi-english transliteration task . the results show that our technique perfoms better than the existing transliteration system which uses hmm alignment and conditional probabilities derived from counting the alignments .

