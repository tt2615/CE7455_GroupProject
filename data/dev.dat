
A speech feature based on Bark frequency warping-the non-uniform linear prediction (NLP) cepstrum

Auto Curation on FaceNet Embeddings with Gamma and Gaussian Distribution to Predict Model Performance in Actual Industrial Deployment
Abstract:Many AI applications, such as face recognition [1] and NLP, rely heavily on data embedding as an intermediate representation on which further processing is made. However few of these applications gain insights to such intermediate representation, and thus have difficulties in data analytic or designing efficient models, or both. The resulting models accordingly designed are thus hard to analyze for performance tuning and optimization. We deeply dived into the embedding of FaceNet in an actual industrial deployed site, and propose a closed-loop solution with data representation, data curation, data modeling on these intermediate data, as to do performance prediction for 1:1 and 1:N scenarios [2]. Our results shows our prediction of the model, in the range of interest of application, achieved 0.4% error in predicting True Positive Rates, and 2.8% error in predicting False Positive Rates.

Abstract:This paper proposes a novel method of Chinese word segmentation utilizing morphology information. The method introduces morphology into statistical model to capture structural relationship within word. It improves the conventional Conditional Random Fields (CRFs) models on the ability of representing the structure information. Firstly, a word-segmented Chinese corpus is annotated with morphology tags by a semi-automatic method. The resulting structure-related tags are integrated into the CRFs model. Secondly, a joint CRFs model is trained, which generates both morphology tags and word boundaries. Experiments are carried out on several SIGHAN Bakeoff corpus and show that the morphology information can improve the performance of Chinese word segmentation significantly, especially for the segmentation of out-of-vocabulary words.


Abstract:Twitter has had a prominent role during the Covid-19 pandemic in discussing public healthcare-related events, including vaccine efficacy. In this context, the sentiments of individuals about the vaccine have varied between supporters of vaccination programs, and opponents who advocate others not to vaccinate. Being able to identify anti-vaxxers through their tweets could enable public health organizations to make wise decisions to enhance vaccine acceptance, by allocating customized awareness campaigns to individuals to reduce anti-vaccination sentiments among different groups. This study aims to provide a smart classification model that can identify the sentiments of individuals in the Middle East region toward Covid-19 vaccinations through the Arabic tweets they publish on their own Twitter accounts, exploiting Natural Language Processing, Machine Learning, and Deep Learning-based models. Nine well-known Machine Learning algorithms and four Deep Learning algorithms were investigated with various feature extraction techniques (TF, TF-IDF, and Glove). The results show that the performance of the TF model outperforms other models in the Multinomial Naïve Bayes with an accuracy of 82.14%. On the other hand, the performance of the TF-IDF model outperforms other models in the Support Vector Machine with an accuracy of 81.81%. Finally, the Glove word embedding model outperforms other models in the LSTM with an accuracy of 81.67%.


CrypTop12: A Dataset For Cryptocurrency Price Movement Prediction From Tweets And Historical Prices
Abstract:Cryptocurrencies are gaining popularity day by day, and their analysis is a fascinating and demanding research topic. The average daily trading volume of Bitcoin was ${\$}67$ billion in May 2021. A peculiar feature of cryptocurrencies is that they are not generally issued by a central authority, making them insusceptible to any governmental impedance. Cryptocurrency rates are closely related to news and influenced by tweets. However, no available dataset can analyze the crypto market adequately. We present CrypTop12, a benchmark dataset for Cryptocurrency Price Movement Prediction based on tweets and historical prices. We collect over 576K tweets related to the top 12 cryptocurrencies, spanning over 1255 days and refine them to filter the tweets that are most relevant to price fluctuations. We also demonstrate use-cases by providing adapted baseline methods and a quantitative results analysis on our dataset.

Change impact analysis for Natural Language requirements: An NLP approach
Abstract:Requirements are subject to frequent changes as a way to ensure that they reflect the current best understanding of a system, and to respond to factors such as new and evolving needs. Changing one requirement in a requirements specification may warrant further changes to the specification, so that the overall correctness and consistency of the specification can be maintained. A manual analysis of how a change to one requirement impacts other requirements is time-consuming and presents a challenge for large requirements specifications. We propose an approach based on Natural Language Processing (NLP) for analyzing the impact of change in Natural Language (NL) requirements. Our focus on NL requirements is motivated by the prevalent use of these requirements, particularly in industry. Our approach automatically detects and takes into account the phrasal structure of requirements statements. We argue about the importance of capturing the conditions under which change should propagate to enable more accurate change impact analysis. We propose a quantitative measure for calculating how likely a requirements statement is to be impacted by a change under given conditions. We conduct an evaluation of our approach by applying it to 14 change scenarios from two industrial case studies.

Automatic Knowledge Extraction to Build Semantic Web of Things Applications
Abstract:The Internet of Things (IoT) primary objective is to make a hyper-connected world for various application domains. However, IoT suffers from a lack of interoperability leading to a substantial threat to the predicted economic value. Schema.org provides semantic interoperability to structure heterogeneous data on the Web. An extension of this vocabulary for the IoT domain (iot.schema.org) is an ongoing research effort to address semantic interoperability for the Web of Things (WoT). To design this vocabulary, a central challenge is to identify the main topics (concepts and properties) automatically from existing knowledge in IoT applications. We designed knowledge extraction for the WoT (KE4WoT) to automatically identify the most important topics from literature ontologies of three different IoT application domains: 1) smart home; 2) smart city; and 3) smart weather-based on our corpus consisting of 4500 full-text conference and journal articles to utilize domain-specific knowledge encoded within IoT publications. Despite the importance of automatically identifying the relevant topics for iot.schema.org, up to know there is no study dealing with this issue. To evaluate the extracted topics, we compare the descriptiveness of these topics for the ten most popular ontologies in the three domains with empirical evaluations of 23 domain experts. The results illustrate that the identified main topics of IoT ontologies can be used to sufficiently describe existing ontologies as keywords.

Automated Mapping of Environmental Higher Education Ranking Systems Indicators to SDGs Indicators using Natural Language Processing and Document Similarity
Abstract:To evaluate the ESHERSs and determine their efficiency to measure environmental sustainability, we tackle this problem as a classification assignment. This study benchmark three ESHERSs: UI GreenMetric, Times Higher Education Impact ranking, and STARS (Sustainability Tracking, Assessment Rating System) by AASHE (the association for the advancement of sustainability in higher education). Next, we recruited a group of experts who mapped the ESHERS indicators to the SDGs indicators. Then, we use NLP techniques to classify (map) the ESHERS indicators to the SDGs indicators. Since most of the ESHERS indicators and the SDGs indicators are in the form of short text, we use the query expansion technique to make the NLP techniques more effective. Each ESHERS indicator and its expanded text represents a document. And, each SDG indicator and its expanded text represents a document. We took the expanded text from the description of the ESHERS indicators and the description of SDG indicators, forming the corpus for our study. Then, we used document similarity to find the similarity between every pair of the corpus documents. We used different similarity measures to see the similarity between the forms. Then, we used a voting system to map the ESHERSs indicators to the SDGs indicators. The proposed system was able to automatically map the underlying ranking systems indicators to the UN SDGs with 99% accuracy compared to the experts mapping.

Abstract:This paper aims to give instructors the opportunity to analyze learners' cognitive behavior. Thus we propose an automatic system to assess learners' CoI-Cognitive presence regarding their social participation within asynchronous online discussion. Using Natural Language Preprocessing and Machine learning techniques, we first make some pre-processing to the textual data in order to make it clean and understandable, then we represent each discussion transcript with a vector using Doc2Vec-based features method; a document embedding method for document representation; according to some context and LIWC features. Finally, the Naïve Bayes algorithm is applied to classify messages according to cognitive presence categories.



Metrics for evaluating phonetics machine translation in Natural Language Processing through modified Edit Distance algorithm-A naïve approach

Multimodal prediction of the audience's impression in political debates
Debates are popular among politicians, journalists and scholars because they are a useful way to foster discussion and argumentation about relevant matters. In these discussions, people try to give a good impression (the immediate effect produced in the mind after a stimulus) by showcasing good skills in oratory and argumentation. We investigate this issue by using data gathered from an audience watching a national election debate and measuring the impression that politicians were giving during it. We then model a multimodal approach for automatically predicting their impression and analyze what modalities are the most important ones for this task. Our results show that the vision modality brings the best results and that fusing modalities at the feature-level is beneficial depending on the setup. The dataset is made publicly available to the community for further research on this topic.

Abstract:Semantic matching is a critical task in the field of natural language processing (NLP). It aims to compare two sentences and identify the relationship between them. A new semantic matching method is proposed in this paper. First, the pre-trained model BERT is used to dynamically encode the input text into the corresponding encoding vectors. Then, the encoding vectors of the two sequences are respectively input to a bidirectional LSTM with attention mechanism in order to obtain the global vectors of two sequences. Finally, the two global vectors are concatenated and input to a linear layer for computing the final semantic result. This paper compares with other representative methods on the Chinese data set LCQMC. The experimental results show that the method proposed in this paper has a significant improvement.


1
 value were used as the evaluation criteria. Experimental data show that this method improves the accuracy of keyword extraction and improves the performance of the original algorithm.

Chinese complex long sentences processing method for Chinese-Japanese machine translation
Abstract:The research on machine translation has been lasting for many years, and now this research field is increasingly a thoroughly refined. In practical machine translation system, the processing of a simple and short Chinese sentence has somewhat good results. However, but the complex long Chinese sentence translation still has difficulties. In this paper a new hierarchical approach processing for Chinese complex long sentence through analysis of Chinese punctuation, conjunctive words and syntax function is proposed. The method synthetically uses semantic characteristic of source Chinese sentence, which includes grammatical features, the length of the source sentence, punctuation and functional words. First phase is conjunctive words for simplified segmentation, and then the syntax analysis, in order to process complex long sentence by the multi-hierarchical approach. A long sentence is divided into several parts; every part gets the correct translation of the result separately, and then is combined by the comprehensive approach to gain the complex long sentence translation result. Experiments show that our approach can significantly reduce the time consumption and numbers of ambiguity, and also improve the accuracy and readability when parsing Chinese complex long sentence.

Book Abstract:Linguistic annotation and text analytics are active areas of research and development, with academic conferences and industry events such as the Linguistic Annotation Workshops and the annual Text Analytics Summits. This book provides a basic introduction to both fields, and aims to show that good linguistic annotations are the essential foundation for good text analytics. After briefly reviewing the basics of XML, with practical exercises illustrating in-line and stand-off annotations, a chapter is devoted to explaining the different levels of linguistic annotations. The reader is encouraged to create example annotations using the WordFreak linguistic annotation tool. The next chapter shows how annotations can be created automatically using statistical NLP tools, and compares two sets of tools, the OpenNLP and Stanford NLP tools. The second half of the book describes different annotation formats and gives practical examples of how to interchange annotations between different formats using XSLT transformations. The two main text analytics architectures, GATE and UIMA, are then described and compared, with practical exercises showing how to configure and customize them. The final chapter is an introduction to text analytics, describing the main applications and functions including named entity recognition, coreference resolution and information extraction, with practical examples using both open source and commercial tools. Copies of the example files, scripts, and stylesheets used in the book are available from the companion website, located at the book website. Table of Contents: Working with XML / Linguistic Annotation / Using Statistical NLP Tools / Annotation Interchange / Annotation Architectures / Text AnalyticsView less


FU Covid-19 AI Agent built on Attention algorithm using a combination of Transformer, ALBERT model, and RASA framework
Potentialized by Natural Language Processing (NLP) technology, we can build a chatbot or an AI Agent to automatically address the need to automatically get credible and timely information, especially in the fight against epidemics. However, Vietnamese understanding is still a big challenge for NLP. This paper introduces an AI Agent using the Attention algorithm and Albert model to implement the question/answering task in the Covid-19 field for the Vietnamese language. In the end, we also built two other modules, one for Vietnamese diacritic auto-correction and another for updating Covid-19 statistics (using RASA framework), to deploy a Covid-19 chatbot application on mobile devices.

On the role of NLP in linguistics
This paper summarizes some of the applications of NLP techniques in various linguistic sub-fields, and presents a few examples that call for a deeper engagement between the two fields.


Optical vortices in nonlinear media: Spirals, forks, and knots


Exploiting Named Entity Recognition via Pre-trained Language Model and Adversarial Training

Abstract:Pre-trained contextual language models such as BERT have dramatically improved performances for many NLP tasks recently. However, few have explored BERT on specific medical domain tasks such as early prediction for Acute Kidney Injury (AKI). Since much of the clinical information is contained in clinical notes that are largely unstructured text, in this paper, we present an AKI domain-specific pre-trained language model based on BERT (AKI-BERT) that could be used to mine the clinical notes for AKI early prediction. Our experiments on MIMIC-III dataset demonstrate that AKI-BERT can yield performance improvements for AKI early prediction.



Automatic learning for semantic collocation

Are they our brothers?: analysis and detection of religious hate speech in the arabic Twittersphere
Religious hate speech in the Arabic Twittersphere is a notable problem that requires developing automated tools to detect messages that use inflammatory sectarian language to promote hatred and violence against people on the basis of religious affiliation. Distinguishing hate speech from other profane and vulgar language is quite a challenging task that requires deep linguistic analysis. The richness of the Arabic morphology and the limited available resources for the Arabic language make this task even more challenging. To the best of our knowledge, this paper is the first to address the problem of identifying speech promoting religious hatred in the Arabic Twitter. In this work, we describe how we created the first publicly available Arabic dataset annotated for the task of religious hate speech detection and the first Arabic lexicon consisting of terms commonly found in religious discussions along with scores representing their polarity and strength. We then developed various classification models using lexicon-based, n-gram-based, and deep-learning-based approaches. A detailed comparison of the performance of different models on a completely new unseen dataset is then presented. We find that a simple Recurrent Neural Network (RNN) architecture with Gated Recurrent Units (GRU) and pre-trained word embeddings can adequately detect religious hate speech with 0.84 Area Under the Receiver Operating Characteristic curve (AUROC).


Compositional matrix-space models of language

Abstract:Interference alignment is a new technique recently proposed in physical layer wireless communications research, which purposely align the interference among multiple users thus increase the efficiency of channel use. This paper proposes a new method to enhance throughput of multi-hop wireless networks by utilizing interference alignment at physical layer. At first, it takes a simple network with only 6 nodes to demonstrate that interference alignment can achieve higher throughput. Subsequently, an optimization problem is formulated to maximize multi-hop wireless network throughput, which is a nonlinear programming (NLP) problem. Among that, finding the current transmission links can be transformed into maximal cliques of a graph. Furthermore, a branch-and-bound framework is provided to obtain the achievable bound of the NLP problem. Numerical results are presented to validate the algorithm and offer insights on the throughput enhancement brought by the interference alignment technique.


Transitivity in semantic relation learning
Abstract:Text understanding models exploit semantic networks of words as basic components. Automatically enriching and expanding these resources is then an important challenge for NLP. Existing models for enriching semantic resources based on lexical-syntactic patterns make little use of structural properties of target semantic relations. In this paper, we propose a novel approach to include transitivity in probabilistic models for expanding semantic resources. We directly include transitivity in the formulation of probabilistic models. Experiments demonstrate that these models are an effective way for exploiting structural properties of relations in learning semantic networks.


Day-Ahead Corrective Adjustment of FACTS Reactance: A Linear Programming Approach


Extracting knowledge from text using SHELDON, a Semantic Holistic framEwork for LinkeD ONtology data

Social media and NLP tasks: Challenges in crowdsourcing linguistic information
Abstract:In the framework of the TraMOOC1(Translation for Massive Open Online Courses) research and innovation project, data collection tasks for parallel translation are implemented using a crowdsourcing platform. The educational genre (videolectures subtitles, forums discussions, course assignments), the type of text (segmentation, misspellings, syntax errors, specialized terminology, scientific formulas, limited knowledge on context) of the source data, and the multilingual approach of the involved activities (the focus is on a total of 12 European and BRIC languages) provides a challenging setting for the success of the project. Experimental trials reveal significant findings for the purposes of Language Technology research as well as limitations in crowdsourcing linguistic data collections for multilingual tasks.


An empirical architecture for verb subcategorization frame: a lexicon for a real-world scale Japanese-English interlingual MT

Automatic sarcasm detection using feature selection
Abstract:Sarcasm is an expression of humor, mockery or criticism with the help of ironic remarks that seems positive. Sarcasm detection in sentiment analysis is essential for understanding the emotions and thoughts of the people. Automatic sarcasm detection refers to the detection of sarcasm in the text written in natural language. Various natural language processing techniques carry out this purpose. Recognition of Sarcasm is of extraordinary significance and is valuable to numerous NLP applications such as Opinion Mining and multiple advertisings. Sarcasm detection is a complex task, because of the challenges involved in determining the nature of the sarcasm bearing text. Automatic sarcasm detection is thus, one of the hardest challenges in Sentiment Analysis, including complex linguistic analyzing and machine learning methods. This paper focuses on various sarcasm analyzing techniques employed for filtering of sarcastic statements from a text and the use of Automatic sarcasm detection in the categorization of tweets and product review texts.

Abstract:We prefer to have more options when we cook. Choosing alternative ingredients or recipes, creating new recipes could be quite challenging. In this research project, we investigated how to apply the state-of-the-art natural language processing techniques such as word embedding to help people choose alternative ingredients/recipes and build language models -N-gram and neural network model to generate new recipes with authentic flavor of certain cuisine style.



Constructing parse forests that include exactly the n-best PCFG trees

$n$
-gram language model to accomplish word prediction tasks. In order to find a set of mathematical equations to properly describe the language modeling, a model based on partial differential equations is proposed. With the appropriate initial conditions, it was found an interpolation model similar to the traditional maximum entropy language model. Improvements in keystroke saved and perplexity over the word-based 

Using SGML as a basis for data-intensive NLP
This paper describes the LT NSL system (McKelvie et al, 1996), an architecture for writing corpus processing tools. This system is then compared with two other systems which address similar issues, the GATE system (Cunningham et al, 1995) and the IMS Corpus Workbench (Christ, 1994). In particular we address the advantages and disadvantages of an SGML approach compared with a non-SGML database approach.

Abstract:Document clustering plays a vital role in text mining fields such as information retrieval, sentiment analysis, and text organizing. Document clustering aims to automatically divide a collection of documents based on some aspects of similarity into groups that are meaningful, useful or both. This paper aims to improve the clustering task for the Arabic documents. Recent studies show that partitioning clustering algorithms are more suitable for clustering process. However, k-means is the most common algorithm that is being used for clustering process because of its simplicity and speed. It can only generate an arbitrary solution because the results depend on the initial centers for the desired clusters “the seeds”. In this paper, a new modified k-means algorithm called PSO K-means, supported by Particle Swarm Optimization (PSO) is applied to enhance the Arabic document clustering process. Then, an intensive comparative study between the proposed model and the standard k-means algorithm is applied. Also, the stemming algorithms those are being used in Arabic language processing were assessed. Through the experiments, an evaluation for the new algorithm is done with three different Arabic data sets. The results demonstrate that the proposed model can produce more accurate results compared to the standard k-means algorithm for Arabic language documents. On the other hand, Arabic light stemmer is more suitable for the stemming step.


Analyzing Tweets on New Norm: Work from Home during COVID-19 Outbreak
Abstract:The COVID-19 pandemic triggered a large-scale work-from-home trend globally in recent months. In this paper, we study the phenomenon of “work-from-home” (WFH) by performing social listening. We propose an analytics pipeline designed to crawl social media data and perform text mining analyzes on textual data from tweets scrapped based on hashtags related to WFH in COVID-19 situation. We apply text mining and NLP techniques to analyze the tweets for extracting the WFH themes and sentiments (positive and negative). Our Twitter theme analysis adds further value by summarizing the common key topics, allowing employers to gain more insights on areas of employee concerns due to pandemic.

Development of thai question answering system
Automatic question answering systems play an important role nowadays. In this paper, we developed a Thai question answering system. From an input question, the system analyzes an expected type of answer (e.g. human, place) then searches for all possible answers from source documents. In order to select the most relevance answer, a word order consistency and a relevance score are proposed to measure similarity of the question and the sentences surrounding the possible answers. Based on an experiment from 210 questions and 50 articles, the proposed system can provide a mean reciprocal answer rank of 0.657.

Unsupervised software-specific morphological forms inference from informal discussions
Informal discussions on social platforms (e.g., Stack Overflow) accumulates a large body of programming knowledge in natural language text. Natural language process (NLP) techniques can be exploited to harvest this knowledge base for software engineering tasks. To make an effective use of NLP techniques, consistent vocabulary is essential. Unfortunately, the same concepts are often intentionally or accidentally mentioned in many different morphological forms in informal discussions, such as abbreviations, synonyms and misspellings. Existing techniques to deal with such morphological forms are either designed for general English or predominantly rely on domain-specific lexical rules. A thesaurus of software-specific terms and commonly-used morphological forms is desirable for normalizing software engineering text, but very difficult to build manually. In this work, we propose an automatic approach to build such a thesaurus. Our approach identifies software-specific terms by contrasting software-specific and general corpuses, and infers morphological forms of software-specific terms by combining distributed word semantics, domain-specific lexical rules and transformations, and graph analysis of morphological relations. We evaluate the coverage and accuracy of the resulting thesaurus against community-curated lists of software-specific terms, abbreviations and synonyms. We also manually examine the correctness of the identified abbreviations and synonyms in our thesaurus. We demonstrate the usefulness of our thesaurus in a case study of normalizing questions from Stack Overflow and CodeProject.

Abstract:This paper describesr the modules of natural language processing (NLP) engine which can be used with Hungarian input. There are many standard NLP engines which have tokenization, part-of-speech (POS) tagging, named entity recognition, parsing modules. Most of them work for universal languages like English. Processing of Hungarian language is a much more difficult and there cannot be found such a complete NLP system which satisfies all tasks for syntactic and semantic analysis of incoming inputs. This paper summarizes the existing solutions and techniques and gives a brief description of a proposed NLP engine using for inputs in Hungarian language.


Converting Mikrokosmos frames into description logics
Mikrokosmos contains an ontology plus a number of lexicons in different languages that were originally developed for machine translation. The underlying representation formalism for these resources is an ad-hoc frame-based language which makes it difficult to inter-operate Mikrokosmos with state-of-the-art knowledge-based systems. In this paper we propose a translation from the frame-based representation of Mikrokosmos into Description logics. This translation allows us to automatically transform Mikrokosmos sources into OWL and thus provide a powerful ontology in the formalism of the semantic web. Furthermore, the reasoning mechanisms of Description Logics may also support knowledge acquisition and maintenance as well as its application in natural language processing systems.


Development of a Morph Analyser for Nepali noun token

Abstract:Sentiment analysis is the study, to classify the text based on customer reviews which can provide valuable information to improve business. Previously the analysis was carried out based on the information provided by the customers using natural language processing and machine learning. In this paper, sentiment analysis on IMDB movie reviews dataset is implemented using Machine Learning (ML) and Deep Learning (DL) approaches to measure the accuracy of the model. ML algorithms are the traditional algorithms that work in a single layer while deep learning algorithms work on multilayers and gives better output. This paper helps the researchers to identify the best algorithm for sentiment analysis. The comparison of the machine learning and deep learning approaches shows that DL algorithms provide accurate and efficient results.



Textual risk mining for maritime situational awareness

Part of speech tagging with Naïve Bayes methods
Abstract:In this paper we have focused on the problem of automatic prediction of parts of speech in sentences. We present an experimental framework which includes the analysis and the implementation of methods for part of speech (POS) labeling (tagging). We have tested three methods that predict the POS without current word's context and also three context awareness statistic methods. The main goal of our work was to evaluate the three statistical methods Forward, Backward and Complete Method in order to analyze their applicability in the problem of automatically prediction of the POS. These methods are derived from the classic Naïve Bayes classifier. In our research we have used the WordNet database and a set of benchmarks called the Brown University Standard Corpus of Present - Day American English. The results obtained by the non-context-awareness methods compared to the results obtained by statistical methods are better but not so reliable like the statistical methods.

Abstract:Facts play a critical role in our lives. We have got radical ways to accumulate and store these facts as meaningful information. A database is an organized collection of facts and information stored in the form of data. A good organization has to maintain lots and lots of data. There are ultimate tools which can collect and store the huge data in its proper format in the main repository but still the users doesn't get benefited from it up to its full potential. Retrieving meaningful information from such repositories requires an extensive knowledge of database languages like SQL. However, not everybody is able to write the complex SQL queries to extract the needful information. This intricate problem can be solved to a great extent by providing a Natural Language Interface to the users using which they can query a database in their own language. In this paper, we propose a Database Intelligent Querying System (DBIQS) which portrays completely automatic, simple, fast and reliable way to query a database.


Abstract:To reduce the effects of modeling imprécisions, in the traditional “Receding Horizon Control” that works with finite horizon lengths, in the consecutive horizon-length cycles, the actually measured state variable is used as the starting point in the next cycle. In this design, within a horizon-length cycle, a cost function is minimized under a constraint that mathematically represents the dynamic properties of the system under control. In the “Nonlinear Programming” (NLP) approach the state variables as well as the control signals are considered over a discrete time-resolution grid, and the solution is computed by the use of Lagrange's “Reduced Gradient” (RG) method. It provides the “estimated optimal control signals” and the “estimated optimal state variables” over this grid. The controller exerts the estimated control signals but the state variables develop according to the exact dynamics of the system. In this paper an alternative approach is suggested in which, instead of exerting the estimated control signals, the estimated optimized trajectory is adaptively tracked within the given horizon. Simulation investigations are presented for a simple “Linear Time-Invariant” (LTI) model with strongly non-linear cost and terminal cost functions. It is found that the transients of the adaptive controller that appear at the boundaries of the finite-length horizons reduce the available improvement in the tracking precision. In contrast to the traditional RHC, in which decreasing horizon length improves the tracking precision, in our case some increase in the horizon length improves the precision by giving the controller more time to compensate the effects of these transients.



Data Classification with k-fold Cross Validation and Holdout Accuracy Estimation Methods with 5 Different Machine Learning Techniques

Abstract:This paper proposes a method of NP tree matching to realize the translation of English-Chinese patent titles. Firstly a bilingual example database for patent titles is built. English parse trees are produced by English parser, forming NP tree database. The input patent title to be translated is firstly parsed into a tree. Then NP trees are searched for which match with the input NP tree in NP tree database. If similar NP trees exist, HowNet is used to find the best NP tree by calculating word semantic similarity. Final translations are obtained through calculating cohesion of candidate words. If there are no similar NP trees, subtrees that match the input NP tree are searched for and translations generate by subtree substitutions recursively. Experimental results show that our method outperforms a baseline Pharaoh, by using BLEU evaluation system.



Optimizations for item-based Collaborative Filtering algorithm

Bilingual random walk models for automated grammar correction of ESL author-produced text
We present a novel noisy channel model for correcting text produced by English as a second language (ESL) authors. We model the English word choices made by ESL authors as a random walk across an undirected bipartite dictionary graph composed of edges between English words and associated words in an author's native language. We present two such models, using cascades of weighted finite-state transducers (wFSTs) to model language model priors, random walk-induced noise, and observed sentences, and expectation maximization (EM) to learn model parameters after Park and Levy (2011). We show that such models can make intelligent word substitutions to improve grammaticality in an unsupervised setting.


Multi-Cuts Outer Approximation Method for Unit Commitment


Plausible Deniability for ISP Log and Browser Suggestion Obfuscation with a Phrase Extractor on Potentially Open Text

UofL: word sense disambiguation using lexical cohesion
One of the main challenges in the applications (i.e.: text summarization, question answering, information retrieval, etc.) of Natural Language Processing is to determine which of the several senses of a word is used in a given context. The problem is phrased as "Word Sense Disambiguation (WSD)" in the NLP community. This paper presents the dictionary based disambiguation technique that adopts the assumption of one sense per discourse in the context of SemEval-2007 Task 7: "Coarse-grained English all-words".

Abstract:Various computational algorithms have been developed for constructing the genetic networks and by using text mining. The rapid growth in the biomedical literature, however, still encourages the importance of improving these algorithms for a better understanding of the protein/gene biochemical interactions. This paper proposes a text mining system that constructs a gene-gene-interaction network for the yeast genome by identifying the co-occurrence frequency of the genes in the biomedical text. The system determines connected genes based on their appearance in several levels of the text (i.e. abstract and sentence). This paper highlights the importance of recognizing the sparsity of biomedical data when designing a text mining prediction system. It does so by employing a rare event classification model that reflects the population using small samples of data. The results show that this method has the potential of improving the prediction accuracy for gene-gene-interactions.


Needs and challenges of care robots in nursing care setting: A literature review
Abstract:This study aims to identify needs and challenges of care robot in nursing care setting through an extensive search of the literature. As the result shows, there exists a shortage of information about results of the introduction of care robots, the needs of recipients and care providers, and relevant ethical problems. To advance our research and to introduce care robots into setting, there are so many things to do; consider the application of natural language processing technology by collaborating with researchers in the robotics field, carry out an investigation, extract the needs, clarify ethical problems and seek solutions, conduct the on-site experiment study, and so on.


Exploring the role of punctuation in parsing natural text

ProFIT: prolog with features, inheritance and templates
ProFIT is an extension of Standard Prolog with Features, Inheritance and Templates. ProFIT allows the programmer or grammar developer to declare an inheritance hierarchy, features and templates. Sorted feature terms can be used in ProFIT programs together with Prolog terms to provide a clearer description language for linguistic structures. ProFIT compiles all sorted feature terms into a Prolog term representation, so that the built-in Prolog term unification can be used for the unification of sorted feature structures, and no special unification algorithm is needed. ProFIT programs are compiled into Prolog programs, so that no meta-interpreter is needed for their execution. ProFIT thus provides a direct step from grammars developed with sorted feature terms to Prolog programs usable for practical NLP systems.

Automated Extraction of Conceptual Models from User Stories via NLP
Abstract:Natural language (NL) is still the predominant notation that practitioners use to represent software requirements. Albeit easy to read, NL does not readily highlight key concepts and relationships such as dependencies and conflicts. This contrasts with the inherent capability of graphical conceptual models to visualize a given domain in a holistic fashion. In this paper, we propose to automatically derive conceptual models from a concise and widely adopted NL notation for requirements: user stories. Due to their simplicity, we hypothesize that our approach can improve on the low accuracy of previous works. We present an algorithm that combines state-of-the-art heuristics and that is implemented in our Visual Narrator tool. We evaluate our work on two case studies wherein we obtained promising precision and recall results (between 80% and 92%). The creators of the user stories perceived the generated models as a useful artifact to communicate and discuss the requirements, especially for team members who are not yet familiar with the project.

Unsupervised concept annotation using latent Dirichlet allocation and segmental methods
Training efficient statistical approaches for natural language understanding generally requires data with segmental semantic annotations. Unfortunately, building such resources is costly. In this paper, we propose an approach that produces annotations in an unsupervised way. The first step is an implementation of latent Dirichlet allocation that produces a set of topics with probabilities for each topic to be associated with a word in a sentence. This knowledge is then used as a bootstrap to infer a segmentation of a word sentence into topics using either integer linear optimisation or stochastic word alignment models (IBM models) to produce the final semantic annotation. The relation between automatically-derived topics and task-dependent concepts is evaluated on a spoken dialogue task with an available reference annotation.

PO
4

Accommodating Transformer onto FPGA: Coupling the Balanced Model Compression and FPGA-Implementation Optimization
Recently, Transformers gradually gain popularity and perform outstanding for many Natural Language Processing (NLP) tasks. However, Transformers suffer from heavy computation and memory footprint, making it difficult to deploy on embedded devices. The field-programmable gate array (FPGA) is widely used to accelerate deep learning algorithms for its advantages. However, the trained Transformer models are too large to accommodate to an FPGA fabric. To accommodate Transformer onto FPGA and achieve efficient execution, we propose an acceleration framework coupling the balanced model compression at the algorithm level and FPGA-implementation optimization at the hardware level. At algorithm level, we adopt a block-balanced pruning and propose an efficient sparse matrix storage format for this pruning technique, named Compressed Block Row (CBR). At the hardware level, we design an accelerator for sparse model. And we also abstract a performance analytic model to evaluate the performance of accelerator. Experiments show that our CBR format perform better than general formats and can significantly save storage space. And our accelerator can achieve $38\times$ and $1.93\times$ speedup compared to other works on CPU and GPU respectively.

Evaluating a statistical CCG parser on Wikipedia
The vast majority of parser evaluation is conducted on the 1984 Wall Street Journal (WSJ). In-domain evaluation of this kind is important for system development, but gives little indication about how the parser will perform on many practical problems. Wikipedia is an interesting domain for parsing that has so far been under-explored. We present statistical parsing results that for the first time provide information about what sort of performance a user parsing Wikipedia text can expect. We find that the C&C parser's standard model is 4.3% less accurate on Wikipedia text, but that a simple self-training exercise reduces the gap to 3.8%. The self-training also speeds up the parser on newswire text by 20%.

State-of-the-art NLP approaches to coreference resolution: theory and practical recipes
The identification of different nominal phrases in a discourse as used to refer to the same (discourse) entity is essential for achieving robust natural language understanding (NLU). The importance of this task is directly amplified by the field of Natural Language Processing (NLP) currently moving towards high-level linguistic tasks requiring NLU capabilities such as e.g. recognizing textual entailment. This tutorial aims at providing the NLP community with a gentle introduction to the task of coreference resolution from both a theoretical and an application-oriented perspective. Its main purposes are: (1) to introduce a general audience of NLP researchers to the core ideas underlying state-of-the-art computational models of coreference; (2) to provide that same audience with an overview of NLP applications which can benefit from coreference information.

Utterance templates merging in automaton-based dialogue systems
Abstract:Many of the implemented dialog systems in industry are based on a state automaton. Most of these systems rely on predefined messages, where a message is an ordered set of utterance templates, in order to produce the output message. The automaton-based dialog manager computes the correspondence of a particular predefined message to a given user's request. However, a rather common event in the dialog system's workflow is the dialog manager's highlighting of multiple messages in response to a user's request. In order to produce an appropriate output message, the templates of all messages need to be merged and restructured. In this paper, we introduce a natural language generation (NLG) module to the automaton based dialog system in order to perform this task.


A Comparative Study on Various Deep Learning Techniques for Thai NLP Lexical and Syntactic Tasks on Noisy Data

Polynomial to linear: efficient classification with conjunctive features
This paper proposes a method that speeds up a classifier trained with many conjunctive features: combinations of (primitive) features. The key idea is to precompute as partial results the weights of primitive feature vectors that appear frequently in the target NLP task. A trie compactly stores the primitive feature vectors with their weights, and it enables the classifier to find for a given feature vector its longest prefix feature vector whose weight has already been computed. Experimental results for a Japanese dependency parsing task show that our method speeded up the svm and llm classifiers of the parsers, which achieved accuracy of 90.84/90.71%, by a factor of 10.7/11.6.

Enjoy the paper: lexical semantics via lexicology
Current research being undertaken at both Cambridge and IBM is aimed at the construction of substantial lexicons containing lexical semantic information capable of use in automated natural language processing (NLP) applications. This work extends previous research on the semi-automatic extraction of lexical information from machine-readable versions of conventional dictionaries (MRDs) (see e.g. the papers and references in Boguraev & Briscoe, 1989; Walker et al., 1988). The motivation for this and previous research using MRDs is that entirely manual development of lexicons for practical NLP applications is infeasible, given the labour-intensive nature of lexicography (e.g. Atkins, 1988) and the resources likely to be allocated to NLP in the foreseeable future. In this paper, we motivate a particular approach to lexical semantics, briefly demonstrate its computational tractability, and explore the possibility of extracting the lexical information this approach requires from MRDs and, to some extent, textual corpora.

XNLP: A Living Survey for XAI Research in Natural Language Processing
We present XNLP: an interactive browser-based system embodying a living survey of recent state-of-the-art research in the field of Explainable AI (XAI) within the domain of Natural Language Processing (NLP). The system visually organizes and illustrates XAI-NLP publications and distills their content to allow users to gain insights, generate ideas, and explore the field. We hope that XNLP can become a leading demonstrative example of a living survey, balancing the depth and quality of a traditional well-constructed survey paper with the collaborative dynamism of a widely available interactive tool. XNLP can be accessed at: https://xainlp2020.github.io/xainlp.

Text-to-Speech Synthesis: Literature Review with an Emphasis on Malayalam Language
Text-to-Speech Synthesis (TTS) is an active area of research to generate synthetic speech from underlying text. The identified syllables are uttered with proper duration and prosody characteristics to emulate natural speech. It falls under the category of Natural Language Processing (NLP), which aims to bridge the gap in communication between human and machine. So far as Western languages like English are concerned, the research to produce intelligent and natural synthetic speech has advanced considerably. But in a multilingual state like India, many regional languages viz. Malayalam is underexplored when it comes to NLP. In this article, we try to amalgamate the major research works performed in the area of TTS in English and the prominent Indian languages, with a special emphasis on the South Indian language, Malayalam. This review intends to provide right direction to the research activities in the language, in the area of TTS.

Abstract:Brill tagging is a classic rule-based algorithm for part-of-speech (POS) tagging that assigns tags, such as nouns, verbs, adjectives, etc., to input tokens. Due to the the intense memory requirements of rule matching, CPU implementations of the Brill tagging algorithm have been found to be slow. We show that Micron's Automata Processor (AP) - a new computing architecture that can perform massively parallel pattern matching - can greatly accelerate the second stage of Brill tagging via rule template matching. The 218 contextual rules are first converted into regular expressions (regex). Regex is used widely in natural language processing (NLP) tasks, thus, this case study involving Brill Tagging also shows how the AP might accelerate other applications that are able to be framed as regexes. We compare single-threaded, and multithreaded versions of Regex matching on an Intel i7 CPU, an Intel XeonPhi co-processor, and the AP. The results show a 63.90X speed-up using the AP as a regex accelerator over the fastest multi-threaded CPU version. We also investigate how performance of regex matching on both CPU architectures varies depending on the complexity of the regex. Taken together, these results demonstrate the potential for significant performance improvements by using accelerators for various NLP computational tasks, particularly those that involve rule-based or pattern-matching approaches.


Subword Contextual Embeddings for Languages with Rich Morphology
Abstract:Morphological information is important for many sequence labeling tasks in Natural Language Processing (NLP). Yet, existing approaches rely heavily on manual annotations or external software to capture this information. In this study, we propose using subword contextual embeddings for languages with rich morphology. Evaluated on Dependency Parsing (DEP) and Named Entity Recognition (NER) tasks, which are shown to benefit highly from morphological information, subword contextual embeddings consistently outperformed other approaches on all languages tested (Hungarian, Finnish, Czech and Turkish). Our proposed method enables achieving state-of-the-art results with little annotation requirements compared to the previous work. Besides, the novel network architecture we propose, coupled with a Bayesian hyperparameter optimization suite, achieved state-of-the-art results for both tasks for the Turkish language. Finally, we experimented with different multi-task learning architectures to analyze the effect of jointly learning the two tasks.

Abstract:National guidelines for a number of health conditions recommend that practitioners assess and reinforce patient's adherence to specific diet and lifestyle modifications. Counseling intervention has shown to have a long-term positive effect on patient adherence but the extent to which physicians comply is unknown. Evidence of counseling provided by practitioner is recorded only as free text in electronic medical records. To identify physicians' counseling practices we developed a natural language processing system to detect text documentation of dietary counseling in gout patients.


Abstract:Text classification is an essential part of the NLP, which aims to predict the categories for given texts in a particular classification system. There are many ways of feature selection and classification models. However, most researchers would like to use the encapsulated methods of third-party libraries to achieve their goals. Therefore, in this paper, we propose to implement code to achieve functions, instead of using third-party libraries. We evaluate our code in different classification models, and the result of our experiment shows that our code is feasible.



A graph-based approach for semantic similar word retrieval


Joint and conditional estimation of tagging and parsing models

Abstract:Failing to identify multi-word expression (MWE) may cause serious problems for many Natural Language Processing (NLP) tasks. Previous approaches heavily depend on language specific knowledge and pre-existing natural language processing (NLP) tools. However, many languages (including Chinese language) have less such resources and tools compared to English. An automatically learn effective features from corpus, without relying on language specific resources is needed. In this paper, we develop a hybrid approach that combines Bidirectional long short-term memory (Bi-LSTM), word correlation degree calculation and weakly supervised K-means cluster to capture both sequence information and correlation degree of phrase from specific contexts, and use them to train a multi-word expression detector for multiple languages without any manually encoded features. Experiment result shows that the extraction results of Chinese and English multi-word expression using this hybrid approach is better than that of baseline algorithm, which verified that the hybrid approach is effective.


Enhanced word embedding with multiple prototypes
Abstract:Word representation is one of the basic word repressentation methods in natural language processing, which mapped a word into a dense real-valued vector space based on a hypothesis: words with similar context have similar meanings. Models like NNLM, C&W, CBOW, Skip-gram have been designed for word embeddings learning, and get widely used in many NLP tasks. However, these models assume that one word had only one semantics meaning which is contrary to the real language rules. In this paper we pro-pose a new word unit with multiple meanings and an algorithm to distinguish them by it's context. This new unit can be embedded in most language models and get series of efficient representations by learning variable embeddings. We evaluate a new model MCBOW that integrate CBOW with our word unit on word similarity evaluation task and some downstream experiments, the result indicated our new model can learn different meanings of a word and get a better result on some other tasks.


Modelling field dependencies on structured documents with fuzzy logic

Abstract:A great number of radiologic reports are created each year which incorporate the expertise of radiologists. This knowledge could be exploited via machine understanding. This could provide valuable statistics and visualization of the reports, and as training data, and it could also contribute to later automatic reporting applications. In our current work, we present our first steps toward the machine understanding of clinical reports of the spinal region, written in the Hungarian language. Our system provides an automatic classification and connection detection for various entities in the text. Our classification is achieved via bi-directional long short-term memory and conditional random fields producing 0.87-0.95 F1-score values, while the extraction of connection relies on linguistic analysis and predefined rules. The extracted information is displayed in an easily comprehensible, well-formed tree-structure.


A Review of Fake News Detection Using Machine Learning Techniques
Abstract:With the growing number of users of the internet people share millions of posts, articles and videos. These posts are shred on a number of social media platforms along with twitter, facebook, youtube and other social networking web sites. It is now a well-known fact that many times any misinformation may even lead to conflicts and it also has a significant influence on public opinion. The propagation of fake news stories on social media platforms and on Internet is duping people to an extent, stopping which is the need of the hour. The research area of fake news detection is gaining interest but at the same time it involves a number of challenges due to unavailability of quality resources such as datasets, published literature etc. The existing systems are not that much efficient in the detection of fake news because of the lack of fake news datasets that are comprehensive and at the same time are community-driven datasets. This has become one of the major roadblocks in the research works related to fake news detection. At the same time there are some restrictions on the input and the news category that makes it less varied. The fake news detection system aims to use data repositories such as Buzzfeed, Politifact, CREDBANK, FakeNewsNet and various classification techniques such as Support Vector Machine (SVM), K-Nearest Neighbor (KNN), Decision tree (DT), Logistic Regression (LR), RandomForests etc. to help to achieve maximum accuracy. This review paper provides a detailed review of various fake news detection techniques used by different researchers, the datasets they have worked upon and various evaluation parameters used by them for performance evaluation of their models. We have also discussed the difficulties and challenges faced in fake news detection.


CorpoMate: A framework for building linguistic corpora from the web

The NVI clustering evaluation measure
Clustering is crucial for many NLP tasks and applications. However, evaluating the results of a clustering algorithm is hard. In this paper we focus on the evaluation setting in which a gold standard solution is available. We discuss two existing information theory based measures, V and VI, and show that they are both hard to use when comparing the performance of different algorithms and different datasets. The V measure favors solutions having a large number of clusters, while the range of scores given by VI depends on the size of the dataset. We present a new measure, NVI, which normalizes VI to address the latter problem. We demonstrate the superiority of NVI in a large experiment involving an important NLP application, grammar induction, using real corpus data in English, German and Chinese.

Abstract:It is remarkable to note that the scope of Natural Language Processing (NLP) is developing and increasing in the area of text mining. Natural Language Processing is a field that covers computer understanding and deals with manipulation of human language. Human language is an unstructured source of information, and hence to use it, as an input to a computer program, it has to be, first, converted into a structured format [3]. Parts of Speech (POS) tagging is one of the steps which assigns a particular part of speech to a respective word. POS is difficult because most words tend to have more than one parts of speech in different cases and some parts of speech are complex or unspoken. This paper aims at developing part of speech tagging model for Konkani language, using the Konkani corpus.



On the power of social networks to analyze threatening trends


Field based case studies on overcurrent relay coordination optimization using GA-NLP approach

Abstract:This paper introduces Fuzzy Support Vector Machines (FSVMs) for Japanese dependency analysis. Japanese dependency analysis based on Support Vector Machines (SVMs) has been proposed and has achieved high accuracy. While regular SVMs try to find a decision hyperplane from two distinct classes of the input examples, FSVMs apply a fuzzy membership to each input example such that different examples can make different contributions to the decision hyperplane. For nonlinear classification problem, FSVMs can achieve good performance by reducing the effect of outliers. In this paper, a new fuzzy membership function is proposed to Japanese dependency analysis. We train an initial classifier with a small training set. The fuzzy membership is calculated by the distance from each input example to the initial hyperplane. In addition, we employ Nivre's algorithm for Japanese dependency analysis since it parses a sentence in linear-time. Experiments using the Kyoto University Corpus show that the parser using Nivre's algorithm outperforms the previous systems, and the proposed FSVMs improve the already excellent performance of SVMs for Japanese dependency analysis.


Large-Scale News Classification using BERT Language Model: Spark NLP Approach
The rise of big data analytics on top of NLP increasing the computational burden for text processing at scale. The problems faced in NLP are very high dimensional text, so it takes a high computation resource. The MapReduce allows parallelization of large computations and can improve the efficiency of text processing. This research aims to study the effect of big data processing on NLP tasks based on a deep learning approach. We classify a big text of news topics with fine-tuning BERT used pre-trained models. Five pre-trained models with a different number of parameters were used in this study. To measure the efficiency of this method, we compared the performance of the BERT with the pipelines from Spark NLP. The result shows that BERT without Spark NLP gives higher accuracy compared to BERT with Spark NLP. The accuracy average and training time of all model's using BERT is 0.9187 and 35 minutes while using BERT with Spark NLP pipeline is 0.8444 and 9 minutes. The bigger model will take more computation resources and need a longer time to complete the tasks. However, the accuracy of BERT with Spark NLP only decreased by an average of 5.7%, while the training time was reduced significantly by 62.9% compared to BERT without Spark NLP.


Applying natural language processing to analyze customer satisfaction

Unlocking Super Bowl Insights: Weighted Word Embeddings for Twitter Sentiment Classification
Sentiment classification plays an important role in Sentiment Analysis. It is challenging to develop an automatic method for classification problems without annotated training data. In this paper, we present a WWE (weighted word embeddings) method, which uses a continuous word representations algorithm (Word2Vec) to train a vector model. According to the cosine similarity between the vector of a word and the vectors of seed words, a polarity score of this word can be calculated. We then use the weighted polarity scores of words to compute a polarity score of the whole tweet. Unlike the previous learning-based approaches, our method does not require annotated data gathered for the purpose of training models. We collected the Super Bowl 50 related tweets to demonstrate the WWE classification method. Experiments are performed with promising outcomes.

An Automated Learning System for Twitter Trends
Abstract:Twitter is the social media platform for real-time broadcasting of information on world events. The microblogging site contains and continues to generate huge amounts of data along with the growing breadth of a geographically diverse user base. Qualitative analysis of this enormous data will require substantial effort on information filtering to successfully drill down to relevant topics and events. This paper presents an automated learning system for trends in twitter to generate a recommendation system for users to understand the contexts in a particular trend. We have devised a framework using Apriori Algorithm and Named Entity Recognition on learned twitter trends. The paper also presents schemes for visual representation of the results using concept hierarchies.

Abstract:Sentiment analysis is a research area where studies focus on the analysis and the processing of opinions found on the web content. It is about finding the polarity of sentiment expressed by internet user in their interaction about a giving subject in order to classify them on positive or negative. In this paper, we propose the implementation of a tool for sentiment analysis able to find the polarity of opinions in reviews extracted from e-commerce magazines and blogs in Arabic language. To do this, we conducted various experiments like testing different techniques of stemming and the performance of some classification algorithms to have the combination that gives us satisfactory results. In spite of the huge difficulties that we found in this research area like the lack of resources in data collection and the complexity of processing the Arabic language dialects, the results were hopeful and satisfying.


Inheritance in natural language processing
In this introduction to the special issues, we begin by outlining a concrete example that indicates some of the motivations leading to the widespread use of inheritance networks in computational linguistics. This example allows us to illustrate some of the formal choices that have to be made by those who seek network solutions to natural language processing (NLP) problems. We provide some pointers into the extensive body of AI knowledge representation publications that have been concerned with the theory of inheritance over the last dozen years or so. We go on to identify the three rather separate traditions that have led to the current work in NLP. We then provide a fairly comprehensive literature survey of the use that computational linguists have made of inheritance networks over the last two decades, organized by reference to levels of linguistic description. In the course of this survey, we draw the reader's attention to each of the papers in these issues of Computational Linguistics and set them in the context of related work.

Abstract:Traditionally most of the steps involved in a Systematic Literature Review (SLR) process are manually executed, causing inconvenience of time and effort, given the massive amount of primary studies available online. This has motivated a lot of research focused on automating the process. Current state-of-the-art methods combine active learning methods and manual selection of primary studies from a smaller set so they can maximize the finding of relevant papers while at the same time minimizing the number of manually reviewed papers. In this work, we propose a novel strategy to further improve these methods whose early success heavily depends on an effective selection of initial papers to be read by researchers using a PCAbased method which combines different document representation and similarity metric approaches to cluster and rank the content within the corpus related to an enriched representation of research questions within the SLR protocol. Validation was carried out over four publicly available data sets corresponding to SLR studies from the Software Engineering domain. The proposed model proved to be more efficient than a BM25 baseline model as a mechanism to select the initial set of relevant primary studies within the top 100 rank, which makes it a promising method to bootstrap an active learning cycle.


Abstract:Transliteration of Indian languages has been a challenging problem., given their complex nature., which has conventionally been handled by rule based systems developed by trained linguists. Scalability is a prominent issue with these systems and given the number of language pairs possible for Indian languages., a scalable pipeline is necessary for swift development of these systems. Deep learning systems are pragmatic for building a scalable pipeline., as they are completely data driven. We experimented with LSTMs and Sequence to Sequence models to find an optimal model for the scalable pipeline by comparing the results. The results show Sequence to Sequence models are a better fit for this solution. We also discuss techniques for pre-processing the data and post processing the output for optimal performance.


SEWordSim: software-specific word similarity database
Measuring the similarity of words is important in accurately representing and comparing documents, and thus improves the results of many natural language processing (NLP) tasks. The NLP community has proposed various measurements based on WordNet, a lexical database that contains relationships between many pairs of words. Recently, a number of techniques have been proposed to address software engineering issues such as code search and fault localization that require understanding natural language documents, and a measure of word similarity could improve their results. However, WordNet only contains information about words senses in general-purpose conversation, which often differ from word senses in a software-engineering context, and the software-specific word similarity resources that have been developed rely on data sources containing only a limited range of words and word uses.   In recent work, we have proposed a word similarity resource based on information collected automatically from StackOverflow. We have found that the results of this resource are given scores on a 3-point Likert scale that are over 50% higher than the results of a resource based on WordNet. In this demo paper, we review our data collection methodology and propose a Java API to make the resulting word similarity resource useful in practice.   The SEWordSim database and related information can be found at http://goo.gl/BVEAs8. Demo video is available at http://goo.gl/dyNwyb.

A comprehensive dictionary of multiword expressions
It has been widely recognized that one of the most difficult and intriguing problems in natural language processing (NLP) is how to cope with idiosyncratic multiword expressions. This paper presents an overview of the comprehensive dictionary (JDMWE) of Japanese multiword expressions. The JDMWE is characterized by a large notational, syntactic, and semantic diversity of contained expressions as well as a detailed description of their syntactic functions, structures, and flexibilities. The dictionary contains about 104,000 expressions, potentially 750,000 expressions. This paper shows that the JDMWE's validity can be supported by comparing the dictionary with a large-scale Japanese N-gram frequency dataset, namely the LDC2009T08, generated by Google Inc. (Kudo et al. 2009).

Abstract:Parallel corpora have become an essential resource for work in multilingual natural language processing. However, sentence aligned parallel corpora are more efficient than non-aligned parallel corpora for cross language information retrieval and machine translation applications. In this paper, we present a new approach to aligning sentences in bilingual parallel corpora based on the text character length between successive punctuates. A probabilistic score is assigned to each proposed correspondence of texts, based on the scaled difference of lengths of the two texts (in characters) and the variance of this difference. Using this score, the time required for punctuates matching decreased and the sentence alignment precision increased. Using this new approach, we could achieve 21.8% improvement over length based approach when applied on English-Arabic parallel documents.


Abstract:In the world of computer research, machine translation is a hot topic. It is fundamental in NLP. Language translation is very important in India because it is a country with myriad languages. The aim of this paper is to look at several various types of machine translation techniques and see how they are used in different translation systems. We begin with some tried-and-true methods before moving on to more modern and hybrid ways that are still being perfected. We also compare these studies based on the following parameters such as the machine translation approaches used, the language pair taken into research and the papers key features. We then offer a language translation model using Hybrid Machine Translation systems to facilitate us in translating from Gujarati to English.


Web service integration for next generation localisation
Developments in Natural Language Processing technologies promise a variety of benefits to the localization industry, both in its current form in performing bulk enterprise-based localization and in the future in supporting personalized web-based localization on increasingly user-generated content. As an increasing variety of natural language processing services become available, it is vital that the localization industry employs the flexible software integration techniques that will enable it to make best use of these technologies. To date however, the localization industry has been slow reap the benefits of modern integration technologies such as web service integration and orchestration. Based on recent integration experiences, we examine how the localization industry can best exploit web-based integration technologies in developing new services and exploring new business models

Filling knowledge gaps in text for machine reading
Texts are replete with gaps, information omitted since authors assume a certain amount of background knowledge. We define the process of enrichment that fills these gaps. We describe how enrichment can be performed using a Background Knowledge Base built from a large corpus. We evaluate the effectiveness of various openly available background knowledge bases and we identify the kind of information necessary for enrichment.

Abstract:Much of the world's knowledge is recorded in natural language text, but making effective use of it in this form poses a major challenge. Information extraction converts this knowledge to a structured form suitable for computer manipulation, opening up many possibilities for using it. In this review, the author describes the processing pipeline of information extraction, how the pipeline components are trained, and how this training can be made more efficient. He also describes some of the challenges that must be addressed for information extraction to become a more widely used technology.


Extracting Concepts from the Software Requirements Specification Using Natural Language Processing
Abstract:Extracting concepts from the software requirements is one of the first step on the way to automating the software development process. This task is difficult due to the ambiguity of the natural language used to express the requirements specification. The methods used so far consist mainly of statistical analysis of words and matching expressions with a specific ontology of the domain in which the planned software will be applicable. This article proposes a method and a tool to extract concepts based on a grammatical analysis of requirements written in English without the need to refer to specialized ontology. These concepts can be further expressed in the class model, which then can be the basis for the object-oriented analysis of the problem. This method uses natural language processing (NLP) techniques to recognize parts of speech and to divide sentences into phrases and also the WordNet dictionary to search for known concepts and recognize relationships between them.


Intelligent Automated Text Processing System - An NLP Based Approach

Applying NLP technologies to the collection and enrichment of language data on the Web to aid linguistic research
The field of linguistics has always been reliant on language data, since that is its principal object of study. One of the major obstacles that linguists encounter is finding data relevant to their research. In this paper, we propose a three-stage approach to help linguists find relevant data. First, language data embedded in existing linguistic scholarly discourse is collected and stored in a database. Second, the language data is automatically analyzed and enriched, and language profiles are created from the enriched data. Third, a search facility is provided to allow linguists to search the original data, the enriched data, and the language profiles in a variety of ways. This work demonstrates the benefits of using natural language processing technology to create resources and tools for linguistic research, allowing linguists to have easy access not only to language data embedded in existing linguistic papers, but also to automatically generated language profiles for hundreds of languages.


Chinese Natural Language Processing Based on Semantic Structure Tree

Enticing notification text & the impact on engagement
Push-notifications are a design tool used by mobile and web apps to alert subscribers to new information. In recent years, due to widespread adoption of the technology and the shrinking level of user attention available, marketing techniques have been deployed to persuade subscribers to engage positively with notifications. One such technique, known as the curiosity gap, exploits Lowenstein's Information-Gap theory. This paper explores the impact of enticing notification text, instilled by the curiosity gap, on subsequent engagement actions. A classifier was defined to identify enticing language in notifications. Features commonly paired with enticing text were identified. Intelligent notification delivery agents, trained using data captured in-the-wild, were evaluated using enticing and non-enticing notifications to demonstrate the influence of enticing text. Additionally, a solution was proposed and briefly evaluated for limiting subscriber susceptibility to enticing notifications.

Abstract:In NLP, the challenging and crucial task is Word Sense Disambiguation (WSD). Many Natural Languages have many ambiguous words with more than one sense. Depending on the context the sense of the ambiguous word is identified, this process is termed as word sense disambiguation (WSD). WSD algorithms are classified as context dependent and context independent algorithms. This article discusses about context dependent algorithms QEWTSS and QEGBCPR and their performances are compared using the evaluation metrics such as Normalized Discounted Cumulative Gain (NDCG) and Mean Average Precision (MAP) metrics. The data set used is Lexical Knowledge Base (LKB), which is developed from training data and is used for evaluation process.


Open Traffic Data Exchange and Collaborative Platform
Abstract:Traffic information is a necessity information for people within a smart city context. Whether we travel by personal cars or public transportations, up-to-date information on traffic situations helps make our travel much more convenient. Thailand is no different from the rest of the world. This is evident that many cities including Bangkok, Chiang Mai, Khon Kaen and Phuket built mobile applications for disseminating their existing transportation data to public. Also, several universities implement mobile applications to give information on campus transportation. However, these implementations are independent and their data cannot be shared or exchanged between them. This paper shows an implementation of an open traffic data exchange that is ongoing between Nectec and Mahidol University. Both Nectec and Mahidol University have their proprietary traffic systems to inform of their transportation. We agree on some sets of simple standards so that collaboration and sharing of information is possible and efficient. It is our intention to make this platform available to all community including R&D and business enterprises.

Abstract:We consider the use of text processing and analysis methods to create an automatic knowledge control system that evaluates the learning and understanding of educational material based on written answers. We indicate the part of the knowledge control system where semantic analysis of texts takes place. the system uses a text similarity algorithm based on distributive relationships, as well as a specific method for calculating the score of foreign words or formulas. The text processing methods used are described; the place of semantic analysis in knowledge control systems is shown, an example of the algorithm's operation on real data is given. The results of this article show that using this system has a significant impact on the educational process and is a viable solution.



Sparse Bayesian Learning Assisted CFO Estimation Using Nonnegative Laplace Priors

An Automated Tool for Generating UML Models from Natural Language Requirements
This paper describes a domain independent tool, named, UML Model Generator from Analysis of Requirements (UMGAR), which generates UML models like the Use-case Diagram, Analysis class model, Collaboration diagram and Design class model from natural language requirements using efficient Natural Language Processing (NLP) tools. UMGAR implements a set of syntactic reconstruction rules to process complex requirements into simple requirements. UMGAR also provides a generic XMI parser to generate XMI files for visualizing the generated models in any UML modeling tool. With respect to the existing tools in this area, UMGAR provides more comprehensive support for generating models with proper relationships, which can be used for large requirement documents.


Comparing the Conceptual Graphs Extracted from Patent Claims


AI-enabled Project Initiation: An approach based on RFP Response Document


Optimal renewable energy project sequencing with transmission expansion

Abstract:In today's information era, people are keen to know about various topics & events but are unable to keep up because of lack of time & information overload. Inspired by the need to say “Let me know”, `LeMeNo' aims to be an application that provides personalized news & event updates to its users without losing out on time or having to go through irrelevant content. This paper presents the event-based approach used by LeMeNo for News Recommendation based on user interest. The Recommendation System is based on both news & users interests. News articles are recommended based on machine learning techniques like clustering of similar articles, predicting their category, content similarity & keyword extraction. The system learns user interests based on time spent on reading an article, whether user likes/dislikes the article as well as user specified levels of interest in various topics. The proposed system provides a unique Event Timeline feature, which filters out multiple news articles related to an event, displays them in chronological order & appends new upcoming related articles, thus providing a journey of that event.


Abstract:Social media has become a valuable tool for users to express their opinion and for researchers to analyze public sentiment more efficiently. People respond much quickly towards any issue on social than any other traditional platform. Observing the patient's opinion on social media about the hospital medical facilities is a new trend that several hospitals are adopting recently in the modern world to improve their heal care facilities. After the pandemic of SARS-CoV-2, it has influenced the health care practices of all the world. Initial investigations indicate that patients with comorbidities are more fragile to this SARS-CoV-2 infection. Medical experts suggested postponing the routine treatment of cancer patients. However, few meta-analyses suggested evidence are not sufficient to hold the claim of the frailty of cancer patients to COVID-19. They were not in favor of shelving the scheduled treatments. On the other hand, some medical experts favored postponing cancer patients' scheduled treatments like chemotherapy, which could be a dangerous decision for cancer patients. We conducted the sentiment analysis of the patients with various comorbidities (diseases like diabetes, obesity, and cancer in which patient has to visit the hospital more often) to understand their point of view whether they were satisfied during the pandemic with their treatment or not? How Covid-19 affected their scheduled appointments. To serve the purpose, we gathered more than 150000 relevant tweets from Twitter (Jan 2020 to April 2020) to analyze the sentiment of cancer patients around the world. Our findings demonstrate a surge in the argument about cancer and its treatment after the outbreak of COVID-19. Most of the tweets are reasonable (52.6%) compared to negative ones (24.3%). We developed polarity and subjectivity distribution to better recognize the positivity/negativity in the sentiment. The results reveal that the polarity range of positive tweets is within the range of 0 to 0.5. That means the tendency in the tweets is not negative (it is above zero) nor so much positive too. It is statistical evidence supporting how natural language processing (NLP) can be used to better understand the patient's behavior in real-time. It may facilitate the medical professionals to make better decisions to organize the routine management of cancer patients.


Co-Training for Demographic Classification Using Deep Learning from Label Proportions
Abstract:Deep learning algorithms have recently produced state-of-the-art accuracy in many classification tasks, but this success is typically dependent on access to many annotated training examples. For domains without such data, an attractive alternative is to train models with light, or distant supervision. In this paper, we introduce a deep neural network for the Learning from Label Proportion (LLP) setting, in which the training data consist of bags of unlabeled instances with associated label distributions for each bag. We introduce a new regularization layer, Batch Averager, that can be appended to the last layer of any deep neural network to convert it from supervised learning to LLP. This layer can be implemented readily with existing deep learning packages. To further support domains in which the data consist of two conditionally independent feature views (e.g. image and text), we propose a co-training algorithm that iteratively generates pseudo bags and refits the deep LLP model to improve classification accuracy. We demonstrate our models on demographic attribute classification (gender and race/ethnicity), which has many applications in social media analysis, public health, and marketing. We conduct experiments to predict demographics of Twitter users based on their tweets and profile image, without requiring any user-level annotations for training. We find that the deep LLP approach outperforms baselines for both text and image features separately. Additionally, we find that co-training algorithm improves image and text classification by 4% and 8% absolute F1, respectively. Finally, an ensemble of text and image classifiers further improves the absolute F1 measure by 4% on average.

Does reusing pre-trained NLP model propagate bugs?
In this digital era, the textual content has become a seemingly ubiquitous part of our life. Natural Language Processing (NLP) empowers machines to comprehend the intricacies of textual data and eases human-computer interaction. Advancement in language modeling, continual learning, availability of a large amount of linguistic data, and large-scale computational power have made it feasible to train models for downstream tasks related to text analysis, including safety-critical ones, e.g., medical, airlines, etc. Compared to other deep learning (DL) models, NLP-based models are widely reused for various tasks. However, the reuse of pre-trained models in a new setting is still a complex task due to the limitations of the training dataset, model structure, specification, usage, etc. With this motivation, we study BERT, a vastly used language model (LM), from the direction of reusing in the code. We mined 80 posts from Stack Overflow related to BERT and found 4 types of bugs observed in clients’ code. Our results show that 13.75% are fairness, 28.75% are parameter, 15% are token, and 16.25% are version-related bugs.

Identifying multi-word expressions by leveraging morphological and syntactic idiosyncrasy
Multi-word expressions constitute a significant portion of the lexicon of every natural language, and handling them correctly is mandatory for various NLP applications. Yet such entities are notoriously hard to define, and are consequently missing from standard lexicons and dictionaries. Multi-word expressions exhibit idiosyncratic behavior on various levels: orthographic, morphological, syntactic and semantic. In this work we take advantage of the morphological and syntactic idiosyncrasy of Hebrew noun compounds and employ it to extract such expressions from text corpora. We show that relying on linguistic information dramatically improves the accuracy of compound extraction, reducing over one third of the errors compared with the best baseline.

Abstract:Stemming is a technique that transforms morphologically similar terms into a unique term without doing a complete morphological analysis. Stemming is used as a preprocessing step in many Natural Language Processing (NLP) applications like Information retrieval (IR), Machine Translation, Parsing, Summarization, etc. The present work explores the application of stemming to the task of information retrieval. In IR, stemming is generally used for two main purposes: decreasing index size and for increasing system performance. This paper presents a stemmer for Marathi language which uses rule-based technique. The average accuracy achieved by the proposed stemmer is 79.97% when tested on a collection of 4500 unique words from the news corpus among nine runs. Since the accuracy of the proposed stemmer is satisfactory it can be effectively useful in several NLP systems for Marathi language.


Abstract:In Artificial Intelligence (AI), the contents of an image are generated automatically which involves computer vision and NLP (Natural Language Processing). The neural model which is regenerative, is created. It depends on computer vision and machine translation. This model is used to generate natural sentences which eventually describes the image. This model consists of Convolutional Neural Network(CNN) as well as Recurrent Neural Network(RNN). The CNN is used for feature extraction from image and RNN is used for sentence generation. The model is trained in such a way that if input image is given to model it generates captions which nearly describes the image. The accuracy of model and smoothness or command of language model learns from image descriptions is tested on different datasets. These experiments show that model is frequently giving accurate descriptions for an input image.



Demo: Automatically Retrainable Self Improving Model for the Automated Classification of Software Incidents into Multiple Classes

Artificial Intelligence in Financial Services – Need to Blend Automation with Human Touch
Abstract:Artificial Intelligence is the latest in the series of disruptions manifested by computer science. AI has been rapidly transforming the dynamics of banking and financial services industry also. The established and emerging capabilities of AI are being combined, reconstituted and re-formulated in unexpected ways and are throwing up new opportunities and new challenges but at the same time posing new threats also. Apart from the ethico-neutral character of technology and its attendant threats like cyber-crimes and macro-financial risks, a major question to be inquired into is its sustainability as it tends to replace humans and the related personal touch which most often is the essence of financial services industry thriving on the art of customization and customer delight. The instant paper attempts to examine relatively under-explored perspective of AI replacing humans in the space of banking and financial services and unmindfully heralding the flight of personal touch and service customization which are the cornerstone of customer satisfaction and delight in industries like banking and financial services known for their fiduciary and responsible character.

DefScriber: a hybrid system for definitional QA
No abstract available.

Abstract:With an increase in targeted attacks such as advanced persistent threats (APTs), enterprise system defenders require comprehensive frameworks that allow them to collaborate and evaluate their defense systems against such attacks. MITRE has developed a framework which includes a database of different kill-chains, tactics, techniques, and procedures that attackers employ to perform these attacks. In this work, we leverage natural language processing techniques to extract attacker actions from threat report documents generated by different organizations and automatically classify them into standardized tactics and techniques, while providing relevant mitigation advisories for each attack. A naïve method to achieve this is by training a machine learning model to predict labels that associate the reports with relevant categories. In practice, however, sufficient labeled data for model training is not always readily available, so that training and test data come from different sources, resulting in bias. A naïve model would typically underperform in such a situation. We address this major challenge by incorporating an importance weighting scheme called bias correction that efficiently utilizes available labeled data, given threat reports, whose categories are to be automatically predicted. We empirically evaluated our approach on 18,257 real-world threat reports generated between year 2000 and 2018 from various computer security organizations to demonstrate its superiority by comparing its performance with an existing approach.


An emotion information processing model based on a mental state transition network
Abstract:A machine that lacks of emotion computing ability cannot realize artificial intelligent sufficiently and cannot meet the increasing demanding of human-computer interaction as well. Though most current research is focusing on physical components of emotions, rarely are they carried out from the view of psychology. In this paper an emotion information processing model based on the mental state transition network and a corpus of common sense are proposed to detect human emotion. By a series of psychological experiments, we present a new way to predict future human's emotions depending on the various current emotional states under various conditions. Besides, people in different sexes, characters and ages are taken into consideration in our experiments. From the psychological experiments data that is abstracted from 250 questionnaires, a Bayesian network for describing the transitions in distribution among the emotions and relationships between internal mental situations and external reinforcements are concluded. Further more, comparing seven relative evaluating experiments we found that the model provided a higher precision average rate of 0.843 respectively for the 50 random data examples.

Abstract:In the era of growing world, Social Sites is one of the platform where lots of people interacted with it. Every individual directly or indirectly connected with the social sites. As we can see, People are used to about taking reviews about anything before doing it-as taking reviews of movies, restaurants, online product shopping and many more. Taking reviews means knowing opinions about things. In a single manner it can state as Sentiment Analysis, or even it can be called as Opinion Mining or Data Mining. Here, the present work is come with the idea of Twitter Sentiment Analysis, in which process is designed to know the person thought about any specific tweet done by them. After knowing the people opinions about any issue, any individual can come up with the conclusion. To do this types of analysis, Present work has taken Natural Language Processing (NLP) in use. So this research paper describe the types of techniques used and also the procedure performed throughout the process.


An algorithmic approach to extract actions and actors (AAEAA)
Use case is a model delivered by requirements engineering phase, which is considered as an input to the forthcoming design phase and test phase. A use case model is a simplest representation of an actor's interactions with the system in which the user is involved. The development of a use case model requires the finding out the use case itself and the actor that uses this use case to interact with the system. These two tasks are achieved manually via analyst's experience, who starts with different sources of data to develop use case model. User requirements document is a common source of data that may be started with to develop use case model. The extracting of actors and their actions (use cases) is subjected to the linguistic properties of each on. The aim of this paper is to define a new algorithmic approach for extracting actors and their use cases by using thematic role technique. This algorithmic approach had been manually tested using known examples, and shown its validity. The success of this technique will lead to develop an Intelligent Computer Aided Software Engineering (I-CASE) tool that automatically extracts actions and actors of use case model from functional requirements by using Semantic Role Labelling (SRL) of Natural Language Processing (NLP) approach.

Perception of syllables pitch contour in Sindhi language
Abstract:Sindhi is an open syllable language cv; most of its words have vocalic ending and the occurrence of irregular consonants. We had digitally investigated eight classes of syllables reported by Jatoi [4]. The object of this paper is to analyze the actual syllable patterns occur in Sindhi and the frequency of syllable patterns. The syllables consist onV, cv, cV, cV v, ccV ccV c, cV c and cvcc. Phonetically, all types of syllables end with short or long vowels. The purpose of the research is to investigate the f0 peak of syllable classes in terms of the different position of short and long vowels for the appropriate syllabification in a Sindhi word. The clear differences of rise size were found due to the placement of both types of vowels in a syllable of Sindhi words. The achieved results of this work are fundament of computational Sindhi speech processing.

Intelligent Medicine Identification System Using a Combination of Image Recognition and Optical Character Recognition
Abstract:This research aims to develop an automatic verification system with deep learning techniques to verify prescription dispensing accuracy. The proposed method will be able to help pharmacies to reduce errors that lead to patients receiving the wrong medicine to patients. The system consists of two models: image classification and text classification. The image classification model uses raw medicine blister pack images, then removes the background to interpret the features based on the pattern recognition for Histograms of Oriented Gradients (HOG) of the model. It is composed of Convolution Neural Network (CNN), Linear Regression, and Logistic Regression. The text classification model uses text extraction to obtain imprints appearing on the blister package then matches the words to a bag of word. The dataset collected two-hundred types of medicine blister packs images inside plastic zip bags as a dataset. It includes 300 high-quality images of front-side medicine blister packages for each type of package in light-controlled conditions with a black background, which are used for training the model. The automatic verification system uses the majority vote based on the confidence of the two models. Experimental results, indicate that the image classification model of CNN with HOG feature extraction has the highest accuracy at 95.83 percent. In-text classification results show that the method using Character Region Awareness For Text detection (CRAFT), Keras-OCR, and text correction gave the highest accuracy at 92 percent. Overall accuracy was 94.23 percent.

Abstract:The approach presented in this paper aims at finding a solution to the problem of conflict-free motion planning for multiple aircraft on the same flight level with trajectory recovery. One contribution of this work is to develop three consistent models, i.e., from a continuous-time representation to a discrete-time linear approximation. Each of these models guarantees separation at all times and trajectory recovery, but they are not equally difficult to solve. A new hybrid algorithm is thus developed to use the optimal solution of a mixed-integer linear program as a starting point when solving a nonlinear formulation of the problem. The significance of this process is that it always finds a solution when the linear model is feasible while still taking into account the nonlinear nature of the problem. A test bed containing numerous data sets is then generated from three virtual scenarios. A comparative analysis with three different initializations of nonlinear optimization validates the efficiency of the hybrid method.


Answer extraction towards better evaluations of NLP systems
We argue that reading comprehension tests are not particularly suited for the evaluation of NLP systems. Reading comprehension tests are specifically designed to evaluate human reading skills, and these require vast amounts of world knowledge and common-sense reasoning capabilities. Experience has shown that this kind of full-fledged question answering (QA) over texts from a wide range of domains is so difficult for machines as to be far beyond the present state of the art of NLP. To advance the field we propose a much more modest evaluation set-up, viz. Answer Extraction (AE) over texts from highly restricted domains. AE aims at retrieving those sentences from documents that contain the explicit answer to a user query. AE is less ambitious than full-fledged QA but has a number of important advantages over QA. It relies mainly on linguistic knowledge and needs only a very limited amount of world knowledge and few inference rules. However, it requires the solution of a number of key linguistic problems. This makes AE a suitable task to advance NLP techniques in a measurable way. Finally, there is a real demand for working AE systems in technical domains. We outline how evaluation procedures for AE systems over real world domains might look like and discuss their feasibility.

VocabChecker: Measuring Language Abilities for Detecting Early Stage Dementia
Recently, dementia patients have been increasing in number worldwide, necessitating the development of techniques to detect dementia as early as possible. Considering that a typical symptom of dementia, especially Alzheimer's disease, is language impairment, speech-based dementia detection approaches have drawn much attention. This paper presents a smartphone-based dementia screening application, VocabChecker, which measures language abilities from a speech narrative via automatic speech recognition (ASR). It measures four language abilities related to dementia: number of tokens (token), number of types (type), type token ratio (TTR), and potential vocabulary size (PVS). We also reported that the use of VocabChecker has distinguished dementia patients from elderly people.

Native judgments of non-native usage: experiments in preposition error detection
Evaluation and annotation are two of the greatest challenges in developing NLP instructional or diagnostic tools to mark grammar and usage errors in the writing of non-native speakers. Past approaches have commonly used only one rater to annotate a corpus of learner errors to compare to system output. In this paper, we show how using only one rater can skew system evaluation and then we present a sampling approach that makes it possible to evaluate a system more efficiently.

Ontology-driven coordination for supply chain system
Abstract:This paper proposes an ontology-driven coordination framework for information interoperation in supply chain system (SCS). To achieve semantic interoperability in heterogeneous enterprise information systems, ontologies have been advised to use. Mapping among multiple ontologies is necessary for an across-enterprises supply chain. In some complex supply chain, it highly demands semantic interoperability to bridge heterogeneous enterprise applications at various domain into an integrative system. In this paper, we elaborate application ontologies and their mapping logic in SCS, and discuss how to use object-oriented technologies to define primitive ontology and describe ontology coordination logic. We also discuss how to design and implement the coordination framework to solving the semantic interoperability for supply chain information system.

Abstract:Aiming at the text data of railway transportation safety, designed the platform of the unstructured text data converte into the structured data, and assist the security text data further mined. Summarize the data characteristics, and design the overall framework and business process of the platform from the perspective of data security and intelligent data operation. The named entity recognition and text classification models that can realize text structural transformation are studied. Some models are built into the platform according to the characteristics of data. The interaction mode between model learning and platform is designed. The platform provides a structured transformation tool for text data analysis in the field of railway transportation security.


Beyond NVD: Cybersecurity meets the Semantic Web.
Cybersecurity experts rely on the knowledge stored in databases like the NVD to do their work, but these are not the only sources of information about threats and vulnerabilities. Much of that information flows through social media channels. In this paper we argue that security experts and general users alike can benefit from the technologies of the Semantic Web, merging heterogeneous sources of knowledge in an ontological representation. We present a system that has an ontology of vulnerabilities at its core, but that is enhanced with NLP tools to identify cybersecurity-related information in social media and to launch queries over heterogeneous data sources. The transformative power of Semantic Web technologies for cybersecurity, which has been proven in the biomedical field, is evaluated and discussed.

Increasing maintainability of NLP evaluation modules through declarative implementations
Computing precision and recall metrics for named entity tagging and resolution involves classifying text spans as true positives, false positives, or false negatives. There are many factors that make this classification complicated for real world systems. We describe an evaluation system that attempts to control this complexity through a set of rules and a forward chaining inference engine.

Identifying the epistemic value of discourse segments in biology texts
To manage the flood of information that threatens to engulf (life-)scientists, an abundance of computer-aided tools are being developed. These tools aim to provide access to the knowledge conveyed within a collection of research papers, without actually having to read the papers. Many of these tools focus on text mining, by looking for specific named-entities that have scientific meaning, and relationships between these. An overview of the current state of the art is given in Rebholz-Schuhmann et al. (2005) and Couto et al. (2003). Typically, these tools identify a list of sentences containing relationships between two specific named-entities that can be found using rules or a thesaurus of synonyms. These sentences represent an overview of the interactions that are known with a specific entity, thus precluding the need for an exhaustive literature study. For example, the following are a few sentences that have been found using a typical text mining tool for the relationship 'p53 activates*': 1. The p53 tumor suppressor protein exerts most of its anti-tumorigenic activity by transcriptionally activating several pro-apoptotic genes. 2. We found that p53 ... activates[,] the promoter of the myosin VI gene.

Third workshop on exploiting semantic annotations in information retrieval (ESAIR): CIKM 2010 workshop
There is an increasing amount of structure on the Web as a result of modern Web languages, user tagging and annotation, and emerging robust NLP tools. These meaningful, semantic, annotations hold the promise to significantly enhance information access, by enhancing the depth of analysis of today's systems. Currently, we have only started exploring the possibilities and only begin to understand how these valuable semantic cues can be put to fruitful use. Unleashing the potential of semantic annotations requires us to think outside the box, by combining the insights of natural language processing (NLP) to go beyond bags of words, the insights of databases (DB) to use structure efficiently even when aggregating over millions of records, the insights of information retrieval (IR) in effective goal-directed search and evaluation, and the insights of knowledge management (KM) to get grips on the greater whole. The Workshop aims to bring together researchers from these different disciplines and work together on one of the greatest challenges in the years to come. The desired result of the workshop will be concrete insight into the potential of semantic annotations, and in concrete steps to take this research forward; synchronize related research happening in NLP, DB, IR, and KM, in ways that combine the strengths of each discipline; and have a lively, interactive workshop were everyone contributes and that inspires attendees to think "outside the box".

Abstract:Acoustic Echo Cancelation is a typical event in today's telecommunication fields. The input speech signal interference caused by acoustic echo is disturbing for both users side and causes a reduction in the quality of the original signal. The adaptive filtering techniques are used to reduce the unwanted echo, and increasing quality of the speech signal. This Paper focus on different Adaptive filters algorithms and characterized by the Normalized Least Mean Square (NLMS) and Variable Step Size Least Mean Square (VSSLMS) and also This Paper actualizes Real Time Acoustic Echo Cancellation on Tms320c6713dsk. The primary modules of this reverberation canceller are a versatile channel framework utilizing Normalized Least Mean Square (NLMS) calculation, a Double-Talk Detector (DTD) and a Non-Linear Processor (NLP). The Setup of NLMS based versatile framework, Modified NLMS with Variable Step Size calculations and Frequency Domain based Fast LMS algorithms are to be actualized on the DSP equipment. This implementation shows fast response, speed, convergence of time and good tracking capability. The calculation of different adaptive algorithms must be analysed utilizing MATLAB.


Multi-domain adaptation for sentiment classification: Using multiple classifier combining methods
Abstract:Sentiment classification is very domain-specific and good domain adaptation methods, when the training and testing data are drawn from different domains, are sorely needed. In this paper, we address a new approach to domain adaptation for sentiment classification in which classifiers are adapted for a specific domain with training data from multiple source domains. We call this new approach dasiamulti-domain adaptationpsila and present a multiple classifier system (MCS) framework to describe and understand it. Under this framework, we propose a new combining method, called Multi-label Consensus Training (MCT), to combine the base classifiers for selecting dasiaautomatically-labeledpsila samples from unlabeled data in the target domain. The experimental results for sentiment classification show that multi-domain adaptation using this method improves adaptation performance.


Role of negative data and spoken corpora in corpus building


Online Dynamic Window (ODW) Assisted 2-Stage LSTM Indoor Localization for Smart Phones

Abstract:Syntactic Parsing has played an important role in Natural Language Processing (NLP). The character of Traditional Mongolian is that “the predicate is generally at the end of the sentence, the other constituent can change location but the meaning of the sentence is not change”. According to this character, the paper propose a “from-bottom-to-top” method to analyze the sentence constituent. Part-of-Speech (POS) tagging is the first step of analyzing the sentence constituent. Marking POS of words and phrases based on the dictionary library and the rule base. After the preprocessing, the sentence is divided into several modules by keywords, “case”, phrase, and so on. Every module use the “from-bottom-to-top” method to analyze and label the sentence constituent.


Integrating natural language processing into e-learning: a case of Czech
The paper deals with the application of NLP technology in e-learning. We report our research on intelligent platforms for computer-mediated education. Some of the methods described in the paper have already taken part in the end-user applications that are in everyday use, others still wait for their implementation in the form of software products. The main message of the paper is that the language technology, even in the imperfect form of the current state of the art, can significantly enhance today's computer-mediated teaching and learning activities. It is true especially for languages different from English, where the adopted learning management systems often do not support even the basic functionality of a language-oriented search and retrieval of learning objects. As a case study, this paper demonstrates the application of the given ideas for e-learning materials in Czech.

Abstract:Information is playing an important role in our lives. One of the major sources of information is databases. Databases and database technology are having major impact on the growing use of computers. In order to retrieve information from a database, one needs to formulate a query in such way that the computer will understand and produce the desired output. Generally, query processing is handled by the Structured Query Language (SQL). But the non IT people cannot be able to write SQL queries as they may not be aware of the SQL as well as structure of the database. So there is a need for non-expert users to query the databases in their natural language instead of working with the values of the attributes. This paper proposes an approach for accessing the database easily using natural language without having any knowledge about the query language. The approach is a rule based approach. The obvious advantage is that it makes a great promise for computer interfaces easier for the use of general people. Because of this, people will be able to communicate to the computer in their own language instead of learning a specialized language or commands. In order to test our approach in an actual computer environment, we have developed a prototype system. We obtained promising results using our system.


Multimodular Text Normalization of Dutch User-Generated Content
As social media constitutes a valuable source for data analysis for a wide range of applications, the need for handling such data arises. However, the nonstandard language used on social media poses problems for natural language processing (NLP) tools, as these are typically trained on standard language material. We propose a text normalization approach to tackle this problem. More specifically, we investigate the usefulness of a multimodular approach to account for the diversity of normalization issues encountered in user-generated content (UGC). We consider three different types of UGC written in Dutch (SNS, SMS, and tweets) and provide a detailed analysis of the performance of the different modules and the overall system. We also apply an extrinsic evaluation by evaluating the performance of a part-of-speech tagger, lemmatizer, and named-entity recognizer before and after normalization.


Exploring Paraphrasing Techniques on Formal Language for Generating Semantics Preserving Source Code Transformations

Affix-augmented stem-based language model for persian
Abstract:Language modeling is used in many NLP applications like machine translation, POS tagging, speech recognition and information retrieval. It assigns a probability to a sequence of words. This task becomes a challenging problem for high inflectional languages. In this paper we investigate standard statistical language models on the Persian as an inflectional language. We propose two variations of morphological language models that rely on a morphological analyzer to manipulate the dataset before modeling. Then we discuss shortcoming of these models, and introduce a novel approach that exploits the structure of the language and produces more accurate. Experimental results are encouraging especially when we use n-gram models with small training dataset.

Abstract:Sentiment Analysis(SA) for Kannada documents has been explored recently. In the recent study [8], the sentiment analysis for Kannada text is explored using Naive Bayes classifier. The objective of this work is to improve the performance of the previous study on the sentiment analyzer for Kannada language explored in the paper [8]. In this work, we propose the ensemble of classifier with random forest technique to identify the polarity of the sentiment and test the performance of the same. Also in this work, some of the limitations of [8] such as handling multi class labels, identification of sentiment polarity of comparative and conditional statements have been addressed. The over all accuracy is improved from 65% to 72 %, indicating our approach based on Random Forest technique is more efficient for SA for Kannada.



Extracting Business Process Models Using Natural Language Processing (NLP) Techniques

Abstract:Current world is growing so fast and communication between nation and different type of people with different language became part of our life. Even from buying product to our social life everything is dependent on communication. Therefore language is the most important part of human life. Though still now there is a language barrier for communication between people. But very soon language will be universal and everyone will be able to communicate in any language worldwide using the NLP technology. For that it is necessary to understand each language individually. This research proposes a new type of text generation of Bangla language using the bi-directional RNN. This technique is used to predict the next possible word in a Bangla text.


Injecting linguistics into NLP through annotation
Over the past 20 years, the size of the L in Computatioonal Linguistics has been shrinking relative to the size of the C. The result is that we are incresingly becoming a community of uniformed but sophisticated engineers, applying to problems very complex machine learning techninques that use very simple (simplistic?) analyses/theories. (Try finding a theoretical account of subjectivity, opinion, entailment, or inference in publications surrounding the associated competitions of the past few years.)

 methodology and curve-fitting technique. Along with the small-signal analysis, both linear and nonlinear programming techniques are utilized for topology feasibility checking. With only a small number of circuit topologies through the fast evaluation stage toward the subsequent detailed sizing and further evaluation, the efficiency of the whole circuit synthesis process can be significantly improved. The experimental results demonstrate high efficiency, strong reliability, and wide applicability of our proposed methods.


Scenic area data analysis based on NLP and ridge regression
Abstract:With the rapid development of Internet technology, many textual evaluation data of tourist destinations have accumulated on the Internet. Using NLP to conduct text mining on the data can effectively improve tourists' satisfaction and has a long-term and positive effect on the scientific supervision of tourism enterprises and the optimal allocation of resources. This paper uses Python to pre-process the comment data, including de-duplication, removal of English text, conversion of traditional Chinese to simplified, text correction, and compression to remove words. The reviews are divided into five categories: service, location, facility, hygiene, and cost-performance. The Paddlehub library is used to calculate the emotional scores of all reviews in the five aspects of each scenic spot and hotel and subsequently calculate the percentage of positive, neutral, and negative reviews. Afterward, use Ridge Regression and k-fold cross-validation to establish a comprehensive evaluation model, which can obtain the total score of each scenic spot and hotel in five aspects, with MSE, RMSE, MAE to verify. Furthermore, a method of extracting characteristic words in scenic spots and hotels is proposed: firstly, use the LDA subject vocabulary mining; next, select the TOP50 words through operations such as extracting keywords, selecting out nouns, filtering out irrelevant words, and synonymous merge; lastly, two parts of words are integrated to get the characteristic words. Finally, according to the total score, the scenic spots and hotels are divided into three levels: high, medium, and low levels, while three groups of scenic spots and hotels of the same type are selected respectively (each group has three scenic spots or hotels of different level). Through the characteristic words and five aspects of the total score, we can compare and analyze the selected three groups of scenic spots and hotels to make a suggestion.

MACAON: an NLP tool suite for processing word lattices
MACAON is a tool suite for standard NLP tasks developed for French. MACAON has been designed to process both human-produced text and highly ambiguous word-lattices produced by NLP tools. MACAON is made of several native modules for common tasks such as a tokenization, a part-of-speech tagging or syntactic parsing, all communicating with each other through XML files. In addition, exchange protocols with external tools are easily definable. MACAON is a fast, modular and open tool, distributed under GNU Public License.

TVis: A Light-weight Traffic Visualization System for DDoS Detection
Abstract:With rapid growth of network size and complexity, network defenders are facing more challenges in protecting networked computers and other devices from acute attacks. Traffic visualization is an essential element in an anomaly detection system for visual observations and detection of distributed DoS attacks. This paper presents an interactive visualization system called TVis, proposed to detect both low-rate and highrate DDoS attacks using Heron’s triangle-area mapping. TVis allows network defenders to identify and investigate anomalies in internal and external network traffic at both online and offline modes. We model the network traffic as an undirected graph and compute triangle-area map based on incidences at each vertex for each 5 seconds time window. The system triggers an alarm iff the system finds an area of the mapped triangle beyond the dynamic threshold. TVis performs well for both low-rate and high-rate DDoS detection in comparison to its competitors.

Tunable plasmonic Yagi-Uda nanoantenna
Abstract:We suggest and theoretically study a novel tunable plasmonic Yagi-Uda nanoantenna driven by means of a feed element consisting of two closely spaced metal nanobars separated by a gap filled with a semiconductor. Through extensive numerical simulations we demonstrate that a variation of the free-carrier density in the semiconductor strongly modifies the antenna's response in both near-field and far-field regions, which opens up ways for active all-optical spectral tuning of the nanoantenna.


Global solution of multi-objective optimal control problems with multi agent collaborative search and direct finite elements transcription

A question-answering system in Bahasa Malaysia
Abstract:The authors present a prototype question-answering system named SIGMA. This system is designed and implemented for interpreting and answering Bahasa Malaysia (Malaysian language) queries in a specific domain. The independent mechanisms of the system, the parser, the analyzer, and the response generator, are described in detail. During a trial run, the system demonstrated fast response times and was able to successfully answer 87.42% of the sentences from users.<


NLP-based Enhancement of Information Security in ITO - A Diffusion of Innovation Theory perspective


Question answering using ontological semantics

Abstract:Attackers often use an executable file (malware) as a tool to obtain sensitive information from specific companies and individuals. Anti-virus software attempts to detect the malware by pattern matching method etc. However, it is difficult to detect unknown malware in these methods. The unknown malware is detected by a sandbox, etc. We consider another method because the sandbox requires much time for running. ASCII strings extracted from executable files are helpful for analyzing malware. With the recent development of natural language processing (NLP) techniques, it is becoming possible to use these strings as a malware detection method. In this paper, we propose a malware detection method using ASCII strings with NLP techniques. Our method divides these strings into words, and distinguishes the difference of the words between benign and malicious executable files. To compare with the arrangement of words or the frequency of appearing words, uncommon words are unnecessary in NLP techniques. Thus, we consider that reducing the uncommon words improves the detection rate. Our method converts a corpus of frequent words into a feature vector with natural language processing techniques. In our experiments, we used a dataset containing more than 23,000 malware samples (more than 2,100 malware families) provided by FFRI and more than 16,000 benign files collected from "download.cnet.com". Our method achieves the F-measure more than 0.85. The experimental results show that our method detects unknown malware with high accuracy.


Abstract:Machine translation means automatic translation which is performed using computer software. There are several approaches to machine translation, some of them need extensive linguistic knowledge while others require enormous statistical calculations. This paper presents a hybrid method, integrating corpus based approach and statistical approach for translating Bengali sentences into English with the help of N-gram language model. The corpus based method finds the corresponding target language translation of sentence fragments, selecting the best match text from the bilingual corpus to acquire knowledge while the N-gram model rearranges the sentence constituents to get an accurate translation without employing external linguistic rules. A variety of Bengali sentences, including various structures and verb tenses are considered to translate through the new system. The performance of the proposed system is evaluated in terms of adequacy, fluency, WER, and BLEU score. The assessment scores are compared with other conventional approaches as well as with Google Translate, a well-known free machine translation service by Google. It has been found that experimental results of the work provide higher scores over Google Translate and other methods with less computational cost.


Leveraging Google BERT to Detect and Measure Innovation Discussed in News Articles
Abstract:In this paper, we leverage non-survey data (i.e., news articles), natural language processing (NLP), and deep learning methods to detect and measure innovation, ultimately enriching innovation surveys. Our dataset is composed of 1.9M news articles published between 2013 and 2018 acquired from Dow Jones Data, News, and Analytics. We use Bidirectional Encoder Representation from Transformers (BERT), a neural network-based technique for NLP pre-training developed by Google. Our methods involve: (i) utilizing Google’s BERT as a binary classifier to identify articles that mention innovation, (ii) developing BERT’s named-entity recognition algorithm to extract company names from these articles, (iii) leveraging BERT’s question and answering capabilities to extract company and product names. As a result, we obtain innovation indicators, i.e., company innovations in the pharmaceutical sector.


A Proposed Approach to Check Project Idea Similarity Using Topic Modelling

TutorBot: contextual learning guide for software engineers
This document is poster submission on using conversational chat bot to guide a software engineer in their learning journey and keeping pace with the technology changes. We describe the motivation, technical approach, and experience of building, piloting such a Bot in a controlled setting and capturing the user feedback. The document also discusses future opportunities to extend and enhance the functionality.

Abstract:With the expanding of State Grid structure, operation instructions logics of power grid are increasingly complex, which makes it even more difficult to compile operation instructions based on the dispatchers' experiences, consequently may lead to great risks in safe running of the power grid. This paper proposes an artificial intelligence method adopting NLP (Natural Language Processing) technology, this method can get the computers understand power grid operation tasks by themselves and automatically generate operation instructions with the deduction method designed by the authors. To reduce the dispatchers' workload and to improve the accuracy and safety of operations, these instructions need to be checked for safety before and after execution.


Abstract:Modern urbanization is demanding smarter technologies to improve a variety of applications in intelligent transportation systems to relieve the increasing amount of vehicular traffic congestion and incidents. Existing incident detection techniques are limited to the use of sensors in the transportation network and hang on human-inputs. Despite of its data abundance, social media is not well-exploited in such context. In this paper, we develop an automated traffic alert system based on Natural Language Processing (NLP) that filters this flood of information and extract important traffic-related bullets. To this end, we employ the fine-tuning Bidirectional Encoder Representations from Transformers (BERT) language embedding model to filter the related traffic information from social media. Then, we apply a question-answering model to extract necessary information characterizing the report event such as its exact location, occurrence time, and nature of the events. We demonstrate the adopted NLP approaches outperform other existing approach and, after effectively training them, we focus on real-world situation and show how the developed approach can, in real-time, extract traffic-related information and automatically convert them into alerts for navigation assistance applications such as navigation apps.



Keyword extraction from Tweets using NLP tools for collecting relevant news

Abstract:In processing ill-formed spontaneous spoken utterance, many state-of-the-art robust parsers achieve robustness by allowing skipping of words and rule symbols. The parser's ability to skip words and rule symbols, however, results in a much bigger search space and greatly increases the parse ambiguity. Previous approaches resolved these issues through manually labeling the types of rule symbols, or by utilizing heuristic scores or statistical probabilities. However, these approaches have certain drawbacks. This paper proposes to exploit embedded machine learning techniques to help with pruning and disambiguation in robust parsers. An embedded machine learning system is integrated with the heuristic score and the strategy of basing the types of rule symbols upon their correspondence to the domain model. This integration can considerably relieve the reliance of robust parser development on linguistic expert handcrafting. Our experiments show that this integration offers stronger capability in ambiguity resolution, thereby enabling the robust parser to achieve better parsing accuracy.


Abstract:Since the coronavirus disease 2019 (COVID-19) outbreak has spread across the country, our research applies to remind the people to wear a face mask when we go outside because a facial image detection and classification method will be used to authentication and authorization. This paper has shown that our created models based on CNN can detect the face mask-wearing, glasses-wearing, and gender with comparison two models. We training model with mix public datasets such as WIDER FACE, AFW, and MAFA. Moreover, we use VGG-Face to pre-train the model for the advance detection rate.


Abstract:Text Mining has emerged as an active domain in the field of NLP (Natural Language Processing) and due to availability of large data sets of reviews, it has become easy to do sentiment analysis and extract the result from it, but Now-a-days the objectives are expressed in different ways making the data massive and difficult to understand for machines. In this research work, machines are first trained (Supervised learning) with the help of the predefined data (or more clearly reviews) and then tested with the reviews available. This Research work will show you the working of a system that uses the supervised training which classifies a product review as positive or negative using various classifier algorithms like KNN, Logistic Regression and Support Vector Machines. The model which will give the more accuracy will be considered as the best model.



DeNom: a tool to find problematic nominalizations using NLP

Abstract:Most modern neural networks for classification fail to take into account the concept of the unknown. Trained neural networks are usually tested in an unrealistic scenario with only examples from a closed set of known classes. In an attempt to develop a more realistic model, the concept of working in an open set environment has been introduced. This in turn leads to the concept of incremental learning where a model with its own architecture and initial trained set of data can identify unknown classes during the testing phase and autonomously update itself if evidence of a new class is detected. Some problems that arise in incremental learning are inefficient use of resources to retrain the classifier repeatedly and the decrease of classification accuracy as multiple classes are added over time. This process of instantiating new classes is repeated as many times as necessary, accruing errors. To address these problems, this article proposes the classification confidence threshold (CT) approach to prime neural networks for incremental learning to keep accuracies high by limiting forgetting. A lean method is also used to reduce resources used in the retraining of the neural network. The proposed method is based on the idea that a network is able to incrementally learn a new class even when exposed to a limited number samples associated with the new class. This method can be applied to most existing neural networks with minimal changes to network architecture.



Implementing a Portable Clinical NLP System with a Common Data Model -- a Lisp Perspective

th
 partitions. Thus machine translation system is able to produce translation output as a continuous stream, sentence by sentence, as soon as each sentence gets translated. The system maintains flow rate of translated sentences stream high enough so that the next translated sentence is produced well before the end user finishes reading the previous sentence, thereby providing very good user experience. There is a class of natural language processing (NLP) applications, viz., machine translation systems, text to speech systems, speech recognition systems, etc., that are functional in nature, and this engineering approach would be equally applicable to them as well.

Extended lexical-semantic classification of English verbs
Lexical-semantic verb classifications have proved useful in supporting various natural language processing (NLP) tasks. The largest and the most widely deployed classification in English is Levin's (1993) taxonomy of verbs and their classes. While this resource is attractive in being extensive enough for some NLP use, it is not comprehensive. In this paper, we present a substantial extension to Levin's taxonomy which incorporates 57 novel classes for verbs not covered (comprehensively) by Levin. We also introduce 106 novel diathesis alternations, created as a side product of constructing the new classes. We demonstrate the utility of our novel classes by using them to support automatic subcategorization acquisition and show that the resulting extended classification has extensive coverage over the English verb lexicon.

Applications of lexical information for algorithmically composing multiple-choice cloze items
We report experience in applying techniques for natural language processing to algorithmically generating test items for both reading and listening cloze items. We propose a word sense disambiguation-based method for locating sentences in which designated words carry specific senses, and apply a collocation-based method for selecting distractors that are necessary for multiple-choice cloze items. Experimental results indicate that our system was able to produce a usable item for every 1.6 items it returned. We also attempt to measure distance between sounds of words by considering phonetic features of the words. With the help of voice synthesizers, we were able to assist the task of composing listening cloze items. By providing both reading and listening cloze items, we would like to offer a somewhat adaptive system for assisting Taiwanese children in learning English vocabulary.

Abstract:Medication is one of the essential parts of a patient's treatment. Therefore, it is important to have good medication storage administration in order to have effective medication storage. This study aimed to find a proper model used for the prediction of medication purchase amount by using machine learning to analyze medication purchasing amounts in the form of time series. In this research, the first 10 medicines in AV group were chosen. Then, Multilayer Perceptron (MLP), Long Shot-Term Memory (LSTM), and 1D Convolutional neural network with LSTM models were used together with Rolling Windows which were used to predict the purchase amount of each model. The periods of prediction were at 1 month, 3 months, and 6 months. The efficacy of each model was compared using their errors. CNN-LSTM model produces the better forecasting results. The result also shows that 1-month forecasting period is suitable for medicines that specific to disease. The 3-month forecasting period is suitable for commonly used medicines. The 6-month forecasting period is suitable for the medicines for chronic diseases.


Drug Safety Intelligence and Automation
Abstract:A huge amount of data about adverse drug reactions are in medical cases, and that is a major challenge for experts to generate reports and analyze it. Time and money are wasted in finding efficient strategies for identifying and extracting this type of information. Our paper focuses on solving problems in report generation of free text cases and reports to support Pharmacovigilance research and decision-making. Hence, Machine learning with the help of NLP is used in the identification of MEDLINE case. An automated system to solve the problem is developed to identify drug reactions and effects from ICSRs (Individual Case Safety Report) and drug safety classification of adverse drug events from free-text electronic patient records and information. A further implementation into an AI-based mass casualty management framework is considered for future work to aid the decision-making right after triage.


An Efficiency Comparison for Predicting of Educational Achievement Based on LMT

Abstract:This paper shows how the Unified Modeling Language(UML) can be used to represent a textual object or sentence of a natural language. Precisely, the representation builds on the UML class. That is data of a textual object - which are component types or "word types", lemmas of components, morphological information, syntactic dependency relations between components, and meanings - take up the compartments of the UML class - which are the compartment of the name, attributes, operations,and responsibilities. In addition to existent UML concepts,the representation introduces new ones to denote syntactic dependency relations and their types, gender and some morphological information of object components.


Vers l'utilisation des méthodes formelles pour le développement de linguiciels
Formal methods have'nt been applied enough in the development process of lingware although their advantages have been proved in many other domains. In this framework, we have investigated some applications dealing with different processing levels (lexical analyses, morphology, syntax, semantic and pragmatic). These investigations has mainly led to the following observations. First of all, we have noticed a lack of use of methodologies that cover all the life cycle of a software development. The formal specification has not been used in the first development phases. In addition, we have noticed the lack of formal validation and consequently the insufficient guarantee of the developed software results. Moreover, there has been no appeal to rigorous methods of integration to solve the dichotomy of data and processing problem. However, the use of the formal aspect in the Natural Language Processing (NLP) has generally been limited to describing the natural language knowledge (i.e., grammars) and specifying the treatments using algorithmic languages. Few are those who have used a high level specification language.This paper focuses on the contributions of formal methods in developing natural language software starting from an experimentation carried out on a real application and which consists in specifying and validating the system CORTEXA (Correction ORthographique des TEXtes Arabes) using the VDM formal method.First of all, we review the advantages of formal methods in the general software development process. Then, we present the experimentation and the obtained results. After that, we place the formal methods advantages in the context of NLP. Finally, we give some methodological criteria that allow the choice of an appropriate formal method.

Examining Religion Bias in AI Text Generators
One of the biggest reasons artificial intelligence (AI) gets a backlash is because of inherent biases in AI software. Deep learning algorithms use data fed into the systems to find patterns to draw conclusions used to make application decisions. Patterns in data fed into machine learning algorithms have revealed that the AI software decisions have biases embedded within them. Algorithmic audits can certify that the software is making responsible decisions. These audits verify the standards centered around the various AI principles such as explainability, accountability, human-centered values, such as, fairness and transparency, to increase the trust in the algorithm and the software systems that implement AI algorithms.

Multilingual extraction and mapping of dictionary entry names in business schema integration
Being a research field for many years, natural language processing (NLP) has gained a lot of attention in recent times due to the quality of translation software like Google Translator or Babylon. In the context of the iGreen project, we are developing a schema integration service (working title: Warp 10), which helps to generate business transformations from individual business schemata. To match the different schemas natural language processing plays an important role. Information coming from user input or files is extracted and mapped to a canonical data model (CDM) based on the CCTS standard. In this paper, we illustrate the use of NLP for the extraction of dictionary entry names (DENs) and indicate some of the problems of NLP and term extraction. Furthermore, we describe the NLP-supported mapping of DENs and outline the problems and approaches in a multilingual setting.

Abstract:Compared with the conventional optimization approaches, Gauss Pseudospectral Method (GPM) can deal better with optimal problems with complex constraints and is less time consuming. As a numerical technique, GPM transforms the optimal control problem to a nonlinear programming problem (NLP) by using discrete approximation. This paper considers a cruise missile's mission which is to hit a fixed target while minimizing the exposure to anti-air defenses. The objective function can be set as integrated altitude along the trajectory. The Gpops tool kit is used to demonstrate GPM on this problem. The case study's results show that the method converges fast and precisely.


Abstract:Representing knowledge in the beginning of the 21st century has become a challenge due to the large volume of information to which we are submitted. Associate intelligent techniques have become a relevant method in this context, however studies are still lacking with the aim of making the process simpler for the user. Considering the current state of the art of NLP text labelers, this proposal is based on texting efficiently and without complexity through the use of free texts. We evaluated this proposal using concept maps to represent knowledge of entity relationships and compared the maps with the approach of the conceptual model. This proposal was able to satisfy the representation with a correct percentage rate of over 90%.


Oxygen hole paramagnetic centers in γ-irradiated DKDP single crystals
Abstract:The ESR (electron spin resonance) investigation of two types of an oxygen hole center in γ-irradiated DKDP (KD

VarSem: declarative expression and automated inference of variable usage semantics
Programmers declare variables to serve specific implementation purposes that we refer to as variable usage semantics (VUS). Understanding VUS is required for various software engineering tasks, including program comprehension, code audits, and vulnerability detection. To help programmers understand VUS, we present a new program analysis that infers a variable's usage semantics from its textual and context information (e.g., symbolic name, type, scope, information flow). To support this analysis, we introduce VarSem, a domain-specific language, in which a variable's semantic category is expressed as a set of declarative rules. VarSem's execution determines which program variables belong to a given semantic category. VarSem translates high-level declarative rules into low-level program analysis techniques, including natural language processing and data flow, and provides a highly extensible architecture for specifying new rules and analysis techniques. We evaluate VarSem with eight real-world systems to identify their personally identifiable information variables. The evaluation results show that VarSem infers variable semantics with satisfying accuracy/precision and passable recall, thus potentially benefiting both software and security engineers.


U-Compare: A modular NLP workflow construction and evaluation system


Passive and active contribution to multilingual lexical resources through online cultural activities

Suffix Based Automated Parts of Speech Tagging for Bangla Language
Abstract:Natural language processing (NLP) is the technique by which we process the human language with the computer. Parts-of-Speech (POS) tagging is one of the fundamental requirements for some NLP applications. It is considered as a solved problem for some foreign languages, such as English, Chinese, due to higher accuracy (97%), where it is still an unsolved problem for Bangla because of its ambiguity. Although making a POS tagger for Bangla is not a new work, but each one of available POS taggers has different kinds of limitations. We choose to develop an unsupervised system rather than a supervised system, because a supervised system needs a huge data resource for training purpose and available resources in Bangla is really poor. Here we develop a POS tagger mainly based on Bangla grammar especially suffixes. Because Bangla is a very inflectional language, where a single word has many variants based on their suffixes. In this POS tagger, we assign 8 base POS tags, where some rules, based on Bangla grammar and suffix, are applied to identify POS tags with the cooperation of verb root dataset. To handle non-suffix words, a dataset of almost 14500 Bangla words, with having their default POS tags, is added with the system, which helps to increase the efficiency of this POS tagger. A modified version of previously used algorithm for suffix analysis is applied, which result in a satisfactory level of about 94.2%.

Domain-Specific Pretraining for Vertical Search: Case Study on Biomedical Literature
Information overload is a prevalent challenge in many high-value domains. A prominent case in point is the explosion of the biomedical literature on COVID-19, which swelled to hundreds of thousands of papers in a matter of months. In general, biomedical literature expands by two papers every minute, totalling over a million new papers every year. Search in the biomedical realm, and many other vertical domains is challenging due to the scarcity of direct supervision from click logs. Self-supervised learning has emerged as a promising direction to overcome the annotation bottleneck. We propose a general approach for vertical search based on domain-specific pretraining and present a case study for the biomedical domain. Despite being substantially simpler and not using any relevance labels for training or development, our method performs comparably or better than the best systems in the official TREC-COVID evaluation, a COVID-related biomedical search competition. Using distributed computing in modern cloud infrastructure, our system can scale to tens of millions of articles on PubMed and has been deployed as Microsoft Biomedical Search, a new search experience for biomedical literature: https://aka.ms/biomedsearch.

Generating Hints for Programming Problems Without a Solution
We propose a novel way of implementing an Intelligent Tutoring System (ITS) to accompany coding problems without accessible solutions. Previous ITS are good at generating feedback for coding problems with known solutions, but there exists a wide variety of coding platforms that do not publish solutions for many of their problems. These platforms are hugely popular with CS students, so a hint generator that does not require a known solution would benefit a large population of students. We attempted this task using Natural Language Processing (NLP) techniques, specifically BERT fine-tuning, to predict solution components. We selected three features that were easy to programmatically detect in solutions - lists, stacks, and queues - and labeled each statement with three booleans denoting whether their solutions contained each feature. Next, we fed the statements through BERT and fit several binary classifiers to the 12th layer to predict which statements had which feature. We were unsuccessful at predicting these features from the problem statements beyond the negative class proportion, possibly due to class imbalance or insufficient BERT pre-training. Because this is a new problem for ITS, we also propose a general benchmark to measure the efficacy of solution-less hint generators in hopes of informing future research.

Abstract:One promising application of natural language processing (NLP) research is in the area of information extraction (IE). In this paper, we present work flow of our IE system for the extraction of semantically rich information from the unstructured or semi-structured Chinese web pages. Knowledge engineering approach and automatic training approach are used to extract pattern and built knowledge repository. General IE system needs to label the unlabeled training Web pages. A novel methodology that does not need to label text is developed, including hierarchy filtration pattern matching based on syntax in best distance method and maximum forward boundary recognition using organization suffix repository and part of speech tagging method. As for applications of IE, a new application system based on IE is built. It is object-level vertical search system and object here is Chinese people, so IE is concerned with extracting people's related attributes from a collection of web pages about Chinese people. The results are displayed as hierarchy directory tree according to people's attributes. The system makes user find people quickly and easily.


Abstract:This study explores a new domain representation method for natural language processing based on an application of possibility theory. In our method, domain-specific information is extracted from natural language documents using a mathematical process based on Rieger's notion of semantic distances, and represented in the form of possibility distributions. We implement the distributions in the context of a possibilistic domain classifier, which is trained using the SchoolNet corpus.


An improved Global Weight Function of Terms based on Pearson's Chi-square statistics
Abstract:Since term frequency, the most popular discriminator used in term weighting of Natural language processing (NLP), is not the only one which is necessary to be considered when calculating the term weight and make it suitable to indicate term importance, we are motivated to investigate other statistical characteristics of terms and found an important discriminator: term distribution. It is found in this this paper that a term close to hypo-dispersion distribution usually contains much contextual information and should be given higher weight than the one close to intensive distribution. Based on this hypothesis, a Pearson's Chi-square Theory based Term Global Weight Function is put forward in this paper. In addition, a text classifier system is developed based on LSA (Latent Semantic Analysis) model and its precision and recall results are used for evaluation, which approve the reliability and efficiency of the algorithm On conclusion, term distribution should be considered into term weighting as a new discriminator and the algorithms in this paper is recommended.


Relation Extraction in Clinical Text using NLP Based Regular Expressions

Real-Time Crisis Mapping of Natural Disasters Using Social Media
Abstract:The proposed social media crisis mapping platform for natural disasters uses locations from gazetteer, street map, and volunteered geographic information (VGI) sources for areas at risk of disaster and matches them to geoparsed real-time tweet data streams. The authors use statistical analysis to generate real-time crisis maps. Geoparsing results are benchmarked against existing published work and evaluated across multilingual datasets. Two case studies compare five-day tweet crisis maps to official post-event impact assessment from the US National Geospatial Agency (NGA), compiled from verified satellite and aerial imagery sources.

Abstract:Machine learning method in text classification has expanded from topic identification to more challenging tasks such as sentiment classification, and it is valuable to explore, compare methods applied in sentiment classification and investigate relevant influence factors. The chief aim of the present work is to compare four machine learning methods to sentiment classification of Chinese review. The corpus is made up of 16000 reviews from website. We investigate the factors which affect the performance: namely feature representation via Word-Based Unigram (WBU), Bigram (WBB) and Chinese Character-Based Bigram (CBB), Trigram (CBT); feature weighting schemes and feature dimensionality. Experimental evaluations show that performance depends on different settings. As a result, we draw a conclusion that Naive Bayes (NB) classifier obtains the best averaging performance when using WBB, CBT as features with bool weighting under different dimensionality to the task.


The Effectiveness of China National Doctoral Dissertations Quality Inspection
National1 Doctoral Dissertations Quality Inspection (NDDQI), implemented by the Ministry of Education of China, is an important but questioned postgraduate education policy. Effectiveness of NDDQI is highly dependent on examiner's reports, which had not been fully demonstrated. This study analyzed 17249 examiner's reports in 2015 NDDQI with help of NLP techniques, and found that 16% of examiners would give five types of problematic reports (i.e. vague report, less-strict report, rigorous report, confused report and dilettante report) which makes 10% of all examiner reports. The existence of problematic reports will largely shake the NDDQI result; the number of unqualified theses may be underestimated. Hence, for the purpose of enhancing the effectiveness of NDDQI, it is suggested that a distinguishing module based on Natural Language Processing technology should also be added on the inspection platform. Meanwhile, this study also showed the flexibility and power of NLP techniques, and as well showed the possibility of applying NLP for policy analysis and large-scale social science research.

A task-oriented dialogue bot using long short-term memory with attention for Thai language
A task-oriented dialogue bot helps users achieve a predefined goal within a closed domain. A neural-network based dialogue bot tracks the user intention in each action, which can reach promising performance compared to a hand-crafted baseline [1] and has a more flexible conversational flow. One such end-to-end architecture is the Hybrid Code Networks (HCNs) [2]. It uses the simulated conversation of human-bot in the domain of restaurant booking to train an LSTM to track dialogue states and predict the next bot response. This research proposes a similar architecture to HCNs with the addition of attention to LSTM [3]. The best results are obtained by our model on both original and Thai translated versions of bAbI task 5.

Investigating the Impact of Group Size on Non-Programming Exercises in CS Education Courses
Computer science (CS) courses are taught with increasing emphasis on group work and with non-programming exercises facilitating peer-based learning, computational thinking, and problem solving. However, relatively little work has been done to investigate the interaction of group work and non-programming exercises because collaborative, non-programming work is usually open-ended and requires analysis of unstructured, natural language responses. In this paper, we consider collaborative, non-programming work consisting of online wiki text from 236 groups in nine different CS1 and higher-level courses at a large Midwestern university. Our investigation uses analysis tools with natural language processing (NLP) and statistical analysis components. First, NLP uses IBM Watson Personality Insights to automatically convert students' collaborative wiki text into a Big Five model. This model is useful as a quality metric on group work since Big Five factors such as Openness and Conscientiousness are strongly related to both academic performance and learning. Then, statistical analysis generates regression models on group size and each Big Five trait that make up the factors. Our results show that increasing group size has a significant impact on collaborative, non-programming work in CS1 courses, but not for such work in higher-level courses. Furthermore, increasing group size can have either a positive or negative impact on the Big Five traits. These findings imply the feasibility of using such tools to automatically assess the quality of non-programming group exercises and offer evidence for effective group sizes.


A Deep Learning Based Framework for Textual Requirement Analysis and Model Generation

HOO 2012 error recognition and correction shared task: Cambridge University submission report
Previous work on automated error recognition and correction of texts written by learners of English as a Second Language has demonstrated experimentally that training classifiers on error-annotated ESL text generally outperforms training on native text alone and that adaptation of error correction models to the native language (L1) of the writer improves performance. Nevertheless, most extant models have poor precision, particularly when attempting error correction, and this limits their usefulness in practical applications requiring feedback. We experiment with various feature types, varying quantities of error-corrected data, and generic versus L1-specific adaptation to typical errors using Näive Bayes (NB) classifiers and develop one model which maximizes precision. We report and discuss the results for 8 models, 5 trained on the HOO data and 3 (partly) on the full error-coded Cambridge Learner Corpus, from which the HOO data is drawn.

Massive bio-ontology engineering for NLP
We describe an ontology engineering methodology by which conceptual knowledge is extracted from an informal medical thesaurus (UMLS) and automatically converted into a formally sound description logics system. Our approach consists of four steps: concept definitions are automatically generated from the UMLS source, integrity checking of taxonomic and partonomic hierarchies is performed by the terminological classifier, cycles and inconsistencies are eliminated, and incremental refinement of the evolving knowledge base is performed by a domain expert. We report on knowledge engineering experiments with a terminological knowledge base composed of 164,000 concepts and 76,000 relations.

Depression Detection by Analyzing Social Media Posts of User
Abstract:Depression is a serious mental health issue for people world-wide irrelevant of their ages, genders and races. In this age of modern communication and technology, people feel more comfortable sharing their thoughts in social networking sites (SNS) almost every day. The objective of this paper is to propose a data-analytic based model to detect depression of any human being. In this proposed model data is collected from the users' posts of two popular social media websites: twitter and facebook. Depression level of a user has been detected based on his posts in social media. The standard method of detecting depression of a person is a fully structured or a semi-structured interview method (SDI) [1]. These methods need a huge amount of data from the person. Micro-blogging sites such as twitter and facebook have become so much popular places to express peoples' activity and thoughts. The data screening from tweets and posts show the manifestation of depressive disorder symptoms of the user. In this research, machine learning is used to process the scrapped data collected from SNS users. Natural Language Processing (NLP), classified using Support Vector Machine (SVM) and Naïve Bayes algorithm to detect depression potentially in a more convenient and efficient way.

Questions to be asked & answered as to NLP's role in improving semantic annotation
In the realm of Information Retrieval, why is Semantic Annotation needed? What has changed? Is it the users, the sources, the genres, the technologies, the applications, the queries? If there are differences, why and how can Semantic Annotation help? And more specifically, how and what is Natural Language Processing (NLP) contributing. This talk will share some practical use cases of where and how NLP-based semantic annotation has demonstrated its utility (or a solid promise of utility) in some information access tasks, by virtue of the depth and richness of annotation possible with today's more powerful NLP technologies. What is showing promise is the ability to understand how to utilize the higher levels of language processing to do Semantic Annotation. This is largely through the introduction of the Pragmatic level of language processing -- the functional perspective which provides the extra understanding that comes from the study of language in actual use. Pragmatics is concerned with the aspects of language which require context to be understood. Basically, how situational context is lexicalized and grammaticalized. In Pragmatics, the goal is to recognize the extra meaning that humans read into utterances, which other levels of language processing have not recognized as being encoded in them. Semantic Annotation can then go the next step and amplify current annotations with this additional contextual & intentional knowledge. Examples will be shown of what is being done with NLP today that couldn't be done, or simply wasn't being done in earlier days of IR. In applications of keenest interest today, there is an increased relative emphasis focus on dialogue, interaction, real-time, social, and exploratory search, where understanding the user's intent or plan in their query is key. Applications in exploratory search, eDiscovery, sentiment recognition, collaborative search, along with very, very large scale medical insurance consumer applications will be considered in terms of how and what Semantic Annotations can improve by adding more advanced levels of Natural Language Processing

How well do semantic relatedness measures perform?: a meta-study
Various semantic relatedness, similarity, and distance measures have been proposed in the past decade and many NLP-applications strongly rely on these semantic measures. Researchers compete for better algorithms and normally only few percentage points seem to suffice in order to prove a new measure outperforms an older one. In this paper we present a meta-study comparing various semantic measures and their correlation with human judgments. We show that the results are rather inconsistent and ask for detailed analyses as well as clarification. We argue that the definition of a shared task might bring us considerably closer to understanding the concept of semantic relatedness.

EBL: an approach to automatic lexical acquisition
A method for automatic lexical acquisition is outlined. An existing lexicon that, in addition to ordinary lexical entries, contains prototypical entries for various non-exclusive paradigms of open-class words, is extended by inferring new lexical entries from texts containing unknown words. This is done by comparing the constraints placed on the unknown words by the natural language system's grammar with the prototypes and a number of hand-coded phrase templates specific for each paradigm. Once a sufficient number of observations of the word in different contexts have been made, a lexical entry is constructed for the word by assigning it to one or several paradigm(s).Parsing sentences with unknown words is normally very time-consuming due to the large number of grammatically possible analyses. To circumvent this problem, other phrase templates are extracted automatically from the grammar and domain-specific texts using an explanation-based learning method. These templates represent grammatically correct sentence patterns. When a sentence matches a template, the original parsing component can be bypassed, reducing parsing times dramatically.

A Deep Convolutional Network Demodulator for Mixed Signals with Different Modulation Types
Abstract:In recent years, deep learning is becoming more and more popular. It has been widely applied to fields including image recognition, automatic speech recognition and natural language processing(NLP). In the field of communication, signals are considered to be temporal data, which can be learned with deep learning to recognize its patterns inside. In this paper, a Deep Convolutional Network Demodulator (DCND) is proposed. This model attempts to respectively demodulate symbol sequences from mixing signals. The data composes of signals modulated with different signal modulation techniques and the same carrier frequency for the purpose of our project. In this condition, the proposed model can give contribution to reduce the bit error ratio(BER), demodulate signals successfully which cannot be recognized by correlation demodulation for additive white Gaussian noise(AWGN), and resistance to frequency interference.

Abstract:Striving for reliability of software systems often results in immense numbers of tests. Due to the lack of a generally used annotation, finding the parts of code these tests were meant to assess can be a demanding task. This is a valid problem of software engineering called test-to-code traceability. Recent research on the subject has attempted to cope with this problem applying various approaches and their combinations, achieving profound results. These approaches have involved the use of naming conventions during development processes and also have utilized various information retrieval (IR) methods often referred to as conceptual information. In this work we investigate the benefits of textual information located in software code and its value for aiding traceability. We evaluated the capabilities of the natural language processing technique called Latent Semantic Indexing (LSI) in the view of the results of the naming conventions technique on five real, medium sized software systems. Although LSI is already used for this purpose, we extend the viewpoint of one-to-one traceability approach to the more versatile view of LSI as a recommendation system. We found that considering the top 5 elements in the ranked list increases the results by 30% on average and makes LSI a viable alternative in projects where naming conventions are not followed systematically.


Abstract:With the popularity of social media platforms, patients tend to share their experiences and opinions on them, and patient feedback is key to improving health services. Sentiment analysis techniques have been applied to automatically analyze the patients’ opinions to understand the quality of healthcare. Aspect extraction, which aims to identify the opinion targets in the text, is an important step towards understanding the patient’s opinion towards particular target or entity. However, due to the complex nature of medical domain data, existing approaches take much execution time. To address this, we presents a new approach for aspect extraction and refinement to smooth the sentiment analysis process. Furthermore, this work also introduces an intelligent weighting scheme for classifying the final aspects. For the experimental evaluation, a dataset from Yelp and RateMDs has been utilized. Experimental results show that the proposed model outperforms existing methods.


Abstract:The people information is distributed in various forms such as database, web page, text, and so on, where the world wide web is one of the main sources of publicly-available people information. It has a characteristic that the information on people is intrinsically temporal. Therefore, the reconstruction of the information is needed for an individual or a company to use it efficiently. In order to maintain or manage the temporal people information, it must distinguish the variable information from invariable information of people. However, there have been few examples that construct an ontology from this point of view. In this paper, we propose a method that constructs an ontology based on events to manage the variable people information efficiently. In addition, we present a system which reconstructs people information that satisfies the users' demand with the ontology.


Abstract:The WissKI system provides a framework for ontology-based science communication and cultural heritage documentation. In many cases, the documentation consists of semi-structured data records with free text fields. Most references in the texts comprise of person and place names, as well as time specifications. We present the WissKI tools for semantic annotation using controlled vocabularies and formal ontologies derived from CIDOC Conceptual Reference Model (CRM). Current research deals with the annotations as building blocks for event recognition. Finally, we outline how the CRM helps to build bridges between documentation in different scientific disciplines.


Abstract:The research presented is the first working step towards the goal of developing a domain-independent method for sentiment analysis of German customer feedback in social media. The approach proposes to apply the concept of natural language processing (NLP) to customer language processing (CLP). In this context we hypothesize an indifference in annotator ability in assigning customer reviews of tangible vs. intangible goods and an indifferences within customers' writing styles within their evaluation of these goods. To test these hypotheses, a study was conducted where participants had to assign the sentiment as well as the subject of customer reviews and its evaluative attribute. The results reveal that the inter-rater reliability of annotators does not differ significantly with respect to product groups. However a slight difference with respect to product categories could be observed. Moreover, there occur variations within the inter-rater ability according to the emotional commitment towards products.


Orientation Identification for Chinese Short Text
Abstract:With the rapid development of information technology, huge data are accumulated. A vast number of such data appears as short text. It is very useful to orientation identification for short text. But traditional text filtering technology based on statistics usually is ineffective when it deals with orientation, especially for Chinese short text. This paper proposes a novel method for Chinese short text orientation identification which simulates human's cognition. The approach makes full use of field knowledge, combines tendency dictionary and semantic rules, and takes into account the sentiment orientation of words which constructed by Naive Bayesian model. Experiments show that the proposed method works well in terms of orientation identification for Chinese short text.

The Evaluation of Thai Poem's Content Consistency using Siamese Network
Many research describes Textual Entailment model for compare pair of the sentence but two sentences in term of the poem content consistency are not the same. The content consistency is very important for storytelling in Thai poem composing. In this article, we propose the model and result of The evaluation of Thai poem's content consistency using The Siamese Network 3 models comprise 1) Merge Vector Model 2) Siamese Absolute Different Model and 3) Siamese Dot Vector Model compare with the Basic CNN model. The training data is Thai poem 14,173 pair (batt) and validation data is Thai poem 3,544 pair. All models learn by apply one shot learning technic. The accuracy of Siamese Absolute Different Model near 100%. The macro average of F1-score shows 99.27%. The Area Under Curve shows 0.997 near the perfect value.

Abstract:Personality test and analysis is an important factor in an individual's overall development. Most known personality test followed by people is Myer Briggs Test Indicator. While these tests are conducted by psychologists, it is very easy to deceive them in order to get the personality type of our choice, as these questions are very straight forward. This paper focuses on automating this task with the help of Neural Networks by using images instead of questions. A labelled dataset with user responses on social media along with their personality type is used for analysis. After cleaning, relevant response features are extracted using NLP followed by applying suitable classification algorithms. A model is trained to accurately predict the personality type of users based on their responses, which is then deployed on an interactive website in the form of a personality test.


Texture image retrieval based on gray-primitive co-occurrence matrix
Abstract:The research of texture similarity is very important component of content-based image retrieval system. Firstly the rotation invariance of gray-primitive co-occurrence matrix was proved in this paper, then a new texture image retrieval technique based on gray-primitive co-occurrence matrix was presented. The result of experiment indicates that the algorithm proposed has low computational complexity and certain noise resisting ability.


Forging high-quality User Stories: Towards a discipline for Agile Requirements

Abstract:Machine learning is commonly being used in almost all the areas that involve advanced data analytics and intelligent control. From applications like Natural Language Processing (NLP) to autonomous driving are based upon machine learning algorithms. An increasing trend is observed in the use of Deep Neural Networks (DNNs) for such applications. While the slight inaccuracy in applications like NLP does not have any severe consequences, it is not the same for other safety-critical applications, like autonomous driving and smart healthcare, where a small error can lead to catastrophic effects. Apart from high-accuracy DNN algorithms, there is a significant need for robust machine learning systems and hardware architectures that can generate reliable and trustworthy results in the presence of hardware-level faults while also preserving security and privacy. This paper provides an overview of the challenges being faced in ensuring reliable and secure execution of DNNs. To address the challenges, we present several techniques for analyzing and mitigating the reliability and security threats in machine learning systems.


Application of Chinese sentiment categorization to digital products reviews
Abstract:Sentiment categorization have been widely explored in many fields, such as government policy, information monitoring, product tracking, etc. This paper adopts k-NN, Naive Bayes and SVM classifiers to categorize sentiments contained in on-line Chinese reviews on digital products. Our experimental results show that combining the words and phrases with sentiment orientation as hybrid features, SWM classifier achieves an accuracy of 96,47%, which is words of all parts of speech as features.

Topic Enhanced Word Embedding for Toxic Content Detection in Q&A Sites
Abstract:Increasingly, users are adopting community question-and-answer (Q&A) sites to exchange information. Detecting and eliminating toxic and divisive content in these Q&A sites are paramount tasks to ensure a safe and constructive environment for the users. Insincere question, which is founded upon false premises, is one type of toxic content in Q&A sites. In this paper, we proposed a novel deep learning framework enhanced pre-trained word embeddings with topical information for insincere question classification. We evaluated our proposed framework on a large real-world dataset from Quora Q&A site and showed that the topically enhanced word embedding is able to achieve better results in toxic content classification. An empirical study was also conducted to analyze the topics of the insincere questions on Quora, and we found that topics on “religion”, “gender” and `'politics'' has a highe

Compression of Deep Learning Models for Text: A Survey
In recent years, the fields of natural language processing (NLP) and information retrieval (IR) have made tremendous progress thanks to deep learning models like Recurrent Neural Networks (RNNs), Gated Recurrent Units (GRUs) and Long Short-Term Memory (LSTMs) networks, and Transformer [121] based models like Bidirectional Encoder Representations from Transformers (BERT) [24], Generative Pre-training Transformer (GPT-2) [95], Multi-task Deep Neural Network (MT-DNN) [74], Extra-Long Network (XLNet) [135], Text-to-text transfer transformer (T5) [96], T-NLG [99], and GShard [64]. But these models are humongous in size. On the other hand, real-world applications demand small model size, low response times, and low computational power wattage. In this survey, we discuss six different types of methods (Pruning, Quantization, Knowledge Distillation (KD), Parameter Sharing, Tensor Decomposition, and Sub-quadratic Transformer-based methods) for compression of such models to enable their deployment in real industry NLP projects. Given the critical need of building applications with efficient and small models, and the large amount of recently published work in this area, we believe that this survey organizes the plethora of work done by the “deep learning for NLP” community in the past few years and presents it as a coherent story.

Combining CBIR and NLP for multilingual terminology alignment and cross-language image indexing
In this paper, an overview of an approach for cross-language image indexing and multilingual terminology alignment is presented. Content-Based Image Retrieval (CBIR) is proposed as a means to find similar images in target language documents in the web and natural language processing is used to reduce the search space and find the image index. As the experiments are carried out in specialized domains, a systematic and recursive use of the approach is used to align multilingual terminology by creating repositories of images with their respective cross-language indices.


Computationally Efficient Learning of Quality Controlled Word Embeddings for Natural Language Processing

Abstract:The purpose of this research was to The Develop a System for Detecting and Interpreting Warning Signs, The researcher has proposed to develop a system for detecting and interpreting warning signs by using the theory of color from the sign of the traffic sign. This can be used in analyze and detect traffic signs through the design of applications on smartphones in order to be able to create the traffic sign support system by detecting and interpreting traffic signs. In this case, the driver can clearly and correctly understand the meaning of traffic signs. As a result, the driver can follow the traffic rules. Moreover, it helps reducing accidents on the road as well. The efficiency of the system was tested with test result, it is found that the program was able to detect traffic signs and interpret the objects within the image as traffic signs accurately. Additionally, it is found that the light and distance of the camera affected the quality of data processing in this test to measure performance. In addition, the accuracy of the system is totally 93.5.


Extracting information on pneumonia in infants using natural language processing of radiology reports
Natural language processing (NLP) is critical for improvement of the healthcare process because it has the potential to encode the vast amount of clinical data in textual patient reports. Many clinical applications require coded data to function appropriately, such as decision support and quality assurance applications. However, in order to be applicable in the clinical domain, performance of the NLP systems must be adequate. A valuable clinical application is the detection of infectious diseases, such as surveillance of healthcare-associated pneumonia in newborns (e.g. neonates) because it produces significant rates of morbidity and mortality, and manual surveillance of respiratory infection in these patients is a challenge. Studies have already demonstrated that automated surveillance using NLP tools is a useful adjunct to manual clinical management, and is an effective tool for infection control practitioners. This paper presents a study aimed at evaluating the feasibility of an NLP-based electronic clinical monitoring system to identify healthcare-associated pneumonia in neonates. We estimated sensitivity, specificity, and positive predictive value by comparing the detection with clinicians' judgments and our results demonstrated that the automated method was indeed feasible. Sensitivity (recall) was 87.5%, and specificity (true negative rates) was 94.1%.

Web-based models for natural language processing
Previous work demonstrated that Web counts can be used to approximate bigram counts, suggesting that Web-based frequencies should be useful for a wide variety of Natural Language Processing (NLP) tasks. However, only a limited number of tasks have so far been tested using Web-scale data sets. The present article overcomes this limitation by systematically investigating the performance of Web-based models for several NLP tasks, covering both syntax and semantics, both generation and analysis, and a wider range of n-grams and parts of speech than have been previously explored. For the majority of our tasks, we find that simple, unsupervised models perform better when n-gram counts are obtained from the Web rather than from a large corpus. In some cases, performance can be improved further by using backoff or interpolation techniques that combine Web counts and corpus counts. However, unsupervised Web-based models generally fail to outperform supervised state-of-the-art models trained on smaller corpora. We argue that Web-based models should therefore be used as a baseline for, rather than an alternative to, standard supervised models.

Abstract:This research is a discriminative analysis of conversational dialogues involving individuals suffering from dementia of Alzheimer's type. Several metric analyses are applied to the transcripts of the Carolina Conversation Corpus in order to determine if there are significant statistical differences between individuals with and without Alzheimer's disease. Our prior research suggests that there exist measurable linguistic differences between managed-care residents diagnosed with Alzheimer's disease and their caregivers. This paper presents results comparing managed-care residents diagnosed with Alzheimer's disease to other managed-care residents. Results from the analysis indicate that part-of-speech and lexical richness statistics may not be good distinguishing attributes. However, go-ahead utterances and certain fluency measures provide defensible means of differentiating the linguistic characteristics of spontaneous speech between individuals that are and are not diagnosed with Alzheimer's disease. Two machine learning algorithms were able to classify the speech of individuals with and without dementia of the Alzheimer's type with accuracy up to 80%.


A language-independent anaphora resolution system for understanding multilingual texts
This paper describes a new discourse module within our multilingual NLP system. Because of its unique data-driven architecture, the discourse module is language-independent. Moreover, the use of hierarchically organized multiple knowledge sources makes the module robust and trainable using discourse-tagged corpora. Separating discourse phenomena from knowledge sources makes the discourse module easily extensible to additional phenomena.

Combining open-source with research to re-engineer a hands-on introductory NLP course
We describe our first attempts to re-engineer the curriculum of our introductory NLP course by using two important building blocks: (1) Access to an easy-to-learn programming language and framework to build hands-on programming assignments with real-world data and corpora and, (2) Incorporation of interesting ideas from recent NLP research publications into assignment and examination problems. We believe that these are extremely important components of a curriculum aimed at a diverse audience consisting primarily of first-year graduate students from both linguistics and computer science. Based on overwhelmingly positive student feedback, we find that our attempts were hugely successful.

WikiCheck: An End-to-end Open Source Automatic Fact-Checking API based on Wikipedia
With the growth of fake news and disinformation, the NLP community has been working to assist humans in fact-checking. However, most academic research has focused on model accuracy without paying attention to resource efficiency, which is crucial in real-life scenarios. In this work, we review the State-of-the-Art datasets and solutions for Automatic Fact-checking and test their applicability in production environments. We discover overfitting issues in those models, and we propose a data filtering method that improves the model's performance and generalization. Then, we design an unsupervised fine-tuning of the Masked Language models to improve its accuracy working with Wikipedia. We also propose a novel query enhancing method to improve evidence discovery using the Wikipedia Search API. Finally, we present a new fact-checking system, the WikiCheck API that automatically performs a facts validation process based on the Wikipedia knowledge base. It is comparable to SOTA solutions in terms of accuracy and can be used on low-memory CPU instances.

Proposed Model for Natural Language ABAC Authoring
Authorization policy authoring has required tools from the start. With access policy governance now an executive-level responsibility, it is imperative that such a tool expose the policy to business users' with little or no IT intervention-as natural language. NIST SP 800-162 [1] first prescribes natural language policies (NLPs) as the preferred expression of policy and then implicitly calls for automated translation of NLP to machine-executable code. This paper therefore proposes an interoperable model for the NLP's human expression. It furthermore documents the research and development of a tool set for end-to-end authoring and translation. This R&D journey-focusing constantly on end users' has debunked certain myths, has responded to steadily increasing market sophistication, has applied formal disciplines (e.g. ontologies, grammars and compiler design) and has motivated an informal demonstration of autonomic code generation. The lessons learned should be of practical value to the entire ABAC community. The research in progress' increasingly complex policies, proactive rule analytics, and expanded NLP authoring language support will require collaboration with an ever-expanding technical community from industry and academia.

Knowledge sources for constituent parsing of german, a morphologically rich and less-configurational language
We study constituent parsing of German, a morphologically rich and less-configurational language. We use a probabilistic context-free grammar treebank grammar that has been adapted to the morphologically rich properties of German by markovization and special features added to its productions. We evaluate the impact of adding lexical knowledge. Then we examine both monolingual and bilingual approaches to parse reranking. Our reranking parser is the new state of the art in constituency parsing of the TIGER Treebank. We perform an analysis, concluding with lessons learned, which apply to parsing other morphologically rich and less-configurational languages.

Fast large-scale approximate graph construction for NLP
Many natural language processing problems involve constructing large nearest-neighbor graphs. We propose a system called FLAG to construct such graphs approximately from large data sets. To handle the large amount of data, our algorithm maintains approximate counts based on sketching algorithms. To find the approximate nearest neighbors, our algorithm pairs a new distributed online-PMI algorithm with novel fast approximate nearest neighbor search algorithms (variants of Pleb). These algorithms return the approximate nearest neighbors quickly. We show our system's efficiency in both intrinsic and extrinsic experiments. We further evaluate our fast search algorithms both quantitatively and qualitatively on two NLP applications.


Automatic detection of subject/object drops in Bengali

TransRegex: Multi-modal Regular Expression Synthesis by Generate-and-Repair
Since regular expressions (abbrev. regexes) are difficult to understand and compose, automatically generating regexes has been an important research problem. This paper introduces TRANSREGEX, for automatically constructing regexes from both natural language descriptions and examples. To the best of our knowledge, TransRegex is the first to treat the NLP-and-example-based regex synthesis problem as the problem of NLP-based synthesis with regex repair. For this purpose, we present novel algorithms for both NLP-based synthesis and regex repair. We evaluate TransRegex with ten relevant state-of-the-art tools on three publicly available datasets. The evaluation results demonstrate that the accuracy of our TransRegex is 17.4%, 35.8% and 38.9% higher than that of NLP-based approaches on the three datasets, respectively. Furthermore, TransRegex can achieve higher accuracy than the state-of-the-art multi-modal techniques with 10% to 30% higher accuracy on all three datasets. The evaluation results also indicate TransRegex utilizing natural language and examples in a more effective way.


Fuzzy modeling to conceptual design of mechanisms based on axiomatic design

Do HCI and NLP interact?
We examine the relationship between HCI and Natural Language Processing (NLP) by performing a bibliometric analysis and looking at the specific example of BioNLP. We identify opportunities for HCI to fertilise current NLP research and suggest that HCI will benefit from looking at advances in NLP more closely.

A Deep Learning Model for Source Code Generation
Abstract:Natural Language Processing (NLP) models have been used extensively to study relationship among words in a corpus. Inspired by models such as n-gram we developed a model for analyzing source code via its Abstract Syntax Tree (AST). This study has applications in source code generation, code completion, and software forensics and investigation. The study also benefits software developers and programmers striving to improve code efficiency and speed up development process. With source code analysis, the Natural Language Tool Kit (NLTK) which is very useful in NLP, becomes severely limited by its inability to handle the semantic and syntactic properties of source codes. Therefore, we processed the source code datasets as Abstract Syntax Trees (ASTs) rather than the source code text itself to take advantage of the information-rich structure of the AST. The proposed model is built on the deep learning-based Long Short-Term Memory (LSTM) and Multiple Layer Perceptron (MLP) architectures. Results from our intrinsic evaluation on a corpus of python projects have demonstrated its ability of effectively predicting a sequence of source code tokens and show an improvement over previous work in this field.

Text classification on software requirements specifications using transformer models
Text classification in Software Requirements Specifications (SRS) documents is an essential task for various purposes including automatically extracting requirements and their types as well as identification of duplicate or conflicting information, which all contribute to avoiding potential issues in the later stages of the software development life cycle. While a variety of machine learning approaches have been considered for text classification over SRS documents, many of these fail to provide adequate performance as they often ignore the meaning of software artifacts or integrate domain knowledge for the classification task. Recent advances in deep learning methodology have significantly contributed to Natural Language Processing (NLP) and text classification. One of the main challenges in using deep learning models for various NLP tasks in the software engineering domain is the scarcity of labeled textual data. In addition, even with sufficient data, training from the scratch still requires significant training time and computational resources. Transfer learning is a novel approach that proposes a solution to such reservations by providing pre-trained models that enable fine-tuning with the customized data. In this research, we conduct an empirical analysis on multi-class text classification over SRS documents using different pre-trained transformer models including BERT, DistilBERT, Roberta, AlBERT, and XLNet, and compare their performance. We test the performance of these models using three SRS datasets: DOORS, NFR-PROMISE, and PURE. Our numerical study shows that the transformer models are able to generate highly accurate results to classify all categories except Priority of the requirements. While all models provide a 80% or higher accuracy for other classification tasks, the accuracy of the models to classify the Priority does not exceed 60%.

2
N]), 1-ethyl-3-methylimidazolium trifluoroacetate ([EMIM][CF

Abstract:In the current scenario, the end user of Information Technology Service Management (ITSM) application in software companies has to keep on searching the solution for problems he is facing or else finally generate a ticket since he cannot collaborate with the system by asking questions and getting relevant answers. As a solution to this, we aim to design a chatbot that will be specifically tailored for software firm employees. The chatbot can process input using Natural Language Processing (NLP) and can generate a relevant response which will help the end user to solve his query. The chatbot makes decisions itself to answer user's query with the help of the IBM Watson Conversation APIs. It will also remember the context of the conversation and perform tasks such as creation of ticket on behalf of the user.



Using genetic algorithms for the construction of a space mission automaton

Abstract:This paper describes steps that have been taken to construct a development dataset for the task of Technology Structure Mining. We have defined the proposed task as the process of mapping a scientific corpus into a labeled digraph named a Technology Structure Graph as described in the paper. The generated graph expresses the domain semantics in terms of interdependencies between pairs of technologies that are named (introduced) in the target scientific corpus. The dataset comprises a set of sentences extracted from the ACL Anthology Corpus. Each sentence is annotated with at least two technologies in the domain of Human Language Technology and the interdependence between them. The annotations - technology mark-up and their interdependencies - are expressed at two layers: lexical and termino-conceptual. Lexical representation of technologies comprises varying lexicalizations of a technology. However, at the termino-conceptual layer all these lexical variations refer to the same concept. We have adopted the same approach for representing Semantic Relations, at the lexical layer a semantic relation is a predicate i.e. defined based on the sentence surface structure, however at the termino-conceptual layer semantic relations are classified into conceptual relations either taxonomic or non-taxonomic. Morover, the contexts that interdependencies are extracted from are classified into five groups based on the linguistic criteria and syntactic structure that are identified by the human annotators. The dataset initially comprises of 482 sentences. We hope this effort results in a benchmark that can be used for the technology structure mining task as defined in the paper.


Abstract:In any industry, Customer Relationship Management (CRM) is a very important aspect of the business. In a complex business environment, providing an efficient customer support service is always a challenge. Customer reports the issues/defects in the system to the vendor by sending emails or by creating a ticket in CRM tools like Salesforce.com. The content of such reports includes detailed technical problems or complex workflow issues due to system failures. In the industrial automation systems, a commissioning engineer or a field operating engineer generally reports such issues. Understanding and responding to the customer issues/defects and providing quick customer support is not an easy task. These CRM tools are not sufficiently astute to classify the defects into predefined classes. Text classification techniques are used to automatically identify and categorize the defects from the text messages. In this paper, five different machine learning classifiers (i.e. SVM, MNB, Decision tree, Random forest and K-nearest neighbors) are applied to perform multiclass text classification. The text messages are classified into predefined twelve technical system defects. The comparative analysis of five different classifiers on Customer Support dataset shows that the Support Vector Machine (SVM) has a better accuracy score in identifying the defects.


Semantic matching of GUI events for test reuse: are we there yet?
GUI testing is an important but expensive activity. Recently, research on test reuse approaches for Android applications produced interesting results. Test reuse approaches automatically migrate human-designed GUI tests from a source app to a target app that shares similar functionalities. They achieve this by exploiting semantic similarity among textual information of GUI widgets. Semantic matching of GUI events plays a crucial role in these approaches. In this paper, we present the first empirical study on semantic matching of GUI events. Our study involves 253 configurations of the semantic matching, 337 unique queries, and 8,099 distinct GUI events. We report several key findings that indicate how to improve semantic matching of test reuse approaches, propose SemFinder a novel semantic matching algorithm that outperforms existing solutions, and identify several interesting research directions.

Abstract:This paper presents a natural language-based approach for requirements specification of OLAP systems. In this approach, requirements are specified through a template representing the concepts of the decision making process. In addition, it provides for the acquisition of analytical queries written according to a linguistic pattern. This latter facilitates the automatic analysis of the queries without being too restrictive to the writing styles. In addition, it allows the coherence verification and validation of the requirements with respect to a given source. Furthermore, it allows the automatic generation of conceptual schemas for the target system, that is mapped to the data source used in the loading step.


Abstract:Summary form only given. The distribution network planning under active network management (ANM) schemes is becoming of interest due to substantial benefits in facilitating the increasing integration of renewable energy sources. This paper presents various potential ANM schemes based on the photovoltaic inverter control (PVIC) considering enhanced utilization of the inverter reactive power capability. Depending on the active power generation of PV arrays, inverter size and desired reactive power settings, several PVIC schemes are proposed. The PVIC schemes are incorporated in the optimal power flow (OPF) and formulated as a nonlinear programming (NLP) problem. In this study, the PVIC schemes are applied to maximize the total wind-distributed generation (DG) penetration on a typical U.K. distribution system. Various case studies are presented and compared to evaluate the performance. The results show that the proposed schemes can significantly increase the wind penetration levels by 45.4% and up to 92.3%.


Multilevel Classification of Pakistani News using Machine Learning
Abstract:The availability of innumerable sources of online news has benefitted the masses as they have opportunity to gather news from a diverse set of sources. However, classification of this huge data being generated on regular basis has never been a simple task. This textual information can be invaluable only when it is processed to maximize its usefulness which is possible with automated text classification. Natural Language Processing (NLP) and Machine learning techniques have been extensively applied in this particular domain to address this challenge. Text classification is helpful in several scenarios such as product mining, emotions or sentiment analysis, etc. News classification is one of its applications through which content of news is processed and analyzed to assign predefined label(s). This research is focused on classification of Pakistani news obtained from dataset available on Open Data Pakistan. We have applied various machine learning algorithms including Logistic Regression, Random Forest, Support Vector Machine, and Naïve Bayes for first-level classification and Logistic Regression for multilevel classification. Comparative analysis of these algorithms is also presented. We achieved a maximum of 97.8% accuracy through Support Vector Machine in single-level classification and 83% through Logistic Regression in multilevel text classification.


ClassRoute: An English to Punjabi Educational Video Translation Pipeline for Supporting Punjabi Mother-Tongue Education

Abstract:This work demonstrates an experimental implementation of a helper bot using IBM Watson. It is primarily aimed at people who know English as a second language. With the help of IBM Watson Assistant tool, the chatbot uses APIs like Google Translate API, Text to Speech API, SimpleWIki and Musixmatch API, to provide features like rich responses, translation to regional languages, text to speech conversion facilities, useful information in simpler English, and displaying music lyrics for music in regional languages. This is particularly helpful for those who are newly learning English and are more comfortable in their regional language.


Abstract:Visual question answering (VQA), visual dialogs, visual chat bot are multi-discipline exploration problems, which is a blend of Natural Language Processing (NLP), Image feature extraction and Knowledge Reasoning (KR). Rather than captioning, which is naïve approach of computer vision, VQA problems enhances the perspective by providing interactivity to ask domain specific as well as open ended questions to images and give us the insights based on image features or characteristics. Our research is the evaluate the performance of VQA on counting problems. Given an image, VQA model is expected to answer "how many" question type. We have used few pre-trained models for VQA and visual dialog and tabulated the findings of accuracy of predicted answer with the pre-defined ground truth.


Another look at nominal compounds
We present a progress report on our research on nominal compounds (NC's). Recent approaches to this problem in linguistics and natural language processing (NLP) are reviewed and criticized. We argue that the notion of "role nominal", which is at the interface of linguistic and extralinguistic knowledge, is crucial for characterizing NC's as well as other linguistic phenomena. We examine a number of constraints on the semantic interpretation rules for NC's. Proposals are made that should improve the capability of NLP systems to deal with NC's.

Abstract:In this article, we propose a novel architecture called hierarchical-task reservoir (HTR) suitable for real-time applications for which different levels of abstraction are available. We apply it to semantic role labeling (SRL) based on continuous speech recognition. Taking inspiration from the brain, this demonstrates the hierarchies of representations from perceptive to integrative areas, and we consider a hierarchy of four subtasks with increasing levels of abstraction (phone, word, part-of-speech (POS), and semantic role tags). These tasks are progressively learned by the layers of the HTR architecture. Interestingly, quantitative and qualitative results show that the hierarchical-task approach provides an advantage to improve the prediction. In particular, the qualitative results show that a shallow or a hierarchical reservoir, considered as baselines, does not produce estimations as good as the HTR model would. Moreover, we show that it is possible to further improve the accuracy of the model by designing skip connections and by considering word embedding (WE) in the internal representations. Overall, the HTR outperformed the other state-of-the-art reservoir-based approaches and it resulted in extremely efficient with respect to typical recurrent neural networks (RNNs) in deep learning (DL) [e.g., long short term memory (LSTMs)]. The HTR architecture is proposed as a step toward the modeling of online and hierarchical processes at work in the brain during language comprehension.


Abstract:The main goal of this research is to offer a new perspective of Dante Alighieri and his most famous work: The Divine Comedy based on natural language processing. We seek to provide rigorous evidence to enhance literary analysis on The Divine Comedy. We utilized sentiment analysis, text classification and topic modeling. We analyzed both the original Divine Comedy written in Italian, and the Divine Comedy translated to English. We also used natural language processing to compare the Divine Comedy to a variety of Shakespeare's plays and poems.


Question matching based on Fuzzy set
Abstract:Sentence similarity computing plays an important role in the Question Answering (QA) System. Because there are many question expressions for one meaning, we present a new approach to match question based on Fuzzy set. In this paper, we establish a library of standard questions. Each standard question is relative with a series of Keywords. The main focus of this paper lies with matching of standard questions and questions asked by users. An experimental system based on the proposed method has been built, and the results of our experiments shows the proposed method is effective for question matching.

Information credibility analysis of web content
General users write daily news about themselves and post information they consider interesting as digital documents for blogs and SNS. Such digital content includes both valuable information as well as worthless, false, and demagogic information. Ordinary web search engines can display web pages in a particular order. The ranking method evaluates the score of web content and generates a ranked list. The top-ranked web content on search engines is often relevant to the user's query, though, in some cases, the content may not be credible or valuable. Nevertheless, readers often trust the authenticity of the displayed information. Even if users believe that the content is useful, the search engine cannot evaluate the retrieved digital content, and users have to retrieve a variety of content using different keywords. The need for an information analysis technology that helps find credible and valuable information from large amounts of Web content is progressively growing. In Japan, the NICT (National Institute of Information and Communications Technology) initiated the ''Information Credibility Criteria Project'' in 2oo6, and the MIC (Ministry of Internal Affairs and Communications), too, initiated the ''Research and Development of Information Credibility Verification Technology for Telecommunication Service'' in 2007. The NICT's project addresses the issue of information credibility by analyzing credibility based on the following criteria: (1) content, (2) sender, (3) appearance, and (4) authenticity of content. We believe that the understanding of texts by a machine is important and that an NLP (Natural Language Processing) approach is very effective in evaluating the credibility criteria. The MIC's project aims to develop methods to analyze not only text information but also multimedia content using NLP, information retrieval and data mining approaches. By using different methods for analyzing the information credibility criteria, credible information can be acquired, which eventually becomes valuable knowledge. This talk will throw light on the activities of both projects in Japan.


Development of a WWW image retrieval system using the image knowledge database

Abstract:Authorship Attribution, (AA) is a process of determining a particular document is written by which author among a list of suspected authors. Authorship attribution has been the problem from last six decades; when there were handwritten documents needed to be identified for the genuine author. Due to the technology advancement and increase in cybercrime and unlawful activities, this problem of AA becomes forth most important to trace out the author behind online messages. Over the past, many years research has been conducted to attribute the authorship of an author on the basis of their writing style as all authors possess different distinctiveness while writing a piece of document. This paper presents a comparative study of various machine learning approaches on different feature sets for authorship attribution on short text. The Twitter dataset has been used for comparison with varying sample size of a dataset of 10 prolific authors with various combinations of feature sets. The significance and impact of combinations of features while inferring different stylometric features has been reflected. The results of different approaches are compared based on their accuracy and precision values.


Abstract:Part of speech (POS) tagging is a basic subject for Chinese information processing. In general, the existence of multi-category words greatly affects the processing quality of corpora. High efficient methods and automatically correcting techniques for multi-category word tagging are the keys for improving tagging precision. In this paper, for part of speech correcting of multi-category word, a modeling method is introduced based on an incomplete decision table and two algorithms for attribute reduction and object reduction used for automatically acquiring correcting rules are presented based on attribute significance. The results of testing show the validity of our method for improving part of speech tagging precision in large corpora engineering.



Fitting document representation to specific datasets by adjusting membership functions

Abstract:Other people judgment, sentiment, opinion and think about actions or product or speech are important for person or company or institutions for decision making processes. Today people declare their opinions, feeling and judgment about a product or a service or a film or a speech on social media platforms. Social media are easily accessible platforms and provide remarkable sources for analysis and evaluation. Sentiment analysis find out the feeling of people on an object. The feeling covers attitudes, emotion and opinions. Sentiment is subjective impression; not facts. Sentiment analysis is basically a text classifying problem. Therefore, use Natural Language Processing (NLP), statistics, or machine learning methods to extract, identify, or otherwise characterize the sentiment content of a text.


The idiom-reference connection
Idiom processing and reference resolution are two complex aspects of text processing that are commonly treated in isolation. However, closer study of the reference needs of some idioms suggests that these two phenomena will need to be treated together to support high-end NLP applications. Using evidence from Russian and English, this article describes a number of classes of idioms according to their reference needs and suggests a method of lexical encoding which, supplemented by procedural semantic routines, can adequately support the full semantic and referential interpretation of these idioms.

A simple measure to assess non-response
There are several tasks where is preferable not responding than responding incorrectly. This idea is not new, but despite several previous attempts there isn't a commonly accepted measure to assess non-response. We study here an extension of accuracy measure with this feature and a very easy to understand interpretation. The measure proposed (c@1) has a good balance of discrimination power, stability and sensitivity properties. We show also how this measure is able to reward systems that maintain the same number of correct answers and at the same time decrease the number of incorrect ones, by leaving some questions unanswered. This measure is well suited for tasks such as Reading Comprehension tests, where multiple choices per question are given, but only one is correct.

2
-test, t-test, co-occurrence frequency, log-likelihood ratio and mutual information. Experiments show that our approach leads to a significant performance improvement in comparison with individual basic methods in both precision and recall.


NLP-Based Prediction of Medical Specialties at Hospital Admission Using Triage Notes

Abstract:Human Computer Interaction (HCI) is a field of study to interact between humans (user) and computers on the design of computer technology. Question Answering (QA) system is one of the parts of HCI and a process of Information Retrieval (IR) in Natural Language Processing (NLP). In this research, it is attempted for a Bangla Question Answering System with simple sentences and experimented the system for both Bangla and English language. And it is tried to perform with semantic and syntactical analysis. Furthermore, for Bangla, a word net is constructed to demonstrate the system process. Our proposed method is a model in which it is easy for users to get most possible exact answer to their question easily and reduces the complexity of using noun instead of pronoun for the requested answer with respect to the given question queries for Bangla. It improves better answer extraction than naive approach.



Navigation-orientated natural spoken language understanding for intelligent vehicle dialogue

Abstract:In this paper, we propose a hierarchical attention model for summarization. Normally, sentences have relations among another sentence and it is important to consider these relations in summarizing. Our proposed model can make each sentence vectors from document composed of multi sentences and get relations among sentences from these vectors by the incorporated operation. As an operation of taking relations, we use self-attention and gated convolutional neural network. It has been reported that these operations can get dependencies among words, and self-attention is particularly powerful. Therefore we adopted these operations expecting the same work in sentences. We conducted an experiment of title generation by using Japanese news articles. We evaluated the performance of our proposed model by Rouge and visualized the relations among sentences.


Toward a text classification system for the quality assessment of software requirements written in natural language
Requirements Engineering (RE) is concerned with the gathering, analyzing, specifying and validating of user requirements that are documented mostly in natural language. The artifact produced by the RE process is the software requirements specification (SRS) document. The success of a software project largely depends on the quality of SRS documentation, which serves as an input to the design, coding and testing phases. This paper approaches the problem of the automatic quality assessment of textual requirements from an innovative point of view, namely the use of the Natural Language Processing (NLP) text classification technique. The paper proposes a quality model for the requirements text and a text classification system to automate the quality assessment process. A large study evaluating the discriminatory power of the quality characteristics and the feasibility of a tool for the automatic detection of ambiguities in requirements documentation is presented. The study also provides a benchmark for such an evaluation and an upper bound on what we can expect automatic requirements quality assessment tools to achieve. The reported research is part of a larger project on the applicability of NLP techniques to assess the quality of artifacts produced in RE.

Abstract:In the era of smartphones and the Internet of Things (IoT) we can possibly do everything on our handheld devices. There are so many applications of Android, IOS, and more that can interact and operate nearly to all types of electronic devices. There are unique types of applications, which can operate a computer's software/functionalities through mobile phones only. In this paper, we have highlighted other available applications and introduced and proposed a new application, which interacts and operates a computer through voice commands and has many other functionalities related to IoT. As compared to other existing applications, it uses AO* machine learning algorithms which can reduce the delay of communicating the actions to any computer hardware, that is why it works efficiently as compare to its peer applications, and also has the functionality of controlling the performance embedded devices and the hardware resources of a computer system.


Verb-particle constructions and lexical resources
In this paper we investigate the phenomenon of verb-particle constructions, discussing their characteristics and their availability for use with NLP systems. We concentrate in particular on the coverage provided by some electronic resources. Given the constantly growing number of verb-particle combinations, possible ways of extending the coverage of the available resources are investigated, taking into account regular patterns found in some productive combinations of verbs and particles. We discuss, in particular, the use of Levin's (1993) classes of verbs as a means to obtain productive verb-particle constructions, and discuss the issues involved in adopting such an approach.

Predicting COVID-19 in China Using Hybrid AI Model
Abstract:The coronavirus disease 2019 (COVID-19) breaking out in late December 2019 is gradually being controlled in China, but it is still spreading rapidly in many other countries and regions worldwide. It is urgent to conduct prediction research on the development and spread of the epidemic. In this article, a hybrid artificial-intelligence (AI) model is proposed for COVID-19 prediction. First, as traditional epidemic models treat all individuals with coronavirus as having the same infection rate, an improved susceptible–infected (ISI) model is proposed to estimate the variety of the infection rates for analyzing the transmission laws and development trend. Second, considering the effects of prevention and control measures and the increase of the public’s prevention awareness, the natural language processing (NLP) module and the long short-term memory (LSTM) network are embedded into the ISI model to build the hybrid AI model for COVID-19 prediction. The experimental results on the epidemic data of several typical provinces and cities in China show that individuals with coronavirus have a higher infection rate within the third to eighth days after they were infected, which is more in line with the actual transmission laws of the epidemic. Moreover, compared with the traditional epidemic models, the proposed hybrid AI model can significantly reduce the errors of the prediction results and obtain the mean absolute percentage errors (MAPEs) with 0.52%, 0.38%, 0.05%, and 0.86% for the next six days in Wuhan, Beijing, Shanghai, and countrywide, respectively.

Abstract:Alcohol use is one of the main risk factors related to many diseases. However, alcohol use information is buried in the patient's clinical records, and extracting this information from narrative text requires substantial manual labor. This work aims to develop an automated system for detecting alcohol use status from patients' discharge summaries. A combination of machine learning and rule-based techniques has been employed in order to identify alcohol status in three stages. In the first stage, the proposed system detects alcohol-related sentences by utilizing a keyword search technique. The second stage distinguishes between the negative and positive alcohol sentences and identifies the temporal status. In this stage different machine learning classifiers have been employed in order to achieve the best performance. Finally, the document level alcohol use status is aggregated from the sentence-level for each patient's record. The proposed system exhibits high performance in identifying alcohol use status, achieving an Fl-score up to 0.99 in identifying alcohol use related records, 0.96 in detecting negative records and 0.89 identifying temporal status.



Improving Self-Attention Networks With Sequential Relations

Simulation of Autonomous Mobile Robot System for Food Delivery in In-patient Ward with Unity
Abstract:Logistic management is crucial for effective and efficient transportation of various items in hospitals. During pandemic situations, especially COVID-19, special in-patient cohort ward is established to treat patients who require special treatment due to the quarantine protocol. Autonomous Mobile Robot (AMR) is used for delivering food and medical supplies to individual patients in order to keep the physical distance between patients and health workers. In this research, delivery by using multiple AMRs working in the in-patient ward is simulated. The simulation software is developed in Unity platform to study the operations of AMRs in various scenarios.

Abstract:Circuit partitioning is a very extensively studied problem. Our proposed methodology easily extends to multiple constraints that are very dominant in the design of large scale VLSI systems. In this paper we formulate the problem as a nonlinear program (NLP). The NLP is solved for the objective of minimum cutset size under the constraints of pins, area, and timing. We have tested the unified framework for area, timing, and pin constraints. The NLP is solved using the commercial LP/NLP solver MINOS. We have done extensive testing using large scale RT level benchmarks and have shown that our methods can be used for exploring the design space for obtaining constraint satisfying system designs. We also provide extensions for solving system design problems where a choice between multiple technologies, packaging components, performance, cost, yield, and more can be the constraints for design related decisions.


Abstract:Out of vocabulary (OOV), which is a word that does not exist in a predefined vocabulary. How to deal with OOV is an important research topic in the field of natural language processing. The existence of OOV directly affects the performance of many NLP systems. For example, in some common scenarios such as machine translation, sentiment analysis, and intelligent question answering, the existence of OOV can greatly affect the key performance of the system. In recent years, with the advent of the word vector algorithm word2vec based on the principle of word morphology, the word embedding path of the NLP system has improved significantly. We combine LSTM with NLM, taking the morphemes of words as the basic processing unit, while taking into account the global context information. The results obtained are better than the existing OOV processing strategies, and the performance of commonly used NLP systems is generally improved. Finally, it is experimentally proved that our model is generally better than the existing models in the problem of unregistered word processing.


Methods and Applications for Relation Detection Potential and Limitations of Automatic Learning in IE
Abstract:The detection of relation instances is a central functionality for the extraction of structured information from unstructured textual data and for gradually turning texts into semi-structured information. Experience from many years of shared-task efforts in the MUC and ACE frameworks has led to promising initial results but also to frustrating barriers. But the systematic collective efforts have also yielded valuable insights into the complexity of the task and the limitations of existing approaches. An entire research area has emerged from the numerous efforts to increase the sophistication of the various approaches to relation extraction and from the obtained empirical results. In the meantime, it has become clear that there cannot be a single best method for relation extraction since there are many types of relations differing in complexity and in their reflection in the vocabulary of the language. The scale of complexity ranges from simple binary relations of frequent entity types all the way to complex embeddings of relations of various arity. Types of opinions and complex events are just special types of relations. For some relations the language provides prepositions, verbs or other lexemes that allow a concise and compact encoding. Others have to be described by a combination of different words and constructions. Relation extraction tasks do not only differ in the complexity and the linguistic inventory associated with the relevant relations. They also differ with respect to the size and nature of the available data for training and application. With respect to the applied mathematical methods, we find discrete (or symbolic) and non-discrete approaches. The latter are usually statistical methods. We also witness a growing tendency to combine different methods. With respect to the acquisition of the classifiers or detection grammars, the existing approaches fall in three large categories: i. detection by classifiers/grammars acquired through intellectual human labor ii. detection by classifiers/grammars acquired through supervised learning iii. detection by classifiers/grammars acquired through unsupervised or minimally supervised learning In the talk we will provide examples for the classes of approaches and summarize their respective advantages and disadvantages. We will argue that different relation detection tasks require different methods or even different combinations of methods.


An NLP tool for decoding the ATC Phraseology from English to Bengali

Abstract:Internet forums and public social media, such as online healthcare forums, provide a convenient channel for users (people/patients) concerned about health issues to discuss and share information with each other. In late December 2019, an outbreak of a novel coronavirus (infection from which results in the disease named COVID-19) was reported, and, due to the rapid spread of the virus in other parts of the world, the World Health Organization declared a state of emergency. In this paper, we used automated extraction of COVID-19-related discussions from social media and a natural language process (NLP) method based on topic modeling to uncover various issues related to COVID-19 from public opinions. Moreover, we also investigate how to use LSTM recurrent neural network for sentiment classification of COVID-19 comments. Our findings shed light on the importance of using public opinions and suitable computational techniques to understand issues surrounding COVID-19 and to guide related decision-making. In addition, experiments demonstrated that the research model achieved an accuracy of 81.15% - a higher accuracy than that of several other well-known machine-learning algorithms for COVID-19-Sentiment Classification.


Abstract:Automatic question-answer system is a very hot research in natural language processing realm. But it also exist some problems. The developing period of the Q/A system in different fields is too long, and the recycle rate is so low. To deal with it, the paper researches the generator of automatic Q/A system. First, the paper introduces the answer shape measure and the module of Q/A system. It brings elaborately forward the main idea of generator. Second, it puts forward the method of answer extraction and the translation method of the heterogeneous data in the automatic Q/A system. Automatic Q/A system make the corresponding answer by the principle of the maximal weigh. The sentences are regard as the basic unit of the result matching. It considerably enhances the intelligence degree of Q/A system. Finally, it gives an example of QA system.On the base of above theory and algorithms, we implement a generator of Q/A system.



#Walangpasok on Twitter: Natural language processing as a method for analyzing tweets on class suspensions in the Philippines

Automated web development: theme detection and code generation using Mix-NLP
A website helps a business to grow by using different marketing strategies. This paper describes a novel approach to develop a website by just providing the text (description of the website) or an image as input. Using Text Input it will suggest template (screenshots) after identifying the theme of the site inferred from the input. Those templates are converted into code for further customizations for their personal use. Current problem was that a web developer will take more than 15 days only to just make the basic structure of a website. This issue is resolved by our work which will generate the complete code of the webpage/ website in less amount of time. In this paper, it will tokenize each word to find their synonyms and then mapped it with root words for the theme identification and uses deep learning model to convert templates into code.

Off-topic essay detection using short prompt texts
Our work addresses the problem of predicting whether an essay is off-topic to a given prompt or question without any previously-seen essays as training data. Prior work has used similarity between essay vocabulary and prompt words to estimate the degree of ontopic content. In our corpus of opinion essays, prompts are very short, and using similarity with such prompts to detect off-topic essays yields error rates of about 10%. We propose two methods to enable better comparison of prompt and essay text. We automatically expand short prompts before comparison, with words likely to appear in an essay to that prompt. We also apply spelling correction to the essay texts. Both methods reduce the error rates during off-topic essay detection and turn out to be complementary, leading to even better performance when used in unison.

Abstract:Appearance of unknown words is one of the frequently occurring problems facing in part of speech (POS) tagging process, i.e., the words that appear in sentences, but are not contained within the training corpus. New words are continually coined to the language, and people will often use words that are parsing, the system may not expect. This problem get worse when NLP systems are used for more and more on-line computer applications. New words are continually entering the language, Acronyms and proper names are created very often and new nouns and verbs are adding to the language in a surprising rate. So it is impossible to train the tagger for every possible word in the language. So unknown words are non-negligible in POS tagging. Therefore, in order to build a complete tagger, tagger must be incurred with some knowledge of suggesting the tag for an unknown word.


Learning, an incremental algorithm, for constructing the sentence boundary detection model using different features based on local context. Although the model can be easily trained on any genre of text and on any alphabet language, we emphasize the ability that the classifier is adaptable to text with domain and topic shifts without retraining the whole model from scratch. Empirical results indicate that the performance of proposed system is comparable to that of similar systems.


The relevance of a cognitive model of the mental lexicon to automatic word sense disambiguation: invited talk
Supervised word sense disambiguation requires training corpora that have been tagged with word senses, and these word senses typically come from a pre-existing sense inventory. Space limitations imposed by dictionary publishers have biased the field towards lists of discrete senses for an individual lexeme. Although some dictionaries use hierarchical entries to emphasize relations between senses, many do not. WordNet, which has been the default choice of NLP researchers for sense tagging because of its broad coverage and easy accibility, does not have hierarchical entries. Could the relations between senses that are captured by a hierarchy be useful to NLP systems? Concerns have also been raised about whether or not WordNet's word senses are unnecessarily fine-grained. WSD systems are obviously more successful in distinguishing coarse-grained senses than fine-grained ones (Navigli, 2006), but important information could be lost if fine-grained distinctions are ignored. Recent psycholinguistic evidence seems to indicate that closely related word senses may be represented in the mental lexicon much like a single sense, whereas distantly related senses may be represented more like discrete entities (Brown, 2008). These results suggest that, for the purposes of WSD, closely related word senses can be clustered together into a more general sense with little meaning loss. This talk will describe this psycholinguistic research and its current implications for automatic word sense disambiguation, as well as plans for future research and its possible impact.

Facilitating Knowledge Sharing from Domain Experts to Data Scientists for Building NLP Models
Data scientists face a steep learning curve in understanding a new domain for which they want to build machine learning (ML) models. While input from domain experts could offer valuable help, such input is often limited, expensive, and generally not in a form readily consumable by a model development pipeline. In this paper, we propose Ziva, a framework to guide domain experts in sharing essential domain knowledge to data scientists for building NLP models. With Ziva, experts are able to distill and share their domain knowledge using domain concept extractors and five types of label justification over a representative data sample. The design of Ziva is informed by preliminary interviews with data scientists, in order to understand current practices of domain knowledge acquisition process for ML development projects. To assess our design, we run a mix-method case-study to evaluate how Ziva can facilitate interaction between domain experts and data scientists. Our results highlight that (1) domain experts are able to use Ziva to provide rich domain knowledge, while maintaining low mental load and stress levels; and (2) data scientists find Ziva’s output helpful for learning essential information about the domain, offering scalability of information, and lowering the burden on domain experts to share knowledge. We conclude this work by experimenting with building NLP models using the Ziva output for our case study.


user2agent: 2nd Workshop on User-Aware Conversational Agents

Abstract:This paper implements a system that provides a user an authenticated score of any online product computed with the help of the NLP technique called Sentiment Analysis and Opinion Mining. Sentimental Analysis is the anatomization of the judgment of a user on a particular product. It scans through the sentences in search of keywords that deliver definite emotions. In our system, we have used a Support Vector Machine (SVM) classifier to generate a fine-grained Sentimental Analysis report. This classifier helps us to categorize the reviews collected from the product’s website URL into predefined categories to affirm the user’s thought process and generate a rating from 1 to 5 (1 being the lowest, 5 being the highest). The system makes use of this technique to gather all the accrued reviews and convert the textual meaning into a meaningful average grade. This process makes it possible for any prospective customer to have a definitive conclusion on the review of the product.


Turkish labeled text corpus
Abstract:A labeled text corpus made up of Turkish papers' titles, abstracts and keywords is collected. The corpus includes 35 number of different disciplines, and 200 documents per subject. This study presents the text corpus' collection and content. The classification performance of Term Frequcney — Inverse Document Frequency (TF-IDF) and topic probabilities of Latent Dirichlet Allocation (LDA) features are compared for the text corpus. The text corpus is shared as open source so that it could be used for natural language processing applications with academic purposes.

1
=83.34).

Abstract:Parsing is an essential step in most of Natural Language Processing (NLP) systems. It is mainly concerned with determining the grammatical position of each word in a sentence, and how words can be put together to form a correct sentence according to a certain grammar. In this paper, we present the architecture of a bottom-up semantic parser for traditional Arabic language. This parser is based on a Unification Based Grammar (UBG) representation of traditional Arabic grammar, and is implemented using Extensible Markup Language (XML). Moreover, we provide a set of traditional Arabic grammar rules that can be used in future results.



Recommendation of Indian Cuisine Recipes Based on Ingredients

Abstract:Image captions represent manual semantic annotation of images. These act as essential cues to represent the semantics of an image. This paper describes the process of representing, discovering, storing the semantics in a knowledge base, and then applying the semantics to aid the retrieval of visual information. We exploit a Natural Language Processing (NLP) framework in order to extract the knowledge from image captions and to transform those unstructured data into a semantic model. The novelty of the proposed framework is to use a semantic model to find implicit relationships among the concepts of photographs which are not mentioned directly in text captions. Latent Semantic Indexing (LSI) is deployed to handle ontology imperfections. Experiments tested and validated the major hypotheses of this approach.


Abstract:This research work involves a highly accurate directional overcurrent coordination using the complex method–Rosen's gradient projection nonlinear programming approach with the genetic algorithm-particle swarm optimization (GA-PSO) metaheuristic optimization algorithm. To accelerate the optimization process, manual tuning steps are proposed to remove miscoordinations quickly and to determine the relay type in the case that different relay curves are given for coordination. In this manner, the metaheuristic and the deterministic parts share their advantages so that their combination leads to a significant tradeoff between exploration and exploitation. In addition, different objective functions for each part are introduced. For various conditions, the proposed method is applied to the eight-bus transmission and 33-kV distribution part of the 30-bus IEEE power system. Next, the superiority of the proposed algorithm over other researches is verified by the observation that the results are in the range of dual setting schemes.


ATP-OIE: An Autonomous Open Information Extraction Method
This paper describes an innovative Open Information Extraction method known as ATP-OIE1. It utilizes extraction patterns to find semantic relations. These patterns are generated automatically from examples, so it has greater autonomy than methods based on fixed rules. ATP-OIE can also summon other methods, ReVerb and ClausIE, if it is unable to find valid semantic relations in a sentence, thus improving its recall. In these cases, it is capable of generating new extraction patterns online, which improves its autonomy. It also implements different mechanisms to prevent common errors in the extraction of semantic relations. Lastly, ATP-OIE was compared with other state-of-the-art methods in a well known texts database: Reuters-21578, obtaining a higher precision than with other methods.

College Enquiry Chatbot using Rasa Framework
Abstract:The growth of technologies like Artificial Intelligence (AI), Big Data & Internet of Things (IoT), etc. has marked many advancements in the technological world since the last decade. These technologies have a wide range of applications. One such application is “Chatterbot or “Chatbot”. Chatbots are conversational AIs, which mimics the human while conversing & eliminates the need of human by automating mundane tasks. In the study undertaken, we have created a chatbot in education domain & it is named as “College Enquiry Chatbot”, This chatbot is a web-based application that analyses and understands user's queries and provides an instant and accurate response. Rasa technology is used to construct this chatbot. It's an open-source technology, which uses its two main packages i.e., Rasa Core & Rasa Natural Language Understanding (NLU) in order to build a Contextual AI Chatbot. NLU is used to infer the intent and to extract the necessary entities from user input & the Rasa Core provides the output by building a probabilistic model with the help of Recurrent Neural Network (RNN). Evaluation of the model is done by getting a confusion matrix and performance measures like Precision, Accuracy & F1 Score which come out to be 0.628, 0.725 and 0.669 respectively on average basis. This chatbot's accuracy, lack of dependability on human resources, 24 x 7 accessibility and low maintenance creates various opportunities for its implementation. This conversational agent can not only be used in educational institutions but also in places where enquiry becomes a tedious task.

Abstract:Platforms with multiple Field Programmable Gate Arrays (FPGAs), such as Amazon Web Services (AWS) F1 instances, can efficiently accelerate multi-kernel pipelined applications, e.g., Convolutional Neural Networks for machine vision tasks or transformer networks for Natural Language Processing tasks. To reduce energy consumption when the FPGAs are underutilized, we propose a model to (1) find off-line the minimum-power solution for given throughput constraints, and (2) dynamically reprogram the FPGA at runtime (which is complementary to dynamic voltage and frequency scaling) to match best the workloads when they change. The off-line optimization model can be solved using a Mixed-Integer Non-Linear Programming (MINLP) solver, but it can be very slow. Hence, we provide two heuristic optimization methods that improve result quality within a bounded time. We use several very large designs to demonstrate that both heuristics obtain comparable results to MINLP, when it can find the best solution, and they obtain much better results than MINLP, when it cannot find the optimum within a bounded amount of time. The heuristic methods can also be thousands of times faster than the MINLP solver.


Media File Descriptor Using Deep Learning
Abstract:Media files like audios and videos are very useful sources of information, entertainment etc. Humans watch/listen to media files almost everyday for some purpose or another. There are lots of videos,audio and images on every content on the internet and other platforms. Sometimes, just the title can be misleading and humans end up utilizing time on irrelevant media files and often when they realize this isn't the relevant media file they have already wasted some time. Our Media File Descriptor is a generalized platform used to provide a summary of any media file that is audio, video or image quickly so that the user has optimum information of the media file beforehand by reading the summary and can decide if the file is relevant to what they are looking for. If not, the user doesn't end up wasting time on the wrong media file.

Abstract:Parts of Speech (POS) tagging is one of the most well-studied problems in the field of Natural Language Processing (NLP). In this paper, a Neural Network Language Models (NNLM) such as Recurrent Neural Network (RNN) and Long-Short Term Memory (LSTM) have been trained and assessed to address the POS tagging problem for the Turkish Language. The performance is compared to the state-of-art methods. The results show that LSTM outperforms RNN with 88.7% F1-score. This study is the first study that contributes to the literature utilizing word embedding and NNLM for the Turkish language.



Natural Language Processing in Business Process Identification and Modeling: A Systematic Literature Review

Abstract:Kale is a popular ingredient in Thai cuisine and can be grown year-round. However, kale requires particular care, especially pests. Therefore, this study applies the Internet of Things to propose the KaleCare, a smart farm management system for kale with four main functions including automatic watering based on weather forecasting, automatic fertilizing, reporting, and pest detection for cutworms, and aphids. There are three processes to create the pest classification models for pest detection function. Firstly, the raw images were applied to the GrabCut to remove the background. Secondly, data augmentation was applied to generate images due to the small amount of raw data. Finally, the modified GoogLeNet reduced the original GoogLeNet structure is proposed to classify both types of pests. The experimental results show that the proposed model outperforms with 0.8903 and 0.7959 in average classification rate and 0.886 and 0.7965 in average F1-score to classify cutworm and aphid, respectively.


A tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing
Dual decomposition, and more generally Lagrangian relaxation, is a classical method for combinatorial optimization; it has recently been applied to several inference problems in natural language processing (NLP). This tutorial gives an overview of the technique. We describe example algorithms, describe formal guarantees for the method, and describe practical issues in implementing the algorithms. While our examples are predominantly drawn from the NLP literature, the material should be of general relevance to inference problems in machine learning. A central theme of this tutorial is that Lagrangian relaxation is naturally applied in conjunction with a broad class of combinatorial algorithms, allowing inference in models that go significantly beyond previous work on Lagrangian relaxation for inference in graphical models.


Determining Physical Location of Wireless Access Point using Smart Devices

Using the distribution of performance for studying statistical NLP systems and corpora
Statistical NLP systems are frequently evaluated and compared on the basis of their performances on a single split of training and test data. Results obtained using a single split are, however, subject to sampling noise. In this paper we argue in favour of reporting a distribution of performance figures, obtained by resampling the training data, rather than a single number. The additional information from distributions can be used to make statistically quantified statements about differences across parameter settings, systems, and corpora.

Abstract:Mining bilingual parallel sentence pair from Web data is the most effective way to get large-scale of bilingual corpus. In this paper, we put forward both the set of method and the series of process for extracting parallel sentence pair from nonspecific web date source. considering 1.1 billion page as the web data input, with a sequence of steps we get several sentences pair which has 81% recall and 85% precision, on this basis we bring up a parameter for measure quality of sentence pair. After filter sentence pair by this parameter, we get 850 thousand unique sentence pairs. On filtering by this parameter, the precision increase to 95%, meanwhile the recall only decrease by 1%.


Abstract:In the last two decades, significant effort has been put into annotating linguistic resources in several languages. Despite this valiant effort, there are still many languages left that have only small amounts of such resources. The goal of this article is to present and investigate a method of propagating information (specifically mentions) from a resource-rich language such as English into a relatively less-resource language such as Arabic. We compare also this approach to its equivalent counterpart using monolingual resources. Part of the investigation is to quantify the contribution of propagating information in different conditions - based on the availability of resources in the target language. Experiments on the language pair Arabic-English show that one can achieve relatively decent performance by propagating information from a language with richer resources such as English into Arabic alone (no resources or models in the source language Arabic). Furthermore, results show that propagated features from English do help improve the Arabic system performance even when used in conjunction with all feature types built from the source language. Experiments also show that using propagated features in conjunction with lexically-derived features only (as can be obtained directly from a mention annotated corpus) brings the system performance at the one obtained in the target language by using feature derived from many linguistic resources, therefore improving the system when such resources are not available.



The Building of a CBD-Based Domain Ontology in Chinese

Human–Robot Communications of Probabilistic Beliefs via a Dirichlet Process Mixture of Statements
Abstract:This paper presents a natural framework for information sharing in cooperative tasks involving humans and robots. In this framework, all information gathered over time by a human-robot team is exchanged and summarized in the form of a fused probability density function (pdf). An approach for an intelligent system to describe its belief pdfs in English expressions is presented. This belief expression generation is achieved through two goodness measures: semantic correctness and information preservation. In order to describe complex, multimodal belief pdfs, a Mixture of Statements (MoS) model is proposed such that optimal expressions can be generated through compositions of multiple statements. The model is further extended to a nonparametric Dirichlet process MoS generation, such that the optimal number of statements required for describing a given pdf is automatically determined. Results based on information loss, human collaborative task performances, and correctness rating scores suggest that the proposed method for generating belief expressions is an effective approach for communicating probabilistic information between robots and humans.

Abstract:Nowadays, due to a large amount of tourism information such as attractions or activities, it is difficult for users to decide where to go or do specific activities without getting suggestions from experts, books, websites, etc. To support users wisely in choosing tourism activities, a recommendation or suggestion system using Natural Language Processing (NLP), which is a subdomain of Artificial Intelligence (AI), is proposed. NLP plays an important part in the pre-processing task of the recommender system to enhance the performance of the suggested output. This paper explores and compares the NLP techniques that are currently applied to the existing recommendation systems. Challenges and trends using NLP for suggestion systems in tourism are discussed.


NLP Oriented Japanese Pun Classification
Abstract:In this paper we describe a phonetic classification of Japanese puns (dajare). Basing on real life examples gathered from available sources (books, Internet), we divided Japanese puns into 12 groups with numerous subgroups, according to phonetic changes that occur within them. This classification was prepared for the NLP purpose, i.e. to be used in humor processing. Its usefulness was shown in a research project, aimed at constructing a humor-equipped conversational system for Japanese.


Identifying Sponsored Content in YouTube using Information Extraction

LTP: a Chinese Language Technology Platform
LTP (Language Technology Platform) is an integrated Chinese processing platform which includes a suite of high performance natural language processing (NLP) modules and relevant corpora. Especially for the syntactic and semantic parsing modules, we achieved good results in some relevant evaluations, such as CoNLL and SemEval. Based on XML internal data representation, users can easily use these modules and corpora by invoking DLL (Dynamic Link Library) or Web service APIs (Application Program Interface), and view the processing results directly by the visualization tool.

Knowledge discovery in hashtags#
Abstract:Twitter is a breed of social networks that are playing a buoyant role in today's world communication. This paper is an attempt to apply knowledge discovery process on Twitter dataset comprising hashtags along with the visual analytic techniques whose purpose is to provide information to the people in such a way so that they understand concealed knowledge in the data effortlessly and meritoriously. We further analyze tweet text and metadata associated with each tweet for identification of useful patterns like "who talks to whom" and "how much". Our research reveals the impact of visualization and hierarchical clustering technique in analyzing similar groups of users. Further we investigate different social network measures that unveil the influence of users in the particular hashtags.

Natural language processing for software requirement specifications
Software Requirement Specifications (SRS) describe the functionality and expected performance for software products. It is one of the most important documents in the software development life cycle process that affects all the subsequent phases in product development. To manage the software requirements, requirement management tools such as IBM DOORS have been used to facilitate the communication, collaboration and verification throughout the process. Such tools generate a large amount of textual data, which can be processed through various NLP techniques to automate the software development processes. In this regard, the amalgamation of NLP and requirement engineering can catalyze the process of requirement classification (e.g., classifying requirements as functional and non-functional requirements), categorization of software documents, and topic modeling over SRS documents. Moreover, NLP-based approaches can be expanded to analyze the deeper semantics of software requirements for various purposes such as information extraction, automation of UML diagrams, AI-enabled software requirement analysis, defect and error detection in SRS, and solving the natural language ambiguities present in the requirements. In this workshop, university and IBM researchers will give insights into various existing NLP methodologies applied to SRS documents, e.g., for detecting the ambiguities and defects in the SRS documents. The prospects of various NLP-based automation techniques for conflict/duplicate detection as well as other NLP applications in requirement engineering will also be presented.

Automatic identification of arabic dialects in social media
Modern Standard Arabic (MSA) is the formal language in most Arabic countries. Arabic Dialects (AD) or daily language differs from MSA especially in social media communication. However, most Arabic social media texts have mixed forms and many variations especially between MSA and AD. This paper aims to bridge the gap between MSA and AD by providing a framework for AD classification using probabilistic models across social media datasets. We present a set of experiments using the character n-gram Markov language model and Naive Bayes classifiers with detailed examination of what models perform best under different conditions in social media context. Experimental results show that Naive Bayes classifier based on character bi-gram model can identify the 18 different Arabic dialects with a considerable overall accuracy of 98%. This work is a first-step towards an ultimate goal of a translation system from Arabic to English and French, within the ASMAT project

Abstract:In this paper, we propose a novel Chinese-English organization name translation method with the assistance of mix-language web resources. Firstly, all the implicit out-of-vocabulary terms in the input Chinese organization name are recognized by a CRFs model. Then the input Chinese organization name is translated without considering these recognized out-of-vocabulary terms. Secondly, we construct some efficient queries to find the mix-language web pages that contain both the original input organization name and its correct translation. At last, a similarity matching and limited expansion based translation identification approach is proposed to identify the correct translation from the returned web pages. Experimental results show that our method is effective for Chinese organization name translation and can improve performance of Chinese organization name translation significantly.



WIT: Workshop on deriving Insights from user-generated Text


Feature extraction and analysis of MISING speech vowels


Natural language parsing using Fuzzy Simple LR (FSLR) parser

Abstract:Internet technology has occupied an important part of human lives. Users often face the problem of the available excessive information. Recommandation system (RS) are deployed to help users cope up with the information explosion. RS is mostly used in digital entertainment, such as Netflix, prime video, and IMDB, and e-commerce portals such as Amazon, Flipkart, and eBay. The two traditional methods namely, collaborative filtering (CF) and content-based approaches consist of few limitations individually. However, any hybrid system, which utilizes the advantage of both the systems to leverage better results. Some fundamental issues faced by movie recommendation systems such as scalability, cold start problem, data sparsity and practical usage feedback and verification based on real implementation are still neglected. Other issues that require significant research attention are accuracy and time complexity problem, which could make RS, a bad candidate for real-world recommendation systems. This literature survey aims to consolidate and structurally categorize all the major drawbacks present in the most common and popular commercial movie recommendation systems.



Collective intelligence & sentimental analysis of twitter data by using StandfordNLP libraries with software as a service (SaaS)


Searching the Web for Cross-lingual Parallel Data

Abstract:To analyze a massive and extremely large amount of text information efficiently we use text mining techniques. It has been received a lot of attention because of the increasing demands of various organizations and companies to manage the large datasets of information which are available in text documents. In this digital world, text mining is very essential. The main goal is to discover information which is unknown, which is something till now not aware of anyone and so it has not been written down. Text mining is much similar to the web search but there are minor differences between the two of them. The major difference between text mining and web search is that in web search the browser searches the information which the user wants and that is already known that is it is written by someone else whereas in text mining we don't know exactly what we are looking for. The main objective of text mining is to discover new patterns and new trends among data, associations in entities and predictive rules, etc which combine together to extract information which is required by the user. This research is based on the exploration of the capabilities of text mining using R language which is an open source software which is basically used for statistical computing and graphics.



The formalization of ‘temporal adverbials +ZHE imperfective’ sentences


Constructing parse forests that include exactly the n-best PCFG trees

Automatic semantic grouping in a spoken language user interface toolkit
With the rapid growth of real application domains for NLP systems, there is a genuine demand for a general toolkit from which programmers with no linguistic knowledge can build specific NLP systems. Such a toolkit should provide an interface to accept sample sentences and convert them into semantic representations so as to allow programmers to map them to domain actions. In order to reduce the workload of managing a large number of semantic forms individually, the toolkit will perform what we call semantic grouping to organize the forms into meaningful groups. In this paper, we present three semantic grouping methods: similarity-based, verb-based and category-based grouping, and their implementation in the SLUI toolkit. We also discuss the pros and cons of each method and how they can be utilized according to the different domain needs.

Introduction to classification: likelihoods, margins, features, and kernels: tutorial for NAACL-HLT 2007
Statistical methods in NLP have exploited a variety of classification techniques as core building blocks for complex models and pipelines. In this tutorial, we will survey the basic techniques behind classification. We first consider the basic principles, including the principles of maximum likelihood and maximum margin. We then discuss several core classification technologies: naive Bayes, perceptrons, logistic regression, and support vector machines. The discussion will include the key optimization ideas behind their training and the empirical trade-offs between the various classifiers. Finally, we consider the extension to kernels and kernelized classification: what can kernels offer and what is their cost? The presentation is targeted to NLP researchers new to these methods or those wanting to understand more about how these techniques are interconnected.

Session details: Text mining and NLP applications
No abstract available.

 and (ii) a higher accuracy on using D
n

Natural language analysis of patent claims
We propose a NLP methodology for analyzing patent claims that combines symbolic grammar formalisms with data-intensive methods while enhancing analysis robustness. The output of our analyzer is a shallow interlingual representation that captures both the structure and content of a claim text. The methodology can be used in any patent-related application, such as machine translation, improving readability of patent claims, information retrieval, extraction, summarization, generation, etc. The methodology should be universal in the sense that it could be applied to any language, other parts of patent documentation and text as such.


A neural model for generating natural language summaries of program subroutines

Abstract:The condition of the pilot eyes is one of the important safety factors for the mission critical aviation business. There are so many eye diseases that can effect pilot normal working namely cataract, glaucoma, crossed eyes etc. In this paper we propose advanced analysis of eye diseases for the safety of the pilot operations. Our analysis involves using different semantic techniques in order to conclude for the presence of the mentioned eye diseases using clinical notes of the pilot. We propose using different medical ontologies and SWRL along with NLP techniques in order to predict the most prominent eye disease.


Towards a Performant Multilingual Model Based on Ensemble Learning to Enhance Sentiment Analysis
Abstract:The aim of sentiment analysis, known as opinion mining, is to discover subjective information by understanding the meaning over public opinions, standpoints and attitudes from the shared text, such as consumers feedback, which focuses on automated tools. In this paper, we introduce a novel model based on a learning approach to enhance the rate of understanding and predicting the sentiment of a text. Our proposed learning approach establishes an effective penalty mechanism to map out the links between the analyzed context in which the sentiment is similar. With a gold standard corpus we released, the results obtained are better in terms of precision, recall, and computation time cost using a multithreading model.


An Effective Approach for Speech Enhancement by Multi-band MMSE Spectral Subtraction

Abstract:This paper focus to present a prediction model for drug addiction of the accused in type 1 drug abuse case as amphetamine. Case studies in the Suan Phueng police station area Ratchaburi province. The data set that used for modeling is obtained from the collection from Suan Phueng police station from 2016-2018. The data set 1,598 items consist of gender, age, number of offenses, education status, nationality, occupation, and non-drug abuse. For our contribute a prediction model into two classes as “take” and “untake” by using data mining techniques namely Bayes Network classifier. In our experimental, a group of bayes are used to comparison such as Bayes Network, Naive Bayes and Naive Bayes Updateable. The results displayed the Bayes Network classifier shown the highest accuracy rate, Naive Bayes and Naive Bayes Updateable with 81.53, 80.85 and 80.85 respectively.


Leveraging Sentiment Analysis for Classifying Patient Complaints
Unsolicited patient complaints expressed in patients' own words provide an evidential basis for identifying and mitigating patient safety and financial risks associated with physicians and other practitioners. Classifying patient complaints is complicated by the complexity of linguistic representation. Current practice relies upon manual classification, which limits scalability. An automatic approach can potentially improve response time and scale, thereby enhancing opportunities to promote physician accountability for safe and respectful care. This research seeks to automate the classification of patient complaints to improve triage and response. We process a corpus of patient complaints data collected by the Patient Advocacy Reporting System (PARS) developed at Vanderbilt and associated institutions. Our method is to map each complaint to a vector based on enhanced Linguistic Inquiry and Word Count (LIWC) lexicons and to train a Naive Bayes classifier over those vectors. We compare it to both Term Frequency-Inverse Document Frequency (TF-IDF) and the best case results of any classifier over bag of words features. Our classifier outperforms traditional complaint analysis approaches, which disregard sentiment. Our classifier yields 3% greater accuracy overall than traditional approaches. For the SAFETY OF ENVIRONMENT label, our classifier had an accuracy of 84% (compared to 50% for traditional) and a sensitivity of 96% (compared to 0% for traditional). We conclude that patient sentiments conveyed in complaints are often overlooked yet can be valuable in analyzing such complaints to identify and mitigate patient safety and provider financial risks. We demonstrate that inferring the complaint sentiment leads to improved classification accuracy.

Where should I comment my code?: a dataset and model for predicting locations that need comments
Programmers should write code comments, but not on every line of code. We have created a machine learning model that suggests locations where a programmer should write a code comment. We trained it on existing commented code to learn locations that are chosen by developers. Once trained, the model can predict locations in new code. Our models achieved precision of 74% and recall of 13% in identifying comment-worthy locations. This first success opens the door to future work, both in the new where-to-comment problem and in guiding comment generation. Our code and data is available at http://groups.inf.ed.ac.uk/cup/comment-locator/.

Abstract:Detection and correction of errors in Bengali text is essential. In general, Bengali text error can be classified into non-word error and semantic error (also known as context sensitive error). Till date, auto-correction for semantic error in Bengali sentence is challenging since there is no significant research works on this very topic. In this paper, we bring out the concept of Semantic Error detection and correction. We have developed a method that can detect and correct this kind of errors. Semantic error includes typographical error, grammatical errors, homophone errors, homonym error etc. Our goal to this study is to develop an approach to handle multiple semantic errors in a sentence. We have used our own built confused word list by edit distance and apply Naïve Bayes Classifier to detect and correct typographical and homophone error. For a candidate word from a sentence, we pick out a set of words which is a collection of confused words. We use all other neighbor words as features for each word from confusion set. Then we apply naïve theorem to calculate the probability and decide whether a target word is error or not. We have used 28,057 sentences to evaluate our model and we have achieved more than 90% accuracy. All data corpora used to evaluate the model are built by us. We strongly believe that the problem we have solved may shed light on the advancement of Bengali language processing significantly.


Prototype machine translation system from text-to-Indian sign language
This paper presents a prototype Text-To-Indian Sign Language (ISL) translation system. The system will help dissemination of information to the deaf people in India. This paper also presents the SL-dictionary tool, which can be used to create bilingual ISL dictionary and can store ISL phonological information.

A Matrix-Vector Recurrent Unit Model for Capturing Compositional Semantics in Phrase Embeddings
The meaning of a multi-word phrase not only depends on the meaning of its constituent words, but also the rules of composing them to give the so-called compositional semantic. However, many deep learning models for learning compositional semantics target specific NLP tasks such as sentiment classification. Consequently, the word embeddings encode the lexical semantics, the weights of the networks are optimised for the classification task. Such models have no mechanisms to explicitly encode the compositional rules, and hence they are insufficient in capturing the semantics of phrases. We present a novel recurrent computational mechanism that specifically learns the compositionality by encoding the compositional rule of each word into a matrix. The network uses a recurrent architecture to capture the order of words for phrases with various lengths without requiring extra preprocessing such as part-of-speech tagging. The model is thoroughly evaluated on both supervised and unsupervised NLP tasks including phrase similarity, noun-modifier questions, sentiment distribution prediction, and domain specific term identification tasks. We demonstrate that our model consistently outperforms the LSTM and CNN deep learning models, simple algebraic compositions, and other popular baselines on different datasets.

Contextual Analysis of Social Media: The Promise and Challenge of Eliciting Context in Social Media Posts with Natural Language Processing
While natural language processing affords researchers an opportunity to automatically scan millions of social media posts, there is growing concern that automated computational tools lack the ability to understand context and nuance in human communication and language. This article introduces a critical systematic approach for extracting culture, context and nuance in social media data. The Contextual Analysis of Social Media (CASM) ap-proach considers and critiques the gap between inadequacies in natural language processing tools and differences in geographic, cultural, and age-related variance of social media use and communication. CASM utilizes a team-based approach to analysis of social media data, explicitly informed by community expertise. We use of CASM to analyze Twitter posts from gang-involved youth in Chicago. We designed a set of experiments to evaluate the performance of a support vector machine us-ing CASM hand-labeled posts against a distant model. We found that the CASM-informed hand-labeled data outperforms the baseline distant labels, indicating that the CASM labels capture additional dimensions of information that content-only methods lack. We then question whether this is helpful or harmful for gun violence prevention.

Going beyond traditional QA systems: challenges and keys in opinion question answering
The treatment of factual data has been widely studied in different areas of Natural Language Processing (NLP). However, processing subjective information still poses important challenges. This paper presents research aimed at assessing techniques that have been suggested as appropriate in the context of subjective - Opinion Question Answering (OQA). We evaluate the performance of an OQA with these new components and propose methods to optimally tackle the issues encountered. We assess the impact of including additional resources and processes with the purpose of improving the system performance on two distinct blog datasets. The improvements obtained for the different combination of tools are statistically significant. We thus conclude that the proposed approach is adequate for the OQA task, offering a good strategy to deal with opinionated questions.

Abstract:The growing pace of information technology demands fast operation for communication and other related applications. After having a successful Machine Translation System [MTS], it has been felt to optimize the performance of Machine Translation for its real-time uses. The considered MTS is Tree Adjoining Grammar [TAG] based system. An approach has been experimented to use File model instead of Database model for fast streaming of grammar into memory and operation. This model provides an efficient and a systematic way of encapsulating language resource with engineering solution to develop the speedy MTS. The computational experiments demonstrate that substantial performance in terms of time and memory has been obtained by using this approach.


Abstract:Threat and abusive languages spread quickly through social media which can be controlled if we can detect and remove them. Since there exist many social media like Facebook, Twitter, Instagram etc and a huge number of social media users, we need a robust and effective automatic system to identify threat and abusive languages. In our proposed system Machine Learning and Natural Language Processing techniques have been implemented to build an automatic system. Previous research on Bengali abusive language detection used Multinomial Näıve Bayes (MNB), Support Vector Machine(SVM) algorithms and considered Bengali Unicode characters to build their system. We considered both Unicode emoticons and Unicode Bengali characters as valid input in our proposed system. Besides MNB and SVM algorithm, we implemented Convolutional Neural Network (CNN) with Long Short Term Memory(LSTM). Among three algorithms, SVM with linear kernel performed best with 78% accuracy.



Natural Language Processing for Productivity Metrics for Software Development Profiling in Enterprise Applications

A Computational Analysis of News Media Bias: A South African Case Study
News media in South Africa is assumed to be unbiased and objective in their reporting of the news. Indeed, editors are required to uphold an objective and balanced view with no favour to external political or corporate interests. This assumption of objectivity is tested on a large scale by computationally analysing 30 000 articles published by five media houses: News24, SABC, EWN, ENCA, and IOL. Using topic modelling, 38 topics are extracted from the corpus, and sentiment is computed for each topic. The study highlights various cases of both over and under-reporting by media houses on particular topics. We also identify various tonality biases by media houses.

Abstract:In this paper, a novel kernel, called position weight subsequences kernel (PWSK), is introduced for identifying gene sequences. String subsequences kernel (SSK), which is based on string alignment, performs well for text categorization problems. For gene sequences identification, not only the comprised subsequences but also the positions of them are important. To integrate the position information, the decay factor of match position in SSK was replaced by position weight in PWSK. By doing this, PWSK can integrate both the content and position information of subsequences. This kernel was used for splice site identification and the experimental results demonstrated its efficiency. The sensitivities for donor sites and acceptor sites are 94% and 95%, respectively, and the specificities for them are 96% and 96%. The performance is better than that of SSK. The reason is that the content of sequence alone is not enough to interpret splicing, and it is necessary to include the position information. Compared with the existing approaches, PWSK achieves better sensitivities for both the donor sites and the acceptor sites, and the specificities of them are comparable.


Abstract:In recent years, to encourage citizens to use the E-Government system more widely, the number of innovative research topics in this area has grown tremendously to bridge the information gap between citizens and the government. However, recently, concerns have been increasing, especially in developing countries, as to how to simplify the submission of a request for an E-Government service and how to link these requests to build an online application without asking the citizen to input the data more than once or run around to submit their information in person. Thus, the challenge for the Government in this situation is to provide an e-system which is fast and simple for the citizen to use. This research proposes a method for simplifying the information retrieval process from the E-Government system. This method uses Natural Language Processing (NLP), concept indexing and data integration methods for data analyzing and building. To assist the retrieval of information, the government concept is used to match everyday language to the formal language which is used in the government's E-Documents. As well, the mapping is based on the enriched citizen's requests to structure a multi-layered E-Document. To demonstrate its usefulness, we applied our methodology to sample data of an online request for a vehicle registration procedure.


An adaptive diversity strategy for particle swarm optimization
Abstract:In this paper, we present a diversity strategy for particle swarm optimizer. The modified algorithm re-initializes part of particles with poorer fitness during the searching process. It is empirically tested and compared with other published methods on many famous benchmark functions. The experimental results illustrate that the proposed algorithm has the potential to achieve higher success ratio and better solution quality. It is very competitive for hard multimodal function optimization.

Text analytics for security: tutorial
Computing systems that make security decisions often fail to take into account human expectations. This failure occurs because human expectations are typically drawn from in textual sources (e.g., mobile application description and requirements documents) and are hard to extract and codify. Recently, researchers in security and software engineering have begun using text analytics to create initial models of human expectation. In this tutorial, we provide an introduction to popular techniques and tools of natural language processing (NLP) and text mining, and share our experiences in applying text analytics to security problems. We also highlight the current challenges of applying these techniques and tools for addressing security problems. We conclude the tutorial with discussion of future research directions.

Abstract:In recent years, the social web has been increasingly used for health information seeking, sharing, and subsequent health-related research. Women often use the Internet or social networking sites to seek information related to pregnancy in different stages. They may ask questions about birth control, trying to conceive, labor, or taking care of a newborn or baby. Classifying different types of questions about pregnancy information (e.g., before, during, and after pregnancy) can inform the design of social media and professional websites for pregnancy education and support. This research aims to investigate the attention mechanism built-in or added on top of the BERT model in classifying and annotating the pregnancy-related questions posted on a community Q&A site. We evaluated two BERT-based models and compared them against the traditional machine learning models for question classification. Most importantly, we investigated two attention mechanisms: the built-in self-attention mechanism of BERT and the additional attention layer on top of BERT for relevant term annotation. The classification performance showed that the BERT-based models worked better than the traditional models, and BERT with an additional attention layer can achieve higher overall precision than the basic BERT model. The results also showed that both attention mechanisms work differently on annotating relevant content, and they could serve as feature selection methods for text mining in general.


Transforming standard Arabic to colloquial Arabic
We present a method for generating Colloquial Egyptian Arabic (CEA) from morphologically disambiguated Modern Standard Arabic (MSA). When used in POS tagging, this process improves the accuracy from 73.24% to 86.84% on unseen CEA text, and reduces the percentage of out-of-vocabulary words from 28.98% to 16.66%. The process holds promise for any NLP task targeting the dialectal varieties of Arabic; e.g., this approach may provide a cheap way to leverage MSA data and morphological resources to create resources for colloquial Arabic to English machine translation. It can also considerably speed up the annotation of Arabic dialects.


Cyber Attack Detection Method Based on NLP and Ensemble Learning Approach

Abstract:This paper highlights an E-learning system created using Moodle which is an open-source Learning Management System (LMS) that enables a better learning environment between the tutors and students. This system detects two learner profiles i.e. students with Learning Disability (LD) and without Learning Disability (Non-LD) using dedicated courses designed on the basis of various aspects of an LD student. This work also multiple stages of our approach for informal testing used to capture the learning parameters for Dyslexic students. The first stage i.e. data collection has two approaches where the first approach pertains to a smaller age group of 8-10 years with limited parameters whereas the second approach pertains to the age group 11-13 years i.e. grades 6-8 with more parameters. Natural Language Processing (NLP) has been used to perform Speech-to-Text (STT) conversion on the audio responses of the users. The analysis of these responses have been performed in python language. To detect whether the user has LD (Dyslexia in this case) or not, Machine Learning (ML) is used. Two ML algorithms namely Logistic Regression (LR) and Support Vector Machine (SVM) are used to perform binary classification with LD (1) and Non-LD (0) as the two classes of the dataset. The results are shown for both the approaches and comparative analysis shows that the dataset generated in the final approach for capturing parameters involving NLP is better and more robust. LR algorithm for ML shows better results as compared to SVM for performing detection based on the generated dataset.


Abstract:In recent years Natural language processing is one of the most active areas of research especially with the emergence of deep learning algorithms. More attention has been given to Latin descendent languages e.g English, French, and Spanish given the availability of high-quality datasets and compute resources. In this paper, we present a moroccan News Articles Corpus collected from four of the major moroccan news websites. The corpus contains more than 418k news articles corresponding to 19 different categories, thus considered to be one of the largest Arabic news articles corpora. A description of the collection and processing steps were presented and exploration analysis was performed. To prove the utility of the dataset. An evaluation step was conducted in the context of text classification using four different Machine Learning baselines: Random Forest (RF), Multinomial Naive Bayes (MNB), Support Vector Machine (SVC), and Gradient Boosting (GradBoost) Classifiers. The experimental results are presented in terms of accuracy, F1-score, and confusion matrix.



A context-related vocabulary trainer in the integrated intelligent computer-assisted language learning (iiCALL) environment

3CAP: categorizing the cognitive capabilities of Alzheimer’s patients in a smart home environment
Alzheimer’s disease is a progressive illness that affects more than 5.5 million people in the United States with no effective cure or treatment. Symptoms of the disease include declines in memory and speech abilities and increases in aggression and insomnia. Recent research suggests that NLP techniques can detect early cognitive decline as well as monitor the rate of decline over time. The processed data can be used in a smart home environment to enhance the level of home care for Alzheimer’s patients. This paper proposes early-stage research in software engineering and natural language processing for quantifying and evaluating the patient’s cognitive state to determine the required level of support in a smart home.

Abstract:Automated text information extraction from cancer pathology reports is an active area of research to support national cancer surveillance. A well-known challenge is how to develop information extraction tools with robust performance across cancer registries. In this study we investigated whether transfer learning (TL) with a convolutional neural network (CNN) can facilitate cross-registry knowledge sharing. Specifically, we performed a series of experiments to determine whether a CNN trained with single-registry data is capable of transferring knowledge to another registry or whether developing a cross-registry knowledge database produces a more effective and generalizable model. Using data from two cancer registries and primary tumor site and topography as the information extraction task of interest, our study showed that TL results in 6.90% and 17.22% improvement of classification macro F-score over the baseline single-registry models. Detailed analysis illustrated that the observed improvement is evident in the low prevalence classes.


Automatic generation of narrative content for digital games
Abstract:Interactive simulation games used for training usually require a large amount of coherent narrative content. An effective and efficient solution to the narrative content creation problem is to use Natural Language Generation (NLG) systems. The use of NLG systems, however, requires sophisticated linguistic and sometimes programming knowledge. For this reason, NLG systems are typically not accessible to the game designers who write narrative content. We have designed and implemented a visual environment for creating and modifying NLG templates that requires no programming knowledge, and can operate with a minimum of linguistic knowledge. It allows specifying templates with any number of variables and dependencies between them. It automatically generates all the sentences that follow the created template. It uses SimpleNLG to provide the linguistic background knowledge. We tested the performance of our system in the context of an interactive simulation game.

Abstract:Lack of Information Communication Technology (ICT) knowledge and the cost have been identified as challenges to Small and Medium Enterprises (SMEs) to adapt ICT. However there are tools and systems freely available to generate information systems automatically, but those require the database structure of the system. Database conceptualization based on the system requirement specification in natural language (SRS-NL) is the most significant landmark in the process of database design. Therefore it is desirable to have a tool, so that the non-technical people in SMEs could use to generate a quality conceptual database model (CDM) based on NL-SRS automatically. Comprehensive literature survey was conducted and evaluated the identified tools by analyzing usability, affordability and quality of the outcomes. Analysis showed that some limitations of the tools which cannot be used by non-technical people in SMEs.


A Multimodal Deep Framework for Derogatory Social Media Post Identification of a Recognized Person
In today’s era of digitization, social media platforms play a significant role in networking and influencing the perception of the general population. Social network sites have recently been used to carry out harmful attacks against individuals, including political and theological figures, intellectuals, sports and movie stars, and other prominent dignitaries, which may or may not be intentional. However, the exchange of such information across the general population inevitably contributes to social-economic, socio-political turmoil, and even physical violence in society. By classifying the derogatory content of a social media post, this research work helps to eradicate and discourage the upsetting propagation of such hate campaigns. Social networking posts today often include the picture of Memes along with textual remarks and comments, which throw new challenges and opportunities to the research community while identifying the attacks. This article proposes a multimodal deep learning framework by utilizing ensembles of computer vision and natural language processing techniques to train an encapsulated transformer network for handling the classification problem. The proposed framework utilizes the fine-tuned state-of-the-art deep learning-based models (e.g., BERT, Electra) for multilingual text analysis along with face recognition and the optical character recognition model for Meme picture comprehension. For the study, a new Facebook meme-post dataset is created with recorded baseline results. The subject of the created dataset and context of the work is more geared toward multilingual Indian society. The findings demonstrate the efficacy of the proposed method in the identification of social media meme posts featuring derogatory content about a famous/recognized individual.

Abstract:This paper presents a Bangla corpus specifically targeted for sentiment analysis and made available to researchers under an open-source licensing scheme
1

Knowledge Graph based Learning Guidance for Cybersecurity Hands-on Labs
Hands-on practice is a critical component of cybersecurity education. Most of the existing hands-on exercises or labs materials are usually managed in a problem-centric fashion, while it lacks a coherent way to manage existing labs and provide productive lab exercising plans for cybersecurity learners. With the advantages of big data and natural language processing (NLP) technologies, constructing a large knowledge graph and mining concepts from unstructured text becomes possible, which motivated us to construct a machine learning based lab exercising plan for cybersecurity education. In the research presented by this paper, we have constructed a knowledge graph in the cybersecurity domain using NLP technologies including machine learning based word embedding and hyperlink-based concept mining. We then utilized the knowledge graph during the regular learning process based on the following approaches: 1. We constructed a web-based front-end to visualize the knowledge graph, which allows students to browse and search cybersecurity-related concepts and the corresponding interdependence relations; 2. We created a personalized knowledge graph for each student based on their learning progress and status; 3. We built a personalized lab recommendation system by suggesting more relevant labs based on students' past learning history to maximize their learning outcomes. To measure the effectiveness of the proposed solution, we have conducted a use case study and collected survey data from a graduate-level cybersecurity class. Our study shows that, by leveraging the knowledge graph for the cybersecurity area study, students tend to benefit more and show more interests in cybersecurity area.

Abstract:We present an analysis of the problem of identifying biological context and associating it with biochemical events described in biomedical texts. This constitutes a non-trivial, inter-sentential relation extraction task. We focus on biological context as descriptions of the species, tissue type, and cell type that are associated with biochemical events. We present a new corpus of open access biomedical texts that have been annotated by biology subject matter experts to highlight context-event relations. Using this corpus, we evaluate several classifiers for context-event association along with a detailed analysis of the impact of a variety of linguistic features on classifier performance. We find that gradient tree boosting performs by far the best, achieving an F1 of 0.865 in a cross-validation study.


Semantic frames as an anchor representation for sentiment analysis
Current work on sentiment analysis is characterized by approaches with a pragmatic focus, which use shallow techniques in the interest of robustness but often rely on ad-hoc creation of data sets and methods. We argue that progress towards deep analysis depends on a) enriching shallow representations with linguistically motivated, rich information, and b) focussing different branches of research and combining ressources to create synergies with related work in NLP. In the paper, we propose SentiFrameNet, an extension to FrameNet, as a novel representation for sentiment analysis that is tailored to these aims.

Diagnosing meaning errors in short answers to reading comprehension questions
A common focus of systems in Intelligent Computer-Assisted Language Learning (ICALL) is to provide immediate feedback to language learners working on exercises. Most of this research has focused on providing feedback on the form of the learner input. Foreign language practice and second language acquisition research, on the other hand, emphasizes the importance of exercises that require the learner to manipulate meaning. The ability of an ICALL system to diagnose and provide feedback on the meaning conveyed by a learner response depends on how well it can deal with the response variation allowed by an activity. We focus on short-answer reading comprehension questions which have a clearly defined target response but the learner may convey the meaning of the target in multiple ways. As empirical basis of our work, we collected an English as a Second Language (ESL) learner corpus of short-answer reading comprehension questions, for which two graders provided target answers and correctness judgments. On this basis, we developed a Content-Assessment Module (CAM), which performs shallow semantic analysis to diagnose meaning errors. It reaches an accuracy of 88% for semantic error detection and 87% on semantic error diagnosis on a held-out test data set.


Chatbot User Interface for Customer Relationship Management using NLP models


Implementing public health analytical services: Grid enabling of MetaMap

Abstract:Considering the wide spectrum of both practical and research applicability, opinion mining has attracted increased attention in recent years. This article focuses on breaking the domain-dependency barrier which occurs in supervised opinion mining strategies by using a semi-supervised approach, which ensures domain independence. Our work devises a generalized methodology by considering a set of grammar rules for identification of the opinion bearing words. We focus on tuning our method for the best tradeoff between precision and recall and time. Moreover, as the seed words are not specific to a given domain, we claim again that the approach is domain independent.


The Effect of POS Tag Information on Sentence Boundary Detection in Turkish Texts
Abstract:Recently, Natural language processing (NLP) applications have been crucial by the increase in the amount of digitized written and oral text documents. As sentence boundary detection is the first step of most of the NLP applications, it has high importance. In this study, the effects of using POS (Part-of-Speech) tags on the performance of machine learning methods based sentence boundary detection from Turkish texts have been studied. To reach our goal, a dataset which contains 30000 instances such that 15000 of them are sentences and the remaining 15000 instances are non-sentence samples has been drawn from a subset of TNC (Turkish National Corpus). The sub-corpus has 10.000.000 words in total, and to develop the dataset, the characters which may represent the end of a sentence are searched from the sub-corpus, then the text is divided into pieces from these characters. Each piece is checked manually to label as sentence and non-sentence, and randomly 30000 instances are selected to form the dataset. Each instance in the dataset is converted to a vector by using total 9 attributes that are used in the rule-based sentence boundary detection studies and proposed in this study. After that two more attributes that are POS tags of the terms before and after the haracter that may represent the end of the sentence are included to the attribute set, and then the dataset is again converted to vectors by using these 11 attributes. The two datasets are classified by using Back Propagation Neural Network, RBF Network, Naive Bayes classifier, Decision Tree, and Support Vector Machines to evaluate the performance of supervised learning methods on the sentence boundary detection. After the experimental evaluation we observed that, when POS tags are included, success of sentence boundary detection increases for all classifiers, and the most successful classifier is decision tree with classification accuracy which is improved from 84.7% to 86.2% when POS tags are considered.


Sixth workshop on exploiting semantic annotations in information retrieval (ESAIR'13)


Identifying assertions in text and discourse: the presentational relative clause construction

Abstract:This paper presents a portable phenotyping system that is capable of integrating both rule-based and statistical machine learning based approaches. Our system utilizes UMLS to extract clinically relevant features from the unstructured text and then facilitates portability across different institutions and data systems by incorporating ODHSI's OMOP Common Data Model (CDM) to standardize necessary data elements. Our system can also store the key components of rule-based systems (e.g., regular expression matches) in the format of OMOP CDM, thus enabling the reuse, adaptation and extension of many existing rule-based clinical NLP systems. We experimented our system on the corpus from i2b2's Obesity Challenge as a pilot study. Our system facilitates portable phenotyping of obesity and its 15 comorbidities based on the unstructured patient discharge summaries, while achieving a performance that often ranked among the top 10 of the challenge participants. This standardization enables a consistent application of numerous rule-based and machine learning based classification techniques downstream.


Abstract:For the data-driven approach in Natural Language Processing (NLP) applications, good quality linguistic resources considered as a main factor to obtain good results. Although Arabic language is one of the main languages in the world, it is considered as low-resourced language in term of good quality and free linguistic resources. This work presents the first stage of building a new open source dependency treebank for Arabic language. It describes the prototype of the new dependency treebank that are inspired by Lexical Functional Grammar (LFG). This paper shows a main approach of developing a newly treebank and put lines the future work needed to complete this novel linguistic resource.


Ten Social Dimensions of Conversations and Relationships
Decades of social science research identified ten fundamental dimensions that provide the conceptual building blocks to describe the nature of human relationships. Yet, it is not clear to what extent these concepts are expressed in everyday language and what role they have in shaping observable dynamics of social interactions. After annotating conversational text through crowdsourcing, we trained NLP tools to detect the presence of these types of interaction from conversations, and applied them to 160M messages written by geo-referenced Reddit users, 290k emails from the Enron corpus and 300k lines of dialogue from movie scripts. We show that social dimensions can be predicted purely from conversations with an AUC up to 0.98, and that the combination of the predicted dimensions suggests both the types of relationships people entertain (conflict vs. support) and the types of real-world communities (wealthy vs. deprived) they shape.

Fake News Detection Using Machine Learning
Fake news has been an issue in every generateon. As the technology evolves, the problem of detecting accurate data for the unreliable news evolves with it and its resolution becomes more significant. This paper explores various feature sets, wherein two new features are introduced to develop an automated fake news detector on news articles. Experiment obtained 96.60% on using XGBoost that has been noted to be comparable to existing works.

A generalised hybrid architecture for NLP
Many tasks in natural language processing require that sentences be classified from a set of discrete interpretations. In these cases, there appear to be great benefits in using hybrid systems which apply multiple analyses to the test cases. In this paper, we examine a general principle for building hybrid systems, based on combining the results of several, high precision heuristics. By generalising the results of systems for sentiment analysis and ambiguity recognition, we argue that if correctly combined, multiple techniques classify better than single techniques. More importantly, the combined techniques can be used in tasks where no single classification is appropriate.

Abstract:Traditional analysis for logs generated by wireless telecommunication systems requires a priori knowledge to explore the common patterns and structures. To overcome this difficulty, this paper presents a new method to analyze logs. We first implement natural language processing (NLP) techniques and machine learning (ML) models to process logs and directly vectorize logs. Then we train a classification model to determine the states of logs. Experiments are carried out to confirm the effectiveness of our method.


Interpretable Sentiment Analysis based on Deep Learning: An overview
Abstract:Sentiment analysis (SA) or emotion AI or opinion mining uses natural language processing (NLP). Sentiment Analysis identify, study, quantify, obtain, tacit states and subject related information. Broad spectrum of areas influenced due to Sentiment Analysis such as policy making by the government, finding mental health of individuals, finding misuse of drugs in healthcare, fraud detection in the financial sector, covid-19 awareness and impact, Cyber-crime etc. As the amplitude of social media data increases day by day, there is a need to automatically address sentiment analysis. Deep learning handles it very well. It gives very good accuracy but incomprehensibility in decision strategy. For better decision-making trust, believe, fairness, reliability, and unbiasing is important. This paper explores the work done in this area along with popular techniques to address interpretability in sentiment analysis and its evaluation criteria.

Abstract:Sarcasm is a sharp and often ironic utterance that is meant to convey contempt or mock. In today's world, one of the challenging problems for opinion mining task is sarcasm detection. Many researchers are exploring the properties of sarcasm like semantic properties, syntactical properties, lexical feature etc, to design algorithms for sarcasm detection. We aim at using a recurrent neural network (RNN) model for sarcasm detection because it automatically extracts features required for machine learning approaches. Along with the recurrent neural network, this model also uses long short-term memory (LSTM) cells on tensorflow to capture syntactic and semantic information over Twitter tweets to detect sarcasm. Finally, we present the result of this model and a statistical overview of the dataset.



Unified expectation maximization

Abstract:Machine Learning and NLP (Natural Language Processing) have aided the development of new and improved user experience features in many applications. We address the problem of automatically identifying the "Start Reading Location" (SRL) of eBooks, i.e. the location of the logical beginning or start of main content. This improves eBook reading experience by taking users automatically to the logical start location without requiring them to flip through several front-matter sections such as "Dedication" and "About the Author". Automatic identification of SRL is complex since many eBooks do not adhere to any well-defined convention with respect to section naming, formatting and layout patterns. We formulate SRL as a classification problem based on detailed rule-based and NLP-based classification schemes. Our models are being used in production for Kindle eBooks and have led to a 400% increase in coverage (number of books which had SRL stamped) compared to what could be achieved earlier through an entirely manual process, while also maintaining a high accuracy of 95%.


Abstract:An English-language fragment L/sub NS/, which is used for writing program specifications, is defined using the algebraic language ASL/*. In order to define the semantics of L/sub NS/, the translation from each sentence in L/sub NS/ into a formula in a formal system is defined. If a sentence s in L/sub NS/ is translated into a formula t, then the semantics of s is defined to be the semantics of t in the system. The definition of nonlogical words such as nouns and verbs are given as axioms. A processing system for natural-language specifications that is based on this method has been implemented. Since the number of nonlogical words is very large and their semantics depend on problem domains, it seems to be difficult to define the semantics of all the words from the beginning. The system not only translates specifications into formulas but also supports dictionary expansion. Several specifications have been analyzed using the system, and the nonlogical words have been defined and stored in the dictionary.<
>View less

CHI 99 special interest group on natural language in computer-human interaction
With the growing interest in human-computer interfaces that use natural language in some way, researchers and practitioners who work on these interfaces are finding that two general fields of research, CHI and natural language processing (NLP), are complementary and converging. In the CHI research community, there have been investigations on a number of related issues such as usability of text and graphics in on-line documentation, hypertext, spoken-dialogue interfaces, and language/audio resources. In the NLP research community, there is increasing interest in use of natural language, both spoken and written, in intelligent multimodal and multimedia interfaces, e.g., International Symposium on Spoken Dialogue (ISSD-96), COOP 98 Workshop on The Use of Herbert H. Clark's Models of Language Use for the Design of Cooperative Systems, 1998 AAAI Workshop on Representations for Multi-Modal Human-Computer Interaction, and Coling-ACL'98 Workshop on Content Visualizations and Intermedia Representations (CVIR'98).Some technical issues of possible interest to both communities are:1. For what and under what conditions is NL effective in the human-computer interface? For what types of tasks or communication? How does modality influence its effectiveness? How does its effectiveness in computer media differ from that in traditional forms of communication such as face-to-face conversation and print media? How do performance limitations of NLP technologies (e.g., .speech recognition errors) influence effectiveness?2. What are the critical technical requirements for NLP to be effective in the human-computer interface, e.g., coordination of generated text and graphics, incremental and robust interpretation, and modeling turn-taking and initiative in dialogue? What technical requirements arise in transferring technology developed for one language to systems for users of another language (e.g., languages using different writing systems)? How should effectiveness of NLP technologies be evaluated?

Automatic detection of tags for political blogs
This paper describes a technique for automatically tagging political blog posts using SVM's and named entity recognition. We compare the quality of the tags detected by this approach to earlier approaches in other domains, observing effects from the political domain and benefits from NLP techniques complementary to the core SVM method.

Measuring topic homogeneity and its application to dictionary-based word sense disambiguation
The use of topical features is abundant in Natural Language Processing (NLP), a major example being in dictionary-based Word Sense Disambiguation (WSD). Yet previous research does not attempt to measure the level of topic cohesion in documents, despite assertions of its effects. This paper introduces a quantitative measure of Topic Homogeneity using a range of NLP resources and not requiring prior knowledge of correct senses. Evaluation is performed firstly by using the WordNet::Domains package to create word-sets with varying levels of homogeneity and comparing our results with those expected. Additionally, to evaluate each measure's potential value, the homogeneity results are correlated against those of 3 co-occurrence/dictionary-based WSD techniques, tested on 1040 Semcor and SENSEVAL sub-documents. Many low-moderate correlations are found to exist with several in the moderate range (above .40). These correlations surpass polysemy and senseentropy, the 2 most cited factors affecting WSD. Finally, a combined homogeneity measure achieves correlations of up to .52.

Boosting performance of gene mention tagging system by classifiers ensemble
Abstract:To further improve the tagging performance of single classifiers, a classifiers ensemble experimental framework is presented for gene mention tagging. In the framework, six classifiers are constructed by four toolkits (CRF++, YamCha, Maximum Entropy (ME) and MALLET) with different training methods and feature sets and then combined with a two-layer stacking algorithm. The recognition results of different classifiers are regarded as input feature vectors to be incorporated, and then a high-powered model is obtained. Experiments carried out on the corpus of BioCreative II GM task show that the classifiers ensemble method is effective and our best combination method achieves an F-score of 88.09%, which outperforms most of the top-ranked Bio-NER systems in the BioCreAtIvE II GM challenge.


Augean Artificial Neural Network


A statistical method for Uyghur tokenization

News topic detection based on hierarchical clustering and named entity
Abstract:News topic detection is the process of organizing news story collections and real-time news/broadcast streams into news topics. While unlike the traditional text analysis, it is a process of incremental clustering, and generally divided into retrospective topic detection and online topic detection. This paper considers the feature changes of modern news data experienced from the past, and presents a new topic detection strategy based on hierarchical clustering and named entities. Topic detection process is also divided into retrospective and online steps, and named entities in the news stories are employed in the topic clustering algorithm. For the online step's efficiency and precision, this paper first clusters news stories in each time window into micro-clusters, and then extracts three representation vectors for each micro-cluster to calculate the similarity to existing topics. The experimental results show remarkable improvement compared with recently most applied topic detection method.

Report on the fourth workshop on exploiting semantic annotations in information retrieval (ESAIR'11)
There is an increasing amount of structure on the Web as a result of modern Web languages, user tagging and annotation, and emerging robust NLP tools. These meaningful, semantic, annotations hold the promise to significantly enhance information access, by increasing the depth of analysis of today's systems. Currently, we have only started to explore the possibilities and only begun to understand how these valuable semantic cues can be put to fruitful use. The workshop had an interactive format consisting of keynotes, boasters and posters, breakout groups and reports, and a final discussion, which was prolonged into the evening. There was a strong feeling that we made substantial progress. Specifically, each of the breakout groups contributed to our understanding of the way forward. First, annotations and use cases come in many different shapes and forms depending on the domain at hand, but at a higher level there are remarkable commonalities in annotation tools, indexing methods, user interfaces, and general methodology. Second, we got insights in the "exploitation" aspects, leading to a clear separation between the low-level annotations giving context or meaning to small units of information (e.g., NLP, sentiments, entities), and annotations bringing out the structure inherent in the data (e.g., sources, data schema's, document genres). Third, the plan to enrich ClueWeb with various document level (e.g., pagerank and spam scores, but also reading level) and lower level (e.g., named entities or sentiments) annotations was embraced by the workshop as a concrete next step to promote research in semantic annotations.


Automatic identification of assamese and bodo multiword expressions

Abstract:Bug reports are a popular target for natural language processing (NLP). However, bug reports often contain artifacts such as code snippets, log outputs and stack traces. These artifacts not only inflate the bug reports with noise, but often constitute a real problem for the NLP approach at hand and have to be removed. In this paper, we present a machine learning based approach to classify content into natural language and artifacts at line level implemented in Python. We show how data from GitHub issue trackers can be used for automated training set generation, and present a custom preprocessing approach for bug reports. Our model scores at 0.95 ROC-AUC and 0.93 F1 against our manually annotated validation set, and classifies 10k lines in 0.72 seconds. We cross evaluated our model against a foreign dataset and a foreign R model for the same task. The Python implementation of our model and our datasets are made publicly available under an open source license.


Performance of QAM signals in optical fiber links in the presence of nonlinear phase noise
Abstract:The amplified spontaneous emission (ASE) noise of optical amplifiers and nonlinear phase (NLP) noise generated due to interaction of ASE noise and Kerr nonlinearity are the two main sources of noise in optical fiber communications systems. This paper presents an analysis of the effects of ASE noise and NLP noise on the symbol error rate performance of M-ary quadrature amplitude modulated (QAM) signals with rectangular signal constellations. The analysis is carried out assuming that ASE noise and NLP noise are statistically independent, and approximating the probability density function (pdf) of NLP noise by a Gaussian pdf. An expression for the symbol error probability conditional on the NLP noise is derived and then averaging over NLP noise is carried out using numerical integration. Numerical results for the average symbols error probability are presented for M=4, 16, 64, and 256. The comparison of error probabilities with the simulated results shows that Gaussian approximation is sufficiently accurate to predict the error probabilities for QAM signals, when NLP noise is treated as an independent process.

Knowledge Base Collecting Using Natural Language Processing Algorithms
Abstract:Natural language processing (NLP) is one of the most complicated and fast developing area in Computer Science. There are solutions in this area for special cases, but developing one general solution is impossible due to variety of grammatical, syntactic and semantic forms in different languages. The NLP algorithms and methods are used in speech recognition, text analyzing and understanding, speech generation. This paper is focused on application of NLP approaches to understand quasi-structured or unstructured data with subsequent inclusion in a knowledge base. The article covers the usage of a graph database as a knowledge base, that allows to show and visualize relationships between different pieces of text according to specified data patterns.

Sentiment Analysis of Tamil-English Code-Switched Text on Social Media Using Sub-Word Level LSTM
Abstract:Social media are the ultimate platforms to express the opinion and to facilitate the creation and sharing of information, ideas, career interests and other forms of expression via virtual communities and networks. Analysing the sentiment features in these ideas in the public posts of social media users will lead to building more accurate behavioural patterns. Importance of these behavioural patterns with respect to the marketing and business perspective has been focused here. When considering the traditional Facebook marketing platform, efficiency and effectiveness of the marketing are very low since the advertisers do not happen to have a proper understanding of the customers that they should address. Thus, to overcome this issue, a system is proposed to identify the behavioural patterns of Facebook users by analysing their social media contents such as posts, comments, interactions, and also reviews and critics on products to enhance the effectiveness of the Facebook marketing. This system mainly focuses on Facebook users in Sri Lanka. Natural language processing is used to process text-based posts (uploaded and shared) and comments of users in order to build a behavioural profile for the users. This system process text data which is composed by using both English and Tamil languages, in code-switching language pattern.

Extraction and Classification of Semantic Data from Twitter
Twitter is a social network and microblogging service in which registered users read and post messages called Tweets. Tweets have a maximum of 280 characters and cover every conceivable subject, from simple activity updates and news coverage to opinions about arbitrary topics. In this way, Twitter emerges as a valuable data source to get information about what people think and feel about the most different subjects. In this context, this work presents different approaches for extracting and processing information from Twitter using Natural Language Processing (NLP) and Machine Learning techniques, examining tools and methods to collect and analyze semantic information from Tweets.

An integrated development environment for spoken dialogue systems
Development environments for spoken dialogue processing systems are of particular interest because the turn-around time for a dialogue system is high while at the same time a considerable amount of components can be reused with little or no modifications. We describe an Integrated Development Environment (IDE) for spoken dialogue systems. The IDE allows application designers to interactively specify reusable building blocks called dialogue packages for dialogue systems. Each dialogue package consists of an assembly of data sources, including an object-oriented domain model, a task model and grammars. We show how the dialogue packages can be specified through a graphical user interface with the help of a wizard.

Abstract:In the increasingly digitized and connected world, advantages also bring equally important challenges that often require new thinking and approach. One of the primary challenges is data that is being generated in huge quantity at enormous pace and in variety of forms. Social medium has blow up as a sort of online discussion where people create and share the information at a substantial rate. The outlook of social media user is now noticeably used for taking the decisions. Evaluating sentiments, opinions and emotions by a group of people in the form of reviews pertaining to a certain event can carve out the niche of providing insights into the review text analytics, especially when text data is large. On one hand sentiments shows the conformity, disparity or objectivity among the masses whereas on the other hand emotions coming out from text clusters the group feedback. In this paper, the authors analyses the sentiments of patients pertaining to ayurvedic spa treatment. User reviews from the top rated websites are extracted to obtain the sentiments. The authors explore the possible ways to analysis the user sentiments using NLTK and Python programming libraries.


Abstract:In this paper, we applied a statistical machine translation (SMT) approach to generate Burmese paraphrases of input sentences and words in Burmese. The system trained 89K sentence pairs that are manually collected from Facebook Comments and daily conversation corpus and also 89K Burmese Paraphrase Words are collected from Burmese Wiktionary. We implemented three different statistical machine translation models; phrase-based, hierarchical phrase based, and the operation sequence model. Moreover, we used two segmentation units; character and syllable segmentation for comparing the machine translation performance. The performance of machine translation or paraphrase generation was measured in terms of BLEU, RIBES, chrF++, and WER scores for all experiments. However, automatic evaluation metrics are weak for judging whether the generated Burmese sentences and words “is a paraphrase” or “is not a paraphrase': And thus, we also conducted a human evaluation on both sentence-to-sentence and word-toword paraphrase generation results. We found that the results obtained using the BLEU and RIBES automatic evaluation metrics were misleading and as the human evaluation result the machine translation approach is suitable for Burmese paraphrase generation.


Arabic Terminology Extraction and Enrichment Based on Domain-Specific Text Mining
Abstract:Information Retrieval (IR) involves several disciplines to improve the search quality. In this paper, we focus on terminology extraction which includes critical tasks especially for highly ambiguous languages like Arabic. Properly retrieving a list of relevant terms for a given domain and enrich it is a persistent problem. Therefore, and in order to improve the coverage of terminology extraction and enrichment, we must strengthen our research by text-mining technologies based on well-founded methods. In this article, we present a new Arabic terminology extraction and enrichment approach which exploits the corpus structure as a first step to extract a minimal terminology and uses text-mining techniques to enrich it in a second step.


ReSCo-CC: Unsupervised Identification of Key Disinformation Sentences

Aletheia: A Fake News Detection System for Hindi
“Fake News” and Misinformation can have far-reaching negative social impacts. Scalable fake news classification techniques for resource-poor languages such as Hindi are in their infancy due to the lack of data sets and lack of robust NLP libraries in these languages. We present Aletheia, a Fake News classification system for Hindi. We curate a dataset of approximately 13,000 news articles by media organizations that flag authentic and fake news. We present preliminary results using several Machine Learning models on this dataset. We also developed a system accessible over the web (http://responsible-tech.bits-hyderabad.ac.in/aletheia/demo/) using which users can test if a given piece of news is fake or authentic. We also use the website to collect crowd-sourced labelled news data and present additional information on the dataset and the models to the users.

Predicting health patterns using sensor sequence similarity and NLP
Abstract:Health information technology has been used in long-term care to improve outcomes and reduce cost. In Tiger Pace, an aging in place facility from Columbia, MO, we deployed sensor networks together with an electronic health record (EHR) system to provide early illness recognition. In this paper, we describe a methodology for early illness based on non-wearable sensor data and concepts extracted from nursing notes using Natural Language Processing (NLP). The methodology is inspired from genomie sequence annotation using BLAST. First, we extract a set of Unified Medical Language System (UMLS) concepts from each nursing note using Metamap, a NLP tool provided by UMLS. Then, we associate each daily sensor sequence with the medical concepts related to the nursing notes issued that day for that patient. Finally, to infer the health concepts for an unknown day, we compute the similarity between its sensor sequence and those available in the database. The challenges presented by this method are finding the most suitable multi-attribute time sequence similarity and aggregation of the retrieved concepts. On a pilot dataset from three Tiger Place residents, with a total of 1685 sensor days and 358 nursing records, we obtained an average precision of 0.34 and a recall of 0.52.

Automatic Recognition of Chinese Organization Name Based on Conditional Random Fields
Abstract:Person, location and organization have been always mentioned as a bottleneck of a named entity recognition (NER) system. Automatic recognition of Chinese organization name is the most difficult problem in NER tasks. This paper presents a new approach of Chinese organization name recognition based on cascaded conditional random fields. In the proposed approach, we first recognize the person name and location name before recognizing organization. The model structure has been designed with the cascade way, the result then is passed to the high model and suppose the decision of high model for recognition of the complicated organization names. And we proposed the new feature to realize this task. We evaluate our approach on large-scale corpus with open test method using People's Daily (January. 1998). Chinese ORG recalling rate achieves 88.78% and the precision rate is 82.35%. The evaluation results show that our approach based on cascaded conditional random fields significantly outperforms previous approaches.

Abstract:NeuroinformaticsNatural Language Processing (NeuroNLP) relies on clustering and classification for information categorization of biologically relevant extraction targets and for interconnections to knowledge-related patterns in event and text mined datasets. The accuracy of machine learning algorithms depended on quality of text-mined data while efficacy relied on the context of the choice of techniques. Although developments of automated keyword extraction methods have made differences in the quality of data selection, the efficacy of the Natural Language Processing (NLP) methods using verified keywords remain a challenge. In this paper, we studied the role of text classification and document clustering algorithms on datasets, where features were obtained by mapping to manually verified MESH terms published by National Library of Medicine (NLM). In this study, NLP data classification involved comparing 8techniques and unsupervised learning was performed with 6 clustering algorithms. Most classification techniques except meta-based algorithms namely stacking and vote, allowed 90% or higher training accuracy. Test accuracy was high (=>95%) probably due to limited test dataset. Logistic Model Trees had 30-fold higher runtime compared to other classification algorithms including Naive Bayes, AdaBoost, Hoeffding Tree. Grouped error rate in clustering was 0-4%. Runtime-wise, clustering was faster than classification algorithms on MESH-mapped NLP data suggesting clustering methods as adequate towards Medline-related datasets and text-mining big data analytic systems.


R2DE: a NLP approach to estimating IRT parameters of newly generated questions
The main objective of exams consists in performing an assessment of students' expertise on a specific subject. Such expertise, also referred to as skill or knowledge level, can then be leveraged in different ways (e.g., to assign a grade to the students, to understand whether a student might need some support, etc.). Similarly, the questions appearing in the exams have to be assessed in some way before being used to evaluate students. Standard approaches to questions' assessment are either subjective (e.g., assessment by human experts) or introduce a long delay in the process of question generation (e.g., pretesting with real students). In this work we introduce R2DE (which is a Regressor for Difficulty and Discrimination Estimation), a model capable of assessing newly generated multiple-choice questions by looking at the text of the question and the text of the possible choices. In particular, it can estimate the difficulty and the discrimination of each question, as they are defined in Item Response Theory. We also present the results of extensive experiments we carried out on a real world large scale dataset coming from an e-learning platform, showing that our model can be used to perform an initial assessment of newly created questions and ease some of the problems that arise in question generation.


Research on Adaptive Step Decoding in Segment-based LVCSR

Experiments with artificially generated noise for cleansing noisy text
Recent works show that the problem of noisy text normalization can be treated as a machine translation (MT) problem with convincing results. There have been supervised MT approaches which use noisy-regular parallel data for training an MT model, as well as unsupervised models which learn the translation probabilities in alternative ways and try to mimic the MT-based approach. While the supervised approaches suffer from data annotation and domain adaptation difficulties, the unsupervised models lack a holistic approach catering to all types of noise. In this paper, we propose an algorithm to artificially generate noisy text in a controlled way, from any regular English text. We see this approach as an alternative to the unsupervised approaches while getting the advantages of a parallel corpus based MT approach. We generate parallel noisy text from two widely used regular English datasets and test the MT-based approach for text normalization. Semi-supervised approaches were also tried to explore different ways of improving the parallel corpus (manually annotated) based MT approach by using the generated noisy text. An extensive analysis based on comparison of our approaches with both the supervised as well as unsupervised approaches is presented.

Abstract:In this paper, we present and analyze the results of the application of Arabic query-based text summarization system - AQBTSS - in an attempt to produce a query-oriented summary for a single Arabic document. For this task, we adapted the traditional vector space model (VSM) and the cosine similarity measure to find the most relevant passages extracted form Arabic document to produce a text summary. We aim at using the short summaries in some natural language (NL) tasks such as generating answers for Arabic open domain question answering system (AQAS) as well as experimenting with categorizing Arabic scripts. The obtained results indicate that our simple approach for text summarization is promising.


Abstract:With the increasing popularity and evolution of Computer Supported Collaborative Learning systems, the need for developing a tool that automatically assesses instant messaging conversations has become imperative. The main reasons are the high volume of data and the increased amount of time spent for manually assessing conversations. We propose an automated analysis system based on Natural Language Processing (centered on Latent Semantic Analysis and Social Network Analysis) and optimize its runtime performance by means of distributed computing. Moreover, we provide a unique grading mechanism based on a multilayered architecture and induce an increase of speedup by deploying a Replicated Worker architecture. Load balancing and fault tolerance represent key aspects of this approach, besides the actual increase in performance.


LogEA: Log Extraction and Analysis Tool to Support Forensic Investigation of Linux-based System
Abstract:System logs provide valuable information about the events that are taking place in your operating system. It stores information regarding user actions, kernel events and security-related events. It allows you to see almost any operation performed on your system. While investigating a compromised system, the investigator first examines the log files for abnormal events. The events found in logs can assist the investigator in the construction of a forensic timeline. The forensic timeline gives a summary of the events that took place around the security incident. There are many helpful techniques and tools for building a timeline, but nothing significant for identifying events of interest or abnormalities with limited historical data. This paper presents the LogEA (Log Extraction and Analysis) tool to support the forensic investigation of Linux-based logs. The LogEA tool uses natural language processing with machine learning to identify the log messages with negative sentiment. It considers negative log messages as events of interest within the system. LogEA can perform log extraction, log parsing, log maintenance and log analysis. With static log analysis, it is also helpful for real-time log monitoring, and it aims to assist an investigator by reducing the investigation time of a computer security incident.

Going beyond traditional QA systems: challenges and keys in opinion question answering
The treatment of factual data has been widely studied in different areas of Natural Language Processing (NLP). However, processing subjective information still poses important challenges. This paper presents research aimed at assessing techniques that have been suggested as appropriate in the context of subjective - Opinion Question Answering (OQA). We evaluate the performance of an OQA with these new components and propose methods to optimally tackle the issues encountered. We assess the impact of including additional resources and processes with the purpose of improving the system performance on two distinct blog datasets. The improvements obtained for the different combination of tools are statistically significant. We thus conclude that the proposed approach is adequate for the OQA task, offering a good strategy to deal with opinionated questions.

A Study on Sentiment Analysis of Product Reviews
Abstract:Sentiment analysis is an intellectual way of identifying users' sentiments towards certain entities. Natural Language Processing (NLP) has the process of analyzing sentiments as one of its prominent fields. NLP is a branch of computer science and artificial intelligence that deals with human-computer language interaction. The growth in e-commerce has led to increasing customer reviews about various products which are available on the internet. These product reviews offer a massive amount of valuable information which can be used for various decision- making purposes. Several methods have been developed in recent years in order to accomplish this task. In this paper, we discuss various levels of sentiment analysis followed by comparison among different approaches to sentiment analysis.

Why will my question be closed?: NLP-based pre-submission predictions of question closing reasons on stack overflow
Closing a question on a community question answering forum such as Stack Overflow is a highly divisive event. On one hand, moderation is of crucial importance in maintaining the content quality indispensable for the future sustainability of the site. On the other hand, details about the closing reason might frequently appear blurred to the user, which leads to debates and occasional negative behavior in answers or comments. With the aim of helping the users compose good quality questions, we introduce a set of classifiers for the categorization of Stack Overflow posts prior to their actual submission. Our binary classifier is capable of predicting whether a question will be closed after posting with an accuracy of 71.87%. Additionally, in this study we propose the first multiclass classifier to estimate the exact reason of closing a question to an accuracy of 48.55%. Both classifiers are based on Gated Recurrent Units and trained solely on the pre-submission textual information of Stack Overflow posts.

Language Distance using Common N-Grams Approach
Abstract:Within the framework of historical linguistics, the prevailing view of the linguist experts is that language similarity cannot be measured. However, there has been significant research to attempt to quantify this phenomenon. We extend experiments conducted by Gamallo et al. from 2017 to compare forty-one European languages. We use a distance measure that was proven to work well in authorship attribution tasks. We build a distance network where the nodes represent the languages, and the weighted edges represent distances. We additionally provide statistical analysis that confirms the stability of our method across different experimental setups.

Design of a predictor for COVID-19 misinformation prediction
Abstract:Due to the increase of social media usage, the online sharing of content has been extremely increased. As a result, the spread of misinformation on social media platforms has also increased. To address this issue, we proposed an approach that predicts the news is fake or real. In our approach, we select the top k ranked features through a filter base algorithm and feed them to the classifier. The main objective of this research is to provide two things. First, to provide an approach, which compares the benchmark performance results of the evolutionary detection approach on the Koirala dataset. The second is to build, publicly available dataset through web scraping for the classification of COVID-19 fake news articles. Our method significantly uplifts the F1-score with 14.88 percent for the same number of features selected 605 for the already existing approach. Also, stated the number of features 5000 on which the approach showed the best results with a margin of F1-score of 20.4 percent, respectively. Similarly, on the self-build dataset, this approach also outshines and achieved 99.66 percent of F1-score, respectively. Our experimental results show that our robust approach by comparing with other classifiers and existing approach, Max-Min Ratio (MMR) along with support vector machine (SVM) outperformed on both of these datasets. Hence, feature selection plays a vital role in the performance of the model rather than deeply tuning and training the classifier.

Story assembly in the R2aft dyslexia fluency tutor
To overcome their substantial barriers to fluent reading, students with dyslexia need to be enticed to read more, and to read texts with carefully controlled lexical content. We describe and show examples from a prototype of the new R2aft story assembly engine, which generates an interactive text that has A) variable plot and B) lexical content which is individualized by decoding pattern.


A comparison of windowless and window-based computational association measures as predictors of syntagmatic human associations

Abstract:We present CCRFs (cascaded conditional random fields): a cascaded approach to scale conditional random fields (CRFs) for Chinese POS tagging (labeling). General CRFs worked well on POS tagging, but met difficulty when dealing with a large training dataset and tag set because of high computation cost for training. CCRFs organize all tags in a hierarchy and run CRFs on each node of the hierarchy. In CCRFs framework, similar tags were treated as the same one in an upper layer, while they were distinguished in lower layer with more information from upper layer. The analysis of computation complexity shows that CCRFs can highly reduce both space complexity and time complexity for training. The experiments show that CCRFs can deal with large data sets from SIGHAN 2008 with common PCs, which cannot be handled by general CRFs under the same computational condition. Furthermore, we try to keep the good performance of CCRFs for POS tagging by selecting proper features for hierarchical POS tags clustering and introducing more rich and accurate features in the training and tagging phases. The final results show that CCRFs outperform maximum entropy model on all the three test datasets, and outperform enhanced maximum entropy model on the two of the three datasets.



Enhancing Tokenization by Embedding Romanian Language Specific Morphology

Statistical metaphor processing
Metaphor is highly frequent in language, which makes its computational processing indispensable for real-world NLP applications addressing semantic tasks. Previous approaches to metaphor modeling rely on task-specific hand-coded knowledge and operate on a limited domain or a subset of phenomena. We present the first integrated open-domain statistical model of metaphor processing in unrestricted text. Our method first identifies metaphorical expressions in running text and then paraphrases them with their literal paraphrases. Such a text-to-text model of metaphor interpretation is compatible with other NLP applications that can benefit from metaphor resolution. Our approach is minimally supervised, relies on the state-of-the-art parsing and lexical acquisition technologies distributional clustering and selectional preference induction, and operates with a high accuracy.

Abstract:This contribution demonstrates how to apply concepts of social network analysis on educational data. The main aim of this approach is to provide a deeper insight into the structure of courses and/or other learning units that belong to a given curriculum in order to improve the learning process. The presented work can help us discover communities of similar study disciplines (based on the similarity measures of textual descriptions of their contents), as well as identify important courses strongly linked to others, and also find more independent and less important parts of the curriculum using centrality measures arising from the graph theory and social network analysis.


Concept Driven Search and Visualization System for Exploring Scientific Repositories
In this paper we present a scientific document retrieval system targeted at answering specific information needs for a diverse set of professionals. This became a crying need in the wake of the pandemic when researchers, clinicians, virologists, epidemiologists and health policy makers are regularly exploring a large collection of scientific literature related SARS-COV2 and COVID-19. The system facilitates exploration and easy comprehension of the results at multiple granularity through visual and textual summaries. The system uses novel NLP techniques to extract, aggregate and analyse textual information. The system also creates unique visual components to view key information of interest at individual and aggregate levels. The system is implemented as a web application.

Abstract:Privacy concerns are constantly increasing in different sectors. Regulations such as the EU's General Data Protection Regulation (GDPR) are pressuring organizations to handle the individual's data with reinforced caution. As information systems deal with increasingly large amounts of personal data in essential services, there is a lack of mechanisms to help organizations in protecting the involved data subjects. In this paper, we propose and evaluate the use of Named Entity Recognition as a way to identify, monitor and validate Personally Identifiable Information. In our experiments, we used three of the most well-known Natural Language Processing tools (NLTK, Stanford CoreNLP, and spaCy). First, we assess the effectiveness of the tools with a generic dataset. Then, machine learning models are trained and evaluated with datasets built on data that contain personally identifiable information. The results show that models' performance was highly positive in accurately classifying both generic and more context-specific data. We observe the relationship between the datasets' training size and respective performance and estimate the appropriate size for model training within this context. Furthermore, we discuss how our proposal can effectively act as a Privacy Enhancing Technology as well as the potential risks and associated impacts.



PhraseNet: towards context sensitive lexical semantics

Abstract:When processing text documents in natural language, you may encounter such problem as polysemy of words. If the algorithm determines the terms regardless of the semantic component of the words, considering homonymous words as one term, there is a loss of data. As the analysis of the literature data shows, at present no algorithm can unambiguously divide polysemous words into separate terms by semantic groups. In this paper, we propose a method of eliminating polysemy of words, based on the clustering of word contexts, which will improve the quality of processing text documents in natural language.


Abstract:In this paper, we depict hybrid approach, i.e., combination of rule based approach and machine learning techniques, i.e Conditional Random Fields (CRF) for Named Entity Recognition (NER). The main objective of Named Entity Recognition is to categorize all Named Entities (NE) in a document into predefined classes like Person name, Location name, Organization name. This paper first outlines the Named Entity Recognizer using rule based approach. In this approach we prepared Gazette lists for names of persons, locations and organizations, some suffix and prefix features and dictionary consist of 200000 words to recognize the category of names entities. Further, we used Machine learning technique, i.e., CRF in order to improve the accuracy of the system.



Privacy Preserving Chatbot Conversations


Clinical Trial Information Extraction with BERT

Multimedia Text Summary Generator For Visually Impaired
Abstract:With the advancing methodologies in the field of NLP, text summarization have been evolved to its best since the beginning and is still under research. As the visually impaired have become a part of our society it gives us an opportunity to make use of the technological advances to make their lives easier. As even a normal person cannot sit listening to long audio news and doesn't have time reading long news articles, this gave us an inspiration that led to the idea of our project to generate concise and short summaries for the visually impaired people. This research work attempts to implement NLP summarization techniques to come up with the short summaries for the text extracted from the long audio files. The text from the audio files are to be extracted first using various api's which would be sent to summarize. Additionally this research work also plans to summarize multiple documents to a single short summary by getting its references from different news articles. To make it user friendly we are converting the generated summary back to audio format, which will be easier to listen to anywhere and anytime. It becomes our responsibility to contribute something back to society and this project will make the news reading easier and interesting to the people.

Abstract:Social network, e-commerce sites, blogs are new emerging platforms for people to express their opinion. These sites contain huge amount of text which can be used for different purpose like Sentiment Analysis. Sentiment Analysis is a growing field in natural language processing. Sentiment analysis is major focused on company's improvement. But sentiment analysis can be useful in recommendation system also. Based on various performance measures, this paper compares the results of machine learning algorithms like Multinomial Naive Bayes algorithm, Logistic Regression, SVM Classifier, Decision Tree and Random Forest. These algorithms are used for sentiment analysis of reviews and in turn for product recommendation. In proposed system, Random Forest shows outstanding performance. To create suitable recommendations using the analysis of emotions, there is a need to use polarity obtained through the reviews.


Abstract:Information shared by millions of people in social networks. It is an important way to know the views on political issues, the vision of social problems, the latest trends, commercial websites, etc. In particular, Twitter is an important source of knowledge. It is useful to perform a moral assessment of the service. The classification of emotions on tweets has given a financial and effective form to determine public sentiment. In this research, more than 30,000 location-based tweets are collected from the Twitter page also emotions (emoji‘s) and the abbreviated form of words are used for the sentiment classification by replacing in top form, then perform the opinion classification in 5 categories, extreme positive, positive, neutral, negative and extreme negative classes. Suppliers and shareholders help to take into account the opinions of modes, services, transactions, etc from a particular geographical location. It also performs a logical analysis of many products or services on the same tool.



Sinusoidal + all-pole modification based spectral smoothing for concatenative speech synthesis

Extending the integrated intelligent computer-assisted language learning (iiCALL) environment
The Integrated Intelligent Computer-Assisted Language Learning (iiCALL) environment offers options to learn natural languages with the use of common working environments like Web browsers or e-mail clients. Therefore, we designed a generic data model and developed a software framework handling language learning processes and information exchanges. A corresponding prototype, implemented as Firefox plug-in, shows the applicability of the generic data model to a specific learning scenario within an iiCALL environment. For developers, the paper proves extensibility by describing the framework and the way of extending iiCALL to add new learning scenarios and functionalities.

Tell Them Apart: Distilling Technology Differences from Crowd-Scale Comparison Discussions
Abstract:Developers can use different technologies for many software development tasks in their work. However, when faced with several technologies with comparable functionalities, it is not easy for developers to select the most appropriate one, as comparisons among technologies are time-consuming by trial and error. Instead, developers can resort to expert articles, read official documents or ask questions in Q&A sites for technology comparison, but it is opportunistic to get a comprehensive comparison as online information is often fragmented or contradictory. To overcome these limitations, we propose the diffTech system that exploits the crowdsourced discussions from Stack Overflow, and assists technology comparison with an informative summary of different comparison aspects. We first build a large database of comparable software technologies by mining tags in Stack Overflow, and locate comparative sentences about comparable technologies with NLP methods. We further mine prominent comparison aspects by clustering similar comparative sentences and represent each cluster with its keywords. The evaluation demonstrates both the accuracy and usefulness of our model and we implement a practical website for public use.

An improved method of keywords extraction based on short technology text
Abstract:Keywords are the critical resources of information management and retrieval, automatic text classification and clustering. The keywords extraction plays an important role in the process of constructing structured text. Current algorithms of keywords extraction have matured in some ways. However the errors of word segmentation which caused by unknown words have been affected the performance of Chinese keywords extraction, particularly in the field of technological text. In order to solve the problem, this paper proposes an improved method of keywords extraction based on the relationship among words. Experiments show that the proposed method can effectively correct the errors caused by segmentation and improve the performance of keywords extraction, and it can also extend to other areas.

Computer-aided generation of multiple-choice tests
This paper describes a novel computer-aided procedure for generating multiple-choice tests from electronic instructional documents. In addition to employing various NLP techniques including term extraction and shallow parsing, the program makes use of language resources such as a corpus and WordNet. The system generates test questions and distractors, offering the user the option to post-edit the test items.

Abstract:The evolution of the Arabic language from antiquity to the present days has given birth to several linguistic registers ascribed to the great periods of the history of the Arabic language. They can be classified as: Old Arabic, Classical Arabic and Modern Standard Arabic. In this work, we propose a method that aims to disambiguate words in Modern Standard Arabic. This method consists of measuring the semantic relation between the context of use of the ambiguous word and its sense definitions. Within the context of creating a historical dictionary for Arabic, and to disambiguate a word, we need to take into consideration the historical period in which the word appeared. This method disambiguates Arabic words takes into account that a word may have an old meaning but appears in a modern document.


STiki: an anti-vandalism tool for Wikipedia using spatio-temporal analysis of revision metadata
STiki is an anti-vandalism tool for Wikipedia. Unlike similar tools, STiki does not rely on natural language processing (NLP) over the article or diff text to locate vandalism. Instead, STiki leverages spatio-temporal properties of revision metadata. The feasibility of utilizing such properties was demonstrated in our prior work, which found they perform comparably to NLP-efforts while being more efficient, robust to evasion, and language independent. STiki is a real-time, on-Wikipedia implementation based on these properties. It consists of, (1) a server-side processing engine that examines revisions, scoring the likelihood each is vandalism, and, (2) a client-side GUI that presents likely vandalism to end-users for definitive classification (and if necessary, reversion on Wikipedia). Our demonstration will provide an introduction to spatio-temporal properties, demonstrate the STiki software, and discuss alternative research uses for the open-source code.


Automatic Segmentation and tagging of facts in French for automated fact-checking


Direct trajectory optimization by a Chebyshev pseudospectral method

Abstract:This paper provides a novel and totally statistical method to search similar questions from a large question archive for a given queried question. Firstly, a word relevance model is trained based on the whole question archive which is made up of millions of natural language questions proposed by users on the Web. The word relevance model is utilized to find most semantically related words to a specific word. Secondly, in order to find semantically similar questions for a queried question, each non-stop word in a question is expanded with the help of word relevance model and represented as a word vector. Elements of the vector include the word itself and some semantically related words to it. Elements of the word vector are weighted by combining both classical IR term weighting method and word transformation probability learned from the relevance model. Then the question is mapped to a question vector as the normalized center of the word vectors representing these words contained in it. The problem of question retrieval can be solved by comparing the similarity between question vectors. The method is actually a simple question expansion based Kernel approach. Experimental results indicate the proposed method outperforms the baseline methods such as Vector Space Model (VSM) and Language Model for Information Retrieval (LMIR).


Big data and learning analytics in higher education: Demystifying variety, acquisition, storage, NLP and analytics
Abstract:Different sectors have sought to take advantage of opportunities to invest in big data analytics and Natural language processing, in order to improve their productivity and competitiveness. Current challenges facing the higher education sector include a rapidly changing and evolving environment, which necessitates the development of new ways of thinking. Interest has therefore increased in analytics as part of the solution to many issues in higher education, including rate of student attrition and learner support. This study provides a comprehensive discussion on big data, learning analytics and use of NLP in higher education. In addition, it introduces an integrated learning analytics solution leveraging a distributed technology system capable of supporting academic authorities and advisors at educational institutions in making decisions concerning individual students.

Abstract:Multi-agent system (MAS) is an important research field of distributed artificial intelligence. MAS negotiation is one of the key contents in MAS research. Combining with traditional contract net protocol, acquaintance model, acquaintance coalition inner task allocation distributed optimization algorithm, bulletin board model of agent information exchange with different knowledge and agent mental state parameters, this paper presents a new multi-agent system negotiation model based on expanded contract net protocol and introduces the design and implementation of it in detail.


Abstract:Word sense disambiguation has always been a key problem in Natural Language Processing. In the paper, we use the method of Information Gain to calculate the weight of different position's context, which affect to ambiguous words. And take this as the foundation. We select the ahead and back six position’s context of ambiguous words to construct the feature vectors. The feature vectors are endued with different value of weight in Bayesian Model. Thus, the Bayesian Model is improved. We use the sense of the HowNet to describe the meaning of ambiguous words. The average accuracy rate of the experiments of 10 Chinese ambiguous words was 95.72% in close test and the average accuracy rate was 85.71% in open test. The results showed that the method was proposed in this paper were very effective.


Abstract:Mnemonic phrases may help engineering students remember list information more easily. This paper describes the current progress of work to develop, implement and test two methods for the automatic generation of mnemonic phrases by computer. Techniques from artificial intelligence (AI) are drawn upon; more specifically from natural language processing (NLP) and genetic algorithms (GA). The first method approaches mnemonic phrase generation directly using NLP; the second approaches it as an optimisation problem to be solved by GA.



Analyzing behavior in nursing training toward grasping trainee’s situation remotely

Abstract:We present an approach to recognizing sentiment polarity in Chinese reviews based on topic sentiment sentences. Considering the features of Chinese reviews, we firstly identify the topic of a review using an n-gram matching approach. To extract candidate topic sentiment sentences, we compute the semantic similarity between a given sentence and the ascertained topic and meanwhile determine whether the sentence is subjective. A certain number of these sentences are then selected as representatives according to their semantic similarity value with relation to the topic. The average value of the representative topic sentiment sentences is calculated and taken as the sentiment polarity of a review. Experiment results show that the proposed method is feasible and can achieve relatively high precision.


Are they our brothers?: analysis and detection of religious hate speech in the arabic Twittersphere
Religious hate speech in the Arabic Twittersphere is a notable problem that requires developing automated tools to detect messages that use inflammatory sectarian language to promote hatred and violence against people on the basis of religious affiliation. Distinguishing hate speech from other profane and vulgar language is quite a challenging task that requires deep linguistic analysis. The richness of the Arabic morphology and the limited available resources for the Arabic language make this task even more challenging. To the best of our knowledge, this paper is the first to address the problem of identifying speech promoting religious hatred in the Arabic Twitter. In this work, we describe how we created the first publicly available Arabic dataset annotated for the task of religious hate speech detection and the first Arabic lexicon consisting of terms commonly found in religious discussions along with scores representing their polarity and strength. We then developed various classification models using lexicon-based, n-gram-based, and deep-learning-based approaches. A detailed comparison of the performance of different models on a completely new unseen dataset is then presented. We find that a simple Recurrent Neural Network (RNN) architecture with Gated Recurrent Units (GRU) and pre-trained word embeddings can adequately detect religious hate speech with 0.84 Area Under the Receiver Operating Characteristic curve (AUROC).

Domain Specific Text Preprocessing for Open Information Extraction
Preprocessing is an integral part of Natural Language Processing (NLP) based applications. Standard preprocessing steps consist of removal of irrelevant, unwanted characters or parts of the text based on several observed patterns, while preserving the original intent of the text. We introduce domain-specific preprocessing to filter domain-irrelevant parts of the text while preserving the intended, semantically relevant meaning and syntactic correctness of the text. For this, we define multiple patterns using the dependency tree that represents the Natural Language text based on its dependency grammar. We applied this technique and the patterns to the United States retirement domain documents for open information extraction task as a pre-cursor for mining business product information and rules, and were able to reduce the document data aka information for analysis and mining by at least 13%, which enhanced the F1-score of relation extraction by a minimum of 16%.


SVM-based audio scene classification

An interface for rapid natural language processing development in UIMA
This demonstration presents the Annotation Librarian, an application programming interface that supports rapid development of natural language processing (NLP) projects built in Apache Unstructured Information Management Architecture (UIMA). The flexibility of UIMA to support all types of unstructured data -- images, audio, and text -- increases the complexity of some of the most common NLP development tasks. The Annotation Librarian interface handles these common functions and allows the creation and management of annotations by mirroring Java methods used to manipulate Strings. The familiar syntax and NLP-centric design allows developers to adopt and rapidly develop NLP algorithms in UIMA. The general functionality of the interface is described in relation to the use cases that necessitated its creation.


Hierarchical verb clustering using graph factorization

Semantic interpretation of prepositions for NLP applications
The proper interpretation of prepositions is an important issue for automatic natural language understanding. We present an approach towards PP interpretation as part of a natural language understanding system which has been successfully employed in various NLP tasks for information retrieval and question answering. Our approach is based on the so-called Multi-Net paradigm, a knowledge representation formalism especially designed for the representation of natural language semantics. The paper describes how the information about the semantic interpretation of PPs is represented in the lexicon and in PP interpretation rules and how this information is used during semantic analysis. Moreover, we report on experiments that evaluate the impact of using this information about PP interpretation on the CLEF question answering task.

Towards the design of a Conceptual Framework for the operation of Intensive Care Units based on Big Data Analysis
The development of Big Data Analytics (BDA) technology and the maturity of the Machine Learning (ML) sector offer great opportunities for applications in Intensive Care Units (ICUs). This paper describes a Conceptual Framework and proposes its use in designing architectures and big data applications in ICUs. The Conceptual Framework is based on BDA,MLNatural Language Processing (NLP) and consists of the following subsystems: The "Big Data Integration and ICUs" module, the "ICUs and critical care services" module, the "Use of standards and ICUs" module, the "Machine Learning and ICUs" module, and the “NLP and ICUs” module. The framework is developed using Soft System Methodology (SSM) and Design Science Research Methodology (DSRM).


Derivational relations in Czech WordNet

An open distributed architecture for reuse and integration of heterogeneous NLP components
The shift from Computational Linguistics to Language Engineering is indicative of new trends in NLP. This paper reviews two NLP engineering problems: reuse and integration, while relating these concerns to the larger context of applied NLP. It presents a software architecture which is geared to support the development of a variety of large-scale NLP applications: Information Retrieval, Corpus Processing, Multilingual MT, and integration of Speech Components.

DeepParse: A Trainable Postal Address Parser
Abstract:Postal applications are among the first beneficiaries of the advancements in document image processing techniques due to their economic significance. To automate the process of postal services, it is necessary to integrate contributions from a wide range of image processing domains, from image acquisition and preprocessing to interpretation through symbol, character and word recognition. Lately, machine learning approaches are deployed for postal address processing. Parsing problem has been explored using different techniques, like regular expressions, Conditional Random Fields (CRFs), Hidden Markov Models (HMMs), Decision Trees and Support Vector Machines (SVMs). These traditional techniques are designed on the assumption that the data is free from OCR errors which decreases the adaptability of the architecture in the real-world scenarios. Furthermore, their performance is affected in the presence of non-standardized addresses resulting in intermixing of similar classes. In this paper, we present the first trainable neural network based robust architecture DeepParse for postal address parsing that tackles these issues and can be applied to any Named Entity Recognition (NER) problem. The architecture takes the input at different granularity levels: characters, trigram characters and words to extract and learn the features and classify the addresses. The model was trained on a synthetically generated dataset and tested on the real-world addresses. DeepParse has also been tested on the NER dataset i.e. CoNLL2003 and gave the result of 90.44% which is on par with the state-of-art technique.


Fast stochastic predictive control for building temperature regulation

Improvement of the dotplotting method for linear text segmentation
Abstract:The dotplotting method, employed by Reynar (1994), is a state-of-the-art algorithm for automatic linear text segmentation. However, several problems are found in its measure for assessing density that represents topical coherence: the density function is asymmetric, leading to the apparent false conclusion that forward scan may result in different segmentation with backward scan; besides, while determining next boundary, the assessing strategy doesn't adequately take the previously located boundaries into account. In this paper we propose modified models that remedy these problems. We also make use of segment length to improve segmentation performance. Experimental results show that the modified models achieve considerable improvement in P/sub k/ value and precision and recall over the original dotplotting method.

Of mice and terms: clustering algorithms on ambiguous terms in folksonomies
Developed using the principles of the Model-View-Controller architectural pattern, FolksEngine is a parametric search engine for folksonomies that allows us to test arbitrary search improvement algorithms by specifying them in three phases: expansion, where the original query is converted in multiple ones according to semantic rules associated to the query terms, search, executing the queries on a standard folksonomy search engine such as Delicious, and ranking, sorting the results according to rules. In this paper we extend our previous studies using FolksEngine and offer a new query expansion algorithms based on Natural Language Processing techniques, and a new view for the results based on Semantic Web technologies. We also describe some tests of the algorithms developed, in order to obtain a clear and effective evaluation of them.

Summarising legal texts: sentential tense and argumentative roles
We report on the SUM project which applies automatic summarisation techniques to the legal domain. We pursue a methodology based on Teufel and Moens (2002) where sentences are classified according to their argumentative role. We describe some experiments with judgments of the House of Lords where we have performed automatic linguistic annotation of a small sample set in order to explore correlations between linguistic features and argumentative roles. We use state-of-the-art NLP techniques to perform the linguistic annotation using XML-based tools and a combination of rule-based and statistical methods. We focus here on the predictive capacity of tense and aspect features for a classifier.

TestNMT: function-to-test neural machine translation
Test generation can have a large impact on the software engineering process by decreasing the amount of time and effort required to maintain a high level of test coverage. This increases the quality of the resultant software while decreasing the associated effort. In this paper, we present TestNMT, an experimental approach to test generation using neural machine translation. TestNMT aims to learn to translate from functions to tests, allowing a developer to generate an approximate test for a given function, which can then be adapted to produce the final desired test.   We also present a preliminary quantitative and qualitative evaluation of TestNMT in both cross-project and within-project scenarios. This evaluation shows that TestNMT is potentially useful in the within-project scenario, where it achieves a maximum BLEU score of 21.2, a maximum ROUGE-L score of 38.67, and is shown to be capable of generating approximate tests that are easy to adapt to working tests.

Grammar-driven versus data-driven: which parsing system is more affected by domain shifts?
In the past decade several parsing systems for natural language have emerged, which use different methods and formalisms. For instance, systems that employ a handcrafted grammar and a statistical disambiguation component versus purely statistical data-driven systems. What they have in common is the lack of portability to new domains: their performance might decrease substantially as the distance between test and training domain increases. Yet, to which degree do they suffer from this problem, i.e. which kind of parsing system is more affected by domain shifts? Intuitively, grammar-driven systems should be less affected by domain changes. To investigate this hypothesis, an empirical investigation on Dutch is carried out. The performance variation of a grammar-driven versus two data-driven systems across domains is evaluated, and a simple measure to quantify domain sensitivity proposed. This will give an estimate of which parsing system is more affected by domain shifts, and thus more in need for adaptation techniques.


Text Sentiment Analysis Based on ResGCNN

SucupiraBot: An Interactive Question-Answering System for the Sucupira Platform
Conversational user interfaces have been an increasingly popular way to obtain quick information in natural language, especially with advances in the area of natural language processing (NLP) fueled by the surge of deep neural networks. Real-world cases include IBM Watson and Azure Microsoft chatbots, among others. Such examples, however, are most present in the commercial spectrum, with few being actively used by the scientific community. In this work, we develop an Interactive Question Answering System (IQA) for the Sucupira platform, Brazil's biggest open platform of postgraduate content, deployed as conversational interfaces for two popular messaging platforms: Telegram and Discord. We also propose the use of multilingual embeddings for ordering answers by a language-independent measuer of semantic similarity. Our IQA was evaluated with an study with 16 participants. The results were mostly positive, indicating that the use of conversational user interfaces, along with NLP techniques, may be a valid alternative for retrieving scientific information from the Sucupira plataform.


Minerva II: A Novel Entity Discovery Tool

Pulling their weight: exploiting syntactic forms for the automatic identification of idiomatic expressions in context
Much work on idioms has focused on type identification, i.e., determining whether a sequence of words can form an idiomatic expression. Since an idiom type often has a literal interpretation as well, token classification of potential idioms in context is critical for NLP. We explore the use of informative prior knowledge about the overall syntactic behaviour of a potentially-idiomatic expression (type-based knowledge) to determine whether an instance of the expression is used idiomatically or literally (token-based knowledge). We develop unsupervised methods for the task, and show that their performance is comparable to that of state-of-the-art supervised techniques.

Abstract:Sentence similarity computing plays an important role in the nature language processing. Many different methods are proposed to calculate sentence similarity including word, semantic, syntax and so on. In this paper, we proposed a sentence similarity method for travel question answering system by combining the word context information and semantic similarity together. We searched a series of context structures for keywords in a sentence. Experiment has been carried out to show the effectiveness of our method.



Topic categorization of Tamil News Articles using PreTrained Word2Vec Embeddings with Convolutional Neural Network

QTIP: Multi-Agent NLP and Privacy Architecture for Information Retrieval in Usable Web Privacy Software
We present a generic natural language processing (NLP) architecture, acronym QTIL, based on a system of cooperating multiple agents (Q/A, T, I, and L agents) which can be used in any information system incorporating Internet Information Retrieval. We then introduce a hybrid multi-agent system (MAS) architecture, acronym QTIP, for the privacy domain through integrating the PeCAN (Personal Context Agent Networking) and QTIL MAS architectures. There are two areas where NLP is used: in the user-MAS interaction and in the process of resource indexing and matching. These two areas map to the Q/Aagent and to the I-agents. We propose using a lightweight Head-driven Phrase Structure Grammar (HPSG) natural language method for the Q architectural layers and qualitatively justify its applicability. We provide an example of employing the HPSG formalism for Information Retrieval using natural language capability via Privacy Web Services in one instantiation of the QTIP architecture. Independent preliminary results for HPSG on the Q level show that our approaches for enhancing the usability of PET tools are promising.

Abstract:In functional User Interface testing, test scenarios are written with respect to the requirements that are specified by test analysts. Usually, a test analyst focuses on base URLs and HTML components while collecting requirements of User Interface test scenarios. A base URL is essentially a unit segment of large scale graph data. It has mostly dynamic shape and is used to navigate pages amongst application's pages. We argue that even though dynamic URLs have additional important information about the content of the page, they are not being utilized in generating User Interface test scenarios. In this study, we address this lack of capability and focus on the development of a methodology that can support the usage of large-scale dynamic URL datasets in UI test script generation. Our proposed methodology is designed as an add-on tool that can be used on the top of the existing UI test automation tools to improve testing quality. We introduce a higher quality testing methodology to make the results more accurate, and we discuss the proposed methodology and give an overview of the implementation details followed by the evaluation results. We perform various performance evaluations to investigate how well the proposed algorithms scale under increasing data sizes. The results are promising and show the usability of the proposed methodology.


Abstract:Text summarization is a process of summarize any text or document. There are many summarization tools for English language. There are also a few works for automated Bengali text or document summarization. The tools are seemed not much appropriate from application point of view. Summarization is categorized in two ways: extractive and abstractive approach. Most of the summarizer methods for Bengali text summarization are extractive. Those proposed methods can't extract whole theme of a text document. Reader can be satisfied about summary if it gives full important information of input document. Our proposed method introduces an enhanced summarization method that can improved the quality of outputs. The proposed method is modeled combining a set mathematical rules and Bengali grammatical rules. This method also solves many problems of extractive summarizer and it also introduces the path of abstractive summarization methods. Although the method has been developed for Bengali language, it is a generic and platform independent approach and can flexible be extended for other languages.


Online search interface for the Sejong Korean-Japanese bilingual corpus and auto-interpolation of phrase alignment
A user-friendly interface to search bilingual resources is of great help to NLP developers as well as pure-linguists. Using bilingual resources is difficult for linguists who are unfamiliar with computation, which hampers capabilities of bilingual resources. NLP developers sometimes need a kind of workbench to check their resources. The online interface this paper introduces can satisfy these needs. In order to implement the interface, this research deals with how to align Korean and Japanese phrases and interpolates them into the original bilingual corpus in an automatic way.

Deep-Confidentiality: An IoT-Enabled Privacy-Preserving Framework for Unstructured Big Biomedical Data
Due to the Internet of Things evolution, the clinical data is exponentially growing and using smart technologies. The generated big biomedical data is confidential, as it contains a patient’s personal information and findings. Usually, big biomedical data is stored over the cloud, making it convenient to be accessed and shared. In this view, the data shared for research purposes helps to reveal useful and unexposed aspects. Unfortunately, sharing of such sensitive data also leads to certain privacy threats. Generally, the clinical data is available in textual format (e.g., perception reports). Under the domain of natural language processing, many research studies have been published to mitigate the privacy breaches in textual clinical data. However, there are still limitations and shortcomings in the current studies that are inevitable to be addressed. In this article, a novel framework for textual medical data privacy has been proposed as Deep-Confidentiality. The proposed framework improves Medical Entity Recognition (MER) using deep neural networks and sanitization compared to the current state-of-the-art techniques. Moreover, the new and generic utility metric is also proposed, which overcomes the shortcomings of the existing utility metric. It provides the true representation of sanitized documents as compared to the original documents. To check our proposed framework’s effectiveness, it is evaluated on the i2b2-2010 NLP challenge dataset, which is considered one of the complex medical data for MER. The proposed framework improves the MER with 7.8% recall, 7% precision, and 3.8% F1-score compared to the existing deep learning models. It also improved the data utility of sanitized documents up to 13.79%, where the value of the k is 3.


Teaching Modelling Literacy: An Artificial Intelligence Approach

Abstract:As Internet technology has become a part of the lifestyle of the common man, research efforts are extensively made in the fields of Natural Language Processing (NLP) and Information Retrieval. Studying regional languages for developing the system to store, retrieve, extract the information from the database has gained lots of prominence nowadays. Case studies show that Ontological Information Retrieval has many advantages over keyword-based approach. In this paper we have focused on the general architecture of ontology-based Information Retrieval used for Kannada.


An application of latent semantic analysis to word sense discrimination for words with related and unrelated meanings
We present an application of Latent Semantic Analysis to word sense discrimination within a tutor for English vocabulary learning. We attempt to match the meaning of a word in a document with the meaning of the same word in a fill-in-the-blank question. We compare the performance of the Lesk algorithm to Latent Semantic Analysis. We also compare the performance of Latent Semantic Analysis on a set of words with several unrelated meanings and on a set of words having both related and unrelated meanings.

Abstract:Today's world is getting flooded with an increasing amount of articles and links to choose from. As this data grows, the importance of semantic density does as well. How can one say the most important things in the shortest amount of time? Having a generated summary lets one decide whether they want to deep dive further or not. Conversion of lengthy texts into short and meaningful sentences is the main idea behind text summarization. To achieve this, various algorithms are present. Machine Learning models are trained, first to understand the given document and then create a summary of it. These models achieve this task either by extracting important words out of the document or by creating human-like sentences to form the summary.


Aspects of Information Integration in a Wiki
Information integration means pulling information pieces from various sources together with as little loss, as little overlap and as little redundancy of information as is possible. This paper explores different aspects of information integration using as an example a wiki that contains a set of documents to which we want to add suitable further pictures. We take into account a number of important aspects such as: (i) Pictures outside the domain of the wiki are ignored. (ii) Pictures that already occur in the wiki or very similar to existing ones are also ignored. (iii) Each new picture has to be associated with a suitable document in the wiki subject to three restraints: if a suitable document does not exist, a new one is created; if the document size due to pictures gets too large some additional actions are necessary; an image may be associated with more than one document. Each of the above points offer many challenges which are discussed.


Slack variable-based control variable parameterization method for constrained engineering optimization

Sentiment analysis using a novel human computation game
In this paper, we propose a novel human computation game for sentiment analysis. Our game aims at annotating sentiments of a collection of text documents and simultaneously constructing a highly discriminative lexicon of positive and negative phrases. Human computation games have been widely used in recent years to acquire human knowledge and use it to solve problems which are infeasible to solve by machine intelligence. We package the problems of lexicon construction and sentiment detection as a single human computation game. We compare the results obtained by the game with that of other well-known sentiment detection approaches. Obtained results are promising and show improvements over traditional approaches.

A noise robust front-end using Wiener filter, probability model and CMS for ASR
Abstract:A novel and noise robust front-end based on the combination of spectral noise reduction and probability model-based feature compensation and cepstral mean subtraction (CMS) is proposed. Mel filter-bank outputs can be affected by additive noise primarily because of the vulnerable spectral valleys. An instantaneous Wiener filter is used to improve SNR of the spectral valley. Because the compensated MFCC is an approximation of the clean one and retains a residual mismatch, features are further processed by CMS in order to remove the global shift of the mean. In the presence of additive noise, ASR experiments reveal that a cascade fashion use of these techniques improves recognition performance greatly. For the 863 continuous Chinese speech databases, the average recognition rate across different noise types is improved from 34.63% (using unmodified MFCCs) to 84.63% (using the proposed techniques) at best.


Complicating the Social Networks for Better Storytelling: An Empirical Study of Chinese Historical Text and Novel

The TermiNet project: an overview
Linguistic resources with domain-specific coverage are crucial for the development of concrete Natural Language Processing (NLP) systems. In this paper we give a global introduction to the ongoing (since 2009) TermiNet project, whose aims are to instantiate a generic NLP methodology for the development of terminological wordnets and to apply the instantiated methodology for building a terminological wordnet in Brazilian Portuguese.

Text to Code Conversion Using Deep Learning for NLP
Abstract:Semantic parsing is the problem of mapping natural language sentences to a formal representation like lambda calculus expressions. We work on a specific sub-problem in semantic parsing: producing python code snippets from natural language descriptions. We address this sub-problem using the CoNaLa Dataset in order to improve precision and to help developers in their development phases. For this task, we use Deep Learning (DL) for Natural Language Processing (NLP), based mainly on an Artificial Neuron Networks (ANN). Unlike the simple ANN, which learns in one direction, we use a Recurrent Neural Networks with LSTM cells since it performs better in Machine Translation (MT) task. We get a BLEU score of 15.2 beating the previously reported baseline of the CoNaLa challenge.

Extensible multimodal annotation markup language (EMMA): invited talk
This talk will introduce the W3C Multimodal Interaction Activity, whose goal is to design a framework of specifications to enable access to the Web using multi-modal interaction. In particular we will introduce the Extensible MultiModal Annotation (EMMA) language specification. EMMA is an XML language for describing the interpretation of user input, combining transcriptions of raw signals into words with metadata to help applications resolve uncertainties and contradictions in interpretations. We will also discuss the issues encountered by the Working Group as the language is being developed, such as whether to use RDF or not, how best to combine interpretations, or what metadata properties to represent.

Automatic generation of system test cases from use case specifications
In safety critical domains, system test cases are often derived from functional requirements in natural language (NL) and traceability between requirements and their corresponding test cases is usually mandatory. The definition of test cases is therefore time-consuming and error prone, especially so given the quickly rising complexity of embedded systems in many critical domains. Though considerable research has been devoted to automatic generation of system test cases from NL requirements, most of the proposed approaches re- quire significant manual intervention or additional, complex behavioral modelling. This significantly hinders their applicability in practice. In this paper, we propose Use Case Modelling for System Tests Generation (UMTG), an approach that automatically generates executable system test cases from use case spec- ifications and a domain model, the latter including a class diagram and constraints. Our rationale and motivation are that, in many environments, including that of our industry partner in the reported case study, both use case specifica- tions and domain modelling are common and accepted prac- tice, whereas behavioural modelling is considered a difficult and expensive exercise if it is to be complete and precise. In order to extract behavioral information from use cases and enable test automation, UMTG employs Natural Language Processing (NLP), a restricted form of use case specifica- tions, and constraint solving.

NLP-based enhancement of information security in ITO: a diffusion of innovation theory perspective
Information technology outsourcing (ITO) has grown significantly in recent decades and is now over a USD trillion-dollar industry. Service provider organisations are striving to improve the efficiencies of their service deliveries. Natural language processing (NLP) provides an opportunity to bring efficiencies through automation in understanding and processing information. Since information security risk management (ISRM) in ITO is a growing concern of both, client and service provider organisations, they are adopting to improve ISRM in ITO using NLP. This paper explores those ISRM improvement scenarios. It also investigates the information security risks (ISRs) that result from the use of NLP in ITO and proposes strategies to manage those ISRs. To gain insights into the problem, a qualitative research approach is followed using the case study method. Six semi-structured interviews were conducted from participants in three organisations in the ICT industry, engaged in an ITO relationship. To the best of our knowledge, it is the first study to investigate the use of NLP for enhancing ISRM in ITO.

Natural language processing in Watson
Open domain Question Answering (QA) is a long standing research problem. Recently, IBM took on this challenge in the context of the Jeopardy! game. Jeopardy! is a well-known TV quiz show that has been airing on television in the United States for more than 25 years. It pits three human contestants against one another in a competition that requires answering rich natural language questions over a very broad domain of topics. The development of a system able to compete to grand champions in the Jeopardy! challenge led to the design of the DeepQA architecture and the implementation of Watson. The DeepQA project shapes a grand challenge in Computer Science that aims to illustrate how the wide and growing accessibility of natural language content and the integration and advancement of Natural Language Processing, Information Retrieval, Machine Learning, Knowledge Representation and Reasoning, and massively parallel computation can drive open-domain automatic Question Answering technology to a point where it clearly and consistently rivals the best human performance. Natural Language Processing (NLP) plays a crucial role in the overall Deep QA architecture. It allows to "make sense" of both question and unstructured knowledge contained in the large corpora where most of the answers are located. That's why we decided to focus this tutorial on the NLP technology adopted by Watson and on how it fits in the general Deep QA architecture.


An Efficient Manifold Ranking Approach for Monolithic Graphs and Semantic Networks

A web survey on the use of active learning to support annotation of text data
As supervised machine learning methods for addressing tasks in natural language processing (NLP) prove increasingly viable, the focus of attention is naturally shifted towards the creation of training data. The manual annotation of corpora is a tedious and time consuming process. To obtain high-quality annotated data constitutes a bottleneck in machine learning for NLP today. Active learning is one way of easing the burden of annotation. This paper presents a first probe into the NLP research community concerning the nature of the annotation projects undertaken in general, and the use of active learning as annotation support in particular.

Clinical Text Analysis Using Interactive Natural Language Processing
Natural Language Processing (NLP) systems are typically developed by informaticists skilled in machine learning techniques that are unfamiliar to end-users. Although NLP has been widely used in extracting information from clinical text, current systems generally do not provide any provisions for incorporating feedback and revising models based on input from domain experts. The goal of this research is to close this gap by building highly-usable tools suitable for the analysis of free text reports.

Abstract:This article is devoted to the problem of increasing the effectiveness of teaching students, the use of new teaching methods, with regard to advanced educational forms, modern methods of educational process, enhancing the role of psychological and pedagogical support of educational process.


Fuzzy Systems for Computational Linguistics and Natural Language
A natural language (NL) is any of the languages naturally used by humans, i.e. not an artificial such as programming languages. Natural language generation systems convert information from computer databases into normal-sounding human language and natural language understanding systems convert samples of human language into more formal representations that are easier for computer programs to manipulate. Natural language is basically a system for describing perceptions. In this paper we discuss use of fuzzy logic in natural language processing and computation, the general process of NLP as well as some common techniques used, computational linguistic, linguistic approximation.


Mixed-initiative development of language processing systems

Abstract:For finding patterns in data, machine learning models are being trained. Gender relations psychology looks for social norms like inter dimensionality, beliefs, social experience and self-perception, and self-respect. Training on gender based text NLP models unknowingly become acquainted with unusual patterns. In this paper, we represent gender recognition by using Bengali conventional full names. We present a review and interpretation of gender classification based on individual names in this correspondence. These days, NLP has demonstrated excellent execution in identifying human gender. In the field of knowledge, gender classification is a demonstrative binary classification phenomenon. We've used a total of seven algorithms in this research. We were added to the dataset with details regarding which features are currently used for prediction along with that it determines how these features are affected by data preprocessing model initialization and architecture selection. Our research compares those classifiers, examines the impact of pretraining moreover, assesses the robustness of the alignment preprocessing through the confusion matrix.. The proposed Neural Network outperforms most approaches and is much more reliable than other models. This model has the best weighted precision of all the models, with such a 73.04 % accuracy score.


Abstract:Compared with people's urgent desire for information, the current information retrieval still has problems such as slow speed and low precision. In response to this problem, this paper proposes an information retrieval method based on NLP. Firstly, the semi-supervised learning algorithm is used to describe the natural language under the manifold condition, and the undirected graph is constructed for all the data. For the undirected graph after construction, we use the label propagation algorithm to simplify the undirected graph to reduce the computational complexity in the information retrieval process. Then the keywords will be extracted by TextRank algorithm to achieve information retrieval. Finally the experimental results show that the retrieval efficiency can be improved by using this algorithm.



Contextual scene segmentation of driving behavior based on double articulation analyzer

Natural Language Processing for EHR-Based Computational Phenotyping
This article reviews recent advances in applying natural language processing NLP to Electronic Health Records EHRs for computational phenotyping. NLP-based computational phenotyping has numerous applications including diagnosis categorization, novel phenotype discovery, clinical trial screening, pharmacogenomics, drug-drug interaction DDI, and adverse drug event ADE detection, as well as genome-wide and phenome-wide association studies. Significant progress has been made in algorithm development and resource construction for computational phenotyping. Among the surveyed methods, well-designed keyword search and rule-based systems often achieve good performance. However, the construction of keyword and rule lists requires significant manual effort, which is difficult to scale. Supervised machine learning models have been favored because they are capable of acquiring both classification patterns and structures from data. Recently, deep learning and unsupervised learning have received growing attention, with the former favored for its performance and the latter for its ability to find novel phenotypes. Integrating heterogeneous data sources have become increasingly important and have shown promise in improving model performance. Often, better performance is achieved by combining multiple modalities of information. Despite these many advances, challenges and opportunities remain for NLP-based computational phenotyping, including better model interpretability and generalizability, and proper characterization of feature relations in clinical narratives.


Deep-Dual-Learning-Based Cotask Processing in Multiaccess Edge Computing Systems

Abstract:Numerous advantages have been offered by e-Learning, for example consistency, cost effectiveness, flexibility, remote operability, simplicity, and various others. The implementation of smart technologies and tools effectively facilitates convenient and uncomplicated education without the obstacles of time and space. The objectives of this study are to 1) to formulate a conceptual framework of e-Learning for learners with multiple intelligences that is collaborative, and 2) to conduct the development of e-Learning activities that are based on the mathematical/logical intelligence aspect of multiple intelligences in order to promote critical thinking. The data analysis of students provides the rules upon which rule-based activities are formed in order to guide learners. Four chapters from the subject of Secondary I Education (S1) at the Demonstration School of Phayao University are applied to perform the research experiments. It is indicated by the results that activities involving eLearning can effectively promote the critical thinking intelligence that is a part of the multiple intelligences.


Abstract:Every day, number of pages gets added on web which makes tracking of their links cumbersome. Due to this, problem of overloaded data has come up. This issue led researchers to thoroughly go through different aspects of Web Usage Mining (WUM). Another issue of traditional system is of recommendations which are also a part of WUM and Web logs. This paper proposed a system of recommendations which uses tokenization to separate the users and information is conveyed to Resource Description Framework (RDF) for Semantic data generation and display. Session based clusters are formed and frequency of items is noted. Algorithms are also proposed for smooth working of the system. Web log information is needed for understanding the general mentality or behaviour of the user. We have also proposed Natural Language Processing (NLP) techniques for conditions where users preferences regarding products are not generated.


Abstract:Question is a crucial construct of natural language. Systematic, error free question is a basic need of different applications of natural language. Many research works have been focused on ‘statement’ formation but the issue of ‘systematic question’ formation is less focused. This research work resolves above issue through systematization process using Template based approach which is accompanied by Dictionary approach and powerful NLP technique like Maximum Entropy based POS Tagging technique. Systematization process aims to reform proper flawless question from the erroneous input question by removing existing errors present in order of words, word spelling and removing ambiguous synonyms of the words. This work deals with domain specific WH-questions of English language. Additionally it also works on imperative questions. Template based approach is supported with a key concept of ‘Question Templates’ which are designed with human intelligence keeping detail knowledge of various lingual constructs, their grammar and domain specific questionnaire. This work is useful in various fields, for example in academics to set question papers, to assist English learners, to produce intermediate output for complex systems like question-answering system to retrieve correct answer from a huge dataset.



An Approach for Detection and Correction of Missing Word in Bengali Sentence

Abstract:Brahmi is one of ancient scripting languages used in India, South and Central Asia. Brahmi inscriptions are the main source of getting information about ancient Sri Lanka. But manual translation process is very time consuming because Brahmi alphabet, word pattern, meaning and grammar are completely differing than the current Sinhala context. As these inscriptions have been discovered under the drip ledges of caves, rocks and slabs, it is a big challenge for Archaeologists to getting the data from these inscriptions and translating into Sinhala sentences because of various noises. This research paper is containing the after process of OCR character recognition phase, which proposed and implemented an efficient system which can take recognized Brahmi character array without word boundaries as input and convert it into meaningful Sinhala sentences by overcoming the OCR errors and word identification scoring method. Full system which consist of four modules is taking inputs of Brahmi inscription images, and this research module is the third module which takes the recognised Brahmi character array from the output of second module and mainly focused on word detection, overcome OCR errors using 3 error correction methods, Brahmi language model using 2 Bigram models and 1 Trigram model and translation.


TSNLP: Test Suites for Natural Language Processing
The growing language technology industry needs measurement tools to allow researchers, engineers, managers, and customers to track development, evaluate and assure quality, and assess suitability for a variety of applications.The TSNLP (Test Suites for Natural Language Processing) project has investigated various aspects of the construction, maintenance and application of systematic test suites as diagnostic and evaluation tools for NLP applications. The paper summarizes the motivation and main results of TSNLP: besides the solid methodological foundation of the project, TSNLP has produced substantial (i.e. larger than any existing general test suites) multi-purpose and multi-user test suites for three European languages together with a set of specialized tools that facilitate the construction, extension, maintenance, retrieval, and customization of the test data.The publicly available results of TSNLP represent a valuable linguistic resource that has the potential of providing a wide-spread pre-standard diagnostic and evaluation tool for both developers and users of NLP applications.

de novo
 assembly algorithms have difficulty reconstructing contiguous genome sequences using short reads due to both repetitive and difficult-to-sequence regions in these genomes. Some of the short read assembly challenges are mitigated by scaffolding assembled sequences using paired-end reads. However, unresolved sequences in these scaffolds appear as “gaps”. Here, we introduce GapPredict – An implementation of a proof of concept that uses a character-level language model to predict unresolved nucleotides in scaffold gaps. We benchmarked GapPredict against the state-of-the-art gap-filling tool Sealer, and observed that the former can fill 65.6% of the sampled gaps that were left unfilled by the latter with high similarity to the reference genome, demonstrating the practical utility of deep learning approaches to the gap-filling problem in genome assembly.

Towards the reuse of lingware systems: a proposed approach with a practical experiment
We are going to present in this document a generic approach for lingware systems reuse. This approach is based on reverse engineering technique in order to wrap up an existing lingware system with web services. This approach permits the reuse of lingware systems regardless of programming languages, development environments and the structures of linguistic resources. In order to preserve the interoperability between the reused lingware systems, the proposed approach performs the unification and the standardization of exchanged linguistic data using the Natural Language Processing (NLP) standards and consensus. Doing so, we facilitate the integration and the composition of lingware services in order to create a new application that treats several linguistic levels. In order to consolidate the given approach, we developed the LIngware Reuse Environment (LIRE). A practical experiment was carried out using LIRE environment on an automatic application summary of Arabic texts.

A Framework for Context-Aware Query Processing
Abstract:Using natural language processing (NLP) and artificial intelligence techniques to handle database queries can make query results more useful and accurate, especially in context-aware applications, where their query results depend on the user and the surrounding environment settings. The existing research work [1] can be extended by adding context-aware services to the Location Based Services (LBSS) server of each cell. A new framework has been proposed for handling context-aware queries based on analysis of a user's query text, and a General Query Format (GQF) has been suggested to exchange queries between different database systems (Relational, NoSQL). A prototype has been implemented by using Python 3.6.10 (Anaconda package) and NLTK 3.5 library to analyze the dependency between text words of a user's query and convert the query text into various formats. The main contributions of this paper are summarized as follows: Selecting a cell location-based service server to provide context-aware services to users in its cell. Suggesting to unify query formats among database systems. Using natural language techniques to analyze the user's query text written in the user's natural language to determine which database will be accessed, and convert it into a standard query format that can be executed by database systems, and a technique for handling the context-aware quires.

Abstract:This paper presents an ontology-driven Chinese-English machine translation prototype system. We construct a small ontology called "SCIENTIST", based on which we develop a Chinese-English MT system. We introduce the basic rules and steps we follow to build the ontology and how we use ontology to represent meanings and deal with ambiguation. We show that ontology is necessary in semantic analysis and disambiguation. It provides a world model with meaning units and structures for meaning representation and reasoning. It provides a new perspective to handle the semantic analysis of MT.


Abstract:This paper presents a novel feature selection algorithm for supervised verb sense disambiguation. The algorithm disambiguates and aggregates WordNet synsets of a verb's noun phrase (NP) arguments in the training data. It was then used to filter out irrelevant WordNet semantic features introduced by the ambiguity of verb NP arguments. Experimental results showed that our new feature selection method boosted our system's performance on verbs whose meanings depended heavily on their NP arguments. Furthermore, our method outperformed two standard feature selection methods, indicating its effectiveness and advantages, especially for small-sample machine learning tasks like supervised WSD.


Abstract:In this paper, we propose a method is to improve the performance of information retrieval systems (IRS) by increasing the selectivity of relevant documents on the web. Indeed, a significant number of relevant documents on the web are not returned by an IRS (specifically a search engine), because of the richness of natural language Arabics. For this purpose the search engine does not reach high performance and does not meet the needs of users. To remedy this problem, we propose a method of enrichment of the query. This method relies on many steps. First, identification of significant terms (simple and composed) present in the query. Then, generation of a descriptive list and its assignment to each term that has been identified as significant in the query. A descriptive list is a set of linguistic knowledge of different types (morphological, syntactic and semantic). In this paper we are interested in the statistical treatment, based on the similarity method. This method exploits the weighting functions of Salton TF-IDF and TF-IEF on the list generated in the previous step. TF-IDF function identifies relevant documents, while the TF-IEF's role is to identify the relevant sentence. The terms of high weight (which are terms which may be correlated to the context of the response) are incorporated into the original query. The application of this method is based on a corpus of documents belonging to a closed domain.



Vector representation of internet domain names using a word embedding technique

Classification Benchmarks for Under-resourced Bengali Language based on Multichannel Convolutional-LSTM Network
Abstract:Exponential growths of social media and micro-blogging sites not only provide platforms for empowering freedom of expressions and individual voices, but also enables people to express anti-social behavior like online harassment, cyberbul-lying, and hate speech. Numerous works have been proposed to utilize these data for social and anti-social behavior analysis, document characterization, and sentiment analysis by predicting the contexts mostly for highly resourced languages like English. However, some languages are under-resources, e.g., South Asian languages like Bengali, Tamil, Assamese, Malayalam, that lack of computational resources for natural language processing. In this paper


LegalBERT-th: Development of Legal Q&A Dataset and Automatic Question Tagging

Abstract:The rise of Artificial Intelligence(AI) and Natural Language Processing (NLP) gave a perception of the use of computers in the Healthcare industry. The introduction of voice-assisted chatbots and digital data storage aided health organizations as well as the dependents. Currently, the interactions between doctors and patients are undocumented, and the prescription, in some cases, creates chaos for understanding and long-term maintenance. The project introduces an interactive virtual assistant system for searching, analyzing, and recording clinical data and laboratory data, maintaining electronic health records. It uses a Named Entity Extraction (NER) model for classifying the doctor-patient conversation, i.e., symptoms, detected disorder, prescription of drugs, and stores them in a Structured database. The system poses an ability to understand and respond to limited user queries and imitate human discourse to stimulate a conversation that resembles conversing with a real person. The response includes the recorded history of patients, overcoming the issue of file management, supporting clinical decisions. Generates automated prescriptions eliminating the problems of handwritten prescriptions, and provides live news updates with the web scraper module. Moreover, it plays a vital role in reminding patients about their deadlines for vaccinations and follow-ups.


Abstract:Morphological analysis is an essential component in Natural Language Processing (NLP) applications ranging from spell checker to machine translation. When performing a morphological analysis it leads to segmentation of a word into morphemes, combined with an analysis of the attachments of these morphemes. In English language the complexity of the formation of words is not much higher compared with Indic languages. Hence, Tamil language too does have its complexities when building up a NLP application. The morphemes in the language, the rules how these morphemes are connected and the changes occur when they attach together are the important factors that need to be considered when building up a Morphological Analyzer for any language. Our “Morphological Analyzer and Generator for Tamil Language” will be generating the word forms of a stem/ root, given a particular context and at the same time, a surface form in Tamil language should get analyzed into its proper context. This model tries to cover only the nouns and verbs in the Tamil language. This paper illustrates how the lexicon and the orthographic rules of Tamil language have been written as regular expressions using only finite state operations and how this approach has been implemented to develop a morphological analyzer/generator. This model is built using the Xerox toolkit, which uses “Two-level Morphology”, and almost 2000 noun stems and 96 verb stems have been incorporated into the network. A noun stem now produces about 40 different forms and a verb stem produces up to 240 forms. We have also defined our own transliteration scheme for this purpose.



An Enterprise Public Opinion Emergency Response System

Eigenwords: spectral word embeddings
Spectral learning algorithms have recently become popular in data-rich domains, driven in part by recent advances in large scale randomized SVD, and in spectral estimation of Hidden Markov Models. Extensions of these methods lead to statistical estimation algorithms which are not only fast, scalable, and useful on real data sets, but are also provably correct. Following this line of research, we propose four fast and scalable spectral algorithms for learning word embeddings -- low dimensional real vectors (called Eigenwords) that capture the "meaning" of words from their context. All the proposed algorithms harness the multi-view nature of text data i.e. the left and right context of each word, are fast to train and have strong theoretical properties. Some of the variants also have lower sample complexity and hence higher statistical power for rare words. We provide theory which establishes relationships between these algorithms and optimality criteria for the estimates they provide. We also perform thorough qualitative and quantitative evaluation of Eigenwords showing that simple linear approaches give performance comparable to or superior than the state-of-the-art non-linear deep learning based methods.

Abstract:In Natural Language Processing (NLP) tasks lexical and semantic resources such as WordNet, Open Multilingual WordNet, Wikipedia, Wiktionary, OmegaWiki, BabelNet, Babelfy and so on are played an important role in terms of knowledge extraction. This study presents usage of Babelfy that, given a Turkish hotel review dataset, which contains 1517 reviews, returns all collocations the dataset contains. When the extracted collocations are evaluated according to human annotated collocations it has been seen that from 425 collocations Babelfy extract 216 collocations with 0.67 F-measure. 195 of the extracted collocations are labeled as restricted expression, 2 of the them are labeled as named-entity and 19 of them are labeled as light verbs.


Abstract:The following topics are dealt with: fuzzy set theory; pattern clustering; Internet of Things; data analysis; vocabulary; text analysis; word processing; natural language processing; information retrieval; rough set theory.



Adaptation of a named entity recognition system for the ESTER 2 evaluation campaign

A Unified Framework for Improving Misclassifications in Modern Deep Neural Networks for Sentiment Analysis
Abstract:Deep Neural Networks (DNNs) have achieved high accuracy in multiple Natural Language Processing (NLP) applications. The great success lies in the test data is drawn from the same distribution of the training samples. However, researches have found that the current models classify out-of-distribution, adversarial, and erroneous samples incorrectly with high confidence. Researchers also find the problem comes from the softmax layer of DNN. In this paper, we address this issue and propose a method that ignores the softmax layer in the DNN architecture. Specifically, we estimate the training samples' parameters of the output of the pre-softmax layer of DNN using the Dirichlet Process Gaussian Mixture Model (DPGMM). Then, we compute the distance between a test sample and the distribution of the training samples using Mahalanobis distance to get the classification results. We evaluate our method on a classic NLP task, sentiment analysis, by conducting extensive experiments on different models across several real-world datasets. The results demonstrate that our method assigns correct labels to the samples that are misclassified by current DNNs with softmax layer. Our method can be generalized to any pre-trained DNN without the need to re-train the models and it also does not need supervision learning.

Named entity recognition in tweets: an experimental study
People tweet more than 100 Million times daily, yielding a noisy, informal, but sometimes informative corpus of 140-character messages that mirrors the zeitgeist in an unprecedented manner. The performance of standard NLP tools is severely degraded on tweets. This paper addresses this issue by re-building the NLP pipeline beginning with part-of-speech tagging, through chunking, to named-entity recognition. Our novel T-ner system doubles F1 score compared with the Stanford NER system. T-ner leverages the redundancy inherent in tweets to achieve this performance, using LabeledLDA to exploit Freebase dictionaries as a source of distant supervision. LabeledLDA outperforms co-training, increasing F1 by 25% over ten common entity types. Our NLP tools are available at: http://github.com/aritter/twitter_nlp

. Furthermore, we train and evaluate supervised machine learning models to automatically identify the annotation labels. Linear Support Vector classifier (LinearSVC) performs better compared to other baseline models, and achieves an accuracy F
1

Myanmar POS Resource Extension Effects on Automatic Tagging Methods
Abstract:Part-of-speech (POS) tagging is the process of assigning the part-of-speech tag or other lexical class marker to each word in a sentence. It is also one of the most important steps in Natural Language Processing (NLP) task pipeline. There are several research works in Myanmar POS tagging implemented with different approaches. However, there is only one publicly available tagged corpus named myPOS corpus. The size of this corpus is only 11 thousand sentences. It is not enough to train downstream NLP tasks, such as machine learning. For this reason, we manually extended the original myPOS corpus as myPOS version 2.0 and the size of the extended corpus becomes approximately triple size of the original myPOS corpus. To evaluate the effects of the extended corpus versus the original corpus, the accuracies of four supervised tagging algorithms, namely, Conditional Random Fields (CRFs), Hidden Markov Model (HMM), Ripple Down Rules based (RDR), and neural sequence labeling approach of Conditional Random Fields (NCRF


A patent document retrieval system addressing both semantic and syntactic properties

Hate, Obscenity, and Insults: Measuring the Exposure of Children to Inappropriate Comments in YouTube
Social media has become an essential part of the daily routines of children and adolescents. Moreover, enormous efforts have been made to ensure the psychological and emotional well-being of young users as well as their safety when interacting with various social media platforms. In this paper, we investigate the exposure of those users to inappropriate comments posted on YouTube videos targeting this demographic. We collected a large-scale dataset of approximately four million records and studied the presence of five age-inappropriate categories and the amount of exposure to each category. Using natural language processing and machine learning techniques, we constructed ensemble classifiers that achieved high accuracy in detecting inappropriate comments. Our results show a large percentage of worrisome comments with inappropriate content: we found 11% of the comments on children’s videos to be toxic, highlighting the importance of monitoring comments, particularly on children’s platforms.

A gold standard corpus of early modern German
This paper describes an annotated gold standard sample corpus of Early Modern German containing over 50,000 tokens of text manually annotated with POS tags, lemmas, and normalised spelling variants. The corpus is the first resource of its kind for this variant of German, and represents an ideal test bed for evaluating and adapting existing NLP tools on historical data. We describe the corpus format, annotation levels, and challenges, providing an example of the requirements and needs of smaller humanities-based corpus projects.

A uniform method of grammar extraction and its applications
Grammars are core elements of many NLP applications. In this paper, we present a system that automatically extracts lexicalized grammars from annotated corpora. The data produced by this system have been used in several tasks, such as training NLP tools (such as Supertaggers) and estimating the coverage of hand-crafted grammars. We report experimental results on two of those tasks and compare our approaches with related work.

Abstract:In this article we present a deep learning question answering (QA) setting that can work for any natural language. We recognize the problem of low-resource languages, i.e. most languages other than English, which lack appropriately sized datasets or cutting-edge NLP tools. To address this problem, we have designed and implemented a QA dataset and system that are independent from language use; specifically, we test our solution on the Polish language which is both low-resource and grammatically complex. Both these features make the task of QA significantly harder. To the best of our knowledge, this is the first attempt to train a deep learning QA system in a language-agnostic setting.


