cross - dataset design discussion mining
being able to identify software discussions that are primarily about design — which we call design mining — can improve documentation and maintenance of software systems . existing design mining approaches have good classification performance using natural language processing ( nlp ) techniques , but the conclusion stability of these approaches is generally poor . a classifier trained on a given dataset of software projects has so far not worked well on different artifacts or different datasets . in this study , we replicate and synthesize these earlier results in a meta — analysis . we then apply recent work in transfer learning for nlp to the problem of design mining . however , for our datasets , these deep transfer learning classifiers perform no better than less complex classifiers . we conclude by discussing some reasons behind the transfer learning approach to design mining . 

a modular toolkit for machine translation based on layered charts
we present a freely available toolkit for building machine translation systems for a large variety of languages . the toolkit uses standard linguistic data representation based on charts and typed feature structures ; a modular open architecture based on standardized interfaces and processing architecture , enabling the addition of external language processing components and the configuration of new applications ( plug - and - play) ; an open library of basic parameterizable language processing components including a morphological finite - state processor , dictionary components , an island chart parser , chart generator , and chart - based transfer engine ( for mt systems) . it is open - source : the c ++ source code is available , and portable : targeted systems are unix and windows systems . 

automated threat report classification over multi - source data
with an increase in targeted attacks such as advanced persistent threats ( apts ) , enterprise system defenders require comprehensive frameworks that allow them to collaborate and evaluate their defense systems against such attacks . mitre has developed a framework which includes a database of different kill - chains , tactics , techniques , and procedures that attackers employ to perform these attacks . in this work , we leverage natural language processing techniques to extract attacker actions from threat report documents generated by different organizations and automatically classify them into standardized tactics and techniques , while providing relevant mitigation advisories for each attack . a na ï ve method to achieve this is by training a machine learning model to predict labels that associate the reports with relevant categories . in practice , however , sufficient labeled data for model training is not always readily available , so that training and test data come from different sources , resulting in bias . a na ï ve model would typically underperform in such a situation . we address this major challenge by incorporating an importance weighting scheme called bias correction that efficiently utilizes available labeled data , given threat reports , whose categories are to be automatically predicted . we empirically evaluated our approach on 18 , 257 real - world threat reports generated between year 2000 and 2018 from various computer security organizations to demonstrate its superiority by comparing its performance with an existing approach . 

nlp methods for automatic candidate ’ s cv segmentation
the problem of cv ( or resume ) segmentation and automatic extraction becomes increasingly relevant nowadays as long as it could simplify candidate selection process . the paper proposes a new method of automatic cv segmentation and parsing . the described algorithm is based on natural language processing and machine learning methods . the proposed procedure allows to extract information related to the candidates ' work experience and education from their cvs which come in pdf or docx format . in particular , cv segmentation into 3 blocks ( basic information , education and work experience ) is performed . 

embedding nonlinear optimization in rrt * for optimal kinodynamic planning
some of the latest developments in motion planning methods have addressed the merging of optimal control with sampling - based approaches , to handle the problem of optimal kinodynamic motion planning for complex robot systems in cluttered environments . these include embedding the linear quadratic regulator method in an rrt * context , or solving the kinematic problem with an rrt algorithm first and then feeding the solution to an nlp solver . an alternative approach is presented here , in which nlp is embedded in an rrt * context from the start . the resulting methodological features are illustrated with numerical examples . these include problems in which differential constraints play a fundamental role . 

incremental deep neural network learning using classification confidence thresholding
most modern neural networks for classification fail to take into account the concept of the unknown . trained neural networks are usually tested in an unrealistic scenario with only examples from a closed set of known classes . in an attempt to develop a more realistic model , the concept of working in an open set environment has been introduced . this in turn leads to the concept of incremental learning where a model with its own architecture and initial trained set of data can identify unknown classes during the testing phase and autonomously update itself if evidence of a new class is detected . some problems that arise in incremental learning are inefficient use of resources to retrain the classifier repeatedly and the decrease of classification accuracy as multiple classes are added over time . this process of instantiating new classes is repeated as many times as necessary , accruing errors . to address these problems , this article proposes the classification confidence threshold ( ct ) approach to prime neural networks for incremental learning to keep accuracies high by limiting forgetting . a lean method is also used to reduce resources used in the retraining of the neural network . the proposed method is based on the idea that a network is able to incrementally learn a new class even when exposed to a limited number samples associated with the new class . this method can be applied to most existing neural networks with minimal changes to network architecture . 

applying data miming to automatically acquire user preferences in commercial web sites
data mining are an important held of research . however , there is an important challenge to apply the data mining technique to nlp applications . an integrated data mining system for nlp in chinese commercial web sites is presented in this paper . it firstly extracted the raw data using nlp technology and then presented the data mining process in detail to acquire the user preferences . more importantly , it puts forward a new enhanced and integrated method to acquire user preferences . data mining applied to nlp provides a scientific basis for e - commerce and decision - making systems . 

named entity recognition through bidirectional lstm in natural language texts obtained through audio interfaces
this article describes artificial neural network use case for named entity recognition for text obtained through audio interfaces . the efficiency of the training method based on modified data set was shown . 

trajectory planning for automated driving based on ordinal optimization
this paper proposes an approach based on ordinal optimization ( oo ) to solve trajectory planning for automated driving . as most planning approaches based on candidate curves optimize the trajectory curve and the velocity profile separately , this paper formulates the problem as an unified non - linear programming ( nlp ) model , optimizing the trajectory curve and the acceleration profile ( acceleration is the derivative of velocity ) simultaneously . then a hybrid optimization algorithm named oode , developed by combining the idea of oo and differential evolution ( de ) , is proposed to solve the nlp model . with the acceleration profile optimized “ roughly ” , oode computes and compares “ rough ” ( biased but computationally - easier ) curve evaluations to select the best curve from candidates , so that a good enough curve can be obtained very efficiently . then the acceleration profile is optimized again “ accurately ” with the selected curve . simulation results show that good enough solutions are ensured with a high probability and our method is capable of working in real time . 

full - stack application of skin cancer diagnosis based on cnn model
convolutional neural network ( cnn ) is a subset of deep neural networks , and it has commonly applied to analyze images . skin cancer is a disease that can be observed without the help of expensive and professional instruments . at the same time , consulting a doctor in a hospital is not a cheap thing for many people . to tackle this issue , this paper introduced a skin cancer detection application based on cnn . in this application , the graphical user interface is implemented by swift ui , and the backend is expressjs . it loads a keras cnn model through tensorflow js to detect and classify skin cancer . the cnn model used in the application is created through tensorflow and trained with the ham10000 skin cancer data set . it also integrated natural language process ( nlp ) function , so that users can ask some questions and consult doctors online . the current version of this application can successfully operate in the ios environment , and fully achieve those functions above . also , the experimental result demonstrated that the accuracy of cnn model is around 75 % for ham10000 test set . 

hybridization of nonlinear and mixed - integer linear programming for aircraft separation with trajectory recovery
the approach presented in this paper aims at finding a solution to the problem of conflict - free motion planning for multiple aircraft on the same flight level with trajectory recovery . one contribution of this work is to develop three consistent models , i . e . , from a continuous - time representation to a discrete - time linear approximation . each of these models guarantees separation at all times and trajectory recovery , but they are not equally difficult to solve . a new hybrid algorithm is thus developed to use the optimal solution of a mixed - integer linear program as a starting point when solving a nonlinear formulation of the problem . the significance of this process is that it always finds a solution when the linear model is feasible while still taking into account the nonlinear nature of the problem . a test bed containing numerous data sets is then generated from three virtual scenarios . a comparative analysis with three different initializations of nonlinear optimization validates the efficiency of the hybrid method . 

formation of sql from natural language query using nlp
today , everyone has their own personal devices that connects to the internet . every user tries to get the information that they require through internet . most of the information is in the form of a database . a user who wants to access a database but having limited or no knowledge of database languages faces a challenging and difficult situation . hence , there is a need for a system that enables the users to access the information in the database . this paper aims to develop such a system using nlp by giving structured natural language question as input and receiving sql query as the output , to access the related information from the railways reservation database with ease . the steps involved in this process are tokenization , lemmatization , parts of speech tagging , parsing and mapping . the dataset used for the proposed system has a set of 2880 structured natural language queries on train fare and seats available . we have achieved 98 . 89 per cent accuracy . the paper would give an overall view of the usage of natural language processing ( nlp ) and use of regular expressions to map the query in english language to sql . 

detailed study of deep learning models for natural language processing
natural language processing involves computational processing , and understanding of human languages . with the increase in computation power , deep learning models are being used for various nlp tasks . further availability of large datasets of various languages enables the training of deep learning models . multiple processing layers are used by the deep learning methods for learning representations of data which are hierarchical in nature and which gives excellent results for different nlp tasks . this paper reviews the important models and methods in deep learning which are applied to natural language problems . in particular , convolutional neural network , recurrent neural network , long short - term memory , gated recurrent unit , recursive neural network have been described . also , their advantages and suitability to various natural language processing applications such as text classification , sentiment analysis , etc . have been reviewed . 

road navigation system using automatic speech recognition ( asr ) and natural language processing ( nlp ) 
in a highly evolving technical era , voice - based navigation systems play a major role to bridge the gap between human and machine . to overcome the difficulty in taking and understanding user ' s voice commands , simulating the natural language , process the route with user ' s turn by turn directions while mentioning key entities like street names , landmarks , point of interests , junctions and map the route in an interactive interface , we propose a user - centric roadmap navigation mobile application called “ direct me” . the approach of generating the user preferred route , system will first convert the audio streams into text through automatic speech recognizer ( asr ) using pocket sphinx library , followed by natural language processing ( nlp ) by utilizing stanford corenlp framework to retrieve the navigation - associated information and process the route in the map using google map api upon the user request . this system is used to provide an efficient approach to translate natural language directions to a machine - understandable format and will benefit the development of voice - based navigation - oriented humanmachine interface . 

using parsbert on augmented data for persian news classification
text classification is a fundamental task in natural language processing ( nlp ) . although many works have been done to perform text classification in english , the number of studies on persian text classification is limited . previous works on persian text classification often use classic machine learning methods such as naive bayes , support vector machines , decision trees , etc . while these methods are fast and straightforward , they need feature engineering , and their performance heavily depends on the selected features . in this paper , we first augment the input words with their stem form and then use a pre - trained language model for the persian language ( parsbert ) to classify the text . augmenting the input words with their stem form enables the proposed classifier to generalize well to the new unseen data . we compare the performance of our proposed model with that of traditional machine learning algorithms . the results show that the proposed model achieves a 0 . 91 accuracy and outperforms the traditional machine learning algorithm by at least + 0 . 4 absolute on both accuracy and f1 score . 

automatic summarisation of legal documents
we report on the sum project which applies automatic summarisation techniques to the legal domain . we describe our methodology whereby sentences from the text are classified according to their rhetorical role in order that particular types of sentence can be extracted to form a summary . we describe some experiments with judgments of the house of lords : we have performed automatic linguistic annotation of a small sample set and then hand - annotated the sentences in the set in order to explore the relationship between linguistic features and argumentative roles . we use state - of - the - art nlp techniques to perform the linguistic annotation using xml - based tools and a combination of rule - based and statistical methods . we focus here on the predictive capacity of tense and aspect features for a classifier . 

automated checking of conformance to requirements templates using natural language processing
templates are effective tools for increasing the precision of natural language requirements and for avoiding ambiguities that may arise from the use of unrestricted natural language . when templates are applied , it is important to verify that the requirements are indeed written according to the templates . if done manually , checking conformance to templates is laborious , presenting a particular challenge when the task has to be repeated multiple times in response to changes in the requirements . in this article , using techniques from natural language processing ( nlp ) , we develop an automated approach for checking conformance to templates . specifically , we present a generalizable method for casting templates into nlp pattern matchers and reflect on our practical experience implementing automated checkers for two well - known templates in the requirements engineering community . we report on the application of our approach to four case studies . our results indicate that : ( 1 ) our approach provides a robust and accurate basis for checking conformance to templates ; and ( 2 ) the effectiveness of our approach is not compromised even when the requirements glossary terms are unknown . this makes our work particularly relevant to practice , as many industrial requirements documents have incomplete glossaries . 

ontogene in biocreative ii . 5
we describe a system for the detection of mentions of protein - protein interactions in the biomedical scientific literature . the original system was developed as a part of the ontogene project , which focuses on using advanced computational linguistic techniques for text mining applications in the biomedical domain . in this paper , we focus in particular on the participation to the biocreative ii . 5 challenge , where the ontogene system achieved best - ranked results . additionally , we describe a feature - analysis experiment performed after the challenge , which shows the unexpected result that one single feature alone performs better than the combination of features used in the challenge . 

thesaurus - based efficient example retrieval by generating retrieval queries from similarities
in example - based nlp , the problem of computational cost of example retrieval is severe , since the retrieval time increases in proportion to the number of examples in the database . this paper proposes a novel example retrieval method for avoiding full retrieval of examples . the proposed method has the following three features , 1 ) it generates retrieval queries from similarities , 2 ) efficient example retrieval through the tree structure of a thesaurus , 3 ) binary search along subsumption ordering of retrieval queries . example retrieval time drastically decreases with the method . 

tutorial : text analytics for security
computing systems that make security decisions often fail to take into account human expectations . this failure occurs because human expectations are typically drawn from in textual sources ( e . g . , mobile application description and requirements documents ) and are hard to extract and codify . recently , researchers in security and software engineering have begun using text analytics to create initial models of human expectation . in this tutorial , we will provide an introduction to popular techniques and tools of natural language processing ( nlp ) and text mining , and share our experiences in applying text analytics to security problems . we will also highlight the current challenges of applying these techniques and tools for addressing security problems . we conclude with discussion of future research directions . 

key to crossword solving : nlp
the struggle of technology to understand natural language has been one of the greatest hurdles of humankind . various techniques and algorithms have contributed to significant advancements in the field of nlp . one of the most primary challenges that nlp faces is being able to determine the meaning and essence of a sentence which may have multiple variations of syntax and semantics . crossword solving involves utilizing the knowledge one has over the language and applying it with the various constraints that a clue will provide . as opposed to generic brute force crossword solvers , this work aims to implement automated solving of a crossword using knowledge - based concepts of nlp . the solver provides the user with a potential solution set which contains a collection of words out of which the user may choose the answer which he or she feels is most appropriate . thus , the entire crossword can be solved by using the knowledge obtained from natural language processing , and this method stays true to the essence of crossword solving . 

understanding financial transaction documents using natural language processing
in this paper , we share our experiences creating nlp based ai platform for finance - appzen ( http :// www . appzen . com) . appzen ' s auditing technology is being utilized by over 500 enterprise customers including multiple fortune 500 companies for auditing employee expenses . appzen ' s technology can process , analyze and identify relationships between various kinds of transaction documents such as - receipts , invoices , contracts and purchase orders . each type of transaction document requires custom processing and analysis due to the diversity in language and structure of the document . contracts typically require deep understanding of the content such as identifying sentence structures , identifying entities and relationships between them compared to receipts and invoices , which are somewhat semi - structured and require a different kind of processing . we elaborate on the challenges we have experienced and use of nlp in conjunction with a lightweight semantic layer to alleviate these challenges . 

linked data for toponym linking in french literary texts
the present article discusses first experiments in toponym linking of modern french digital editions aiming to provide an external referent to linked data sources . we have so far focused on testing two knowledge bases - french dbpedia and geonames - for recall . results highlight quality issues in these data sets for usage in nlp - tasks in domain - specific heritage texts . 

bagging to find better expansion words
the supervised learning has been applied into the query expansion techniques , which trains a model to predict the “ goodness ” or “ utility ” of the expanded term to the retrieval system . there are many features to measure the relatedness between the expanded word and the query , which can be incorporated in the supervised learning to select the expanded terms . the training data set is generated automatically by a tricky method . however , this method can be affected by many aspects . a severe problem is that the distribution of the features is query - dependent , which has not been discussed in previous work . with a different distribution on the features , it is questionable to merge these training instances together and use the whole data set to train one single model . in this paper , we first investigate the statistical distribution of the auto - generated training data and show the problems in the training data set . based on our analysis , we proposed to use the bagging method to ensemble several regression models in order to get a better supervised model to make prediction on the expanded terms . we conducted the experiments on the trec benchmark test collections . our analysis on the training data reveals some interesting phenomena about the query expansion techniques . the experiment results also show that the bagging approach can achieve the state - of - art retrieval performance on the standard trec data set . 

nlp support for faceted navigation in scholarly collections
hierarchical faceted metadata is a proven and popular approach to organizing information for navigation of information collections . more recently , digital libraries have begun to adopt faceted navigation for collections of scholarly holdings . a key impediment to further adoption is the need for the creation of subject - oriented faceted metadata . the castanet algorithm was developed for the purpose of ( semi ) automated creation of such structures . this paper describes the application of castanet to journal title content , and presents an evaluation suggesting its efficacy . this is followed by a discussion of areas for future work . 

an industrial study of natural language processing based test case prioritization
in mobile application development , the frequentsoftware release limits the testing time resource . in order todetect bugs in early phases , researchers proposed various testcase prioritization ( tcp ) techniques in past decades . in practice , considering that some test case is described or contains text , theresearchers also employed natural language processing ( nlp ) to assist the tcp techniques . this paper conducted an extensiveempirical study to analyze the performance of three nlp basedtcp technologies , which is based on 15059 test cases from 30industrial projects . the result shows that all of these threestrategies can help to improve the efficiency of software testing , and the risk strategy achieved the best performance across thesubject programs . 

rdf ( s )/ xml linguistic annotation of semantic web pages
although with the semantic web initiative much research on web page semantic annotation has already been done by ai researchers , linguistic text annotation , including the semantic one , was originally developed in corpus linguistics and its results have been somehow neglected by ai . the purpose of the research presented in this proposal is to prove that integration of results in both fields is not only possible , but also highly useful in order to make semantic web pages more machine - readable . a multi - level ( possibly multi - purpose and multi - language ) annotation model based on eagles standards and ontological semantics , implemented with last generation semantic web languages ( rdf ( s ) /xml ) is being developed to fit the needs of both communities ; the present paper focuses on its semantic level . 

atp - oie : an autonomous open information extraction method
this paper describes an innovative open information extraction method known as atp - oie1 . it utilizes extraction patterns to find semantic relations . these patterns are generated automatically from examples , so it has greater autonomy than methods based on fixed rules . atp - oie can also summon other methods , reverb and clausie , if it is unable to find valid semantic relations in a sentence , thus improving its recall . in these cases , it is capable of generating new extraction patterns online , which improves its autonomy . it also implements different mechanisms to prevent common errors in the extraction of semantic relations . lastly , atp - oie was compared with other state - of - the - art methods in a well known texts database : reuters - 21578 , obtaining a higher precision than with other methods . 

a comparative study on unsupervised feature selection methods for text clustering
text clustering is one of the central problems in text mining and information retrieval area . for the high dimensionality of feature space and the inherent data sparsity , performance of clustering algorithms will dramatically decline . two techniques are used to deal with this problem : feature extraction and feature selection . feature selection methods have been successfully applied to text categorization but seldom applied to text clustering due to the unavailability of class label information . in this paper , four unsupervised feature selection methods , df , tc , tvq , and a new proposed method tv are introduced . experiments are taken to show that feature selection methods can improves efficiency as well as accuracy of text clustering . three clustering validity criterions are studied and used to evaluate clustering results . 

automatic detection of tags for political blogs
this paper describes a technique for automatically tagging political blog posts using svm ' s and named entity recognition . we compare the quality of the tags detected by this approach to earlier approaches in other domains , observing effects from the political domain and benefits from nlp techniques complementary to the core svm method . 

gobo : quantizing attention - based nlp models for low latency and energy efficient inference
attention - based models have demonstrated remarkable success in various natural language understanding tasks . however , efficient execution remains a challenge for these models which are memory - bound due to their massive number of parameters . we present gobo , a model quantization technique that compresses the vast majority ( typically 99 . 9 % ) of the 32 - bit floating - point parameters of state - of - the - art bert models and their variants to 3 bits while maintaining their accuracy . unlike other quantization methods , gobo does not require fine - tuning nor retraining to compensate for the quantization error . we present two practical hardware applications of gobo . in the first gobo reduces memory storage and traffic and as a result inference latency and energy consumption . this gobo memory compression mechanism is plug - in compatible with many architectures ; we demonstrate it with the tpu , eyeriss , and an architecture using tensor cores - like units . second , we present a co - designed hardware architecture that also reduces computation . uniquely , the gobo architecture maintains most of the weights in 3b even during computation , a property that : ( i ) makes the processing elements area efficient , allowing us to pack more compute power per unit area , ( ii ) replaces most multiply - accumulations with additions , and ( iii ) reduces the off - chip traffic by amplifying on - chip memory capacity . 

iot device auto - tagging using transformers
the iot platform can identify a device matching the data only when securing the information on a large amount of iot devices in advance . however , in a situation where a lot of companies release a variety type of iot , it is not easy to retain information about all iot devices . that is , it is a difficult situation where the current iot platform has difficulty in analyzing unstandardized tagging information . therefore , this paper provides a technique of ` tagging ' that automatically identifies the device based on information collected from iot devices or sensor devices . this technique was developed based on the natural language processing algorithm based on the attention mechanism among the machine learning models . 

predicting decisions of the philippine supreme court using natural language processing and machine learning
for the past decades , philippine courts have been experiencing severe court congestion and case backlog problems . this study aims to provide a solution to alleviate these problems by predicting the outcome of court cases . as the philippine supreme court case decisions are the only readily available data online , we use this as our dataset . we use natural language processing , particularly the bag - of - words model to represent the case text into n - grams . spectral clustering is also used to group these n - grams into topics . these n - gram and topic features are then input to the machine learning algorithms such as linear support vector machines and random forest classifiers . linear support vector machine results reached 45 % on the n - gram datasets and 55 % on the topic datasets . the best result we obtained is 59 % on the topic datasets using a random forest classifier . this is the first systematic study in predicting philippine supreme court decisions based purely on textual content . 

mining semantic representation from medical text : a bayesian approach
machine learning is a subfield of artificial intelligence that deals with the exploration and construction of systems that can learn from data . machine learning trains the computers to manage the critical situations via examining , self - training , inference by observation and previous experience . this paper provides an overview of the development of an efficient classifier that represents the semantics in medical data ( medline ) using a machine learning ( ml ) perspective . in recent days people are more concerned about their health and explore ways to identify health related information . but the process of identifying the semantic representation for the medical terms is a difficult task . the main goal of our work was to identify the semantic representation for the medical abstracts in the medline repository using machine learning and natural language processing ( nlp ) . 

constructing an anaphorically annotated corpus with non - experts : assessing the quality of collaborative annotations
this paper reports on the ongoing work of phrase detectives , an attempt to create a very large anaphorically annotated text corpus . annotated corpora of the size needed for modern computational linguistics research cannot be created by small groups of hand - annotators however the esp game and similar games with a purpose have demonstrated how it might be possible to do this through web collaboration . we show that this approach could be used to create large , high - quality natural language resources . 

detecting compositionality in multi - word expressions
identifying whether a multi - word expression ( mwe ) is compositional or not is important for numerous nlp applications . sense induction can partition the context of mwes into semantic uses and therefore aid in deciding compositionality . we propose an unsupervised system to explore this hypothesis on compound nominals , proper names and adjective - noun constructions , and evaluate the contribution of sense induction . the evaluation set is derived from wordnet in a semisupervised way . graph connectivity measures are employed for unsupervised parameter tuning . 

extracting context - rich entailment rules from wikipedia revision history
recent work on textual entailment has shown a crucial role of knowledge to support entailment inferences . however , it has also been demonstrated that currently available entailment rules are still far from being optimal . we propose a methodology for the automatic acquisition of large scale context - rich entailment rules from wikipedia revisions , taking advantage of the syntactic structure of entailment pairs to define the more appropriate linguistic constraints for the rule to be successfully applicable . we report on rule acquisition experiments on wikipedia , showing that it enables the creation of an innovative ( i . e . acquired rules are not present in other available resources ) and good quality rule repository . 

natural language processing for mediawiki : the semantic assistants approach
we present a novel architecture for the integration of natural language processing ( nlp ) capabilities into wiki systems . the vision is that of a new generation of wikis that can help developing their own primary content and organize their structure by using state - of - the - art technologies from the nlp and semantic computing domains . the motivation for this integration is to enable wiki users - - novice or expert - - to benefit from modern text mining techniques directly within their wiki environment . we implemented these ideas based on mediawiki and present a number of real - world application case studies that illustrate the practicability and effectiveness of this approach . 

when conset meets synset : a preliminary survey of an ontological lexical resource based on chinese characters
this paper describes an on - going project concerning with an ontological lexical resource based on the abundant conceptual information grounded on chinese characters . the ultimate goal of this project is set to construct a cognitively sound and computationally effective character - grounded machine - understandable resource . philosophically , chinese ideogram has its ontological status , but its applicability to the nlp task has not been expressed explicitly in terms of language resource . we thus propose the first attempt to locate chinese characters within the context of ontology . having the primary success in applying it to some nlp tasks , we believe that the construction of this knowledge resource will shed new light on theoretical setting as well as the construction of chinese lexical semantic resources . 

the nlp task at inex 2005
with xml information retrieval , like in traditional ir , the user ' s information need is loosely defined , linguistic variations are frequent , and answers are a ranked list of relevant elements . like in database querying , structure is of importance and a simple list of keywords may not be sufficient to define an xml query . structured query languages for xml have been developed , but appear to be difficult to use even by system - level users , let alone end - users . therefore developing natural language interfaces for xml - ir requires innovative solutions . inex provides a framework ( documents , topics and relevance assessments ) for independent evaluation of xml - ir systems and approaches . in 2002 and 2003 systems in the ad - hoc task accepted formal language queries ( i . e . < title > elements ) and produced results lists of relevant xml elements ( usually well below the document root ) . in 2004 inex introduced a natural language processing task that operated in parallel with the ad - hoc task ( using the same topics and assessments ) except that systems operated on xml natural language queries ( i . e . < description > elements ) . in this report we describe the motivation for using nlp in xml ir , the general approaches that were tested at inex 2005 , and comment on the results . 

exploring grammatical error correction with not - so - crummy machine translation
to date , most work in grammatical error correction has focused on targeting specific error types . we present a probe study into whether we can use round - trip translations obtained from google translate via 8 different pivot languages for whole - sentence grammatical error correction . we develop a novel alignment algorithm for combining multiple round - trip translations into a lattice using the terp machine translation metric . we further implement six different methods for extracting whole - sentence corrections from the lattice . our preliminary experiments yield fairly satisfactory results but leave significant room for improvement . most importantly , though , they make it clear the methods we propose have strong potential and require further study . 

improving skip - gram embeddings using bert
contextualized embeddings such as bert and gpt have been shown to give significant improvement in nlp tasks . on the other hand , static embeddings such as skip - gram and glove still have desirable characteristics such as low computational cost , easy deployment and freedom from severe contextualized variation in representation . there has been some recent attempt enhancing the skip - gram model by adding syntactic information of context using gcn . we investigate the use of bert embeddings instead for stronger context representation , which contains not only syntactic and surface features , but also rich knowledge from large - scale pre - training . results show that bert - enhanced skip - gram embeddings outperform gcn - enhanced embeddings on a range of tasks . such embeddings also outperform recent effort distilling bert embeddings into context - independent vectors . 

unsupervised concept annotation using latent dirichlet allocation and segmental methods
training efficient statistical approaches for natural language understanding generally requires data with segmental semantic annotations . unfortunately , building such resources is costly . in this paper , we propose an approach that produces annotations in an unsupervised way . the first step is an implementation of latent dirichlet allocation that produces a set of topics with probabilities for each topic to be associated with a word in a sentence . this knowledge is then used as a bootstrap to infer a segmentation of a word sentence into topics using either integer linear optimisation or stochastic word alignment models ( ibm models ) to produce the final semantic annotation . the relation between automatically - derived topics and task - dependent concepts is evaluated on a spoken dialogue task with an available reference annotation . 

introduction to the special issue on finite - state methods in nlp
no abstract available . 

automatic text classification using modified centroid classifier
this work proposes an approach to address the problem of inductive bias or model misfit incurred by the centroid classifier assumption to enhance the automatic text classification task . this approach is a trainable classifier , which takes into account tfidf as a text feature . the main idea of the proposed approach is to take advantage of the most similar training errors to the classification model to successively update it based on a certain threshold . the proposed approach is simple to implement and flexible . the proposed approach performance is measured at several threshold values on the reuters - 21578 text categorization test collection . the experimental results show that the proposed approach can improve the performance of centroid classifier . 

the xmlization of a dependency treebank in conll format for evaluating linguistic queries using xquery
treebanks are essential resources for both data - driven approaches to natural language processing ( nlp ) and empirical linguistic researches . developing these resources is time - and cost - consuming and requires specialized expertise . therefore , they should be designed to be reused for different purposes . currently , there are several dependency treebanks for some languages which are annotated in conll format . for some languages , such as persian , they are the few available linguistic resources . these treebanks are more suitable for the input of data - driven parsers , and querying linguistic data in them is not easy . in recent years , xml has been widely used for formatting treebanks , and there are various tools available for querying and annotating a linguistic croups in this format . in this paper , we present a tool for converting a dependency treebank in conll format to an appropriate xml format . we designed the xml scheme to be particularly suitable for writing linguistic queries in xquery syntax . 

fif : a nlp - based feature identification framework for data warehouses
in a data warehouse , selecting the relevant features is an iterative process that is laborious , time - consuming , and error - prone due to selection bias introduced by either the data expert or the data - analyst . in order to address this challenge , this paper introduces fif , a feature identification framework that uses natural language processing ( nlp ) to analyze the hypotheses , identify the relevant feature space and predict the appropriate data mining task and model . the fif is designed on the principles of microservices architecture pattern , comprising of five core groups of microservices : ( a ) nlp pre - processor , ( b ) attribute identifier , ( c ) feature identifier , ( d ) topic modeller , and ( e ) data mining task evaluator . finally , fif is evaluated with five hypotheses against our data warehouse . 

hidden markov model based part of speech tagging for nepali language
natural language processing ( nlp ) is mainly concerned with the development of computational models and tools of aspects of human ( natural ) language processing . part of speech tagging ( pos ) is well studied topic and also one of the most fundamental preprocessing steps for any language in nlp . natural language processing of nepali is still lack significant research efforts in the area of nlp in india . pos tagging of nepali is a necessary component for most nlp applications in nepali , which analyses the construction of the language , behavior of the language and can be used to develop automated tools for language processing . from the literature survey and related works , it has been found that , not much work has been done previously on pos tagging for nepali language in india due to lack of comprehensive set of tagged corpus or correct hand written rules . in this paper , hidden markov model ( hmm ) based part of speech ( pos ) tagging for nepali language has been discussed . hmm is the most popular used statistical model for pos tagging that uses little amount of knowledge about the language , apart from contextual information of the language . the evaluation of the tagger has been done using the corpora , which are collected from tdil ( technology development for indian languages ) and the bis tagset of 42 tags . tagset has been designed to meet the morph - syntactic requirements of the nepali language . apart from corpora and the tagset , python programming language and the nltk ' s ( natural language toolkit ) library has been used for implementation . the tagger achieves accuracy over 96 % for known words but for unknown words , the research is still continuing . 

source models of long - period seismic events at galeras volcano , colombia
long - period ( lp ) seismic events have occurred repeatedly at galeras volcano , colombia , during the transition from effusive dome formation to explosive vulcanian eruptions . since 1989 , two types of lp events have been observed there : one characterized by long - lasting , decaying harmonic oscillations ( nlp events ) and the other by non - harmonic oscillatory features ( blp events ) . nlp events are attributed to resonances of a dusty gas - filled crack in the magma plugging the eruptive conduit . sixteen episodes of nlp events occurred at galeras during 1992 – 2010 , each characterized by systematic temporal variations in the frequencies and quality factors of nlp events . our and previous estimates of crack model parameters during three of those nlp episodes indicate that the similar temporal variations in crack geometry and fluid properties can be explained by an increase in the ash content within the crack and a decrease in crack volume . we found that nlp events , associated with low so 2 fluxes , are anticorrelated with blp events , which are accompanied by high so 2 emissions . from our observations and analytical results , we inferred that blp events are generated by resonances of open cracks in the uppermost magma plug , corresponding to tuffisite veins , that efficiently transfer volcanic gases . after sufficient degassing and densification , the magma plug effectively seals the conduit . the growing overpressure in the deeper magma is then released through a shear fracture along the conduit margin . the intrusion of deeper , vesiculated magma into the shear fracture depressurizes and fragments the magma , producing a dusty gas and triggering the crack resonances that generate nlp events . our results thus indicate that the evolution of the properties of the magma plug controls the occurrences of blp and nlp events at galeras . although nlp events do not always precede explosive eruptions , they indicate that an important overpressure is building in the shallow conduit . 

automatic nlp - based enrichment of e - learning content for english language learning
the creation of quality content for e - learning resources is a time - consuming task . to simplify the process of content creation for language learning and enable easy adaptability for different requirements and language levels we strive to add as much automation as possible . in order to still obtain high quality , we present in this paper our approaches to enrich e - learning - based english vocabulary tests , which support blended learning and improve direct user feedback . we integrate openly available language resources for selecting and appending usage example sentences for a given vocabulary corpus . furthermore we discuss our results and suggest to acquire natural language processing ( nlp ) based techniques to improve the generation of language related contents in general and to overcome some of the weaknesses of our current solution . 

annotating esl errors : challenges and rewards
in this paper , we present a corrected and error - tagged corpus of essays written by non - native speakers of english . the corpus contains 63000 words and includes data by learners of english of nine first language backgrounds . the annotation was performed at the sentence level and involved correcting all errors in the sentence . error classification includes mistakes in preposition and article usage , errors in grammar , word order , and word choice . we show an analysis of errors in the annotated corpus by error categories and first language backgrounds , as well as inter - annotator agreement on the task . we also describe a computer program that was developed to facilitate and standardize the annotation procedure for the task . the program allows for the annotation of various types of mistakes and was used in the annotation of the corpus . 

real - life human activity recognition with tri - axial accelerometer data from smartphone using hybrid long short - term memory networks
human activity recognition ( har ) has an enthusiastic research field in time - series classification due to its variation of successful applications in various domains . the availability of affordable wearable devices have provided many challenging and interesting research har problems . current researches suggest that deep learning approaches are suited to automated feature extraction from raw sensor data , instead of conventional machine learning approaches that reply on handcrafted features . based on the recent success of long short - term memory ( lstm ) networks for har domains , this work proposes a generic framework for accelerometer data based on lstm networks for real - life har . four hybrid lstm networks have been comparatively studied on a public available real - life har dataset . moreover , we take advantage of bayesian optimization techniques for tuning hyperparameter of each lstm networks . the experimental results indicate that the cnn - lstm network surpasses other hybrid lstm networks . 

computational intelligence framework for automatic quiz question generation
computational intelligence techniques are attracting more and more attention in nlp and text analysis applications . this paper is devoted to their use in automatic question generation based on text analysis with the goal to develop the computational intelligence framework that should automate or semi - automate the process of quiz and exam question generation . the framework operation is based on information retrieval and nlp algorithms . it incorporates the application of production rules , lstm neural network models , and other intelligent techniques . it allows generating multiple choice questions , true and false questions as well as " wh " - type ( what ? when ? how ? ) questions . automation procedures for each type question generation are developed , presented and analyzed . the typical challenges in framework development and application are considered and possible solutions are discussed . the results of the framework application and its use for quiz generation in a real college class are presented and analyzed . 

enticing notification text & the impact on engagement
push - notifications are a design tool used by mobile and web apps to alert subscribers to new information . in recent years , due to widespread adoption of the technology and the shrinking level of user attention available , marketing techniques have been deployed to persuade subscribers to engage positively with notifications . one such technique , known as the curiosity gap , exploits lowenstein ' s information - gap theory . this paper explores the impact of enticing notification text , instilled by the curiosity gap , on subsequent engagement actions . a classifier was defined to identify enticing language in notifications . features commonly paired with enticing text were identified . intelligent notification delivery agents , trained using data captured in - the - wild , were evaluated using enticing and non - enticing notifications to demonstrate the influence of enticing text . additionally , a solution was proposed and briefly evaluated for limiting subscriber susceptibility to enticing notifications . 

ridiculously expensive watches and surprisingly many reviewers : a study of irony
irony is something most people can tell is therewhen they see it , but it is not so easy to define , let alone detectautomatically . in this paper we describe the construction of abalanced corpus of ironic vs . serious watch reviews and show thepromising results achieved by classifiers trained on this corpusin predicting the presence of irony or lack thereof in productreviews from a manually labeled corpus . we try to find commonfeatures in the two corpora and outline our next steps towardsa model which would detect ironic utterances in more general contexts . 

using synthetic clinical data to train an hmm - based pos tagger
the accuracy of part of speech ( pos ) tagging reported in medical natural language processing ( nlp ) literature is typically very high when training and testing data sets are from the same domain and have similar characteristics , but is lower when these differ . this presents a problem for clinical nlp , where it is difficult to obtain large corpora of training data suitable for localized tasks . we experimented with implementing the tnt pos tagger and training it on a manually tagged small corpus of publicly available synthetic clinical reports supplemented with widely used public corpora ( genia and penn treebank ) . we describe this implementation and report the evaluation results on mipacq , a large corpus of manually tagged clinical text . our tagger achieves accuracy comparable to pos taggers trained on large amounts of real clinical data ( 91 - 93% ) . this demonstrates that medical nlp developers do not need to rely on large restricted resources for pos tagging . 

depression detection in tweets from urban cities of malaysia using deep learning
this document was inspired by how the usage of social media platforms in malaysia such as twitter have drastically increased ever since the recent covid - 19 pandemic . while practicing social distancing and other pandemic regulations was for the betterment and prevention of physical health , mental health of most was affected negatively . people generally revolve around with having interactions with other humans and once the physical form of it was cut , people tend to turn to social media . a twitter sentiment analysis approach was used to find the casual link between social media and mental health . this project aims to utilise the broaden scope of social media - based mental health measures since research proves the evidence of a link between depression and specific linguistic features as well . therefore , the research entails on how the problem statement of this project on developing an algorithm that can predict text - based depression symptoms using deep learning and natural language processing ( nlp ) can be achieved . the objective of the project is to identify depressive tweets using nlp and deep learning in the urban cities of malaysia within the beginning of the covid - 19 period to enable individuals , their caregivers , parents , and even medical professionals to identify the linguistic clues that point towards to signs of mental health deterioration . additionally , this paper also researches to make the proposed system to identify words that represent depression and categorize them accordingly as well as improve the accuracy of the system in identifying tweets that display the depression related words based on its specific location . this objective will be achieved following the methodology using the deep learning approach and natural language processing technique . a recurrent neural network approach was implemented in this project known as the long - term short memory , which is a form of advanced rnn , that allows information to be preserved . conducting an analysis on the linguistic indicators from tweets allows for a low - profile assessment that can supplement traditional services which then consequently would allow for a much earlier detection of depressive symptoms . since this research entails on finding the link between tweets and machine learning ' s ability to detect depressive symptoms , the success this project brings forth a meaningful help towards those who are mentally affected but are unable to seek help or are unsure on diagnosing themselves as this project helps alert the government and psychologist on the need for it . the project thus far has an accuracy rate of 94% , along with , precision rate of 0 . 94 , recall of 0 . 96 and an f1 score of 0 . 95 . 

exploring the benefits of utilizing conceptual information in test - to - code traceability
striving for reliability of software systems often results in immense numbers of tests . due to the lack of a generally used annotation , finding the parts of code these tests were meant to assess can be a demanding task . this is a valid problem of software engineering called test - to - code traceability . recent research on the subject has attempted to cope with this problem applying various approaches and their combinations , achieving profound results . these approaches have involved the use of naming conventions during development processes and also have utilized various information retrieval ( ir ) methods often referred to as conceptual information . in this work we investigate the benefits of textual information located in software code and its value for aiding traceability . we evaluated the capabilities of the natural language processing technique called latent semantic indexing ( lsi ) in the view of the results of the naming conventions technique on five real , medium sized software systems . although lsi is already used for this purpose , we extend the viewpoint of one - to - one traceability approach to the more versatile view of lsi as a recommendation system . we found that considering the top 5 elements in the ranked list increases the results by 30 % on average and makes lsi a viable alternative in projects where naming conventions are not followed systematically . 

incorporating pv inverter control schemes for planning active distribution networks
summary form only given . the distribution network planning under active network management ( anm ) schemes is becoming of interest due to substantial benefits in facilitating the increasing integration of renewable energy sources . this paper presents various potential anm schemes based on the photovoltaic inverter control ( pvic ) considering enhanced utilization of the inverter reactive power capability . depending on the active power generation of pv arrays , inverter size and desired reactive power settings , several pvic schemes are proposed . the pvic schemes are incorporated in the optimal power flow ( opf ) and formulated as a nonlinear programming ( nlp ) problem . in this study , the pvic schemes are applied to maximize the total wind - distributed generation ( dg ) penetration on a typical u . k . distribution system . various case studies are presented and compared to evaluate the performance . the results show that the proposed schemes can significantly increase the wind penetration levels by 45 . 4 % and up to 92 . 3 % . 

content - aware tweet location inference using quadtree spatial partitioning and jaccard - cosine word embedding
inferring locations from user texts on social media platforms is a non - trivial and challenging problem relating to public safety . we propose a novel non - uniform grid - based approach for location inference from twitter messages using quadtree spatial partitions . the proposed algorithm uses natural language processing ( nlp ) for semantic understanding and incorporates cosine similarity and jaccard similarity measures for feature vector extraction and dimensionality reduction . we chose twitter as our experimental social media platform due to its popularity and effectiveness for the dissemination of news and stories about recent events happening around the world . our approach is the first of its kind to make location inference from tweets using quadtree spatial partitions and nlp , in hybrid word - vector representations . the proposed algorithm achieved significant classification accuracy and outperformed state - of - the - art grid - based content - only location inference methods by up to 24 % in correctly predicting tweet locations within a 161km radius and by 300km in median error distance on benchmark datasets . 

conversion between dependency structures and phrase structures using a head finder algorithm
this paper proposes how to convert projective dependency structures into flat phrase structures with language - independent syntactic categories , and use a head finder algorithm to convert these phrase structures back into dependency structures . the head finder algorithm is implemented by a maximum entropy approach with constraint information . the converted phrase structures can be parsed using a hierarchical coarse - to - fine method with latent variables . experimental results show that the approach finds 98 . 8 % heads of all phrases , and our algorithm achieves state - of - the - art dependency parsing performance in english treebank . 

wrs : a novel word - embedding method for real - time sentiment with integrated lstm - cnn model
artificial intelligence ( ai ) is a research - focused technology in which natural language processing ( nlp ) is a core technology in ai . sentiment analysis ( sa ) aims to extract and classify the people ' s opinions by nlp . the machine learning ( ml ) and lexicon dictionaries have limited competency to efficiently analyze massive live media data . recently , deep learning methods significantly enrich the accuracy of recent sentiment models . however , the existing methods provide the aspect - based extraction that reduces individual word accuracy if a sentence does not follow the aspect information in real - time . therefore , this paper proposes a novel word embedding method for the real - time sentiment ( wrs ) for word representation . the wrs ' s novelty is a novel word embedding method , namely , word - to - word graph ( w2wg ) embedding that utilizes the word2vec approach . the wrs method assembles the different lexicon resources to employ the w2wg embedding method to achieve the word feature vector . robust neural networks leverage these features by integrating lstm and cnn to improve sentiment classification performance . lstm is utilized to store the word sequence information for the effective real - time sa , and cnn is applied to extract the leading text features for sentiment classification . the experiments are conducted on twitter and imdb datasets . the results demonstrate our proposed method ' s effectiveness for real - time sentiment classification . 

krushi – the farmer chatbot
as per the reports of november 2020 , around 58 % of india ' s population earns a primary source of living from agriculture . but , close to 10 , 000 farmers every year succumb to the harsh conditions in the agricultural sector . these conditions arise when the product of crops is not as desired , spoilt crops , which leads to large loans on the farmers . in this paper , we present an artificial intelligence ( al ) chatbot that assists the farmers by providing solutions to agricultural queries . some of the questions are concerning crop sowing , crop diseases , seasons related issues . thus , benefit the farmers in making the right decisions regarding their crops , thereby increasing their yield . krushi - the farmer chatbot is an end - to - end trainable learning model to create a conversational system with minimum error and answer questions about current conditions . the chatbot is build using artificial intelligence ( ai ) and machine learning ( ml ) techniques . the dataset for the chatbot is used from kisan call centre ( kcc ) . the proposed system , answers queries related to weather , plant protection , animal husbandry , market price , fertilizer uses , government schemes , soil testing with an overall accuracy of 96 . 1 % using rasa x . 

towards a syntactically and semantically enriched lexicon for vietnamese processing
lexicon is an important resource in natural language processing ( nlp ) , as it provides nlp systems with lexical information at different levels , from morphology to semantics . for vietnamese , lexical resources are available for several basic tools such as word segmentation , part - of - speech tagging and syntactic parsing . in this paper , we discuss the construction of a lexicon enriched with syntactic and semantic information , based on an existing computational lexicon for vietnamese . this lexicon is designed to serve for a syntactic and semantic parser using the tree adjoining grammar ( tag ) formalism . 

joining statistics with nlp for text categorization
automatic news categorization systems have produced high accuracy , consistency , and flexibility using some natural language processing techniques . these knowledge - based categorization methods are more powerful and accurate than statistical techniques . however , the phrasal pre - processing and pattern matching methods that seem to work for categorization have the disadvantage of requiring a fair amount of knowledge - encoding by human beings . in addition , they work much better at certain tasks , such as identifying major events in texts , than at others , such as determining what sort of business or product is involved in a news event . statistical methods for categorization , on the other hand , are easy to implement and require little or no human customization . but they don ' t offer any of the benefits of natural language processing , such as the ability to identify relationships and enforce linguistic constraints . our approach has been to use statistics in the knowledge acquisition component of a linguistic pattern - based categorization system , using statistical methods , for example , to associate words with industries and identify phrases that information about businesses or products . instead of replacing knowledge - based methods with statistics , statistical training replaces knowledge engineering . this has resulted in high accuracy , shorter customization time , and good prospects for the application of the statistical methods to problems in lexical acquisition . 

vocabulary profile as a measure of vocabulary sophistication
this study presents a method that assesses esl learners ' vocabulary usage to improve an automated scoring system of spontaneous speech responses by non - native english speakers . focusing on vocabulary sophistication , we estimate the difficulty of each word in the vocabulary based on its frequency in a reference corpus and assess the mean difficulty level of the vocabulary usage across the responses ( vocabulary profile) . three different classes of features were generated based on the words in a spoken response : coverage - related , average word rank and the average word frequency and the extent to which they influence human - assigned language proficiency scores was studied . among these three types of features , the average word frequency showed the most predictive power . we then explored the impact of vocabulary profile features in an automated speech scoring context , with particular focus on the impact of two factors : genre of reference corpora and the characteristics of item - types . the contribution of the current study lies in the use of vocabulary profile as a measure of lexical sophistication for spoken language assessment , an aspect heretofore unexplored in the context of automated speech scoring . 

distributed modules for text annotation and ie applied to the biomedical domain
biological databases contain facts from scientific literature , which have been curated by hand to ensure high quality . curation is time - consuming and can be supported by information extraction methods . we present a server which identifies biological facts in scientific text and presents the annotation to the curator . such facts are : uniprot , umls and go terminology , identification of gene and protein names , mutations and protein - protein interactions . uniprot , umls and go concepts are automatically linked to the original source . the module for mutations is based on syntax patterns and the one for protein - protein interactions on nlp . all modules work independently of each other in single threads and are combined in a pipeline to ensure proper meta data integration . for fast response time the modules are distributed on a linux cluster . the server is at present available to curation teams of biomedical data and will be opened to the public in the future . 

moving toward semantics for language processing : recent advances in resource construction and application
summary form only given . in this paper the author describes the recent work at various locations , focusing on the resource components required ( including isi ' s symbol definition ontology omega and the bbn - upenn - colorado - isi large corpus ontobank of ( shallow ) meaning representations ) and the resources and methods one needs to build them ( including existing ontologies , human annotation procedures , and a verification methodology , as embodied in a pilot project called learning by reading that involves various prominent researchers ) . this work is bringing together the nlp and ke / kr communities after a separation of some 40 years , and poses challenges for each side : dealing with large amounts of possibly incorrect knowledge is not something ke / kr has comfortable with , and dealing with requirements that knowledge be formalized and consistent is not something that large - scale nlp has been comfortable with . the research discussed here enables numerous experiments on the wide - scale use of ( shallow ) semantics , to the benefit of nlp applications of all kinds . 

extending nlp tools repositories for the interaction with language data resources repositories
this short paper presents some motivations behind the organization of the acl / eacl01 " workshop on sharing tools and resources for research and education " , concentrating on the possible connection of tools and resources repositories . taking some papers printed in this volume and the acl natural language software registry as a basis , we outline some of the steps to be done on the side of nlp tool repositories in order to achieve this goal . 

ten social dimensions of conversations and relationships
decades of social science research identified ten fundamental dimensions that provide the conceptual building blocks to describe the nature of human relationships . yet , it is not clear to what extent these concepts are expressed in everyday language and what role they have in shaping observable dynamics of social interactions . after annotating conversational text through crowdsourcing , we trained nlp tools to detect the presence of these types of interaction from conversations , and applied them to 160m messages written by geo - referenced reddit users , 290k emails from the enron corpus and 300k lines of dialogue from movie scripts . we show that social dimensions can be predicted purely from conversations with an auc up to 0 . 98 , and that the combination of the predicted dimensions suggests both the types of relationships people entertain ( conflict vs . support ) and the types of real - world communities ( wealthy vs . deprived ) they shape . 

qtip : multi - agent nlp and privacy architecture for information retrieval in usable web privacy software
we present a generic natural language processing ( nlp ) architecture , acronym qtil , based on a system of cooperating multiple agents ( q / a , t , i , and l agents ) which can be used in any information system incorporating internet information retrieval . we then introduce a hybrid multi - agent system ( mas ) architecture , acronym qtip , for the privacy domain through integrating the pecan ( personal context agent networking ) and qtil mas architectures . there are two areas where nlp is used : in the user - mas interaction and in the process of resource indexing and matching . these two areas map to the q / a - agent and to the i - agents . we propose using a lightweight head - driven phrase structure grammar ( hpsg ) natural language method for the q architectural layers and qualitatively justify its applicability . we provide an example of employing the hpsg formalism for information retrieval using natural language capability via privacy web services in one instantiation of the qtip architecture . independent preliminary results for hpsg on the q level show that our approaches for enhancing the usability of pet tools are promising . 

parallax : sparsity - aware data parallel training of deep neural networks
the employment of high - performance servers and gpu accelerators for training deep neural network models have greatly accelerated recent advances in deep learning ( dl ) . dl frameworks , such as tensorflow , mxnet , and caffe2 , have emerged to assist dl researchers to train their models in a distributed manner . although current dl frameworks scale well for image classification models , there remain opportunities for scalable distributed training on natural language processing ( nlp ) models . we found that current frameworks show relatively low scalability on training nlp models due to the lack of consideration to the difference in sparsity of model parameters . in this paper , we propose parallax , a framework that optimizes data parallel training by utilizing the sparsity of model parameters . parallax introduces a hybrid approach that combines parameter server and allreduce architectures to optimize the amount of data transfer according to the sparsity . experiments show that parallax built atop tensor - flow achieves scalable training throughput on both dense and sparse models while requiring little effort from its users . parallax achieves up to 2 . 8x , 6 . 02x speedup for nlp models than tensorflow and horovod with 48 gpus , respectively . the training speed for the image classification models is equal to horovod and 1 . 53x faster than tensorflow . 

chinese coding type identification based on sub - sentence length observation
this paper studied the identification algorithm of chinese character coding type by analyzing the sub - sentence length . a sub - sentence definition is given in this paper and the pdf of sub - sentence length is analyzed based on the sentence samples from lancaster corpus . we proposed a new algorithm to recognize the coding type of chinese characters by splitting sentences into sub - sentences using chinese punctuation characters and analyzing the probability of the observed sub - sentence length . in this algorithm we used both bayesian rules and iterated sub - sentence length calculation for trust - region comparison . because the size of chinese punctuation characters set is very small , this algorithm has shown great advantages on the space complexity . time complexity and identification performance are also studied in the end of the paper . 

word sense disambiguation using skip gram model to create a historical dictionary for arabic
the evolution of the arabic language from antiquity to the present days has given birth to several linguistic registers ascribed to the great periods of the history of the arabic language . they can be classified as : old arabic , classical arabic and modern standard arabic . in this work , we propose a method that aims to disambiguate words in modern standard arabic . this method consists of measuring the semantic relation between the context of use of the ambiguous word and its sense definitions . within the context of creating a historical dictionary for arabic , and to disambiguate a word , we need to take into consideration the historical period in which the word appeared . this method disambiguates arabic words takes into account that a word may have an old meaning but appears in a modern document . 

research on risk ranking of participants involved in supply chain network : applications of a nlp method based on multiplicative and fuzzy preference relations
this article focuses on the issue of participant selection in supply networks risk management . though there are many ways and applications which are introduced to selection participant in viewpoint of risk management , the practical effect of these methods is not considerable full and satisfactory . in this paper , we introduce a non - linear programming ( nlp ) method based on multiplicative and fuzzy preference relations to rank the candidates of probable participants in supply networks in viewpoint of risk management . the sound performance of discrimination power , the convience of applications and the relatively accuracy of perception expressions make this method of ranking suitable to participants selection in supply networks risk management . numberial example and demonstration are present at last . 

improved two - stage gauss pseudospectral collocation approach for engineering optimal control problems
to improve the optimization performance of gauss pseudospectral method for bang - bang optimal control problems , a two - stage mesh refinement gauss pseudospectral method based on the sensitivity is proposed . in the first stage , the state and control vector are approximated by global interpolation polynomials . the differential equations are approximated by orthogonal polynomials and the optimal control problem is then transformed into an nlp problem . in the second stage , the relative sensitivity is calculated and the mesh is refined by merging and subdividing collocation points . finally a new nlp problem is obtained with refined gauss collocation points . simulation tests are carried out on two classical bang - bang optimal control problems . the test results show that this method is better than the traditional gauss pseudo - spectral method in both accuracy and time . 

subcategorization acquisition and evaluation for chinese verbs
this paper describes the technology and an experiment of subcategorization acquisition for chinese verbs . the scf hypotheses are generated by means of linguistic heuristic information and filtered via statistical methods . evaluation on the acquisition of 20 multi - pattern verbs shows that our experiment achieved the similar precision and recall with former researches . besides , simple application of the acquired lexicon to a pcfg parser indicates great potentialities of subcategorization information in the fields of nlp . 

a generic template to evaluate integrated components in spoken dialogue systems
we present a generic template for spoken dialogue systems integrating speech recognition and synthesis with ' higher - level ' natural language dialogue modelling components . the generic model is abstracted from a number of real application systems targetted at very different domains . our research aim in developing this generic template is to investigate a new approach to the evaluation of dialogue management systems . rather than attempting to measure accuracy / speed of output , we propose principles for the evaluation of the underlying theoretical linguistic model of dialogue management in a given system , in terms of how well it fits our generic template for dialogue management systems . this is a measure of ' genericness ' or ' application - independence ' of a given system , which can be used to moderate accuracy / speed scores in comparisons of very unlike dmss serving different domains . this relates to ( but is orthogonal to ) dialogue management systems evaluation in terms of naturalness and like measurable metrics ( eg dybkjaer et al 1995 , vilnat 1996 , eagles 1994 , fraser 1995 ) ; it follows more closely emerging qualitative evaluation techniques for nl grammatical parsing schemes ( leech et al 1996 , atwell 1996 ) . 

modeling class diagram using nlp in object - oriented designing
requirement ' s analysis and design is a multifaceted and time - consuming process . the success of software projects critically relies on careful & timely analysis and modeling of system requirements . mostly , the requirements gathered from the stakeholders are written in some language ( probably english ) . in this regard , significant manual efforts are required for the formation of good class model which unfortunately results in time delays in the software industry . the problems associated with the requirement analysis and class modeling can be overcome by the appropriate employment of machine learning . in this paper , we propose a system , requirement engineering analysis & design ( read ) to generate unified modeling language ( uml ) class diagram using natural language processing ( nlp ) and domain ontology techniques . we have implemented the read system in python and it successfully generates the uml class diagram i . e . , class name , attributes methods , and relationships from the textual requirements written in english . to assess the performance of the proposed system , we have evaluated it on publicly available standards and the experimental results show that it outperforms the existing techniques for object - oriented based software designing . 

towards terascale knowledge acquisition
although vast amounts of textual data are freely available , many nlp algorithms exploit only a minute percentage of it . in this paper , we study the challenges of working at the terascale . we present an algorithm , designed for the teraxale , for mining is - a relations that achieves similar performance to a state - of - the - art linguistically - rich method . we focus on the accuracy of these two systems as a function of processing time and corpus size . 

combining a statistical language model with logistic regression to predict the lexical and syntactic difficulty of texts for ffl
reading is known to be an essential task in language learning , but finding the appropriate text for every learner is far from easy . in this context , automatic procedures can support the teacher ' s work . some tools exist for english , but at present there are none for french as a foreign language ( ffl) . in this paper , we present an original approach to assessing the readability of ffl texts using nlp techniques and extracts from ffl textbooks as our corpus . two logistic regression models based on lexical and grammatical features are explored and give quite good predictions on new texts . the results shows a slight superiority for multinomial logistic regression over the proportional odds model . 

a web service for automatic word class acquisition
in this paper we present a web service for building nlp resources to construct semantic word classes in japanese . the system takes a few seed words belonging to the target class as input and uses automatic class expansion to suggest semantically similar training samples for the user to label . the system automatically generates random negative training samples as well , and then trains a supervised classifier on this labeled data to generate the target word class from 107 candidate words extracted from a corpus of of 108 web documents . this system eliminates the need for expert machine learning knowledge in creating semantic word classes , and we experimentally show that it significantly reduces the human effort required to build them . 

a hybrid trust model for cloud service provider selection with nlp support and malicious user feedback filtering
with the rapid development of cloud computing and the increase in cloud service providers ( csp ) , there is an immense rise in the cloud services provided to the users . with a mass choice of csps , trust has become a major concern for both csps and the cloud service users ( csu) . it plays an important role in evaluating cloud services and helps in the robust selection of csps without compromising security , performance , and privacy . research has been done in this field that talks about one - way trust between csps and csus but little to no work has been done which includes bi - directional trust in the same model . we propose a trust model for cloud service selection that considers the bi - directional trust i . e . , trust based on user requirements and user feedback , making it more reliable in real - life scenarios . an nlp approach that extracts requirement values from both text and numerical input and a malicious feedback filtering mechanism are proposed . 

on satisfying the android os community : user feedback still central to developers ' portfolios
end - users play an integral role in identifying requirements , validating software features ' usefulness , locating defects , and in software product evolution in general . their role in these activities is especially prominent in online application distribution platforms ( oadps ) , where software is developed for many potential users , and for which the traditional processes of requirements gathering and negotiation with a single group of end - users do not apply . with such vast access to end - users , however , comes the challenge of how to prioritize competing requirements in order to satisfy previously unknown user groups , especially with early releases of a product . one highly successful product that has managed to overcome this challenge is the android operating system ( os ) . while the requirements of early versions of the android os likely benefited from market research , new features in subsequent releases appear to have benefitted extensively from user reviews . thus , lessons learned about how android developers have managed to satisfy the user community over time could usefully inform other software products . we have used data mining and natural language processing ( nlp ) techniques to investigate the issues that were logged by the android community , and how google ' s remedial efforts correlated with users ' requests . we found very strong alignment between end - users ' top feature requests and android developers ' responses , particularly for the more recent android releases . our findings suggest that effort spent responding to end - users ' loudest calls may be integral to software systems ' survival , and a product ' s overall success . 

a word embedding model learned from political tweets
distributed word representations have recently contributed to significant improvements in many natural language processing ( nlp ) tasks . distributional semantics have become amongst the important trends in machine learning ( ml ) applications . word embeddings are distributed representations of words that learn semantic relationships from a large corpus of text . in the social context , the distributed representation of a word is likely to be different from general text word embeddings . this is relatively due to the unique lexical semantic features and morphological structure of social media text such as tweets , which implies different word vector representations . in this paper , we collect and present a political social dataset that consists of over four million english tweets . an artificial neural network ( nn ) is trained to learn word co - occurrence and generate word vectors from the political corpus of tweets . the model is 136mb and includes word representations for a vocabulary of over 86k unique words and phrases . the learned model shall contribute to the success of many ml and nlp applications in microblogging social network analysis ( osn ) , such as semantic similarity and cluster analysis tasks . 

simple interrogative sentence analysis based on crf
 [ objective ] this paper intends to enhance the simple interrogative sentence analysis , which leads question answering system to understand ” what is this question asking? ” . [ methods ] under the condition that simple interrogative sentence analysis is regarded as a sequence labelling problems , conditional random field ( crf ) model can process it well . [ results ] few manual label can lead to promoted result . [ limitations ] for non - factual problems processing needs exceed the defined label system support . [ conclusions ] using conditional random field model to process question analysis problem , which is regarded as a sequential labelling problem , can improve handling capacity with relatively little cost . 

survey on tools and systems to generate er diagram from system requirement specification
lack of information communication technology ( ict ) knowledge and the cost have been identified as challenges to small and medium enterprises ( smes ) to adapt ict . however there are tools and systems freely available to generate information systems automatically , but those require the database structure of the system . database conceptualization based on the system requirement specification in natural language ( srs - nl ) is the most significant landmark in the process of database design . therefore it is desirable to have a tool , so that the non - technical people in smes could use to generate a quality conceptual database model ( cdm ) based on nl - srs automatically . comprehensive literature survey was conducted and evaluated the identified tools by analyzing usability , affordability and quality of the outcomes . analysis showed that some limitations of the tools which cannot be used by non - technical people in smes . 

optimizing to arbitrary nlp metrics using ensemble selection
while there have been many successful applications of machine learning methods to tasks in nlp , learning algorithms are not typically designed to optimize nlp performance metrics . this paper evaluates an ensemble selection framework designed to optimize arbitrary metrics and automate the process of algorithm selection and parameter tuning . we report the results of experiments that instantiate the framework for three nlp tasks , using six learning algorithms , a wide variety of parameterizations , and 15 performance metrics . based on our results , we make recommendations for subsequent machine - learning - based research for natural language learning . 

augmenting case based learning with dynamic language models
this paper describes a novel supporting tool for the case - based learning ( cbl ) . recent advances in deep - learning based language models ( lms ) have enabled highly dynamic interactivity in dialog services and story generation . we leverage the progress in modelling language to develop a technologically augmented cbl pedagogy which we analyze with a standardized assessment . our assessment shows reasonable case interactivity , low rates of factual inaccuracy , and no inappropriate machine - sourced responses . we also compare our assessment results across the case categories of ethics , chemistry , biology , and medicine , but find no statistically significant differences . in summary , we develop a framework for analyzing the ability of lms to augment cbl , apply this framework to the gpt - 3 lm , and discuss some of the challenges and potential solutions to ensuring proper usage in the classroom environment . 

designing a speech interface for voice activated mav ground control station
flying micro aerial vehicle ( mav ) through speech command helps untrained professionals to conveniently control the flight . such a control is achieved through automatic speech recognition ( asr ) and natural language processing ( nlp ) technologies . this paper describes the development of mav ground control station ( gcs ) with a speech interface to enable issuing of voice commands . the paper also presents the performance of the speech interface during a flight trial . 

towards a gamified support tool for requirements gathering in bahasa indonesia
in this modern world , software products have significant impacts on every aspect of peoples ' life . requirements engineering is one of the most critical activities in software development in which the foundation of the product is laid . in order to provide structure in requirements , the user stories concept is often used . the user story concept emphasizes information in three central parts , the stakeholder ' s role , the requirement , and the business value which will be satisfied by the requirement . stakeholders ' participation is crucial in gathering , eliciting , and analyzing requirements ; however , frequently due to circumstances , stakeholders provide minimum information related to the software requirements . in order to increase user ' s participation in providing stories related to the software requirements , our research aims to develop a gamified web - based support tool for the stakeholders and engineers to use in gathering requirements while implementing the user story concept . the gamified support tool is then evaluated to assess the system adoption regarding its use to support users in providing requirements regarding a specific software product . results show that the support tool receives scores as follow , 81 . 1 % for joy , 73 . 4 % for control , 67 . 2 % for focused immersion , 60 . 6 % for temporal dissociation , 80 % for curiosity , 76 . 8 % for perceived ease of use , 72 . 9 % for perceived usefulness , and 76 . 2 % for behavioral intention to use . based on the interview with the users , the support tool shows excellent potential to be further developed to support the requirements engineering activity in software development . 

application - driven statistical paraphrase generation
paraphrase generation ( pg ) is important in plenty of nlp applications . however , the research of pg is far from enough . in this paper , we propose a novel method for statistical paraphrase generation ( spg ) , which can ( 1 ) achieve various applications based on a uniform statistical model , and ( 2 ) naturally combine multiple resources to enhance the pg performance . in our experiments , we use the proposed method to generate paraphrases for three different applications . the results show that the method can be easily transformed from one application to another and generate valuable and interesting paraphrases . 

feasibility of prediction model for internal tumor target volume from 4 - d computed tomography of lung cancer
4 - dimensional computed tomography ( 4dct ) is the most common technique to determine organ movement due to breathing motion . however , the ability of 4dct to acquire ct images as a function of the respiratory phase increases higher radiation dose . to reduce the patient ’ s radiation dose , this study created lung motion prediction models used to estimate tumor target movement in ten respiratory phases by detecting only external organ movement during a complete respiration cycle without radiation with kinect . the average overall amplitude difference between rpm and kinect signals in the phantom experiment was 0 . 02 ± 0 . 1 mm . f1 score of 100 % for all most all classifications except classification 2 , 3 , 6 , 7 and 8 of 85 % , 83 % , 90 % , 84 % , 85 % where irregular breathing pattern . essentially , the proposed tumor movement scheme ’ s total accuracy ( average of f1 scores ) is 92 . 7 % . deep learning model can predict tumor motion range and classification zone by used detection of the external respiratory signalview less

fourth workshop on exploiting semantic annotations in information retrieval ( esair ) 
there is an increasing amount of structure on the web as a result of modern web languages , user tagging and annotation , and emerg - ing robust nlp tools . these meaningful , semantic , annotations hold the promise to significantly enhance information access , by enhancing the depth of analysis of today ' s systems . currently , we have only started exploring the possibilities and only begin to un - derstand how these valuable semantic cues can be put to fruitful use . unleashing the potential of semantic annotations requires us to think outside the box , by combining the insights of natural lan - guage processing ( nlp ) to go beyond bags of words , the insights of databases ( db ) to use structure efficiently even when aggregating over millions of records , the insights of information retrieval ( ir ) in effective goal - directed search and evaluation , and the insights of knowledge management ( km ) to get grips on the greater whole . this workshop aims to bring together researchers from these dif - ferent disciplines and work together on one of the greatest chal - lenges in the years to come . the desired result of the workshop will be to gain concrete insight into the potential of semantic an - notations , and in concrete steps to take this research forward ; to synchronize related research happening in nlp , db , ir , and km , in ways that combine the strengths of each discipline ; and to have a lively , interactive workshop where every participant contributes actively and which inspires attendees to think freely and creatively , working towards a common goal . 

expert and student user evaluation of semantic protocol search
evaluation of intelligent search with support from advanced natural language processing ( nlp ) technologies is labor - intense and related tasks are very trivial . this study introduced user relevance feedback procedures and relevance measures to evaluate our sprit - nlp semantic search system . the historical protocol archives of our organization were annotated using umls ( unified medical language systems ) concepts and indexed by solr to test these evaluation settings . the outcome demonstrated concept - based semantic search is very effective to retrieve many categories of clinical queries . 

analysis on degree words for chinese emotion expressions based on syntactic parse and rules
analysis of emotions in texts has wide - ranging applications . in the analysis of emotional expressions , degree words are important for expressing emotion intensity of emotions . with the support of a large chinese emotion corpus ( ren - cecps) , in this paper , we present analysis on degree words for chinese emotion expressions based on syntactic parse and rules . at first , ren - cecps is used to extract the rules of degree words and their modifying elements . then , we make a contrast analysis on emotion intensities of emotional words with and without modified by degree words . the quantity level for degree words is presented . based on the extracted rules and quantity degree , a method of sentence parse is used to extract the modifying elements of degree words and to get the emotion intensity of emotional words when they are modified by degree words in sentences . 

recognizing medication related entities in hospital discharge summaries using support vector machine
due to the lack of annotated data sets , there are few studies on machine learning based approaches to extract named entities ( nes ) in clinical text . the 2009 i2b2 nlp challenge is a task to extract six types of medication related nes , including medication names , dosage , mode , frequency , duration , and reason from hospital discharge summaries . several machine learning based systems have been developed and showed good performance in the challenge . those systems often involve two steps : 1 ) recognition of medication related entities ; and 2 ) determination of the relation between a medication name and its modifiers ( e . g . , dosage ) . a few machine learning algorithms including conditional random field ( crf ) and maximum entropy have been applied to the named entity recognition ( ner ) task at the first step . in this study , we developed a support vector machine ( svm ) based method to recognize medication related entities . in addition , we systematically investigated various types of features for ner in clinical text . evaluation on 268 manually annotated discharge summaries from i2b2 challenge showed that the svm - based ner system achieved the best f - score of 90 . 05 % ( 93 . 20 % precision , 87 . 12 % recall ) , when semantic features generated from a rule - based system were included . 

using word embedding to enable semantic queries in relational databases
we investigate opportunities for exploiting artificial intelligence ( ai ) techniques for enhancing capabilities of relational databases . in particular , we explore applications of natural language processing ( nlp ) techniques to endow relational databases with capabilities that were very hard to realize in practice . we apply an unsupervised neural - network based nlp idea , distributed representation via word embedding , to extract latent information from a relational table . the word embedding model is based on meaningful textual view of a relational database and captures inter - /intra - attribute relationships between database tokens . for each database token , the model includes a vector that encodes these contextual semantic relationships . these vectors enable processing a new class of sql - based business intelligence queries called cognitive intelligence ( ci ) queries that use the generated vectors to analyze contextual semantic relationships between database tokens . the cognitive capabilities enable complex queries such as semantic matching , reasoning queries such as analogies , predictive queries using entities not present in a database , and using knowledge from external sources . 

reverse engineering variability from natural language documents : a systematic literature review
identifying features and their relations ( i . e . , variation points ) is crucial in the process of migrating single software systems to software product lines ( spl ) . various approaches have been proposed to perform feature extraction automatically from different artifacts , for instance , feature location in legacy code . usually such approaches a ) omit variability information and b ) rely on artifacts that reside in advanced phases of the development process , thus , being only of limited usefulness in the context of spls . in contrast , feature and variability extraction from natural language ( nl ) documents is more favorable , because a mapping to several other artifacts is usually established from the very beginning . in this paper , we provide a multi - dimensional overview of approaches for feature and variability extraction from nl documents by means of a systematic literature review ( slr ) . we selected 25 primary studies and carefully evaluated them regarding different aspects such as techniques used , tool support , or accuracy of the results . in a nutshell , our key insights are that i ) standard nlp techniques are commonly used , ii ) post - processing often includes clustering & machine learning algorithms , iii ) only in rare cases , the approaches support variability extraction , iv ) tool support , apart from text pre - processing is often not available , and v ) many approaches lack a comprehensive evaluation . based on these observations , we derive future challenges , arguing that more effort need to be invested for making such approaches applicable in practice . 

research and development of speech technology & applications for mexican spanish at the tlatoa group
thanks to the advances in today ' s technology in terms of processing speed of computers , storage space and the management of sound and video devices , speech technology is a reality in almost any kind of computerized system . speech applications are being used in personal computers , cellular phones , etc . this makes this interesting technology accessible to almost anyone . among it ' s most useful applications we can find telephone - based information services , banking and computer assisted language learning systems . there exist already a large number of commercial products that use speech interfaces , developed mainly for english , german and japanese . that is why we at tlatoa have focused our efforts on making this technology available in the spanish spoken in mexico . to this effect we perform basic research in the different speech processing techniques , trying to improve the performance of speech recognition and synthesis ( artificial neural networks , hidden markov models ( hmm ' s) , unit selection , etc . ) , as well as , the spanish language , dialogue structure , perception and human - computer interaction approaches , for the development of speech applications . 

the semeval - 2007 weps evaluation : establishing a benchmark for the web people search task
this paper presents the task definition , resources , participation , and comparative results for the web people search task , which was organized as part of the semeval - 2007 evaluation exercise . this task consists of clustering a set of documents that mention an ambiguous person name according to the actual entities referred to using that name . 

a hybrid deep learning model to predict business closure from reviews and user attributes using sentiment aligned topic model
business closure is a very good indicator for success or failure of a business . this will help investors and banks as to whether to invest or lend to a particular business for future growth and benefits . traditional machine learning techniques require extensive manual feature engineering and still do not perform satisfactorily due to significant class imbalance problem and little difference in the attributes for open and closed businesses . we have used historical data besides taking care of the class imbalance problem . transfer learning also has been used to tackle the issue of having small categorical datasets . a hybrid deep learning model has been proposed to predict whether a business would be shut down within a specific period of time . sentiment aligned topic model ( satm ) is used to extract aspect - wise sentiment scores from user reviews . our results show a marked improvement over traditional machine learning techniques . it also shows how the aspect - wise sentiment scores corresponding to each business , computed using satm , help to give better results . 

parsing noun phrases in the penn treebank
noun phrases ( nps ) are a crucial part of natural language , and can have a very complex structure . however , this np structure is largely ignored by the statistical parsing field , as the most widely used corpus is not annotated with it . this lack of gold - standard data has restricted previous efforts to parse nps , making it impossible to perform the supervised experiments that have achieved high performance in so many natural language processing ( nlp ) tasks . we comprehensively solve this problem by manually annotating np structure for the entire wall street journal section of the penn treebank . the inter - annotator agreement scores that we attain dispel the belief that the task is too difficult , and demonstrate that consistent np annotation is possible . our gold - standard np data is now available for use in all parsers . we experiment with this new data , applying the collins ( 2003 ) parsing model , and find that its recovery of np structure is significantly worse than its overall performance . the parser ' s f - score is up to 5 . 69 % lower than a baseline that uses deterministic rules . through much experimentation , we determine that this result is primarily caused by a lack of lexical information . to solve this problem we construct a wide - coverage , large - scale np bracketing system . with our penn treebank data set , which is orders of magnitude larger than those used previously , we build a supervised model that achieves excellent results . our model performs at 93 . 8 % f - score on the simple task that most previous work has undertaken , and extends to bracket longer , more complex nps that are rarely dealt with in the literature . we attain 89 . 14 % f - score on this much more difficult task . finally , we implement a post - processing module that brackets nps identified by the bikel ( 2004 ) parser . our np bracketing model includes a wide variety of features that provide the lexical information that was missing during the parser experiments , and as a result , we outperform the parser ' s f - score by 9 . 04 % . these experiments demonstrate the utility of the corpus , and show that many nlp applications can now make use of np structure . 

towards intelligent search assistance for inquiry - based learning
in online inquiry - based learning ( oibl ) learners search for information to answer driving questions . while learners conduct sequential related searches , the search engines interpret each query in isolation , and thus are unable to utilize task context . consequently , learners usually get less relevant search results . we are developing a nlp - based search agent to bridge the gap between learners and search engines . our algorithms utilize contextual features to provide user with search term suggestions and results re - ranking . our pilot study indicates that our method can effectively enhance the quality of oibl . 

mining tweets of moroccan users using the framework hadoop , nlp , k - means and basemap
the information revolution and exactly the explosion of web 2 . 0 platforms such as discussion forums , blogs , and social networks allow users to share ideas and opinions , express their feelings and much more . this revolution leads to an accumulation of an enormous amount of data that may contain a lot of valuable information . much work has focused on analyzing these data , in particular those provided from social networks platforms like twitter . in this paper , our objective is to propose an approach for analyzing the data generated by moroccan users in the social network twitter , in order to discover the subjects that interest moroccan society and then locate on moroccan map the areas from where come the tweets related to these topics . analyzing the tweets of moroccan users is a real challenge for two main reasons . firstly , moroccan users utilize for their communication in twitter a variety of languages and dialects , such as standard arabic , moroccan arabic “ darija” , moroccan amazigh dialect “ tamazight” , french , spanish , and english . secondly , the moroccan tweets contain a lot of urls , # hashtags , spelling mistakes , reduced syntactic structures , and many abbreviations . in this paper , we propose an approach for detecting the relevant subjects related to moroccan users by extracting the data automatically , and storing it in a distributed file system using hdfs ( hadoop distributed file system ) of framework apache hadoop . then we preprocess this raw data and analyze it by developing a distributed program using three tools , mapreduce of framework apache hadoop , python language , and natural language processing ( nlp ) techniques . afterward , we convert the corpus generated by the previous step into numeric features , and apply the k - means algorithm to cluster all words into general topics . finally , we plot tweets on our moroccan map by using the coordinates extracted from them , in order to have an idea about the geolocation of these subjects . 

cooperation of evolutionary and statistical pos - tagging
part - of - speech tagging which refers to assignment of syntactic categories to words is a fundamental task in natural language processing ( nlp ) . this paper presents a novel algorithm based on bee colony optimization ( bco ) for pos tagging . experimental results indicate that the proposed algorithm outperforms the other evolutionary - based and tested classical part - of - speech - tagging approaches in terms of average accuracy . 

a feasibility study on extracting twitter users ' interests using nlp tools for serendipitous connections
this paper presents our research on the feasibility of extracting twitter users ' interests for suggesting serendipitous connections using natural language processing ( nlp ) technology . defined by andel [ 1 ] as the art of making an unsought finding , serendipity has a positive role in scientific research and people ' s daily lives . applications that facilitate serendipity would bring various benefits to us . in this work , we focus on the mining of users ' interests from twitter messages ( tweets hereafter ) to support the detection of serendipitous connections . to address the challenge , we explore a set of nlp tools to develop a real - time system for automatically extracting the users ' interests in the form of named entities and core terms . we also examine the different contributions of three different information sources with regard to the user ' s interests . furthermore , we examine the issue of determining the additional attribute of surprisingness / unexpectedness of the terms and entities of interest which we deem critical for detecting serendipitous connections . our prototype system was tested with a group of twitter users involving approximately 2 , 300 tweets . our algorithm achieved varying degrees of success on each of the users , demonstrating feasibility of identifying serendipitous interest terms and entities . for example , 27 . 5 % of terms extracted for one of the users were judged to be serendipitous . 

no mining , no meaning : relating documents across repositories with ontology - driven information extraction
far from eliminating documents as some expected , the internet has lead to a proliferation of digital documents , without a centralized control or indexing . thus , identifying relevant documents becomes simultaneously more important and much harder , since what users require may be dispersed across many documents and many repositories . this paper describes ontologic anchoring , a technique to relate documents in domain ontologies , using named entity recognition ( a natural - language processing approach ) and semantic annotation to relate individual documents to elements in ontologies . this approach allows document retrieval using domain - level inferences , and integration of repositories with heterogeneous media , languages and structure . ontological anchoring is a two - way street : ontologies allow semantic indexing of documents , and simultaneously new documents enrich ontologies . the approach is illustrated with an initial deployment for heritage documents in spanish . 

an open - ended question self - explanation classification methodology for a virtual laboratory learning system
scientific experiments are essential for science and technology education . experiments in laboratory cost materials , require preparations , and sometimes cause hazards . a widely used educational tool with many advantages , e . g . cheap , repeatable , suspendable , and safe , virtual laboratory has gradually become a major experimental tool in most elementary and high schools . in educational science experiments , one major challenge is how to initiate students on scientific inquiry and ensure there are multiple opportunities for their formative self - assessment and revision . the self - explanation strategy has proven effective in deepen students ' understanding of the concepts they are trying to learn . using self - explanation strategy in educational science experiments might be an effective way to help students think about the observed results of science experiments and build correct scientific concepts . on the other hand , researches point out that using open - ended questions is better than traditional multiple - choice questions for self - explanation strategy . but when using open - ended question self - explanation strategy , without proper prior knowledge and guidance , a student may go wrong in the processes of deduction and result in constructing misconceptions that will become obstacles in further knowledge constructions . therefore , a learning system that uses open - ended question self - explanation strategy should give proper feedback in order to help students build correct concepts when in self - learning mode . to help students operating in virtual science laboratory and constructing correct concepts from observed results this study constructs an online virtual laboratory learning system with open - ended question self - explanation strategy and proper feedback for natural science course of primary schools . the system uses natural language processing ( nlp ) technology to analyze students ' self - explanation strings , compares the results with coded classification rules , established by an expert from reference explanations , to check the correctness of the strings and possible misconceptions in them , and gives proper learning material , as feedback , for the students to revise possible misconceptions . in the final experiment , the system records and checks all self - explanation strings from 53 students and gives them proper feedback , which reaches an average accuracy of 84 . 45 % after the expert verify the results . 

improving scenario decomposition for multistage mpc using a sensitivity - based path - following algorithm
this letter proposes a computationally efficient algorithm for robust multistage scenario model predictive control ( mpc ) . in multistage scenario mpc , the evolution of uncertainty in the prediction horizon is represented via a scenario tree . the resulting large - scale optimization problem can be decomposed into several smaller subproblems where , for example , each subproblem solves a single scenario . since the different scenarios differ only in the uncertain parameters , the distributed scenario mpc problem can be cast as a parametric nonlinear programming ( nlp ) problem . by using the nlp sensitivity , we do not need to solve all the subproblems as full nlps . instead they can be solved exploiting the parametric nature by a path - following predictor - corrector algorithm that approximates the nlp . this results in a computationally efficient multistage scenario mpc framework . simulation results show that the sensitivity - based distributed multistage mpc provides a very good approximation of the fully centralized scenario mpc . 

maximising the potential of documents , nlp , and ai in an online employment marketplace : technical and non - technical challenges
seek is a leading employment marketplace with a global presence , connecting hundreds of millions of job seekers with job advertisements posted by millions of hirers across apac and the americas . document processing is fundamental to all areas of our business , and multiple teams at seek face daily challenges about how to maximise the potential of our multinational , multilingual , multifaceted corpus of job ads , candidate resumes , and other employment documents . although some of these challenges are familiar technical problems like accuracy , performance , and reliability of services , often the most difficult challenges we face are non - technical . how should large teams organise into autonomous units while avoiding silos of data and expertise ? how can we use data and build ai services in an ethical way that is true to our purpose of helping seek ' s users and protecting their personal data ? in this talk i will discuss several of these challenges that we currently face at seek , and the solutions we have found to deal with them . 

language id in the context of harvesting language data off the web
as the arm of nlp technologies extends beyond a small core of languages , techniques for working with instances of language data across hundreds to thousands of languages may require revisiting and recalibrating the tried and true methods that are used . of the nlp techniques that has been treated as " solved " is language identification ( language id ) of written text . however , we argue that language id is far from solved when one considers input spanning not dozens of languages , but rather hundreds to thousands , a number that one approaches when harvesting language data found on the web . we formulate language id as a coreference resolution problem and apply it to a web harvesting task for a specific linguistic data type and achieve a much higher accuracy than long accepted language id approaches . 

the present use of statistics in the evaluation of nlp parsers
we are concerned that the quality of results produced by an nlp parser bears little , if any , relation to the percentage - results claimed by the various nlp parser - systems presently available for use . to illustrate this problem , we examine one readily available nlp tagging and parsing system , the engcg parser ; and one tagger , the brill tagger . we note responses to both artificially generated and naturally occurring text . the percentage assessments are methodologically flawed , and should be taken with a grain of salt ; instead , assessment of the performance of an nlp parser should be effected by a user , and solely from a consideration of the resulting parses of exactly the input which an nlp user decides to contribute for such an assessment . careful attention to input of whatever corpus the user decides on , is presently the only suitable qualifying test of parsing ability . the parsers available are none of them perfectible yet , despite apparent yields now quoted at 99%+ . we consider the impact of zipf ' s argument of ' least effort ' on percentage assessment ; and we open a discussion on estimating the relative complexities of corpora . 

the genia corpus : an annotated research abstract corpus in molecular biology domain
with the information overload in genome - related field , there is an increasing need for natural language processing technology to extract information from literature and various attempts of information extraction using nlp has been being made . we are developing the necessary resources including domain ontology and annotated corpus from research abstracts in medline database ( genia corpus) . we are building the ontology and the corpus simultaneously , using each other . in this paper we report on our new corpus , its ontological basis , annotation scheme , and statistics of annotated objects . we also describe the tools used for corpus annotation and management . 

manchu handwritten character recognition post processing based on the combination language model
this paper presents a combination manchu language model on post - processing optimization for handwritten manchu characters recognition . the thinking of this model is take corpus as post - processing core and rule assistant correction . the model is based on a manchu machine dictionary . the aim of work is to explore the language model in order to optimize the recognition accuracy rate . present method focuses on the following tasks : a manchu machine dictionary organization ; rule description language ; the contents of form grammar rule base ; and corpus building . experimental results are reported using the combination language model is efficient on the manchu handwritten characters recognition post - processing . 

short text similarity computation method based on feature expansion and siamese network
text similarity   computation issues   is a widely   studied problem in natural language processing   ( nlp ) . short text similarity computation is a new and more challenging problem , which cannot be effectively solved by using previous regular text similarity computation approach . the main reason is that , a short text generally contains limited number of words and fewer features can be extracted . in this paper , we   propose a short text similarity computation method based on feature   expansion and siamese neural network . firstly , a latent dirichlet allocation ( lda )   based model is constructed to expand the features of a short text . then , deep features are extracted by using siamese neural networks model which contains both convolutional neural networks   ( cnn ) and bi - directional long short - term memory   ( bilstm ) . finally , the similarity of two short texts can be achieved by computing the manhattan distance   between generated feature vectors of these two texts . experimental results show that , based on the   data set of   ant financial nlp challenge , our   method achieves higher accuracy and f1 score . 

a development heat stroke detection system integrated with infrared camera
currently , the problem of global warming is increasing heat illness called heat stroke disease , body core temperature has risen over 41 celsius , affected to central nervous system failure and death cased . this research proposed a development heat stroke detection system integrated with infrared camera to detect people with body temperatures above 39 degrees celsius or people who are at risk of heat stroke . the experiment with sample group ' s photo result show that system able to diagnosis human body detection accuracy rate 90 % temperature measurement 60 % heat stroke detected notification 100 % and responsive time 60 % total accuracy validation method summarized 77 . 5 percent . this study is collect a satisfaction measurement about visualize , usage and contribution from program expertise with good satisfaction result . the work can be invention to screen people outdoor activities who have heat stroke ' s risk for first aid assistant . 

recent trends in deep learning based natural language processing [ review article ] 
deep learning methods employ multiple processing layers to learn hierarchical representations of data , and have produced state - of - the - art results in many domains . recently , a variety of model designs and methods have blossomed in the context of natural language processing ( nlp) . in this paper , we review significant deep learning related models and methods that have been employed for numerous nlp tasks and provide a walk - through of their evolution . we also summarize , compare and contrast the various models and put forward a detailed understanding of the past , present and future of deep learning in nlp . 

detection and analysis of the polarity of discourse in moroccan sociolect
today , social media is a term that everyone knows . even the most remote areas of the world have at least heard of facebook and twitter , and are probably using them on a regular basis . we live in a world where people publish and post everything they do or intend to do on social networks . and since we are interested in studying the opinions posted by the moroccan internauts , we were interested by learning this specific language they are using , a language that combines latin letters with numbers , it is what we agreed to call the moroccan sociolect language . i this paper we present the history of the linguistic situation in morocco that gave birth to this moroccan sociolect language , also we present a corpus we have build based on comments extracted from facebook since it is the most used social media in morocco . we also present the result of the application of the classifier of bayes on this corpus with some propositions of improvement specific to this moroccan sociolect language . 

tvis : a light - weight traffic visualization system for ddos detection
with rapid growth of network size and complexity , network defenders are facing more challenges in protecting networked computers and other devices from acute attacks . traffic visualization is an essential element in an anomaly detection system for visual observations and detection of distributed dos attacks . this paper presents an interactive visualization system called tvis , proposed to detect both low - rate and highrate ddos attacks using heron ’ s triangle - area mapping . tvis allows network defenders to identify and investigate anomalies in internal and external network traffic at both online and offline modes . we model the network traffic as an undirected graph and compute triangle - area map based on incidences at each vertex for each 5 seconds time window . the system triggers an alarm iff the system finds an area of the mapped triangle beyond the dynamic threshold . tvis performs well for both low - rate and high - rate ddos detection in comparison to its competitors . 

modified lstm with memory layer for power grid signal classification
in processing of signals coming from power grid system , the major purpose is to filter out the key signals that are highly related to power grid fault or breakdown . power signal analysis and incident summary are normally done by people . the obvious issue with manpower is when multiple incidents take place and huge number of signals emerge , processing all signals manually in a short time becomes virtually impossible . attempts to handle similar massive information situation with neural network models have been proven useful . however , ordinary neural network models that learn and process signals usually suffer “ memory loss ” as the sequence gets longer , which leads to inaccurate classification . this paper proposes a modified version of long - short - term - memory ( lstm ) network that can mitigate the memory dependency issue of recurrent neural network ( rnn ) and basic lstm structure . the focus of the thesis is how the modified lstm network can utilize the additional layer of memory slots to store the entire output sequence from the last lstm cell instead of mere output value in basic lstm architecture . the modified model can incorporate all previous cell information and prevent memory loss in the modeling process . to substantiate the merit of the lstm model with stacked memory layer , a signal classification experiment is done with both modified and basic lstm models and the results are shown . 

a comparative study on various deep learning techniques for thai nlp lexical and syntactic tasks on noisy data
in natural language processing ( nlp ) , there are three fundamental tasks of nlp which are tokenization being a part of a lexical level , part - of - speech tagging ( pos ) and named - entity - recognition ( ner ) being parts of a syntactic level . recently , there have been many deep learning researches showing their success in many domains . however , there has been no comparative study for thai nlp to suggest the most suitable technique for each task yet . in this paper , we aim to provide a performance comparison among various deep learning - based techniques on three nlp tasks , and study the effect on synthesized oov words and the oov handling algorithm with levenshtein distance had been provided due to the fact that most existing works relied on a set of vocabularies in the trained model and not being fit for noisy text in the real use case . our three experiments were conducted on best 2010 i2r , a standard thai nlp corpus on f1 measurement , with the different percentage of noises having been synthesized . firstly , for tokenization , the result shows that synthai , a jointed bidirectional lstm , has the best performance . additionally , for pos , bi - directional lstm with crf has obtained the best performance . for ner , variational bi - directional lstm with crf has outperformed other methods . finally , the effect of noises reduces the performance of all algorithms on these foundation tasks and the result shows that our oov handling technique could improve the performance on noisy data . 

thermal - aware steiner routing for 3d stacked ics
in this paper , we present the first work on the steiner routing for 3d stacked ics . in the 3d steiner routing problem , the pins are located in multiple device layers , which makes it more general than its 2d counterpart . our algorithm consists of two steps : tree construction and tree refinement . our tree construction algorithm builds a delay - oriented steiner tree under a given thermal profile . we show that thermal - aware 3d tree construction involves the minimization of two - variable elmore delay function . in our tree refinement algorithm , we reposition the through - vias while preserving the original routing topology for further thermal optimization under performance constraint . we employ a novel scheme to relax the initial nlp formulation to ilp and consider all through - vias from all nets simultaneously . our related experiments show the effectiveness of our proposed solutions . 

development of thai question answering system
automatic question answering systems play an important role nowadays . in this paper , we developed a thai question answering system . from an input question , the system analyzes an expected type of answer ( e . g . human , place ) then searches for all possible answers from source documents . in order to select the most relevance answer , a word order consistency and a relevance score are proposed to measure similarity of the question and the sentences surrounding the possible answers . based on an experiment from 210 questions and 50 articles , the proposed system can provide a mean reciprocal answer rank of 0 . 657 . 

daily health monitoring chatbot with linear regression
nowadays , elderly people harm their health from daily routines such as the habits of eating junk food , lack of time for exercising , and so on . when age increases , most of elderly people often have encountered with high blood pressure disease . this disease can lead to many dangerous diseases . additionally , population of the doctors in hospital is not so high . the doctor usually takes a short note about symptoms for quick services that might not be enough for diagnosis . therefore , we proposed daily health monitoring chatbot for the elderly people . we need to collect information from the elderly people to create personal health record ( phr) . we develop conversational chatbot to interact with the elderly people via line application . outcomes of this research support the doctor ' s work because after reading daily phr , the doctor diagnoses the diseases and gives advice for medical treatment more accurately . furthermore , linear regression technique was developed to monitor the blood pressure ' s trend of elderly . thus , they can prevent or relieve some diseases from chatbot warning and taking care the health . 

the terminet project : an overview
linguistic resources with domain - specific coverage are crucial for the development of concrete natural language processing ( nlp ) systems . in this paper we give a global introduction to the ongoing ( since 2009 ) terminet project , whose aims are to instantiate a generic nlp methodology for the development of terminological wordnets and to apply the instantiated methodology for building a terminological wordnet in brazilian portuguese . 

term relevance estimation for chinese query expansion
in this paper we propose a novel method to estimate the relevance between query and candidate expansion terms for chinese information retrieval . in previous method , expansion terms are usually selected by counting term co - occurrences in the documents . however , term co - occurrences are not always a good indicator for relevance , whereas some are background terms of the whole collection . in order to remove noise , an em - algorithm is used in our model to estimate two kinds of relevance weight . one is the relevance weight between query and its relevant term extracted from the top - ranked documents in initial retrieval results . the other is the relevance weight between each query term and its relevant terms extracted from the snapshot of google search result when that query term is used as search keyword . the estimated relevance weights are used to select good expansion terms for second retrieval . the experiments on the two test collections show that our query expansion model is more effective than the standard rocchio expansion . 

a proposal for virtual mental health assistant
in these current times , mental health has become one of the most neglected and yet one of the most serious aspects of our overall well being . this paper proposes a system for a virtual mental health assistant owing to financial , time and space constraints and shortage of resources related to in - person therapy . of tentimes , disturbed mental health is a snowball effect built up over time and requires continuous attention and conscious efforts to improve . this is possible with the help of a virtual mental health assistant . the proposed assistant will have a chat feature , psychological assessment , an emotion detection module and a recommendation system for improving the mood of the user . we have used naive bayes classifier and neural networks for sentiment analysis . our system has shown comparatively higher accuracy for naive bayesian model . 

teaching nlp / cl through games : the case of parsing
this paper advocates the use of games in teaching nlp / cl in cases where computational experiments are impossible because the students lack the necessary skills . to show the viability of this approach , three games are described which together teach students about the parsing process . the paper also shows how the specific game formats and rules can be tuned to the teaching goals and situations , thus opening the way to the creation of further teaching games . 

deep neural based name entity recognizer and classifier for english language
named entity recognition ( ner ) is an important and very effective for the machine translation , retrieval ( ir ) , information extraction ( ie ) from huge corpus , question answering ( qa ) system , text mining and text clustering and etc . ner help us to classify or identify the noun and its types such place / location , people , department , ministry , organization , times and etc . the huge data available on social media , websites , news channels and many more sources can be classified so that it can be used in research for nlp processes such as in machine translation , speech technology , information extraction and etc . to process this huge data or corpus we propose recent techniques of machine learning and deep neural network . the deep neural network approach will help to identify the named entity ( ne ) from huge corpus or text by training the corpus using word2vec approach . on the basis of fetched tokens and tag . we categorize these tokens into different grammar categories based of cosine similarity concept of deep neural network . cosine similarity help to find the tag of unknown token or phases by finding its neared vectors which are not trained earlier in word2vec database . we have used the supervised learning ( sl ) techniques to train the network . 

a mixed - methods ethnographic approach to participatory budgeting in scotland
participatory budgeting ( pb ) is already well established in scotland in the form of community led grant - making yet has recently transformed from a grass - roots activity to a mainstream process or embedded ' policy instrument ' . an integral part of this turn is the use of the consul digital platform as the primary means of citizen participation . using a mixed method approach , this ongoing research paper explores how each of the 32 local authorities that make up scotland utilise the consul platform to engage their citizens in the pb process and how they then make sense of citizens ' contributions . in particular , we focus on whether natural language processing ( nlp ) tools can facilitate both citizen engagement , and the processes by which citizens ' contributions are analysed and translated into policies . 

text generation by probabilistic suffix tree language model
during last decade , language modeling has been dominated by neural structures ; rnn , lstm or transformer . these neural language models provide excellent performance to the detriment of very high computational cost . this work investigates the use of probabilistic language model that requires much less computational cost . in particular , we are interested in variable - order markov model that can be efficiently implemented on a probabilistic suffix tree ( pst ) structure . the pst construction is cheap and can be easily scaled to very large dataset . experimental results show that this model can be used to generated realistic sentences . 

design of a morph analyzer for non - declinable adjectives of nepali language
morph analyser is a fundamental tool natural language processing ( nlp ) system which is used to split a given token into its constituents . as per the literature available , very less numbers of works are found in this specific area for nepali language . in this paper , an attempt has been made to develop a successful morph analyser for non - declinable adjectives of nepali language . the technique is developed using finite state grammar approach . it can work with minimum number of linguistic resources . the morph analyser is tested with sufficient number of input text and found satisfactory results . 

sentence boundary detection in colloquial arabic text : a preliminary result
recently , natural language processing tasks are more frequently conducted over online content . this poses a special problem for applications over arabic language . online arabic content is usually written in informal colloquial arabic , which is characterized to be ill - structured and lacks specific linguistic standardization . in this paper , we investigate a preliminary step to conduct successful nlp processing which is the problem of sentence boundary detection . as informal arabic lacks basic linguistic rules , we establish a list of commonly used punctuation marks after extensively studying a large amount of informal arabic text . moreover , we evaluated the correct usage of these punctuation marks as sentence delimiters ; the result yielded a preliminary accuracy of 70% . 

is it fake ? news disinformation detection on south african news websites
disinformation through fake news is an ongoing problem in our society and has become easily spread through social media . the most cost - and time - effective way to filter these large amounts of data is to use a combination of human and technical interventions to identify it . from a technical perspective , natural language processing ( nlp ) is widely used in detecting fake news . social media companies use nlp techniques to identify the fake news and warn their users , but fake news may still slip through undetected . it is especially a problem in more localised contexts ( outside the united states of america ) . how do we adjust fake news detection systems to work better for local contexts such as in south africa . in this work we investigate fake news detection on south african websites . we curate a dataset of south african fake news and then train detection models . we contrast this with using widely available fake news datasets ( from mostly usa website ) . we also explore making the datasets more diverse by combining them and observe the differences in behaviour in writing between nations ’ fake news using interpretable machine learning . 

assessing the challenge of fine - grained named entity recognition and classification
named entity recognition and classification ( nerc ) is a well - studied nlp task typically focused on coarse - grained named entity ( ne ) classes . nerc for more fine - grained semantic ne classes has not been systematically studied . this paper quantifies the difficulty of fine - grained nerc ( fg - nerc ) when performed at large scale on the people domain . we apply unsupervised acquisition methods to construct a gold standard dataset for fg - nerc . this dataset is used to benchmark methods for classifying nes at various levels of fine - grainedness using classical nerc techniques and global contextual information inspired from word sense disambiguation approaches . our results indicate high difficulty of the task and provide a ' strong ' baseline for future research . 

a framework for identifying excessive sadness in students through twitter and facebook in the philippines
natural language processing ( nlp ) can be used to identify a person ' s sentiments or emotions . depression is one sentiment that researchers have tried to identify through natural language processing with little success . depression is an episode of sadness or apathy , along with other symptoms , that lasts for at least two consecutive weeks . depression is especially bad with students due to the amount of stress and anxiety they have to go through . while depression is very difficult to identify and treat , excessive sadness , one of the symptoms that may lead to depression can be identified early and appropriate action can be taken . the philippines is known to have the highest depression count in southeast asia . data mining was performed on twitter and facebook , and with the use of natural language processing ( nlp ) and sentiment analysis , a logistics regression model was devised with the use of emotion lexicons to identify the user ' s state . the latent dirichlet allocation ( lda ) was then used to identify important topics of each user and cluster the data and make sense out of each user ' s excessive sadness . 

the intelligent agent nlp - based customer service system
in nowadays , communication has been more and more important in our lives . it is difficult for us to complete many things without it . with the support of the developing technology , communication is able to become faster and easier . the human - to - machine communication is a creative application used in field of research and industry . to improve the interaction between human and machine , a communication system is specially designed . the technology of natural language processing is implemented in the system to handle the understanding and generation of the chatting language . for higher efficiency , the system is enhanced by designing in the multi - agent system . which let agents deal with the detailed tasks in the process of nlp by interacting and integrating them together . the system is based on sending and receiving so that it is able to communicate asking question and responding answer . provide a textbook to the chatbot , it is capable to understanding the content of book . and then serve as a teaching assistant to help student solve their problems by answering their questions . in this thesis , we will reveal the design and implementation of different parts in the communication system . which consists of natural language processing system , multi - agent system , user interface ( model - view - controller frame) , and knowledge base . system is implemented by java programme language . 

a semantic extraction and sentimental assessment of risk factors ( sesarf ) : an nlp approach for precision medicine : a medical decision support tool for early diagnosis from clinical notes
clinical notes contain information that is crucial for the diagnosis process . however , it is usually not properly manually analyzed due to the tremendous efforts and time it takes . hence , an automated approach is eagerly needed to maximize clinical knowledge management and reduce cost . in this paper , we propose a framework sesarf : a semantic extractor to identify hidden risk factors in clinical notes and a sentimental analyzer to assess the severity levels associated with the identified risk factors . this tool can be customized to any disease using linked open data ( lod ) by selecting a specific disease and collecting its risk factors list from medical ontologies . the extracted knowledge can serve two purposes : 1 ) a feature vector is prepared , for any classifier in machine learning , containing risk factors and their weights based on our semantic enrichment and sentimental analyzer and 2 ) a proper comparison of the extracted information with wearable body sensors that can alert any major changes in a patient ' s health status to personalize treatment . 

building an annotated corpus in the molecular - biology domain
corpus annotation is now a key topic for all areas of natural language processing ( nlp ) and information extraction ( ie ) which employ supervised learning . with the explosion of results in molecular - biology there is an increased need for ie to extract knowledge to support database building and to search intelligently for information in online journal collections . to support this we are building a corpus of annotated abstracts taken from national library of medicine ' s medline database . in this paper we report on this new corpus , its ontological basis , and our experience in designing the annotation scheme . experimental results are shown for inter - annotator agreement and comments are made on methodological considerations . 

online reviews evaluation system for higher education institution : an aspect based sentiment analysis tool
the higher education opportunities have increased significantly over the past decade in sri lanka . today ' s younger generation is keen to study and most of them opt for higher education . choosing the right course at the right private institute is most challenging choice since there are so many options available . in order to find the right institute , students have to surf internet for the reviews and find user comments of particular institution from social network sites like facebook , twitter , google plus and etc . this takes lot of time for reading the comments to understand whether that ratings are good or not on the particular institution . the key information a student wants to get from the review is : whether that institute is good , and what aspects received positive or negative opinions . this task is quite challenging because it is difficult for a human being to extract statistical aspect sentiment information from a massive set of online reviews . as a solution for this problem higher institution aspect based evaluation system which evaluates the institution by considering the reviews given by reviewers is suggested by this project . this system implementation is based on natural language processing . the outcome of this research project , is a system which retrieves review data from the social media networks and gives a rating to an institution by analyzing the sentiment value of the reviews and the features evaluated in them . data gathering and analysis process of this project is made automated as possible and this can be accessed from anywhere , as the client application is developed as a web application . 

speaker identification using linear predictive cepstral coefficients and general regression neural network
a text - independent , closed - set speaker identification method is proposed in this paper . the method uses linear predictive cepstrum coefficients ( lpccs ) as the measured features and follows general regression neural network ( grnn ) approaches based on non - linear partition ( nlp ) algorithm and kernel principal component analysis ( kpca ) . the input speech signal is pre - emphasized , windowed , and lpc analyzed , resulting in a sequence of vectors of lpc derived cepstrum coefficients . to reduce the correlation and dimension of elements in the feature vector , the nlp algorithm is employed to partition the lpccs into several segments . the dimensions of each lpccs segment are reduced by kpca , then fed to a grnn for the classification of speaker identification . the numerical experiments are carried out to verify the theoretical results and clearly show that our identification system has good recognition ability in term of accuracy . 

sentiment analysis of e - commerce customer reviews based on natural language processing
e - commerce can largely boost the economic development and customer behavior analysis is necessary for e - commerce marketing strategy . we used the dataset of women ' s e - commerce clothing reviews to study the sentiment analysis of customer recommendation . five popular machine learning algorithms were applied to solve the problem , including logistic regression , support vector machine ( svm) , random forest , xgboost and lightgbm . these algorithms aimed at figuring out the insight correlation between review features and product recommendation based on natural language processing ( nlp) . the best result was achieved by lightgbm algorithm with highest auc value and accuracy . the precision , recall and f1 score were all 0 . 97 . ridge regression , linear kernel svm and xgboost algorithms which had close performances with the accuracy of 0 . 94 . this research can help generate a deeper comprehension of customer sentiment and grasp customer psychology in e - commerce transaction industry

training and testing low - degree polynomial data mappings via linear svm
kernel techniques have long been used in svm to handle linearly inseparable problems by transforming data to a high dimensional space , but training and testing large data sets is often time consuming . in contrast , we can efficiently train and test much larger data sets using linear svm without kernels . in this work , we apply fast linear - svm methods to the explicit form of polynomially mapped data and investigate implementation issues . the approach enjoys fast training and testing , but may sometimes achieve accuracy close to that of using highly nonlinear kernels . empirical experiments show that the proposed method is useful for certain large - scale data sets . we successfully apply the proposed method to a natural language processing ( nlp ) application by improving the testing accuracy under some training / testing speed requirements . 

an approach towards identification and prevention of riots by analysis of social media posts in real - time
this paper presents an approach for monitoring facebook posts and comments in real - time , with an aim at identifying plausible riots that could spur up as a result . in this research , we employ an nlp and sentiment analysis based algorithm to classify a discussion as likely to cause a riot or not and identify the power groups involved in it . 

towards intelligent arabic text - to - speech application for disabled people
assistive technology customizes speech technology to offer a new communication channel for disabled people such as blind or having speech difficulties . converting written text into natural speech has been addressed in the last decades for some languages such as english , hence , used in many applications such as voice answering machines , reading articles and exploring software for blind people . other languages such as arabic are still not fully served to have high quality text - to - speech applications . this paper describes our effort in developing an intelligent text - to - speech mobile application for arabic . we use a set of statistical language models n - gram for word prediction and auto - completion for easy typing . a large new arabic corpus for daily communication in different domains is constructed which could be used for other purposes . a serious of normalization processing , including spelling correction , is applied to the corpus to maintain the consistency and unify the occurrence of the same words . we use outsource sakhr arabic text - to - speeh voices as one of the best speech synthesizer exist for arabic . to ensure a high usability of the application , we use simple graphical user interface and easy access libraries to favorite phrases with an ability of adding pictures with recorded speech . our experiments shows that word prediction using global and local corpus decries 50 % of keystroke of typing desired sentences with a high prediction of 84 % of bigram model . 

text - to - speech synthesis : literature review with an emphasis on malayalam language
text - to - speech synthesis ( tts ) is an active area of research to generate synthetic speech from underlying text . the identified syllables are uttered with proper duration and prosody characteristics to emulate natural speech . it falls under the category of natural language processing ( nlp ) , which aims to bridge the gap in communication between human and machine . so far as western languages like english are concerned , the research to produce intelligent and natural synthetic speech has advanced considerably . but in a multilingual state like india , many regional languages viz . malayalam is underexplored when it comes to nlp . in this article , we try to amalgamate the major research works performed in the area of tts in english and the prominent indian languages , with a special emphasis on the south indian language , malayalam . this review intends to provide right direction to the research activities in the language , in the area of tts . 

bootstrapping word alignment by automatically generated bilingual dictionary
this paper presents a new approach to improve the word alignment . building a bilingual dictionary is one of the main applications for word alignment . however , the research of using the bilingual dictionary to improve the word alignment is not enough . there are two bottlenecks . the first is that large bilingual dictionary is hard to get . the second is that the normal approach of using bilingual dictionary does not make full use of the dictionary . we designed a bootstrapping algorithm to conquer the bottlenecks , achieving a good result . 

knownet : a proposal for building highly connected and dense knowledge bases from the web
this paper presents a new fully automatic method for building highly dense and accurate knowledge bases from existing semantic resources . basically , the method uses a wide - coverage and accurate knowledge - based word sense disambiguation algorithm to assign the most appropriate senses to large sets of topically related words acquired from the web . knownet , the resulting knowledge - base which connects large sets of semantically - related concepts is a major step towards the autonomous acquisition of knowledge from raw corpora . in fact , knownet is several times larger than any available knowledge resource encoding relations between synsets , and the knowledge that knownet contains outperform any other resource when empirically evaluated in a common multilingual framework . 

automatic generation of sequence diagram from use case specification
in this paper we propose a tool supported automated approach for the generation of sequence diagrams from use case specifications written in some natural language . the approach uses natural language parser to identify problem level objects and interactions between them from use case specification . using three case studies , we evaluate our approach with the existing automated approaches . the results show that the proposed automated approach generates sequence diagrams which are significantly better in terms of correctness and completeness than those generated by the existing automated approaches . 

prediction in speech coding : the modification of the coding of lpc parameters and nonlinear estimation technique by using ann
linear and non - linear prediction techniques of speech signal and the different performance of lpc algorithms are discussed . a new coding plan has been suggested to raise the quantification snr by about 2 db by coding the difference of the log - area ratio ( lar ) of adjacent frames instead of the lar . then , the stable time delay ann is used to carry out nlp ( non - linear prediction ) tasks . experiment results show that only one nlp system is enough to deprive all short - term and long - term correlation from the speech samples and the energy from the output of the nlp system is much smaller than that obtained with the lp system . 

a framework for collocation error correction in web pages and text documents
much of the english in text documents today comes from nonnative speakers . web searches are also conducted very often by non - native speakers . though highly qualified in their respective fields , these speakers could potentially make errors in collocation , e . g . , " dark money " and " stock agora " ( instead of the more appropriate english expressions " black money " and " stock market " respectively) . these may arise due to literal translation from the respective speaker ' s native language or other factors . such errors could cause problems in contexts such as querying over web pages , correct understanding of text documents and more . this paper proposes a framework called collorder to detect such collocation errors and suggest correctly ordered collocated responses for improving the semantics . this framework integrates machine learning approaches with natural language processing techniques , proposing suitable heuristics to provide responses to collocation errors , ranked in the order of correctness . we discuss the proposed framework with algorithms and experimental evaluation in this paper . we claim that it would be useful in semantically enhancing web querying e . g . , financial news , online shopping etc . it would also help in providing automated error correction in machine translated documents and offering assistance to people using esl tools . 

building test suites for uima components
we summarize our experiences building a comprehensive suite of tests for a statistical natural language processing toolkit , cleartk . we describe some of the challenges we encountered , introduce a software project that emerged from these efforts , summarize our resulting test suite , and discuss some of the lessons learned . 

automatic exploitation of multilingual information for military intelligence purposes
intelligence plays an important role in supporting military operations . in the course of military intelligence a vast amount of textual data in different languages needs to be analyzed . in addition to information provided by traditional military intelligence , nowadays the internet offers important resources of potential militarily relevant information . however , we are not able to manually handle this vast amount of data . the science of natural language processing ( nlp ) provides technology to efficiently handle this task , in particular by means of machine translation and text mining . in our research project isaf - mt we created a statistical machine translation ( smt ) system for dari to german . in this paper we describe how nlp technologies and in particular smt can be applied to different intelligence processes . we therefore argue that multilingual nlp technology can strongly support military operations . 

aspects of information integration in a wiki
information integration means pulling information pieces from various sources together with as little loss , as little overlap and as little redundancy of information as is possible . this paper explores different aspects of information integration using as an example a wiki that contains a set of documents to which we want to add suitable further pictures . we take into account a number of important aspects such as : ( i ) pictures outside the domain of the wiki are ignored . ( ii ) pictures that already occur in the wiki or very similar to existing ones are also ignored . ( iii ) each new picture has to be associated with a suitable document in the wiki subject to three restraints : if a suitable document does not exist , a new one is created ; if the document size due to pictures gets too large some additional actions are necessary ; an image may be associated with more than one document . each of the above points offer many challenges which are discussed . 

machine learning classifications of coronary artery disease
coronary artery disease ( cad ) is one of the leading causes of death worldwide , and so it is very important to correctly diagnose patients with the disease . for medical diagnosis , machine learning is a useful tool ; however features and algorithms must be carefully selected to get accurate classification . to this effect , three feature selection methods have been used on 13 input features from the cleveland dataset with 297 entries , and 7 were selected . the selected features were used to train three different classifiers , which are svm , na ï ve bayes and knn using 10 - fold cross - validation . the resulting models evaluated using accuracy , recall , specificity and precision . it is found that the na ï ve bayes classifier performs the best on this dataset and features , outperforming or matching svm and knn in all the four evaluation parameters used and achieving an accuracy of 84% . 

portability of syntactic structure for language modeling
presents a study on the portability of statistical syntactic knowledge in the framework of the structured language model ( slm ) . we investigate the impact of porting slm statistics from the wall street journal ( wsj ) to the air travel information system ( atis ) domain . we compare this approach to applying the microsoft rule - based parser ( nlp - win ) for the atis data and to using a small amount of data manually parsed at upenn for gathering the initial slm statistics . surprisingly , despite the fact that it performs modestly in perplexity ( ppl ) , the model initialized on wsj parses outperforms the other initialization methods based on in - domain annotated data , achieving a significant 0 . 4 % absolute and 7 % relative reduction in word error rate ( wer ) over a baseline system whose word error rate is 5 . 8 % ; the improvement measured relative to the minimum wer achievable on the n - best lists we worked with is 12 % . 

building a voice based image caption generator with deep learning
image processing is used in various industries and it is remaining as one of the most advanced technologies used in google , medical field etc . recently , this technology has also attracted many programmers and developers due to its free and open source tool , which every developer can afford it . image processing also helps in finding out lot of information from a single image since it is currently utilized as a primary method for collecting the information from image and processing it for some purpose and some operations will also be performed on the image . a voice based image caption generation is a task that involves the nlp ( natural language processing ) concept for understanding the description of an image . the combination of cnn and lstm is considered as the best solution for this project ; the main target of the proposed research work is to obtain the perfect caption for an image . after obtaining the description , it will be converted into text and the text into a voice . image description is a best solution used for a visually impaired people who are unable to comprehend visuals . with the use of a voice based image caption generator , the descriptions can be obtained as a voice output , if their vision can ' t be resorted . in future , image processing will emerge as a significant research topic , which will be primarily utilized to save human lives . 

the parallel expert parser ( pep ) : a thoroughly revised descendant of the word expert parser ( wep ) 
in this paper we present pep ( the parallel expert parser , devos 1987) , a radically revised descendant of wep ( the word expert parser , small 1980 ) . wep ' s idea of linguistic entities as interacting processes has been retained , but its adherence to the word as the only entity has been rejected . experts exist at different levels , communicate through rigidly defined protocols and are now fully designed to run in parallel . a prototype of pep is implemented in flat concurrent prolog and runs in a logix environment . 

an improved global weight function of terms based on pearson ' s chi - square statistics
since term frequency , the most popular discriminator used in term weighting of natural language processing ( nlp ) , is not the only one which is necessary to be considered when calculating the term weight and make it suitable to indicate term importance , we are motivated to investigate other statistical characteristics of terms and found an important discriminator : term distribution . it is found in this this paper that a term close to hypo - dispersion distribution usually contains much contextual information and should be given higher weight than the one close to intensive distribution . based on this hypothesis , a pearson ' s chi - square theory based term global weight function is put forward in this paper . in addition , a text classifier system is developed based on lsa ( latent semantic analysis ) model and its precision and recall results are used for evaluation , which approve the reliability and efficiency of the algorithm on conclusion , term distribution should be considered into term weighting as a new discriminator and the algorithms in this paper is recommended . 

deep learning based query expansion for content relevance tracking
search engine ads , which constitute an important part of digital advertising expenditures , are formed by defining keywords to specified ad sets and directing them to the relevant pages . it is very important in terms of ad conversion that the content of the target ad page is compatible with the word set and targets the needs of customers that may change over time . on the other hand , the size of the landing page volume and the high speed of content change cause query and page matching problems . it is time - consuming and costly to evaluate each ad set one by one to detect incorrect matches and ensure correct matches . in this study , a new method called query - page matching model ( qpmm ) has been developed that evaluates matches between search engine ad sets and target page content . qpmm uses deep neural networks ( dnn ) and named entity recognition ( ner ) methods to calculate the match score between query - page groups . according to the results , the ner model achieved 0 . 94 f1 score in the labeled data set consisting of nearly 16 , 000 samples , increasing the performance of the ssem . the dnn model used in the study enabled the system to produce a successful semantic matching score by obtaining 0 . 56 rouge - 1 score . 

sentiment analysis with nlp on twitter data
every social networking sites like facebook , twitter , instagram etc become one of the key sources of information . it is found that by extracting and analyzing data from social networking sites , a business entity can be benefited in their product marketing . twitter is one of the most popular sites where people used to express their feelings and reviews for a particular product . in our work , we use twitter data to analyze public views towards a product . firstly , we have developed a natural language processing ( nlp ) based pre - processed data framework to filter tweets . secondly , we incorporate bag of words ( bow ) and term frequency - inverse document frequency ( tf - idf ) model concept to analyze sentiment . this is an initiative to use bow and tfidf are used together to precisely classify positive and negative tweets . we have found that by exploiting tf - idf vectorizer , the accuracy of sentiment analysis can be substantially improved and simulation results show the efficiency of our proposed system . we achieved 85 . 25 % accuracy in sentiment analysis using nlp technique . 

constructing parse forests that include exactly the n - best pcfg trees
this paper describes and compares two algorithms that take as input a shared pcfg parse forest and produce shared forests that contain exactly the n most likely trees of the initial forest . such forests are suitable for subsequent processing , such as ( some types of ) reranking or lfg f - structure computation , that can be performed ontop of a shared forest , but that may have a high ( e . g . , exponential ) complexity w . r . t . the number of trees contained in the forest . we evaluate the performances of both algorithms on real - scale nlp forests generated with a pcfg extracted from the penn treebank . 

real - time sign language converter for mute and deaf people
deaf people may get irritated due to the problem of not being able to share their views with common people , which may affect their day - to - day life . this is the main reason to develop such system that can help these people and they can also put their thoughts forward similar to other people who don ’ t have such problem . the advancement in the artificial intelligence provides the door for developing the system that overcome this difficulty . so this project aims on developing a system which will be able to convert the speech to text for the deaf person , and also sometimes the person might not be able to understand just by text , so the speech will also get converted to the universal sign language . similarly , for the mute people the sign language which they are using will get converted to speech . we will take help of various ml and ai concepts along with nlp to develop the accurate model . convolutional neural networks ( cnn ) will be used for prediction as it is efficient in predicting image input , also as lip movements are fast and continuous so it is hard to capture so along with cnn , the use of attention - based long short - term memory ( lstm ) will prove to be efficient . data augmentation methods will be used for getting the better results . tensorflow and keras are the python libraries that will be used to convert the speech to text . currently there are many software available but all requires the network connectivity for it to work , while this device will work without the requirement of internet . using the proposed model we got the accuracy of 100 % in predicting sign language and 96 % accuracy in sentence level understanding . 

nonautoregressive encoder - decoder neural framework for end - to - end aspect - based sentiment triplet extraction
aspect - based sentiment triplet extraction ( aste ) aims at recognizing the joint triplets from texts , i . e . , aspect terms , opinion expressions , and correlated sentiment polarities . as a newly proposed task , aste depicts the complete sentiment picture from different perspectives to better facilitate real - world applications . unfortunately , several major challenges , such as the overlapping issue and long - distance dependency , have not been addressed effectively by the existing aste methods , which limits the performance of the task . in this article , we present an innovative encoder - decoder framework for end - to - end aste . specifically , the aste task is first modeled as an unordered triplet set prediction problem , which is satisfied with a nonautoregressive decoding paradigm with a pointer network . second , a novel high - order aggregation mechanism is proposed for fully integrating the underlying interactions between the overlapping structure of aspect and opinion terms . third , a bipartite matching loss is introduced for facilitating the training of our nonautoregressive system . experimental results on benchmark datasets show that our proposed framework significantly outperforms the state - of - the - art methods . further analysis demonstrates the advantages of the proposed framework in handling the overlapping issue , relieving long - distance dependency and decoding efficiency . 

formulation of oligopolistic competition in ac power networks : an nlp approach
in this paper , oligopolistic competition in a centralized power market is characterized by a multi - leader single - follower game , and formulated as a nonlinear programming ( nlp ) problem . an ac network is used to represent the transmission system and is modeled using rectangular coordinates . the follower is composed of a set of competitive suppliers , demands , and the system operator , while the leaders are the dominant suppliers . the ac approach allows one to capture the strategic behavior of suppliers regarding not only active but also reactive power . in addition , the impact of voltage and apparent power flow constraints can be analyzed . different case studies are presented using a three - node system to highlight the features of the formulation . results on a 14 - node system are also presentedview less

managing information extraction : state of the art and research directions
this tutorial makes the case for developing a unified framework that manages information extraction from unstructured data ( focusing in particular on text ) . we first survey research on information extraction in the database , ai , nlp , ir , and web communities in recent years . then we discuss why this is the right time for the database community to actively participate and address the problem of managing information extraction ( including in particular the challenges of maintaining and querying the extracted information , and accounting for the imprecision and uncertainty inherent in the extraction process ) . finally , we show how interested researchers can take the next step , by pointing to open problems , available datasets , applicable standards , and software tools . we do not assume prior knowledge of text management , nlp , extraction techniques , or machine learning . 

a comparative analysis of methods related to automatic ship berthing
automatic berthing has been known as one of the most difficult problems in ship control . during port approach and berthing maneuvers , the ship master takes into account many factors before any maneuver action , i . e . ship speed , wind speed , wind direction , water current direction , available power , heading angle and ship response . however , many methods related to automatic berthing were developed by recent researches , such as artificial neural network , adaptive backstepping , nonlinear programming and proportional - integral - derivative . in this paper a comparative analysis of these methods is presented , determining the advantages and the limitations of each method . 

a long - term reactive power planning framework for transmission grids with high shares of variable renewable generation
the focus of this paper is how to account for the impacts of high shares of variable renewable energy sources such as wind farms and solar pvs on transmission grid steady - state operation in long - term planning of reactive power resources . the increased variability of nodal voltages due to the volatility of such sources will require adequate reactive support capability at the point of connection to the grid as well as throughout the grid if traditional reactive power supplies , i . e . conventional generation , are not available either because they are not dispatched at moments of high availability of wind and solar or they are already decommissioned . an optimization - based approach is proposed , namely a multi - period , multi - scenario corrective security - constrained optimal power flow , to identify the optimal size and location of reactive power sources for long - term planning in scenarios with increasing penetration of variable renewable sources . the proposed reactive power planning formulation optimizes the reactive power investments , considering different energy scenarios ( multi - scenario ) in a given planning horizon in discrete time steps ( multi - period ) . the proposed formulation is demonstrated on a modified ieee 14 - bus system . 

automatic differentiation between legitimate and fake news using named entity recognition
today , the increasing ease of publishing information online combined with a gradual shift of paradigm from consuming news via conventional media to non - conventional media calls for a computational and automatic approach to the identification of an article ' s legitimacy . in this study , we propose an approach for cross - domain fake news detection focusing on the identification of legitimate content from a pool of articles that are of varying degrees of legitimacy . we present a model as a proof of concept as well as data gathered from evaluating the model on fake - news amt , a dataset released for cross - domain fake news detection . the results of our model are then compared against a baseline model which has served as the benchmark for the dataset . we find all results in support of our hypothesis . our proof - of - concept model has also outperformed the benchmark in the domains technology and entertainment as well as when it was run on the whole dataset at once . 

a practical framework for formalizing and extracting chinese collocations
in this paper we argue for a word - sense based formalization for collocation , and proposes a seed - based approach for collocation extraction for specific purposes . the approach uses rfr _ sum model to iteratively classify polysemous word sense in the corpus . the collocation strength is also obtained by rfr . to capture the syntactic relation inside collocations , this paper presents a frame - based collocation extraction method , which uses word - related frames to obtain collocation with structural information automatically from a large - scale corpus with an average accuracy rate of 89 . 69% . 

automatic creation and tuning of context free grammars for interactive voice response systems
in this paper , we focus on the design of an autocfgprocessor procedure to automatically create and tune context free grammars ( cfgs ) for directed dialog speech applications without the use of any domain specific text corpora . a reranking mechanism is used to post - process the large vocabulary continuous speech recognizer ( lvcsr ) n - best lists with additional phonetic and higher level linguistic knowledge for transcribing the user utterances with improved word error rate ( wer ) . we also depict the classification of lvcsr transcriptions into semantic categories and the use of a statistical filtering mechanism on the valid lvcsr - transcriptions for the cfg creation and tuning tasks . we also illustrate the importance of the additional improvements gained by using semantic classification strength in a feedback loop to the transcription mechanism . 

multilingual single document keyword extraction for information retrieval
keywords play an important role in many aspects of information retrieval ( ir ) . from web searches to text summarization good keywords are a necessity . in a typical ir system algorithms are used which require the entire document collection to be built beforehand . while some research has been done on extracting keywords from a single document , the quality of the keywords was not based on how well they perform in ir tasks . moreover , they are designed for only one language and the applicability to other languages is unknown . as such , this paper proposes a new algorithm that is applicable to multiple languages and extracts effective keywords that , to a high degree , uniquely identify a document . it needs only a single document to extract keywords and does not rely on machine learning methods . it was tested on a japanese - english bilingual corpus and a portion of the reuter ' s corpus using a keyword search algorithm . the results show that the extracted keywords do a good job at uniquely identifying the documents . 

legal text analysis of the modification provisions : a pattern oriented approach
one of the main emerging research challenge in the legal documentation is to penetrate in the meaningful and in the semantic of the norm content using nlp techniques and isolate relevant part of the linguistic speech . this paper wants present a methodology for modeling the modificatory provisions in deep in order to provide all the necessary formalization for managing semi - automatically the consolidation process . 

a multimodal mixed reality data exploration framework for tactical decision making
in a data - driven , open - source information space , automatization tools and techniques are indispensable for analyzing the large amount of data and gaining knowledge from it . often however , methods focus solely on the data acquisition methods without providing an appropriate visualization and interactive exploration tool or framework that presents the information directly to the user in a descriptive manner . especially in crisis areas , where it is crucial to keep communication chains short and simple , mixed reality methods can help to present and explore important and relevant records that directly add value to the user in near real - time . this paper presents a novel approach towards data exploration in mixed reality environments , with the aim to enhance tactical decision - making processes and shorten unnecessarily long communication chains . 

analysis of variation on intra - speakers speech recognition performances
even if a speaker uses a speaker - dependent speech recognition system , speech recognition performance varies . however , the relationships between intra - speaker ' s speech variability and speech recognition performance are not clear . to investigate these relationships , we have been collecting speech data since november 2002 . in this paper , we analyze the relationships between intra - speaker ' s speech variability and the phoneme accuracy by a correlation analysis . analyzed results showed the strong negative correlation between the phoneme accuracy and the speaking rate . the correlation coefficient indicated - 0 . 77 . moreover , we can see that the phoneme accuracy is correlated with the temperature in the recording room and the humidity difference . 

guided interactive learning through chatbot using bi - directional encoder representations from transformers ( bert ) 
in this digital era , the smart application is ought to continuously generate a huge amount of data into existence . these data can be utilized to gain a large amount of information that deploys numerous uses . education is one of the key fields that generate huge amounts of data in existence . however , it is difficult to obtain only the required information because of the speed and volume of data being generated from numerous online educational resources . one of the tools that can be useful in extracting useful information from textual data is text summarization and analysis tools . many text summarization tools are being developed but largely focus on summarizing a single document effectively . this project aims to create a text summarization tool using natural language processing techniques that can extract relevant and important information from multiple documents to enable users to learn effectively . this information can be presented to users interactively and effectively through a chatbot interface . the tool also performs multiple analyses on the user responses provided to the chatbot to control the conversational flow and personalize the user experience to enhance learning . 

generating quantifiers and negation to explain homework testing
we describe prograder , a software package for automatic checking of requirements for programming homework assignments . prograder lets instructors specify requirements in natural language as well as explains grading results to students in natural language . it does so using a grammar that generates as well as parses to translate between a small fragment of english and a first - order logical specification language that can be executed directly in python . this execution embodies multiple semantics - - - both to check the requirement and to search for evidence that proves or disproves the requirement . such a checker needs to interpret and generate sentences containing quantifiers and negation . to handle quantifier and negation scope , we systematically simulate continuation grammars using record structures in the grammatical framework . 

software testing and the naturally occurring data assumption in natural language processing
it is a widely accepted belief in natural language processing research that naturally occurring data is the best ( and perhaps the only appropriate ) data for testing text mining systems . this paper compares code coverage using a suite of functional tests and using a large corpus and finds that higher class , line , and branch coverage is achieved with structured tests than with even a very large corpus . 

detection of duplicate defect reports using natural language processing
defect reports are generated from various testing and development activities in software engineering . sometimes two reports are submitted that describe the same problem , leading to duplicate reports . these reports are mostly written in structured natural language , and as such , it is hard to compare two reports for similarity with formal methods . in order to identify duplicates , we investigate using natural language processing ( nlp ) techniques to support the identification . a prototype tool is developed and evaluated in a case study analyzing defect reports at sony ericsson mobile communications . the evaluation shows that about 2 / 3 of the duplicates can possibly be found using the nlp techniques . different variants of the techniques provide only minor result differences , indicating a robust technology . user testing shows that the overall attitude towards the technique is positive and that it has a growth potential . 

tracing contacts with mobile phones to curb the pandemic : topics and stances in people ’ s online comments about the official german contact - tracing app
the covid - 19 pandemic has led to a health crisis with 90 million infections and two million deaths by the end of january 2021 . to prevent an overload of medical capacities , quickly identifying potentially infected persons is vital to stop the spread of the virus . mobile apps for tracing people ’ s contacts seem effective , but raise public concerns , e .  g . , about privacy . hence , they are contested in public discourse . we report a large - scale nlp - supported analysis of people ’ s comments about the german contact - tracing app on news websites , social media and app stores . we identified prevalent topics , stances , and how commenting developed over time . we found privacy to be among the most debated topics discussed from various perspectives . commenting peaked at one point in time , when public discourse centered on the potential tracing protocols and their privacy protection . we encourage further research on the link between the public discussions and actual adoption rates of the app . 

incorporating supervised learning algorithms with nlp techniques to classify bengali language forms
every language has its own root , form , and grammar , and so does bengali . bengali language has two core forms : " sadhu - bhasha " and " cholito - bhasha " which have been widely used from regular communication to literary publications . at present , sadhu - bhasha can be only found in old books and literary publications , whereas cholito - bhasha is mostly used everywhere . however , so many bengali linguists are still researching on these two forms to preserve its root , understand and develop bengali , and also extract knowledge from the historical publications which were mainly written in sadhu - bhasha . unfortunately , till now they do not have any digital tool that can assist their research by automatically identifying these core forms of bengali from the large archive of bengali literature . this study aims to build such an automatic intelligent system that can accurately identify these two language forms by harnessing the power of natural language processing ( nlp) . in this study , we have applied advanced nlp techniques and six supervised learning algorithms to classify " sadhu - bhasha " and " cholito - bhasha " from text corpora . results of this study show that all the six models yielded very promising results , however , the multinomial naive bayes outperformed all the models with 99 . 5 % accuracy , 99 . 0 % precision , 100 % recall , 0 . 995 auc score and , 0 . 995 f1 score . additionally , this study also performs qualitative analysis using t - sne algorithm to visualize the difference between sadhu - bhasha and cholito - bhasha . 

ecovillages , values , and interactive technology : balancing sustainability with daily life in 21st century america
this project seeks to provide a rich account of the adaptive process that occurs as individuals with explicit value commitments interact with information technology . specifically , ethnographic methods are being used to investigate the information technology adaptive process as it unfolds in the daily life of two ecovillages , communities made up of individuals striving to balance their use of technology with a lifestyle that is environmentally , socially , and economically sustainable . anticipated research outcomes include : ( 1 ) an analytic description of information technology adaptive process ; ( 2 ) a categorization of technological functionalities which support or constrain certain values , ( 3 ) an empirical extension of value sensitive design , and ( 4 ) an analysis of the negotiation around tensions which emerge as a community ' s values influence the use of information technology features and , reciprocally , as information technology features influence a community ' s values . most broadly this work contributes to our larger understanding of how the information technology adaptive process influences the human experience . 

direct trajectory optimization by a chebyshev pseudospectral method
a chebyshev pseudospectral method is presented in this paper for directly solving a generic optimal control problem with state and control constraints . this method employs nth degree lagrange polynomial approximations for the state and control variables with the values of these variables at the chebyshev - gauss - lobatto ( cgl ) points as the expansion coefficients . this process yields a nonlinear programming problem ( nlp ) with the state and control values at the cgl points as unknown nlp parameters . numerical examples demonstrate this method yields more accurate results than those obtained from the traditional collocation methods . 

effect of word complexity on l2 vocabulary learning
research has shown that a number of factors , such as maturational constraints , previous language background , and attention , can have an effect on l2 acquisition . one related issue that remains to be explored is what factors make an individual word more easily learned . in this study we propose that word complexity , on both the phonetic and semantic levels , affect l2 vocabulary learning . two studies showed that words with simple grapheme - to - phoneme ratios were easier to learn than more phonetically complex words , and that words with two or fewer word senses were easier to learn that those with three or more . 

recommending features and feature relationships from requirements documents for software product lines
feature models are a key element in software product lines , representing the supported features and their interrelationships within a family of software products . recommendation systems for software engineering ( rsses ) are potentially useful in supporting the extraction , maintenance , and categorization of feature models . this paper focuses on the design and implementation of an rsse to automatically recommend features for software product lines , the types of these features , and how they could be related to each other . such a recommender should save time and tedium over doing the work manually . we present ffre , a prototype recommendation tool for the extraction of features and their relationships from software requirements specification ( srs ) documents . ffre is based on natural language processing ( nlp ) techniques and heuristics . ffre is evaluated qualitatively from four srs documents and compared against other tools and approaches . 

sentiment analysis of the united states senate twitter feeds in election year 2020
sentiment is a powerful motivation which reflects the ideology of politicians . twitter has now become a common platform for the united states members of congress to express and proliferate their agenda to the voter - base . does twitter play a role in the outcome for the senate race elections in 2020 ? is there a relationship between the sentiments of winners and losers on social media ? in this paper , we analyze these key relationships between incumbent senators and their challengers from over thirty states , most of which are key battleground states . the twitter feed for all the politicians is mined , cleaned and analyzed using sentiment analysis and statistical modelling using python and matlab . we find that the difference between the rate of tweet and sentiment scores for sitting senators and their challengers from the republican and democratic party is significantly different . we also report that there is a notable difference between tweet rate and sentiment score for members within a party . 

a non - programming introduction to computer science via nlp , ir , and ai
this paper describes a new cornell university course serving as a non - programming introduction to computer science , with natural language processing and information retrieval forming a crucial part of the syllabus . material was drawn from a wide variety of topics ( such as theories of discourse structure and random graph models of the world wide web ) and presented at some technical depth , but was massaged to make it suitable for a freshman - level course . student feedback from the first running of the class was overall quite positive , and a grant from the ge fund has been awarded to further support the course ' s development and goals . 

identification of noun phrase with various granularities
since noun phrases are the most popular phrases in texts , noun phrase identification is one of vital subtasks of natural language processing . generally chinese noun phrases have hierarchical inner structures . this paper proposes an approach of defining various levels of granularity for noun phrases , catering for different application demands . three levels of granularity noun phrases are proposed , that is , concept noun phrase , base noun phrase and entire noun phrase . the task of noun phrase identification is to label word sequences with phrase tags . all granularity noun phrase identifications are cast as classification problem under certain encoding schemes . the experimental dataset is acquired empirically from chinese penn treebank 5 . 1 . f , measure of concept noun phrase , base noun phrase and entire noun phrase identification reaches 92 . 12 % , 84 . 13 % and 85 . 32 % respectively . 

introducing inference - driven owl abox enrichment
publically available text - based documents ( e . g . news , meeting transcripts ) are a very important source of knowledge for organizations and individuals . these documents refer domain entities such as persons , places , professional positions , decisions , actions , etc . querying these documents ( instead of browsing , searching and finding ) is a very relevant task for any person in general , and particularly for professionals dealing with intensive knowledge tasks . querying text - based documents ' data , however , is not supported by common technology . for that , such documents ' content has to be explicitly and formally captured into knowledge base facts . making use of automatic nlp processes for capturing such facts is a common approach , but their relatively low precision and recall give rise to data quality problems . further , facts existing in the documents are often insufficient to answer complex queries and , therefore , it is often necessary to enrich the captured facts with facts from third - party repositories ( e . g . public lod , private is databases ) . this paper describes the adopted process to identify what data is currently missing from the knowledge base repository and which is desirable to collect from external repositories . the proposed process aims to foster and is driven by owl dl inference - based instance ( abox ) classification , which is supported by the constraints of the tbox . 

boosting performance of gene mention tagging system by classifiers ensemble
to further improve the tagging performance of single classifiers , a classifiers ensemble experimental framework is presented for gene mention tagging . in the framework , six classifiers are constructed by four toolkits ( crf++ , yamcha , maximum entropy ( me ) and mallet ) with different training methods and feature sets and then combined with a two - layer stacking algorithm . the recognition results of different classifiers are regarded as input feature vectors to be incorporated , and then a high - powered model is obtained . experiments carried out on the corpus of biocreative ii gm task show that the classifiers ensemble method is effective and our best combination method achieves an f - score of 88 . 09% , which outperforms most of the top - ranked bio - ner systems in the biocreative ii gm challenge . 

a statistical method for uyghur tokenization
tokenization is very important for uyghur language processing . tokenization of uyghur , an agglutinative language , is quite different from other languages such as chinese and english . in this paper we propose a two - steps statistical tokenization method for uyghur . two related factors , the feature template scheme and the manually tokenized corpora , are also discussed . the preliminary experiment results demonstrate that the proposed method is effective : the f - measure of tokenization reaches 88 . 9 % in the open test . 

convolution kernels with feature selection for natural language processing tasks
convolution kernels , such as sequence and tree kernels , are advantageous for both the concept and accuracy of many natural language processing ( nlp ) tasks . experiments have , however , shown that the over - fitting problem often arises when these kernels are used in nlp tasks . this paper discusses this issue of convolution kernels , and then proposes a new approach based on statistical feature selection that avoids this issue . to enable the proposed method to be executed efficiently , it is embedded into an original kernel calculation process by using sub - structure mining algorithms . experiments are undertaken on real nlp tasks to confirm the problem with a conventional method and to compare its performance with that of the proposed method . 

improving nlp through marginalization of hidden syntactic structure
many nlp tasks make predictions that are inherently coupled to syntactic relations , but for many languages the resources required to provide such syntactic annotations are unavailable . for others it is unclear exactly how much of the syntactic annotations can be effectively leveraged with current models , and what structures in the syntactic trees are most relevant to the current task . we propose a novel method which avoids the need for any syntactically annotated data when predicting a related nlp task . our method couples latent syntactic representations , constrained to form valid dependency graphs or constituency parses , with the prediction task via specialized factors in a markov random field . at both training and test time we marginalize over this hidden structure , learning the optimal latent representations for the problem . results show that this approach provides significant gains over a syntactically un - informed baseline , outperforming models that observe syntax on an english relation extraction task , and performing comparably to them in semantic role labeling . 

are we waves or are we particles ? a new insight into deep semantics in natural language processing
this paper brings conceptually new , empirically based scientific approach to a deeper understanding of human mind cognition , language acquisition , modularity of language and language origin itself . the research presented provides an interactive multilingual associative experiment as an attempt to map the cognitive semantic space : ( csses ) and its basic frames of the essential self in the czech language , collects and compares it to the csses of conceptual language view in czech , russian , english and potentially in other languages . we attempt to merge cognitive metaphor theory with psycholinguistics and psychoanalysis applying associative experiment methodology on the essential self metaphors . the research has two main goals : the first is to build an essential self multilingual wordnet , which serves as the basic lexical resource for artificial intelligence describes the core of the human nature . the second is to create a multilingual 3d semantic network . 

gappredict – a language model for resolving gaps in draft genome assemblies
short - read dna sequencing instruments can yield over 10 12 bases per run , typically composed of reads 150 bases long . despite this high throughput , de novo assembly algorithms have difficulty reconstructing contiguous genome sequences using short reads due to both repetitive and difficult - to - sequence regions in these genomes . some of the short read assembly challenges are mitigated by scaffolding assembled sequences using paired - end reads . however , unresolved sequences in these scaffolds appear as “ gaps” . here , we introduce gappredict – an implementation of a proof of concept that uses a character - level language model to predict unresolved nucleotides in scaffold gaps . we benchmarked gappredict against the state - of - the - art gap - filling tool sealer , and observed that the former can fill 65 . 6 % of the sampled gaps that were left unfilled by the latter with high similarity to the reference genome , demonstrating the practical utility of deep learning approaches to the gap - filling problem in genome assembly . 

multi - task learning with mutual learning for joint sentiment classification and topic detection
recently , advances in neural network approaches have achieved many successes in both sentiment classification and probabilistic topic modelling . on the one hand , latent topics derived from the global context of documents could be helpful in capturing more accurate word semantics and hence could potentially improve the sentiment classification accuracy . on the other hand , the word - level attention vectors obtained during the learning of sentiment classifiers could carry word - level polarity information and can be used to guide the discovery of topics in topic modelling . this paper proposes a multi - task learning framework which jointly learns a sentiment classifier and a topic model by making the word - level latent topic distributions in the topic model to be similar to the word - level attention vectors in the classifier through mutual learning . experimental results on the yelp and imdb datasets verify the superior performance of the proposed framework over strong baselines on both sentiment classification accuracy and topic modelling evaluation results including perplexity and topic coherence measures . the proposed framework also extracts more interpretable topics compared to other conventional topic models and neural topic models . 

an automated method for constructing ontology
this paper proposed an automated method for ontology construction . the techniques used include machine learning and nlp ( natural language processing) . these techniques are used to construct more accurate ontologies that represented as xml format . firstly , na ï ve bayes classification algorithms is used to text categorization which determined what kind of label a document is assigned . secondly , svm algorithm is used to calculate document similarity to cluster similar documents that we imported . thirdly , luhn ' s summarization algorithm is employed in order to shorten the text and kept important essential terms for making the domain ontology . finally , xml format is used to represent a domain ontology , which has the function of describing term ' s meaning and their lexical relationships . additional the method developed by python programming language and natural language toolkit . 

story visualization : generation of scenes sequentially for a given fable using nlp and image processing
humans have a usual tendency to paint pictures in their minds when a story is being told . undoubtedly , a picture conveys information more effectively than words . so it is necessary to instill visualization capability at an early age . in this project story visualization , we have aimed to improve the visualization of fables along with narration of the story . this is achieved by performing pronoun replacement to enhance story character extraction and improving fragmentation of the story to accurately depict the story , which is received as input in the form of text . the extracted attributes for scene generation are then sent to the image processing model which chooses appropriate images from the labeled dataset , transforms them relatively , and positions them based on grids , thereby generating a meaningful scene that nearly depicts the origin segment . furthermore , the story is narrated , and the interactions between the story characters are uniquely voiced while they are exhibited through speech balloons . the comprehensive website presented abstracts nlp model , image processing model , and texttospeech model while giving a fairly simple user interface for users to interact with . to cover a broader audience , we have experimented in indian languages and as proof of concept , kannada language fables are visualized . 

automated extraction of conceptual models from user stories via nlp
natural language ( nl ) is still the predominant notation that practitioners use to represent software requirements . albeit easy to read , nl does not readily highlight key concepts and relationships such as dependencies and conflicts . this contrasts with the inherent capability of graphical conceptual models to visualize a given domain in a holistic fashion . in this paper , we propose to automatically derive conceptual models from a concise and widely adopted nl notation for requirements : user stories . due to their simplicity , we hypothesize that our approach can improve on the low accuracy of previous works . we present an algorithm that combines state - of - the - art heuristics and that is implemented in our visual narrator tool . we evaluate our work on two case studies wherein we obtained promising precision and recall results ( between 80 % and 92 % ) . the creators of the user stories perceived the generated models as a useful artifact to communicate and discuss the requirements , especially for team members who are not yet familiar with the project . 

energy - based lifetime maximization and security of wireless - sensor networks with general nonideal battery models
we study the problem of maximizing the lifetime of a sensor network by means of routing and initial energy allocation over its nodes . we consider a general state space battery model and show that similar results to our previous work with simpler battery dynamics are still valid . in particular , we show that under this general dynamic battery model , there exists an optimal policy consisting of time - invariant routing probabilities in a fixed topology network and these can be obtained by solving a set of nonlinear programming ( nlp ) problems . moreover , we show that the problem can be reformulated as a single nlp problem . in addition , we consider a joint routing and initial energy allocation problem over the network nodes with the same network lifetime maximization objective . we prove that the solution to this problem is given by a policy that depletes all node energies at the same time and that the corresponding energy allocation and routing probabilities are obtained by solving an nlp problem . finally , we examine a network ' s performance under security threats , typified by faked - cost attacks , in terms of its lifetime and its normalized throughput . we illustrate how the optimal routing probabilities , as well as the network lifetime , are robust under such forms of routing attacks even though its normalized throughput can be significantly reduced . 

adaptive horizon multistage nonlinear model predictive control
in this paper , we present a computationally efficient multistage nonlinear model predictive controller ( nmpc ) with a prediction horizon update using nonlinear programming ( nlp ) sensitivities . computational delay is minimized by updating the prediction horizon to a sufficient length at every time step . for a set - point tracking multistage nmpc , we first determine a terminal region around an optimal equilibrium point for each uncertainty realization in offline mode . then using nlp - sensitivity we estimate a sufficient horizon length for the next time step such that all scenarios will be driven into their respective terminal regions . this adaptive horizon multistage nmpc ( ah - msnmpc ) is recursively feasible and input - to - state practically stable . in a simulation study , the ah - msnmpc was used to control a benchmark cooled cstr process under parametric uncertainty . the ah - msnmpc computations take 1 . 4 % and 29 . 5 % of the sampling interval duration for robust horizon of 1 and 2 , respectively . with a robust horizon length of 1 the controller is 15 times faster than ideal - multistage nmpc with a long enough prediction horizon . the computational delay is halved with robust horizon length of 2 . the performance of the two controllers was found to be similar . the improved efficiency is vital in practice for improved control performance and closed - loop stability . it is desired for real - time optimal decision making , and also under limited computing resources such as in embedded systems . 

a new weighted text filtering method
in order to describe the user ' s profile exactly and provide what they need in information filtering , a new method to weight the user ' s characterized demand and filter for him is put forward . in the paper , at first we define the terms ' weight of the user ' s profile according to the order of the proposed sequence , then calculate the similarity between the user ' s profile and the documents using the semantic dictionaries of synonym dictionary and hownet , and at last submit the results to the user sorted in descending order of the similarity . the experiments show that it can improve the precision of information filtering . 

co - training for demographic classification using deep learning from label proportions
deep learning algorithms have recently produced state - of - the - art accuracy in many classification tasks , but this success is typically dependent on access to many annotated training examples . for domains without such data , an attractive alternative is to train models with light , or distant supervision . in this paper , we introduce a deep neural network for the learning from label proportion ( llp ) setting , in which the training data consist of bags of unlabeled instances with associated label distributions for each bag . we introduce a new regularization layer , batch averager , that can be appended to the last layer of any deep neural network to convert it from supervised learning to llp . this layer can be implemented readily with existing deep learning packages . to further support domains in which the data consist of two conditionally independent feature views ( e . g . image and text ) , we propose a co - training algorithm that iteratively generates pseudo bags and refits the deep llp model to improve classification accuracy . we demonstrate our models on demographic attribute classification ( gender and race / ethnicity ) , which has many applications in social media analysis , public health , and marketing . we conduct experiments to predict demographics of twitter users based on their tweets and profile image , without requiring any user - level annotations for training . we find that the deep llp approach outperforms baselines for both text and image features separately . additionally , we find that co - training algorithm improves image and text classification by 4 % and 8 % absolute f1 , respectively . finally , an ensemble of text and image classifiers further improves the absolute f1 measure by 4 % on average . 

nlp - ng — a new nlp system for biomedical text analysis
nlp - ng is a new nlp system consisting of three components : ng - core ( language processing ) , ng - db ( database management ) , and ng - see ( interactive visualization and entry ) . the ultimate goal of nlp - ng is to produce information retrieval systems in which users can choose full - text schema , adding specific items to focus their queries . schema are created by a normalization process which elides adjunctive constructions as well as replacing items by prototypes . biomedical text contains domain - specific constructions which are revealed by normalization . nlp - ng is based on construction grammar . computationally , all representations are integer - based , allowing efficient storage , indexing , and retrieval . see , an ajax web browser client , allows developers , linguists , and users to view a corpus and modify its properties . nlp - ng uses a 300 million word biomed central corpus . nlp - ng does not focus on specific strategies to extract limited classes of information from papers . instead , it is a universal approach that can codify a wide variety of text in papers . 

attention - based recurrent neural model for named entity recognition in chinese social media
ner ( named entity recognition ) is one of the major tasks in nlp ( natural language processing ) . although there are already some methods to solve ner problem , it is rare in chinese social media , and less of them can make good use of external information . the pos ( part - of - speech ) of a word can convey a lot of information in ner , and the attention mechanism can help us successfully focus on these contents . for the above reasons , we propose an attention - based bidirectional lstm ( long short - term memory ) model to address ner problem in chinese social media . the attention mechanism utilizes the pos of every character of a sentence in the learning process . experiments on the dataset of chinese social media demonstrated the performance of the proposed neural model . 

scalable processing of massive text data stores for nlp
the storage and processing of text are integral components of machine learning and natural language processing algorithms . as text - generating sources continue to enlarge the size of natural language data stores , the ability for individual systems to process overwhelming volumes of data becomes challenging . the emergence of systems capable of parallelizing text processing has enabled researchers to rapidly build , train , and deploy intricate nlp and machine learning models . in this tutorial , methods of parallel processing of massive text data stores for nlp and machine learning algorithms will be introduced . tools for constructing models using a variety of approaches , ranging from typical frequency implementations to graph representations of text , will be reviewed . in addition , challenges for both industry and academia will be discussed . 

practical inner codes for batched sparse codes
batched sparse ( bats ) code is a promising technology for reliable end - to - end transmission in multi - hop wireless networks . one main research topic for bats code is how to design an optimal inner code that is typically random linear network code . in this paper , this issue is focus on the number of transmissions from an end - to - end perspective . the problem is formulated as a mixed integer nonlinear programming ( minlp ) problem with the objective of minimizing the total number of transmissions from source to destination . subsequently , the inherent properties of inner codes are exploited to relax the integer restrictions by the means of the regularized incomplete beta function . as a result , a new nonlinear programming ( nlp ) problem is constructed . solving the nlp problem provides a valid lower bound on the optimal solution , and , hence , is used as the performance measure for our heuristic . furthermore , a centralized approximation approach is developed to solve our minlp problem efficiently . the numerical results demonstrate that all solutions developed in the paper are near - optimal with a guaranteed performance bound . 

generating chinese classical poetry with quatrain generation model ( qgm ) using encoder - decoder lstm
chinese classical poetry has strict formats and complicated linguistic rules including harmonious rhyme , level and oblique tones ( pingze ) . therefore , using nlp to generate classical poetry that meets the aforementioned constraints has always attracted much attention . although many studies have shown that deep learning has good results in generating poetry , however , the use of unconstrained natural language may cause the generated poems to fail in conforming to the metric rules , and also make the expression of the meaning of the poems incomplete . in view of this , this study uses the quatrains of the tang and song dynasties as the training sample , adopts encoder decoder lstm as the quatrain generation model ( qgm ) , and then uses latent dirichlet allocation to expand the meaning of related words of each consecutive sentence of the poetry . the features and contributions of this study are as follows : ( 1 ) we propose a qgm which learns poetry knowledge from four consecutive sentences as corpus respectively in the poetry . ( 2 ) the qgm is leveraged to strengthen the meaningfulness , grammaticality , and poeticness of a poetry by extending the keywords to a collection of semantic related words . ( 3 ) we elaborate the evaluation of poetry generation with bleu - 2 metric , cooperating with human evaluation . this study shows that our model can effectively improve the coherence of the sentences with the requirement of the metric rules . 

artificial intelligence in financial services – need to blend automation with human touch
artificial intelligence is the latest in the series of disruptions manifested by computer science . ai has been rapidly transforming the dynamics of banking and financial services industry also . the established and emerging capabilities of ai are being combined , reconstituted and re - formulated in unexpected ways and are throwing up new opportunities and new challenges but at the same time posing new threats also . apart from the ethico - neutral character of technology and its attendant threats like cyber - crimes and macro - financial risks , a major question to be inquired into is its sustainability as it tends to replace humans and the related personal touch which most often is the essence of financial services industry thriving on the art of customization and customer delight . the instant paper attempts to examine relatively under - explored perspective of ai replacing humans in the space of banking and financial services and unmindfully heralding the flight of personal touch and service customization which are the cornerstone of customer satisfaction and delight in industries like banking and financial services known for their fiduciary and responsible character . 

measuring orthogonal mechanics in linguistic annotation games
gamification has been recently growing in popularity among researchers investigating information and communication technologies . scholars have been trying to take advantage of this approach in the field of natural language processing ( nlp ) , developing games with a purpose ( gwaps ) for corpus annotation that have obtained encouraging results both in annotation quality and overall cost . however , gwaps implement gamification in different ways and to different degrees . we propose a new framework to investigate the mechanics employed in the gamification process and their magnitude in terms of complexity . this framework is based on an analysis of some of the most important contributions in the field of nlp - related gamified applications and gwap theory . its primary purpose is to provide a first step towards classifying mechanics that mimic mainstream video games and may require skills that are not relevant to the annotation task , defined as orthogonal mechanics . in order to test our framework , we develop and evaluate spacewords , a linguistic space game for synonymy annotation . 

can we use se - specific sentiment analysis tools in a cross - platform setting ? 
in this paper , we address the problem of using sentiment analysis tools ' off - the - shelf ' , that is when a gold standard is not available for retraining . we evaluate the performance of four se - specific tools in a cross - platform setting , i . e . , on a test set collected from data sources different from the one used for training . we find that ( i ) the lexicon - based tools outperform the supervised approaches retrained in a cross - platform setting and ( ii ) retraining can be beneficial in within - platform settings in the presence of robust gold standard datasets , even using a minimal training set . based on our empirical findings , we derive guidelines for reliable use of sentiment analysis tools in software engineering . 

exploiting the student model to emphasize language teaching pedagogy in natural language processing
one of the typical problems of natural language processing ( nlp ) is the explosive property of the parser and this is aggravated in an intelligent language tutoring system ( ilts ) because the grammar is unconstrained and admits even more analyses . nlp applications frequently incorporate techniques for selecting a preferred parse . computational criteria , however , are insufficient for a pedagogic system because the parse chosen will possibly result in misleading feedback for the learner . preferably , the analysis emphasizes language teaching pedagogy by selecting the sentence interpretation a student most likely intended . in the system described in this paper , several modules are responsible for selecting the appropriate analysis and these are informed by the student model . aspects in the student model play an important pedagogic role in determining the desired sentence interpretation , handling multiple errors , and deciding on the level of interaction with the student . 

towards an understanding of arab women researchers contribution in arabic nlp
arabic language is spoken by around 400 million people living in the middle east , north africa , and the horn of africa . like other languages , literary arabic continues to evolve ; this suggests a large potential audience for arabic natural language processing ( nlp ) and the need for a fair contribution from both men and women arabic nlp researchers . our paper will explore arab women researchers ’ contributions in the field of arabic natural language processing research ( anlp ) by answering 9 research questions . we answer these questions through examining research metadata from digital bibliography & library project ( dblp ) and google scholar . we found that the proportion of arab women researchers in anlp compared to men is similar to the international level . we also observed their publication , collaboration and citation patterns . our results point to the need for gender equity initiatives to increase the number of arab female researchers in anlp . 

using lexical and relational similarity to classify semantic relations
many methods are available for computing semantic similarity between individual words , but certain nlp tasks require the comparison of word pairs . this paper presents a kernel - based framework for application to relational reasoning tasks of this kind . the model presented here combines information about two distinct types of word pair similarity : lexical similarity and relational similarity . we present an efficient and flexible technique for implementing relational similarity and show the effectiveness of combining lexical and relational models by demonstrating state - of - the - art results on a compound noun interpretation task . 

metaextract : an nlp system to automatically assign metadata
we have developed metaextract , a system to automatically assign dublin core + gem metadata using extraction techniques from our natural language processing research metaextract is comprised of three distinct processes : equery and html - based extraction modules and a keyword generator module . we conducted a web - based survey to have users evaluate each metadata element ' s quality . only two of the elements , title and keyword , were shown to be significantly different , with the manual quality slightly higher . the remaining elements for which we had enough data to test were shown not to be significantly different ; they are : description , grade , duration , essential resources , pedagogy - teaching method , and pedagogy - group . 

automatic filtration of multiword units
this paper studies how to filtrate multiword units . we use normalized expectation ( ne ) to extract multiword unit candidates from patent corpus . then the multiword unit candidates are filtrated using stop words , frequency , first stop words , last stop words , and contextual entropy . the experimental result shows that the precision rate of multiword units is improved by 8 . 7 % after filtration . 

 “ can nlp techniques be utilized as a reliable tool for medical science ?” - building a nlp framework to classify medical reports
artificial intelligence persists on being a right - hand tool for many branches of biology . from preliminary advices and treatments , such as understanding if symptoms related to fever or cold , to critical detection of cancerous cell or classification of x - rays , traditional machine learning and deep learning techniques achieved remarkable feats . however , total dependency on machine - based prediction is yet a far fetched concept . in this paper , we provide a framework utilizing several natural language processing ( nlp ) algorithms to construct a comparative analysis . we create an ensemble of top - performing algorithms to accomplish classification task on medical reports . we compare both the traditional machine learning and deep learning techniques and evaluate their probabilities of being reliable on analyzing medical diagnosis . we concluded that an ensemble approach can provide reliable outcomes with accuracy over 92 % and that the current state of the art is unequipped to provide the result with the standard needed for health sectors but an ensemble of these techniques can be a pathway for future research direction . 

on throughput enhancement of multi - hop wireless networks using interference alignment
interference alignment is a new technique recently proposed in physical layer wireless communications research , which purposely align the interference among multiple users thus increase the efficiency of channel use . this paper proposes a new method to enhance throughput of multi - hop wireless networks by utilizing interference alignment at physical layer . at first , it takes a simple network with only 6 nodes to demonstrate that interference alignment can achieve higher throughput . subsequently , an optimization problem is formulated to maximize multi - hop wireless network throughput , which is a nonlinear programming ( nlp ) problem . among that , finding the current transmission links can be transformed into maximal cliques of a graph . furthermore , a branch - and - bound framework is provided to obtain the achievable bound of the nlp problem . numerical results are presented to validate the algorithm and offer insights on the throughput enhancement brought by the interference alignment technique . 

glovenor : glove for node representations with second order random walks
we study the community detection problem by embedding the nodes of a graph into a n - dimensional space such that similar nodes remain close in their representations . there are many state - of - the - art methods , like node2vec and deepwalk to compute node embeddings with the use of second order random walks . these techniques borrow methods like the skip - gram model , used in the domain of natural language processing ( nlp ) to compute word embeddings . this paper explores the idea of porting the glove ( global vectors for word representation ) model , a popular technique for word embeddings , to a new method called glovenor , to compute node embeddings in a graph , and creating a corpus with the use of second order random walks . we evaluate the model ' s quality by comparing it against node2vec and deepwalk on the problem of community detection on five different data sets . we observe that glovenor discovers similar or better communities than the other existing models on all the datasets based on the modularity score . 

slack variable - based control variable parameterization method for constrained engineering optimization
in this paper , a slack variable - based control variable parameterization ( cvp ) method is proposed for solving engineering dynamic optimization problems with inequality path constraints . an improved slack variable transform technique is introduced so that the original problem is converted into an unconstrained dynamic optimization problem ( udop ) . no approximation error generated and the inequality path constraints are satisfied during the whole time intervals . a nonlinear programming ( nlp ) problem , which approximates the original dynamic optimization problem , is obtained by applying the control variable parameterization . the variational method is derived to calculate the gradients so that the transformed nlp problem can be easily solved . test results show that the proposed method performs better than the penalty method in terms of computation accuracy and computation time . 

research of the method for obtaining semantic knowledge in the system of ci - nlu based text processing after speech recognition
how to obtain the high - level knowledge , such as semantic knowledge , to do deep processing of natural language is a focus in the field of natural language processing ( nlp ) these days . in this paper , we do research on the construction of semantic acknowledge base against the background of post - processing of speech recognition . based on reviewing existing methods , authors present and realize a new domain - independent algorithm that applies to a single document and considers the impact of context . in the test , this algorithm shows well performance and the knowledge base can meet the need of project . 

creation of unambiguous centralized knowledge base from umls metathesaurus
efficient clinical information retrieval tasks like entity recognition , relation suggestions , summarization etc , require a comprehensive , extensive , unambiguous and well structured medical knowledge base . one of the largest metathesaurus , umls ( unified medical language system ) , is a repository of biomedical dictionaries developed by the us national library of medicine ( nlm ) and widely used in medical domain . umls metathesaurus includes the dictionaries like snomed - ct , national center for biotechnology information ( ncbi ) taxonomy , medical subject headings ( mesh ) , online mendelian inheritance in man ( omim ) etc . as it has integrated data from different sources , it contains different kinds of ambiguity , which is problematic for all clinical information retrieval tasks that use it . in this paper , we describe our methodology of curating the umls metathesaurus to create a centralized knowledge base that can be used as a knowledge base for a variety of clinical nlp systems . we have also developed a process of updating the curated centralized knowledge base with a newer version of umls such that there is no need to repeat the whole process . we have also presented the comparative results of a clinical entity recognition ( cer ) using our curated centralized knowledge base and original umls database . 

fast uav trajectory optimization using bilevel optimization with analytical gradients
we present an efficient optimization framework that solves trajectory optimization problems by decoupling state variables from timing variables , thereby decomposing a challenging nonlinear programming ( nlp ) problem into two easier subproblems . with timing fixed , the state variables can be optimized efficiently using convex optimization , and the timing variables can be optimized in a separate nlp , which forms a bilevel optimization problem . the challenge is to obtain the gradient of the objective function which itself needs an optimization to compute . whereas finite differences must solve many optimization problems to compute the gradient , our method is based on sensitivity analysis of parametric programming : the dual solution ( lagrange multipliers ) of the lower - level optimization is used to compute analytical gradients . since the dual solution is a by - product of the optimization , the exact gradients can be obtained " for free " . the framework is demonstrated on generating trajectories in safe corridors for an unmanned aerial vehicle . experiments demonstrate that bilevel optimization converges significantly more reliably than a standard nlp solver , and analytical gradients outperform finite differences in terms of computation speed and accuracy . with a 25ms cutoff time , our approach achieves over 8 times better suboptimality than the current state - of - the - art . 

nlp for stress mitigation in employees
stress management in the workplace is a reality that most of us have to face for one reason or another and coping with it is the key to a long - term career success . some careers are more stressful than others . the main causes of workplace stress appear to be linked to deficiencies in the management and organization of work . stress and culture are not only predominantly seen in the industry but are visual even in the service sector . this study directs itself to the study of organization culture and stress in an educational institute . it is normally presumed that the teaching profession is less stressful . with the changing demands in the educational paradigm and expectations from the stakeholders , the study and the findings thereof presented in this paper have significant importance in redefining the premises of the education sector from knowledge management perspective . having observed that employees do demonstrate stress at different strata of hierarchy in an organization for different organizational role stress ( ors ) parameters , an effort is made in this work for stress mitigation using neuro - linguistic programming ( nlp ) intervention in counseling employees . nlp describes the fundamental dynamics between mind ( neuro ) and language ( linguistic ) and how their interplay affects our body and behavior ( programming ) . techniques of nlp such as rapport , anchoring , swish , reframing inter - personal communication and persuasion are very useful towards stress management . in this paper , two groups of employees are counseled with and with out nlp intervention . statistical analysis has been carried out to compare the efficacy of nlp intervention and it is observed that nlp intervention do mitigate employee ' s stress . 

a supervised learning approach to automatic synonym identification based on distributional features
distributional similarity has been widely used to capture the semantic relatedness of words in many nlp tasks . however , various parameters such as similarity measures must be hand - tuned to make it work effectively . instead , we propose a novel approach to synonym identification based on supervised learning and distributional features , which correspond to the commonality of individual context types shared by word pairs . considering the integration with pattern - based features , we have built and compared five synonym classifiers . the evaluation experiment has shown a dramatic performance increase of over 120 % on the f - 1 measure basis , compared to the conventional similarity - based classification . on the other hand , the pattern - based features have appeared almost redundant . 

nlp - qa framework based on lstm - rnn
the paper mainly majors in question answering system ( qa ) . the original natural response theory originated from alan turing ' s turing machine theory in 1950 . nowadays , the best way to implement a qa system is the deep learning . this project is based on the seq2seq model theory , i design and implement an automatic question answering system model based on lstm - rnn algorithm . the paper completely describes the framework and design ideas of the entire system . it realizes the following aspects : 1 ) a seq2seq model based on lstm - rnn . 2 ) we design the qa framework adapts to different chat scene . results show that the average of perplexity is 2 . 92 . and model ' s loss is 1 . 07 . to some extent , it reduces the resistance of deep learning in developing . the design and implementation of the framework simplifies the development process and facilitates the user ' s own data set . 

affix - augmented stem - based language model for persian
language modeling is used in many nlp applications like machine translation , pos tagging , speech recognition and information retrieval . it assigns a probability to a sequence of words . this task becomes a challenging problem for high inflectional languages . in this paper we investigate standard statistical language models on the persian as an inflectional language . we propose two variations of morphological language models that rely on a morphological analyzer to manipulate the dataset before modeling . then we discuss shortcoming of these models , and introduce a novel approach that exploits the structure of the language and produces more accurate . experimental results are encouraging especially when we use n - gram models with small training dataset . 

evolution of semantic similarity — a survey
estimating the semantic similarity between text data is one of the challenging and open research problems in the field of natural language processing ( nlp ) . the versatility of natural language makes it difficult to define rule - based methods for determining semantic similarity measures . to address this issue , various semantic similarity methods have been proposed over the years . this survey article traces the evolution of such methods beginning from traditional nlp techniques such as kernel - based methods to the most recent research work on transformer - based models , categorizing them based on their underlying principles as knowledge - based , corpus - based , deep neural network – based methods , and hybrid methods . discussing the strengths and weaknesses of each method , this survey provides a comprehensive view of existing systems in place for new researchers to experiment and develop innovative ideas to address the issue of semantic similarity . 

nlp approach to knowledge search and information credibility analysis
judgement criteria used by people in their daily life are starting to heavily depend on text - based information in web documents . in the gcoe project , we aim to construct a new generation web search engine that can provide information or knowledge in an organized way , helping users to evaluate information credibility . we strongly believe that the improvement of natural language processing is crucial to realize such a search engine . 

geometric and quantum methods for information retrieval
this paper reviews the recent developments in applying geometric and quantum mechanics methods for information retrieval and natural language processing . it discusses the interesting analogies between components of information retrieval and quantum mechanics . it then describes some quantum mechanics phenomena found in the conventional data analysis and in the psychological experiments for word association . it also presents the applications of the concepts and methods in quantum mechanics such as quantum logic and tensor product to document retrieval and meaning of composite words , respectively . the purpose of the paper is to give the state of the art on and to draw attention of the ir community to the geometric and quantum methods and their potential applications in ir and nlp . 

customizing and evaluating a multilingual discourse module
in this paper , we first describe how we have customized our data - driven multilingual discourse module within our text understanding system for different languages and for a particular nlp application by utilizing hierarchically organized discourse kb ' s . then , we report quantitative and qualitative findings from evaluating the system both with and without discourse processing , and discuss how resolving certain kinds of anaphora affects system performance . 

a multi - agent system using in spatial information sharing on web - based gis
web based geospatial information system ( webgis ) has great strengths in the geospatial information community . as the internet becomes more and more accepted in society as a means to disseminate and gather information , the communication of geographic information over the web using web gis will find its position the evolving medium . the traditional gis are only used by the special occupation , and now many users want and are able to use the spatial data of gis on the web . however , there are two major problems in the web gis construct at present , such as the architecture of webgis and sharing of spatial information . this paper report related works on the strategy consideration of the multi - agent system using in sharing spatial information and solving architectures successfully based on the webgis service . the spatial database adjustment , server composition , xml definition document ( iso / tc211 and gml ) are easy to materialize via the former methods , but the creation of spatial data converter is needed to devise within a new solution , which might transcend the concept of general gis or webgis engines . using the peer - to - peer architecture by the multi - agent system , the sharing spatial data is operated directly among the different systems by the requesting from client . the data can be transferred freely in the system . 

tell them apart : distilling technology differences from crowd - scale comparison discussions
developers can use different technologies for many software development tasks in their work . however , when faced with several technologies with comparable functionalities , it is not easy for developers to select the most appropriate one , as comparisons among technologies are time - consuming by trial and error . instead , developers can resort to expert articles , read official documents or ask questions in qa sites for technology comparison , but it is opportunistic to get a comprehensive comparison as online information is often fragmented or contradictory . to overcome these limitations , we propose the difftech system that exploits the crowdsourced discussions from stack overflow , and assists technology comparison with an informative summary of different comparison aspects . we first build a large database of comparable technologies in software engineering by mining tags in stack overflow , and then locate comparative sentences about comparable technologies with natural language processing methods . we further mine prominent comparison aspects by clustering similar comparative sentences and representing each cluster with its keywords . the evaluation demonstrates both the accuracy and usefulness of our model and we implement our approach into a practical website for public use . 

question answering chatbot using deep learning with nlp
in spite of the number of techniques , models and datasets , question answering is still an exacting problem because of the issues in understanding the question and extracting the correct answer . it refers to creating platforms that when given a question in a natural language by humans , can automatically answer it . while many information retrieval chatbots achieve the task , recently , deep learning has earned a lot of attention to question answering due to its capability to learn optimal representation for the given task . this paper aims to build a closed domain , factoid question answering system . we recruit nlp methods of pattern matching and information retrieval to create an answer candidate pool . before scoring similarities between the question and answers , we map them into some feature space . our approach solves this task through distributional representations of the words and sentences wherein encodings store their lexical , semantic , and syntactic aspects . we use a convolutional neural network architecture to rank these candidate answers . our model learns an optimal representation for the input question and answer sentences and a matching function to relate each such pair in a supervised manner from training data . our model does not require any manual feature engineering or language sensitive data ; hence can be extended to various domains . training and testing on trec qa , a question answering dataset , showed very promising metrics for our model . 

utilization - weighted algorithm for spreading factor assignment in lorawan
long range wide area network ( lorawan ) is one of the leading low power wireless networks that can support thousands of internet of things ( iot ) devices . to enhance the scalability of lorawan , this paper proposes the utilizationweighted ( uw ) algorithm , which is the spreading factor management algorithm designed based on the m / d / 1 queue theory . the main concept of this algorithm is channel utilization balancing that helps form groups of nodes assigned with different spreading factors ( sfs ) . the simulations are performed under two scenarios that are similar and various uplink time interval among sfs . the results show that our uw algorithm can outperform the traditional min - airtime method in both scenarios . the packet received rate ( prr ) of the uw algorithm is clearly higher than that of the min - airtime method for all number of nodes and time intervals . especially in the various time interval simulation of the networks of 120 , 600 , and 1 , 200 nodes , the maximum prr improvements occur at 1 , 3 , and 5 times of the minimum time interval between uplinks , t 0ffl , respectively , and are around 34% , 36% , and 35% , respectively . 

automatic acquisition of subcategorization frames from untagged text
this paper describes an implemented program that takes a raw , untagged text corpus as its only input ( no open - class dictionary ) and generates a partial list of verbs occurring in the text and the subcategorization frames ( sfs ) in which they occur . verbs are detected by a novel technique based on the case filter of rouvret and vergnaud ( 1980 ) . the completeness of the output list increases monotonically with the total number of occurrences of each verb in the corpus . false positive rates are one to three percent of observations . five sfs are currently detected and more are planned . ultimately , i expect to provide a large sf dictionary to the nlp community and to train dictionaries for specific corpora . 

automatic event coding framework for spanish political news articles
today , spanish speaking countries face widespread political crisis . these political conflicts are published in a large volume of spanish news articles from spanish agencies . our goal is to create a fully functioning system that parses realtime spanish texts and generates scalable event code . rather than translating spanish text into english text and using english event coders , we aim to create a tool that uses raw spanish text and spanish event coders for better flexibility , coverage , and cost . to accommodate the processing of a large number of spanish articles , we adapt a distributed framework based on apache spark . we highlight how to extend the existing ontology to provide support for the automated coding process for spanish texts . we also present experimental data to provide insight into the data collection process with filtering unrelated articles , scaling the framework , and gathering basic statistics on the dataset . 

fips , a " deep " linguistic multilingual parser
the development of robust " deep " linguistic parsers is known to be a difficult task . few such systems can claim to satisfy the needs of large - scale nlp applications in terms of robustness , efficiency , granularity or precision . adapting such systems to more than one language makes the task even more challenging . this paper describes some of the properties of fips , a multilingual parsing system that has been for a number of years ( and still is ) under development at latl . based on chomsky ' s generative grammar for its grammatical aspects , and on object - oriented ( oo ) sofware engineering techniques for its implementation , fips is designed to efficiently parse the four swiss " national " languages ( german , french , italian and english ) to which we also added spanish and ( more recently ) greek . 

sentiment analysis of lockdown in india during covid - 19 : a case study on twitter
with the rapid increase in the use of the internet , sentiment analysis has become one of the most popular fields of natural language processing ( nlp ) . using sentiment analysis , the implied emotion in the text can be mined effectively for different occasions . people are using social media to receive and communicate different types of information on a massive scale during covid - 19 outburst . mining such content to evaluate people ' s sentiments can play a critical role in making decisions to keep the situation under control . the objective of this study is to mine the sentiments of indian citizens regarding the nationwide lockdown enforced by the indian government to reduce the rate of spreading of coronavirus . in this work , the sentiment analysis of tweets posted by indian citizens has been performed using nlp and machine learning classifiers . from april 5 , 2020 to april 17 , 2020 , a total of 12 741 tweets having the keywords “ indialockdown ” are extracted . data have been extracted from twitter using tweepy api , annotated using textblob and vader lexicons , and preprocessed using the natural language tool kit provided by the python . eight different classifiers have been used to classify the data . the experiment achieved the highest accuracy of 84 . 4 % with linearsvc classifier and unigrams . this study concludes that the majority of indian citizens are supporting the decision of the lockdown implemented by the indian government during corona outburst . 

nlp - based extraction of modificatory provisions semantics
in this paper we illustrare a research based on nlp techniques aimed at automatically annotate modificatory provisions . we propose an approach which pairs deep syntactic parsing with rule - based shallow semantic analysis relying on a fine - grained taxonomy of modificatory provisions . the implemented system is evaluated on a large dataset hand - crafted by legal experts ; the results are discussed and future directions of the research outlined . 

detecting depression from human conversations
depression is the silent killer of the new age . although depression is considered a mental illness it can affect physical health . most often people ignore it until physical or acute mental symptoms start showing up . one way to early detect symptoms of depression is to analyze what people are talking about - their conversations . this is an attractive solution as it is much less noninvasive than conventional medical tests and has the potential to be quite accurate . in this paper , we explored this idea . we used transcripts of conversation data and analyzed the text using ten different machine learning algorithms including lstm , svm , ensemble methods , random forest , and decision tree . as our goal is to find any patterns in spoken language people used to express depression - screams , cheers , mumbles , whines , stutters , murmurs , etc . , we applied the learning algorithms on original transcripts of the conversations . we found that lstm performed best ( about 94 % accuracy ) in finding depressed dialogues followed by na ï ve bayes ( about 69 . 05 % accuracy ) . 

the contribution of ai for spocs in language learning . the example of spoc + 
the lightning speed of the progress in communication and teaching technologies and the emergence of mobile artefacts offers us the opportunity to propose a teaching system of the french language through natural language processing tools in a modular way . being able to integrate the technologies of artificial intelligence adapted to teaching in the continuity of the mirto project , to allow the creation of a platform for 100 % distance learning through small private online course for different levels of learners for learning french as a foreign language . 

why do enterprises adopt natural language processing services ? startups ’ landscape and opportunities in artificial intelligence
the large number of internet users in indonesia contributes to indonesia ' s growth potential in general in the digital economy . this rapid growth urged the government to plan for the industrial 4 . 0 revolution with artificial intelligence ( ai ) technology as its basis . innovations in the field of ai come from many startup companies . despite many benefits , only 40 percent companies in asia - pacific utilized the ai technology in functional areas , such as chatbot services and customer service robots . this gap needs to be addressed considering that more than 87 percent of internet users in indonesia utilize chat services and social media . ai utilization can be implemented by using the services of companies engaged in ai , including startups . however , ai - based startups in indonesia are not yet mapped . furthermore , practical impact of ai implementation needs to be surveyed as knowledge to implement ai in business . this research mapped 68 indonesian startups of ai service providers . in addition , this study evaluates the implementation of ai in the natural language processing ( nlp ) category for companies from the perspective of the service provider . of all 8 mapped nlp service providers , 4 of them are chosen as the research objects . the impacts of its implementation are categorized into eight categories : motivation , profit , interest , change in strategy , competition , satisfaction , trust , and ethics . recommendations are given to companies related to nlp with the most important things which are defining purposes and dare to try . 

a question answering system for unstructured table images
question answering over tables is a very popular semantic parsing task in natural language processing ( nlp ) . however , few existing methods focus on table images , even though there are usually large - scale unstructured tables in practice ( e . g . , table images ) . table parsing from images is nontrivial since it is closely related to not only nlp but also computer vision ( cv ) to parse the tabular structure from an image . in this demo , we present a question answering system for unstructured table images . the proposed system mainly consists of 1 ) a table recognizer to recognize the tabular structure from an image and 2 ) a table parser to generate the answer to a natural language question over the table . in addition , to train the model , we further provide table images and structure annotations for two widely used semantic parsing datasets . specifically , the test set is used for this demo , from where the users can either choose from default questions or enter a new custom question . 

combining the best of two worlds : nlp and ir for intranet search
natural language processing ( nlp ) is becoming much more robust and applicable in realistic applications . one area in which nlp has still not been fully exploited is information retrieval ( ir ) . in particular we are interested in search over intranets and other local web sites . we see dialogue - driven search which is based on a largely automated knowledge extraction process as one of the next big steps . instead of replying with a set of documents for a user query the system would allow the user to navigate through the extracted knowledge base by making use of a simple dialogue manager . here we support this idea with a first task - based evaluation that we conducted on a university intranet . we automatically extracted entities like person names , organizations and locations as well as relations between entities and added visual graphs to the search results whenever a user query could be mapped into this knowledge base . we found that users are willing to interact and use those visual interfaces . we also found that users preferred such a system that guides a user through the result set over a baseline approach . the results represent an important first step towards full nlp - driven intranet search . 

sewordsim : software - specific word similarity database
measuring the similarity of words is important in accurately representing and comparing documents , and thus improves the results of many natural language processing ( nlp ) tasks . the nlp community has proposed various measurements based on wordnet , a lexical database that contains relationships between many pairs of words . recently , a number of techniques have been proposed to address software engineering issues such as code search and fault localization that require understanding natural language documents , and a measure of word similarity could improve their results . however , wordnet only contains information about words senses in general - purpose conversation , which often differ from word senses in a software - engineering context , and the software - specific word similarity resources that have been developed rely on data sources containing only a limited range of words and word uses . in recent work , we have proposed a word similarity resource based on information collected automatically from stackoverflow . we have found that the results of this resource are given scores on a 3 - point likert scale that are over 50 % higher than the results of a resource based on wordnet . in this demo paper , we review our data collection methodology and propose a java api to make the resulting word similarity resource useful in practice . the sewordsim database and related information can be found at http : / / goo . gl / bveas8 . demo video is available at http : / / goo . gl / dynwyb . 

impact of covid - 19 on education using twitter data
with the rapid spread of covid - 19 cases throughout the world , all the nations have been sentenced to partial or complete lockdowns for their safety . throughout this one section which has been almost completely under lockdown was the education sector where students were hardly allowed to attend physical classes around 1 . 5 years now . our aim in this paper is to use the social media tweets in understanding how the people have felt throughout this course of time , what difficulties they have faced , what are the advantages , disadvantages they feel are of this new normal . we will be extracting data from the social media using natural language processing , text mining techniques , sentiment analysis to determine various insights as to what they have faced and can this new normal be actually normal someday . we have used tableau software to highlight the countries where concerned tweets have been posted more . also using word cloud we have tried to map the main concerns . we see that there have mixed concerns related to education from fear of letting the students attend physical mode lectures to concerns as to what side - effects would be caused due to late conduction of exams and reaction to various government laws regarding the same . 

formalization of the arabic grammatical category ( v - a ) using the nooj platform
we present in this paper1 a morpho - syntactical description with broad coverage of lexical entries of standard / classical arabic . this work will be presented in the form of an electronic dictionary named al - erfan , based on operators of the nooj platform and implemented by local grammars in the form of finite state machines ( fst ) . our work is inspired by the mathematical model of z . harris ( the transformations ) and the linguistic theoretical framework " lexicon grammar " developed by maurice gross . the starting point of our approach is the fundamental fact that arabic is based on the merger between the two components : root / pattern . this is opposed to the set - theoretic approach represented by the formula prefix - lemma - suffix , which is specific to the morpho - syntactic system of the latin languages . our approach consists in the fusion of 480 patterns of arabic which operate on 12400 usual roots constituting the base of any morpho - syntactic derivation of this language . the implementation of this process , via the linguistic - computer techniques of the nooj platform , has enabled us to generate more than 120 million entries including all morpholexical categories . these data are all contained in the electronic dictionary al - erfan developed from a database built manually during the past 20 years in different research laboratories specialized in anlp . we will conclude this article by examining the category v - a extracted from our al - erfan electronic dictionary . 

nlp as an essential ingredient of effective osint frameworks
the immense amount of information that can be collected from open sources have become increasingly important to the academic and business communities over the past two decades . also national and global security is becoming more and more reliant on rapidly making sense of and managing the relevant intelligence data . the enormous size and complexity of the gathered data require advanced and unique data storage , management , analysis , and visualization technologies . the technologies and applications currently adopted typically involve elements like text mining , natural language processing , and stochastic - based algorithms . natural language processing , in particular , has been a prominent research topic for many years now . we have conceptualized an analysis framework with a strong focus on various techniques of natural language processing to aggregate , manipulate , and analyze intelligence information . 

comparative study of twitter sentiment on covid - 19 tweets
recently , the number of tweets on covid - 19 are increasing at an unprecedented rate by including positive , negative and neutral tweets . this diversified nature of tweets has attracted the researchers to perform sentiment analysis and analyze the varied emotions of a large public towards covid - 19 . the traditional sentiment analysis techniques will only find out the polarity and classify it as either positive , negative or neutral tweets . as an advanced step , the proposed research work attempts to find the sentiment of tweets using logistic regression sentiment analysis , vader sentiment analysis and bert sentiment analysis . the proposed analysis methods are more sensitive to sentiment expressions in social media contexts , while it can be generalized on the basis of the domain . even though 3 different algorithms are implemented , all the preprocessing and further steps excluding the sentiment analysis algorithm will remain identical . the identical processing steps will help to compare the proposed three different sentiment analysis algorithms . furthermore , there are many useful applications with this proposed analysis , as this work obtains a public opinion for the government officials or even for the health officials and help them to work on the basis of the obtained results . 

english - japanese example - based machine translation using abstract linguistic representations
this presentation describes an example - based english - japanese machine translation system in which an abstract linguistic representation layer is used to extract and store bilingual translation knowledge , transfer patterns between languages , and generate output strings . abstraction permits structural neutralizations that facilitate learning of translation examples across languages with radically different surface structure characteristics , and allows mt development to proceed within a largely language - independent nlp architecture . comparative evaluation indicates that after training in a domain the english - japanese system is statistically indistinguishable from a non - customized commercially available mt system in the same domain . 

dual - chain unequal - state crf for chinese new word detection and pos tagging
in chinese language processing , new words are particularly problematic . it is impossible to get a complete dictionary as new words can always be created . we proposed a unified dual - chain unequal - state crf model to detect new words together with their part - of - speech in chinese texts regardless of the word types such as compound words , abbreviation , person names , etc . the dual - chain unequal - state crf model has two state chains with unequal number of states . the unequal state chains could model flexible hierarchical lexical information for both chinese new word detection and pos tagging , and also integrate complex context features like the global information . the experimental results show that the proposed method is capable of detecting even low frequency new words and their parts - of - speech synchronously with satisfactory results . 

pure : a dataset of public requirements documents
this paper presents pure ( public requirements dataset ) , a dataset of 79 publicly available natural language requirements documents collected from the web . the dataset includes 34 , 268 sentences and can be used for natural language processing tasks that are typical in requirements engineering , such as model synthesis , abstraction identification and document structure assessment . it can be further annotated to work as a benchmark for other tasks , such as ambiguity detection , requirements categorisation and identification of equivalent re - quirements . in the paper , we present the dataset and we compare its language with generic english texts , showing the peculiarities of the requirements jargon , made of a restricted vocabulary of domain - specific acronyms and words , and long sentences . we also present the common xml format to which we have manually ported a subset of the documents , with the goal of facilitating replication of nlp experiments . 

the clavius on the web project : digitization , annotation and visualization of early modern manuscripts
this paper describes the full procedure adopted in the context of the clavius on the web project , which aims to help web users to appraise the importance of specific manuscripts by going beyond their digital reproduction . the proposed approach is based on the multilayered explication of linguistic , lexical and semantic data representing the innermost nature of the analyzed manuscripts . the final purpose of the project is to gather and display the results of the three layers of analysis through interactive visualization techniques and export them as linked data . all the analyses rely on the xml / tei encoding of the text , followed by a cts - based tokenization . as a working example for this paper , the analysis of a portion of a manuscript provided by historical archives of the pontifical gregorian university will be illustrated . the text is a letter written in latin and sent by botvitus nericius to christophorus clavius in 1598 from madrid . 

fusing contextual word embeddings for concreteness estimation
natural language processing ( nlp ) has a long history , and recent research has focused in particular on encoding meaning in a computable way . word embeddings have been used for this specific purpose , allowing language tasks to be treated as mathematical problems . real valued vectors have been generated or employed as word representations for several nlp tasks . in this work , different types of pre - trained word embeddings are fused together to estimate word concreteness . in the evaluation of this task , we have taken into account how much contextual information can affect final results , and also how to properly fuse different word embeddings in order to maximize their performance . the best architecture in our study surpasses the winning solution in the evalita 2020 competition for the word concreteness task . 

world knowledge in broad - coverage information filtering
no abstract available . 

a robust unsupervised speaker clustering of speech utterances
this paper aims at developing and investigating efficient , robust and unsupervised algorithm for speaker clustering . each utterance is modeled as a single gaussian model distribution . a novel distance metric is proposed in this paper for the purpose of determining stopping criteria . the advantage of the proposed method is that it achieves comparable performance without requiring an adjusting threshold term . in this paper , we adopt the framework of agglomerative hierarchical clustering ( ahc ) with the merging criterion using kullback - leibler ( kl ) distance . the proposed stopping criterion can ensure a right number of speaker clusters . the efficiency of the proposed algorithm is demonstrated with various experiments on data from nist and hub5 , respectively . 

building information extraction system based on computing domain ontology
in this paper , we present an information extraction ( ie ) system , which is built from unstructured text based on computing domain ontology . the ie system comprises four sequential processing steps : preprocessing , topic identifier , building domain specific ontology and extracting information from text corpus . the first two steps perform generic natural language processing ( nlp ) and machine learning tasks , while the last two phases are application - specific and require a thorough understanding of the application domain . furthermore , the paper focuses on evaluating the ie iesystem by selected methods . one of these methods that we introduced here , is comparative . comparative evaluation performed in this study use of key exchange algorithm with the same corpus to contrast results . results generated by such experiments show that this ie system outperforms key exchange algorithm , respectably . 

word sense disambiguation using multi - engine collaborative boostrapping
in this paper we proposed a new word sense disambiguation method , called multi - engine collaborative bootstrapping ( mcb ) that combines different types of corpora and also uses two languages to bootstrapping . mcb contains the bilingual bootstrapping as its kernel algorithm that leads to incremental knowledge acquisition . em model is performed to train parameters in base learner . feature translation model is improved by semantic correlation estimation . in addition we use multi - engine to produce qualified starting seeds from parallel corpora and monolingual corpora . those seeds that are generated through unsupervised machine learning approaches can also ensure bootstrapping effectiveness in contrast with manual selected seeds in spite of their different selection mechanisms . experimental results prove the effectiveness of mcb . some factors including feature space and starting seed number are concerned in our experiments because em algorithm is sensible to starting values . limitation of resources is also concerned . 

nlp - based entity behavior analytics for malware detection
in this research , we formulate malware detection as a large - scale data - mining problem within security information and event management ( siem ) systems . we hypothesize that behavioral analysis of executable / process activities , such as file reads / writes , process creations , network connections , or registry modifications , enables the detection of advanced stealthy malware . to achieve this detection , we model processes behaviors as a set of directed acyclic graph streams and identify outliers in the set of graph streams . we enable this detection by conversion of the behavioral graph streams into documents , embedding using state - of - the - art natural language processing model , and eventually performing novel outlier detection on the high dimensional vector representation of the documents . we evaluate our approach in a real - world setting , next to the siem system of a large - scale international enterprise ( over 3tb of edr logs ) . the proposed method has shown the capability to detect previously unknown threats . 

building annotation rules for text description of endoscopies in romanian - an nlp - free approach
this paper presents an approach for building annotation rules for texts containing natural language descriptions of the endoscopies , in romanian . the annotation rule extraction relies on running the apriori algorithm over some previously annotated texts . it does not employ any natural language processing tools . we show by experiments that , for the relatively small vocabulary employed by those descriptions , the solution proves to offer quite good performance . 

claim stance classification optimized by data augment
as online fora increasingly become the main media for argument and debate , the automatic processing of such data is rapidly becoming more and more important . stance classification , which aims to classify the stance of the claims towards the given topic , can be applied in many application areas such as users ' feelings about services and products . we propose a ensemble model for stance classification with data augment for small sample scenarios , multi - sample dropout for low training speed scenarios , focal loss for imbalance sample scenarios , pseudo labels for self - supervised training scenarios , adversarial training for low robustness scenarios , and all the above can be used in normal scenarios . besides , the ensemble model is composed of task - specific roberta and macbert , which can make more reasonable predictions . we used dataset from nlpcc to validate the model and it worked well . 

towards contradiction detection in german : a translation - driven approach
with the recent advancements in machine learning based natural language processing ( nlp ) , language dependency has always been a limiting factor for a majority of nlp applications . typically , models are trained for the english language due to the availability of very large labeled and unlabeled datasets , which also allow to fine tune models for that language . contradiction detection is one such problem that has found many practical applications in nlp and up to this point has only been studied in the context of english language . the scope of this paper is to examine a set of baseline methods for the contradiction detection task on german text . for this purpose , the well - known stanford natural language inference ( snli ) data set ( 110 , 000 sentence pairs ) is machine - translated from english to german . we train and evaluate four classifiers on both the original and the translated data , using state - of - the - art textual data representations . our main contribution is the first large - scale assessment for this problem in german , and a validation of machine translation as a data generation method . we also present a novel approach to learn sentence embeddings by exploiting the hidden states of an encoder - decoder sequence - to - sequence rnn trained for autoencoding or translation . 

opioid2fhir : a system for extracting fhir - compatible opioid prescriptions from clinical text
background : the opioid crisis is a national public health emergency in us . especially , prescription opioids contributed significantly to drug overdose deaths . to improve the surveillance of prescription opioid overdose , it is critical to accurately collect prescription opioid information and calculate morphine milligram equivalents ( mmes ) . however , plenty of detailed information is only contained in the free text sig component of electronic health record ( ehr ) prescriptions and need to be extracted first . moreover , it is also indispensable to normalize opioid information extracted from multiple heath care facilities to clinical data standards such as fast healthcare interoperability resources ( fhir ) for efficient clinical decision support . however , few efforts are spent in this direction at present . methods : in this study , we designed and implemented a system that can automatically extract opioid information from free text in sig and map them to fhir . the system , named as opioid2fhir , applied multiple natural language processing ( nlp ) techniques for opioid information extraction and normalization . in order to reduce manual efforts , a general - purpose medication ie model was first leveraged . based on 1 , 000 opioid prescription records randomly selected from mimiciii , post - processing rules were designed to adapt the ie model to opioid medications . concept normalization models were also built to transform and map the extracted medication elements to fine - granular standard concepts in fhir . the system was evaluated on another 1 , 000 opioid prescription records in mimiciii . results : opioid2fhir obtained an f - measure of 0 . 963 for medication information extraction and an accuracy of 0 . 987 for medical concept normalization . conclusions : a clinical nlp application to ehr opioid scripts would fill a current gap in available batch script processing tools and would greatly enhance individual prescription processing limitations of prescription drug monitoring programs and clinical mme calculators . 

a hybrid cnn - lstm : a deep learning approach for consumer sentiment analysis using qualitative user - generated contents
with the fastest growth of information and communication technology ( ict ) , the availability of web content on social media platforms is increasing day by day . sentiment analysis from online reviews drawing researchers ’ attention from various organizations such as academics , government , and private industries . sentiment analysis has been a hot research topic in machine learning ( ml ) and natural language processing ( nlp ) . currently , deep learning ( dl ) techniques are implemented in sentiment analysis to get excellent results . this study proposed a hybrid convolutional neural network - long short - term memory ( cnn - lstm ) model for sentiment analysis . our proposed model is being applied with dropout , max pooling , and batch normalization to get results . experimental analysis carried out on airlinequality and twitter airline sentiment datasets . we employed the keras word embedding approach , which converts texts into vectors of numeric values , where similar words have small vector distances between them . we calculated various parameters , such as accuracy , precision , recall , and f1 - measure , to measure the model ’ s performance . these parameters for the proposed model are better than the classical ml models in sentiment analysis . our results analysis demonstrates that the proposed model outperforms with 91 . 3 % accuracy in sentiment analysis . 

shallow syntactic analysis of chinese texts
the paper considers a problem of automatic processing of natural language chinese texts . one of the pressing tasks in this area is automatic fact acquisition from text documents by a query because existing automatic translators are useless at this task . the goal of the work is direct extraction of facts from the text in the original language without its translation . the suggested approach consists of syntactic analysis of sentences with subsequent matching of parts of speech found with a formalized query in the form of subject - object - predicate . a distinctive feature of the proposed algorithm of syntactic analysis is the absence of phase of segmentation into words for the sequence of hieroglyphs that make up the sentences . the bottleneck at this task is a dictionary because the correct interpretation of a phrase can be impossible when a word is absent in the dictionary . to eliminate this problem , we propose to identify a sentence model by function words while limitedness of the dictionary could be compensated by an automatic building of a subject area thesaurus and a dictionary of common words using statistical processing of a document corpus . the suggested approach was approved on a small topic area with a limited dictionary where it demonstrates its robustness . the analysis of temporal characteristics of the developed algorithm was carried out as well . as the proposed algorithm uses a naive inference , the parsing speed at real tasks could be unacceptable low , and this should become a subject for further research . 

designing whisper : a semiautonomous system for database encoding
whisper is a measurably effective user interface for extracting formatted textural data from unformatted text . quantifiable objectives for user productivity , data accuracy and learnability were achieved by a design that integrates direct - manipulation techniques with natural language processing ( nlp ) capabilities . it provides a consistent and extensible user interface that will facilitate the gradual insertion of more powerful nlp as it becomes available , without disrupting established encoding productivity . the authors describe the application problem , the design approach and the empirical evaluation of whisper . < > view less

media file descriptor using deep learning
media files like audios and videos are very useful sources of information , entertainment etc . humans watch / listen to media files almost everyday for some purpose or another . there are lots of videos , audio and images on every content on the internet and other platforms . sometimes , just the title can be misleading and humans end up utilizing time on irrelevant media files and often when they realize this isn ' t the relevant media file they have already wasted some time . our media file descriptor is a generalized platform used to provide a summary of any media file that is audio , video or image quickly so that the user has optimum information of the media file beforehand by reading the summary and can decide if the file is relevant to what they are looking for . if not , the user doesn ' t end up wasting time on the wrong media file . 

the problem of ontology alignment on the web : a first report
this paper presents a general architecture and four algorithms that use natural language processing for automatic ontology matching . the proposed approach is purely instance based , i . e . , only the instance documents associated with the nodes of ontologies are taken into account . the four algorithms have been evaluated using real world test data , taken from the google and looksmart online directories . the results show that nlp techniques applied to instance documents help the system achieve higher performance . 

senet for weakly - supervised relation extraction
sentence relation extraction aims to extract relational facts from sentences , which is an important task in natural language processing ( nlp ) field . a common pipeline in relation extraction ( re ) task is to first encode a sentence into latent representation , and then classify it utilizing a combination of attention mechanism , fully connected ( fc ) layers and softmax layer . therefore a good sentence encoder for relation extraction is essential . previous models use shallow convolution neural networks or recurrent neural networks like bi - directional long short - term memory ( lstm ) to extract features of a sentence . to enhance the representation power of network , in this paper we investigate the capability of squeeze - and excitation ( se ) module on the task of distantly supervised noisy relation extraction . se module could learn the importance of each filter , enhance important filters and restrain less important ones , therefore prevent overfitting . to enhance position information , we also demonstrated the power of a new scheme of pooling named double pooling . experimental results demonstrate the effectiveness of our method compared with baseline models . 

extracting uml class diagrams from software requirements in thai using nlp
in software development , requirements , normally written in natural language , are documents that specify what users want in software products . software developers then analyze these requirements to create domain models represented in uml diagrams in an attempt to comprehend what users need in the software products . these domain models are usually converted into design models and finally carried over into classes in source code . thus , domain models have an impact on the final software products . however , creating correct domain models can be difficult when software developers are not skilled . moreover , even for skilled developers , when requirements are large , wading through all requirements to create domain models can take times and might result in errors . therefore , researchers have studied various approaches to apply natural language processing techniques to transform requirements written in natural language into uml diagrams . those researches focus on requirements written in english . this paper proposes an approach to process requirements written in thai to extract uml class diagrams using natural language processing techniques . the uml class diagram extraction is based on transformation rules that identify classes and attributes from requirements . the results are evaluated with recall and precision using truth values created by humans . future works include identifying operations and relationships from requirements to complete class diagram extraction . our research should benefit thai software developers by reducing time in requirement analysis and also helping novice software developers to create correct domain models represented in uml class diagram . 

concept vector extraction from wikipedia category network
the availability of machine readable taxonomy has been demonstrated by various applications such as document classification and information retrieval . one of the main topics of automated taxonomy extraction research is web mining based statistical nlp and a significant number of researches have been conducted . however , existing works on automatic dictionary building have accuracy problems due to the technical limitation of statistical nlp ( natural language processing ) and noise data on the www . to solve these problems , in this work , we focus on mining wikipedia , a large scale web encyclopedia . wikipedia has high - quality and huge - scale articles and a category system because many users in the world have edited and refined these articles and category system daily . using wikipedia , the decrease of accuracy deriving from nlp can be avoided . however , affiliation relations cannot be extracted by simply descending the category system automatically since the category system in wikipedia is not in a tree structure but a network structure . we propose concept vectorization methods which are applicable to the category network structured in wikipedia . 

automatic identification of chinese multiword chunk based on crf
identifying the chinese multiword chunk automatically is a newly emerged technology in the nlp field . as anew strategy , it can effectively improve the performance of the syntactic parsing . the work follows the standard description system of chinese multiword chunk and has constructed two tag sequence models based on crf model , which are named as ” the syntactic mark tagging list model ” and ” the sequence mark tagging list model ” respectively . the corpus used in the training process is called as ” the chinese multiword chunk bank ” , which is provided by tsinghua university . in the experiments , by selecting appropriate the features and introducing some important rules , the better results are achieved and this system for identifying the chinese multiword chunk can run well in a restricted area . thus , it provides a bridge between syntax and semantic content . 

rule - based extraction of experimental evidence in the biomedical domain : the kdd cup 2002 ( task 1 ) 
below we describe the winning system that we built for the kdd cup 2002 task 1 competition . our system is a rule - based information extraction ( ie ) system . it combines pattern matching , natural language processing ( nlp ) tools , semantic constraints based on the domain and the specific task , and a post - processing stage for making the final curation decision based on the various evidence ( positive and negative ) found within the document . development and implementation were made using the dial ie language and the clearlab development environment . the results achieved were significantly superior than those achieved using categorization approaches . 

classification of book genres using book cover and title
the main purpose of this research was to determine if the cover and title of a book can be used to predict its genre . the main motivation behind this was that since the cover or title individually may not always be suggestive of the type of book , using both of them together may be more indicative of the genre of the book . the idea was to extract features from the cover and title and combine them to predict the genre of a book . a logistic regression model was used for training and prediction . an accuracy of 87 . 2 % was obtained when we used the combined features . we got an accuracy of 67 . 04 % using only cover features and an accuracy of 86 . 2 % using only the title features . this shows that although using both the title and cover features gives better results , the title seems to more expressive of the genre of the book . 

on - line reentry trajectory optimization of hypersonic vehicle based on improved gauss pseudospectral method
the improved gauss pseudospectral method ( igpm ) with the linear final state constraint is proposed to solve the bolza problem . the bolza problem is disrcetized into the nonlinear programming problem ( nlp ) by the approximations of the state and the costate at the legendre - gauss ( lg ) points and the boundary points . the differential of the state can be expressed as the linear combination of the states at the discrete points approximately . therefore , the dynamic equations can be transformed into the algebraic equations and the cost is also expressed as the function of the state and the control . the costate mapping theorem is derived to compute the costate directly . the methods to compute the initial costate and the boundary controls are given for completing the igpm . the nlp is solved by the sequential quadratic programming method . it is easy to check the optimality of the solution . the proposed method is utilized to solve the the reentry trajectory optimization problem of the hypersonic vehicle . the results shown that the the computer time for solving the problem is about 3 seconds . therefore , this method can be used to generate the optimal trajectory of the hypersonic vehicle on line . 

virtual cardiologist — a conversational system for medical diagnosis
in this paper we describe a very preliminary conversational system focused on the automated diagnosis of heart conditions . the system uses a relational ngram model to extract meaning from user queries and answers , and maintains a belief network consisting of 48 different cardiac conditions , symptoms , and other internal system parameters . the end result is a system that can process an initial user query , ask pertinent questions , and to deduce reasonable medical conclusions . although several example output conversations are shown , the medical accuracy of the diagnosis is not the focus of this paper . instead this paper focuses on the system level infrastructure and methodology for building a medical conversation system . 

graphrep : boosting text mining , nlp and information retrieval with graphs
graphs have been widely used as modeling tools in natural language processing ( nlp ) , text mining ( tm ) and information retrieval ( ir ) . traditionally , the unigram bag - of - words representation is applied ; that way , a document is represented as a multiset of its terms , disregarding dependencies between the terms . although several variants and extensions of this modeling approach have been proposed , the main weakness comes from the underlying term independence assumption ; the order of the terms within a document is completely disregarded and any relationship between terms is not taken into account in the final task . to deal with this problem , the research community has explored various representations , and to this direction , graphs constitute a well - developed model for text representation . the goal of this tutorial is to offer a comprehensive presentation of recent methods that rely on graph - based text representations to deal with various tasks in text mining , nlp and ir . 

an interactive system to automatically generate video summaries and perform subtitles synchronization for persons with hearing loss
according to world health organization ( who ) , approximately 328 million of adults and 32 million of children present hearing loss in the world . likewise , the number of people with such impairment increased from 42 million in 1985 to about 360 million in 2011 . however , the most of multimedia and web contents , whether they are educational or leisure , are not accessible for persons with hearing loss . in this line , this paper presents an interactive system aimed at automatically generating video summaries and performing subtitles synchronization for persons with hearing loss . our proposal relies on an educational platform ( moodle ) and natural language processing ( nlp ) to provide an environment fully configurable for these persons . the module that generates the video summaries uses techniques such as latent semantic analysis ( lsa ) and latent dirichlet allocation ( lda ) , whereas the synchronization module is based on forced alignment between audio streams and text . with the aim of validating our environment , we have tested our approach on 15 videos , obtaining a score of 80 % for three criteria related to the summary content : understandability , concordance , and context appropriateness . 

integrating linguistic patterns and term - entity associations in chinese person description extraction
person description extraction is an important task in biography generation , question answering and summarization . most previous extraction approaches select descriptive passages depending on sentence structure and / or word co - occurrence information . in this paper , we focus on chinese person description extraction verification by measuring the associations between the recognized person entities and the surrounding terms , called term - entity associations . the associations are derived from both the semantic knowledge provided in a chinese well - known thesaurus hownet and the term distributional information gathered from the news corpus . relying on term - entity associations , the ineligible extracted descriptions could be filtered out so that the higher precision could be achieved in turn . as far as we know , no work on chinese person description extraction has been reported in the literature . 

using natural language to improve the generation of model transformation in software design
among the present crucial issues in uml modeling , one of the most common is about the fusion of similar models coming from various sources . several similar models are created in software engineering and it is of primary interest to compare them and , when possible , to craft a general model including a specific one , or just identify models that are in fact equivalent . most present approaches are based on model structure comparison and alignment on strings for attributes and classe names . this contribution evaluates the added value of several combined nlp techniques based on lexical networks , pos tagging , and dependency rules application , and how they might improve the fusion of models . topics : use of nlp techniques in practical applications . 

understanding the applications of natural language processing on covid - 19 data
in the digital age , the internet has enabled the circulation of ideas and information and , in turn , has increased awareness among people . however , this does not come with its drawbacks . with the proliferation of online platforms , hoaxers can easily lure people towards their propagandist views or false news . the need to root out such false information and hate speech during this covid - 19 pandemic has never been more essential . the following study presents a survey of various papers that attempt to tackle similar problem statements with fake news , sentiment classification , and topic extraction . the paper focuses on how existing quality research can help improve the current state of research on covid - 19 related datasets by guiding researchers towards valuable procedures to help governmental authorities combat the rise in the spread of false news and malicious and hate comments . 

a definition of " version " for text production data and natural language document drafts
research on differencing and versioning in computer science , writing research , and scholarly editing are all concerned with the evolution of texts and documents through various drafts , versions , and changes between different stages . while the different disciplines all have their own perspectives , on closer inspection , there is a large overlap of concerns . however , there are currently no points of contacts between these disciplines . in this vision paper , we propose a working definition of " version " to reconcile the different views and open the way to closer collaboration . in particular , we propose to model the semantics of changes in what seems to be unstructured text as differences between syntactic parse trees . 

bayesian network , a model for nlp ? 
the nlp systems often have low performances because they rely on unreliable and heterogeneous knowledge . we show on the task of non - anaphoric it identification how to overcome these handicaps with the bayesian network ( bn ) formalism . the first results are very encouraging compared with the state - of - the - art systems . 

unsupervised alignment for segmental - based language understanding
recent years ' most efficient approaches for language understanding are statistical . these approaches benefit from a segmental semantic annotation of corpora . to reduce the production cost of such corpora , this paper proposes a method that is able to match first identified concepts with word sequences in an unsupervised way . this method based on automatic alignment is used by an understanding system based on conditional random fields and is evaluated on a spoken dialogue task using either manual or automatic transcripts . 

mining the demographics of political sentiment from twitter using learning from label proportions
opinion mining and demographic attribute inference have many applications in social science . in this paper , we propose models to infer daily joint probabilities of multiple latent attributes from twitter data , such as political sentiment and demographic attributes . since it is costly and time - consuming to annotate data for traditional supervised classification , we instead propose scalable learning from label proportions ( llp ) models for demographic and opinion inference using u . s . census , national and state political polls , and cook partisan voting index as population level data . in llp classification settings , the training data is divided into a set of unlabeled bags , where only the label distribution of each bag is known , removing the requirement of instance - level annotations . our proposed llp model , weighted label regularization ( wlr ) , provides a scalable generalization of prior work on label regularization to support weights for samples inside bags , which is applicable in this setting where bags are arranged hierarchically ( e . g . , county - level bags are nested inside of state - level bags ) . we apply our model to twitter data collected in the year leading up to the 2016 u . s . presidential election , producing estimates of the relationships among political sentiment and demographics over time and place . we find that our approach closely tracks traditional polling data stratified by demographic category , resulting in error reductions of 28 - 44 % over baseline approaches . we also provide descriptive evaluations showing how the model may be used to estimate interactions among many variables and to identify linguistic temporal variation , capabilities which are typically not feasible using traditional polling methods . 

a case study on experimental - data validation for natural language processing
text data randomly extracted from a particular corpus are usually employed by nlp ( natural language processing ) systems as experimental data . however , it is hard to determine whether the experimental data is appropriate without a reasonable validation process . we in this paper describe a data validation approach for a nlp - based e - learning system we have built before . 

fuzzy clustering and genetic algorithm for clinical pratice guideline execution engines
the clinical practice guideline ( cpg ) is used to support the decision system using general practitioner . the patient symptoms are recommended and followed as per cpg . medical knowledge is framed on the clinical decision making and clinical practice . the nlp is important data source for clinical decision . the nlp system is included in part of speech tagging . fuzzy c means are used to group the symptoms . the fuzzy membership function is used to find the fuzzy c means clustering approach to determine the centroid correctly . owl language is used to map ( onto map ) symptoms and disease in the clinical evaluation . the healthcare , semantic web and ontology ' s for the patients clinical practice is based on cpg . ontology is potentially integrated with health care . ontology ' s are accessed through the user interface and frame work . the gaps between the users requirement is suggested . genetic algorithms ( ga ) via clustering is used for posting the optimization issues . ga provides optimal results when used in complex issues . genetic algorithm is optimized for stomach disease prediction as well . 

vers l ' utilisation des m é thodes formelles pour le d é veloppement de linguiciels
formal methods have ' nt been applied enough in the development process of lingware although their advantages have been proved in many other domains . in this framework , we have investigated some applications dealing with different processing levels ( lexical analyses , morphology , syntax , semantic and pragmatic ) . these investigations has mainly led to the following observations . first of all , we have noticed a lack of use of methodologies that cover all the life cycle of a software development . the formal specification has not been used in the first development phases . in addition , we have noticed the lack of formal validation and consequently the insufficient guarantee of the developed software results . moreover , there has been no appeal to rigorous methods of integration to solve the dichotomy of data and processing problem . however , the use of the formal aspect in the natural language processing ( nlp ) has generally been limited to describing the natural language knowledge ( i . e . , grammars ) and specifying the treatments using algorithmic languages . few are those who have used a high level specification language . this paper focuses on the contributions of formal methods in developing natural language software starting from an experimentation carried out on a real application and which consists in specifying and validating the system cortexa ( correction orthographique des textes arabes ) using the vdm formal method . first of all , we review the advantages of formal methods in the general software development process . then , we present the experimentation and the obtained results . after that , we place the formal methods advantages in the context of nlp . finally , we give some methodological criteria that allow the choice of an appropriate formal method . 

engkoo : mining the web for language learning
this paper presents engkoo , a system for exploring and learning language . it is built primarily by mining translation knowledge from billions of web pages - using the internet to catch language in motion . currently engkoo is built for chinese users who are learning english ; however the technology itself is language independent and can be extended in the future . at a system level , engkoo is an application platform that supports a multitude of nlp technologies such as cross language retrieval , alignment , sentence classification , and statistical machine translation . the data set that supports this system is primarily built from mining a massive set of bilingual terms and sentences from across the web . specifically , web pages that contain both chinese and english are discovered and analyzed for parallelism , extracted and formulated into clear term definitions and sample sentences . this approach allows us to build perhaps the world ' s largest lexicon linking both chinese and english together - at the same time covering the most up - to - date terms as captured by the net . 

virtual manager for medical practitioners
the rise of artificial intelligence ( ai ) and natural language processing ( nlp ) gave a perception of the use of computers in the healthcare industry . the introduction of voice - assisted chatbots and digital data storage aided health organizations as well as the dependents . currently , the interactions between doctors and patients are undocumented , and the prescription , in some cases , creates chaos for understanding and long - term maintenance . the project introduces an interactive virtual assistant system for searching , analyzing , and recording clinical data and laboratory data , maintaining electronic health records . it uses a named entity extraction ( ner ) model for classifying the doctor - patient conversation , i . e . , symptoms , detected disorder , prescription of drugs , and stores them in a structured database . the system poses an ability to understand and respond to limited user queries and imitate human discourse to stimulate a conversation that resembles conversing with a real person . the response includes the recorded history of patients , overcoming the issue of file management , supporting clinical decisions . generates automated prescriptions eliminating the problems of handwritten prescriptions , and provides live news updates with the web scraper module . moreover , it plays a vital role in reminding patients about their deadlines for vaccinations and follow - ups . 

automatic learning for semantic collocation
the real difficulty in development of practical nlp systems comes from the fact that we do not have effective means for gathering " knowledge " . in this paper , we propose an algorithm which acquires automatically knowledge of semantic collocations among " words " from sample corpora . the algorithm proposed in this paper tries to discover semantic collocations which will be useful for disambiguating structurally ambiguous sentences , by a statistical approach . the algorithm requires a corpus and minimum linguistic knowledge ( parts - of - speech of words , simple inflection rules , and a small number of general syntactic rules) . we conducted two experiments of applying the algorithm to diferent corpora to extract different types of semantic collocations . though there are some unsolved problems , the results showed the effectiveness of the proposed algorithm . 

context - aware iot device functionality extraction from specifications for ensuring consumer security
internet of thing ( iot ) devices are being widely used in smart homes and organizations . an iot device has some intended purposes , but may also have hidden functionalities . typically , the device is installed in a home or an organization and the network traffic associated with the device is captured and analyzed to infer high - level functionality to the extent possible . however , such analysis is dynamic in nature , and requires the installation of the device and access to network data which is often hard to get for privacy and confidentiality reasons . we propose an alternative static approach which can infer the functionality of a device from vendor materials using natural language processing ( nlp ) techniques . information about iot device functionality can be used in various applications , one of which is ensuring security in a smart home . we demonstrate how security policies associated with device functionality in a smart home can be formally represented using the nist next generation access control ( ngac ) model and automatically analyzed using alloy , which is a formal verification tool . this will provide assurance to the consumer that these devices will be compliant to the home or organizational policy even before they have been purchased . 

part - of - speech tagger based on maximum entropy model
the maximum entropy ( me ) conditional models don ' t force to adhere to the independence assumption such as in hidden markov generative models , and thus the me - based part - of - speech ( pos ) tagger can depend on arbitrary , non - independent features , which are benefit to the pos tagging , without accounting for the distribution of those dependencies . since me models are able to flexibly utilize a wide variety of features , the sparse problem of training data is efficiently solved . experiments show that the pos tagging error rate is reduced by 54 . 25 % in close test and 40 . 56 % in open test over the hidden - markov - model - based baseline , and synchronously an accuracy of 98 . 01 % in close test and 95 . 56 % in open test are obtained . 

word level language identification of code mixing text in social media using nlp
understanding social media contents has been a primary research topic since the dawn of social networking . especially , contextual understanding of the noisy text , which is characterized by a high percentage of spelling mistakes with creative spelling , phonetic typing , wordplay , abbreviations , and meta tags . thus , the data processing demands a more complex system than traditional natural language processors . also people easily mixing two or more languages together to express their thoughts in social media context . so automatic language identification at word level become as necessary part for analyzing the noisy content in social media . it would help with the automated analysis of content generated on social media . this study uses tamil - english code - mixed data from popular social media posts and comments and provided word level language tags using natural language processing ( nlp ) and modern machine learning ( ml ) technologies . the methodology used for this system is a novel approach implemented as machine learning classifier based on features such as tamil unicode characters in roman scripts , dictionaries , double consonant , and term frequency . different machine learning classifiers such as naive bayes , logistic regression , support vector machines ( svm ) , decision trees and random forest used in training and testing . among that the highest accuracy of 89 . 46 % was obtained in svm classifier . 

the study on quranic surahs ' topic sameness using nlp techniques
study of the structured - ness of quranic surahs has attracted the attention of some researchers in recent years . one of the theories herein is the theory of topic sameness which acknowledges that the inner elements of surahs have tight relationship with each other and that each surah of quran has formed on a single core topic . in this paper , we intend to study the topic sameness in quranic surahs using natural language processing methods . in this regard , based on the two methods of word2vec and roots ' accompaniment in verses , the similarity of quranic roots is calculated . then , the amount of similarity between surahs ' title and the concepts within the surahs is studied . afterwards , the amount of similarity of the concepts within chapters to each other is calculated and compared with the random mode . the results show that the choice of the surah ' s title is based on rational logic , and that could not have been done by the ordinary public of the early islamic era . in addition , the surahs hold the inner coherence between the concepts so that they have formed on a single topic or a few topics tightly related to each other . 

a survey on thesauri application in automatic natural language processing
this paper is devoted to investigate efficiency of thesauri use in popular natural language processing ( nlp ) fields : information retrieval and analysis of texts and subject areas . a thesaurus is a natural language resource that models a subject area and can reflect human expert ' s knowledge in many nlp tasks . the main target of this survey is to determine how much thesauri affect processing quality and where they can provide better performance . we describe studies that use different types of thesauri , discuss contribution of the thesaurus into achieved results , and propose directions for future research in the thesaurus field . 

ontosem and simple : two multi - lingual world views
in this paper we compare programs of work that aim to develop broad coverage cross - linguistic resources for nlp : ontological semantics ( ontosem ) and simple . the approaches taken in these projects differ in three notable respects : the use of an ontology versus a word net as the semantic substrate ; the development of knowledge resources inside of as opposed to outside of a processing environment ; and the development of lexicons for multiple languages based on a single core lexicon or without such a core ( i . e . , in parallel fashion ) . in large part , these differences derive from project - driven , real - world requirements and available resources - - a reflection of their being practical rather than theoretical projects . however , that being said , we will suggest certain preferences regarding the content and development of nlp resources with a view toward both short - and long - term , high - level language processing goals . 

dimensionality reduction for text using lle
dimensionality reduction is a necessary preprocessing step in many fields of information processing such as information retrieval , pattern recognition and data compression . its goal is to discover the representative or the discriminative information residing in raw data . locally linear embedding ( lle) , one of effective manifold learning algorithms , addresses this problem by computing low - dimensional , neighborhood preserving embeddings of high - dimensional data . the embedding is derived from the symmetries for locally linear reconstructions . and the computation of this embedding is related to an eigen - problem in the implement . since lle was proposed , it has been being applied to deal with image data only because it originated from facial recognition . however , the problem of curse of dimensionality is very prevalent . therefore , we here try to apply this algorithm for text processing . in this paper , we introduce the lle briefly and analyze its advantage and latent disadvantages , and the relationship between lsi and lle in the graph embedding framework is then discussed from a theoretic view . finally , the experimental results are show with the datasets of reuters21578 and tdt2 . 

detecting malicious powershell commands using deep neural networks
microsoft ' s powershell is a command - line shell and scripting language that is installed by default on windows machines . based on microsoft ' s . net framework , it includes an interface that allows programmers to access operating system services . while powershell can be configured by administrators for restricting access and reducing vulnerabilities , these restrictions can be bypassed . moreover , powershell commands can be easily generated dynamically , executed from memory , encoded and obfuscated , thus making the logging and forensic analysis of code executed by powershell challenging . for all these reasons , powershell is increasingly used by cybercriminals as part of their attacks ' tool chain , mainly for downloading malicious contents and for lateral movement . indeed , a recent comprehensive technical report by symantec dedicated to powershell ' s abuse by cybercrimials [ 52 ] reported on a sharp increase in the number of malicious powershell samples they received and in the number of penetration tools and frameworks that use powershell . this highlights the urgent need of developing effective methods for detecting malicious powershell commands . in this work , we address this challenge by implementing several novel detectors of malicious powershell commands and evaluating their performance . we implemented both " traditional " natural language processing ( nlp ) based detectors and detectors based on character - level convolutional neural networks ( cnns ) . detectors ' performance was evaluated using a large real - world dataset . our evaluation results show that , although our detectors ( and especially the traditional nlp - based ones ) individually yield high performance , an ensemble detector that combines an nlp - based classifier with a cnn - based classifier provides the best performance , since the latter classifier is able to detect malicious commands that succeed in evading the former . our analysis of these evasive commands reveals that some obfuscation patterns automatically detected by the cnn classifier are intrinsically difficult to detect using the nlp techniques we applied . our detectors provide high recall values while maintaining a very low false positive rate , making us cautiously optimistic that they can be of practical value . 

semi - implicit stochastic recurrent neural networks
stochastic recurrent neural networks with latent random variables of complex dependency structures have shown to be more successful in modeling sequential data than deterministic deep models . however , the majority of existing methods have limited expressive power due to the gaussian assumption of latent variables . in this paper , we advocate learning implicit latent representations using semi - implicit variational inference to further increase model flexibility . semi - implicit stochastic recurrent neural network ( sis - rnn ) is developed to enrich inferred model posteriors that may have no analytic density functions , as long as independent random samples can be generated via reparameterization . extensive experiments in different tasks on real - world datasets show that sis - rnn outperforms the existing methods . 

text analytics for security : tutorial
computing systems that make security decisions often fail to take into account human expectations . this failure occurs because human expectations are typically drawn from in textual sources ( e . g . , mobile application description and requirements documents ) and are hard to extract and codify . recently , researchers in security and software engineering have begun using text analytics to create initial models of human expectation . in this tutorial , we provide an introduction to popular techniques and tools of natural language processing ( nlp ) and text mining , and share our experiences in applying text analytics to security problems . we also highlight the current challenges of applying these techniques and tools for addressing security problems . we conclude the tutorial with discussion of future research directions . 

domain adaptation with structural correspondence learning
discriminative learning methods are widely used in natural language processing . these methods work best when their training and test data are drawn from the same distribution . for many nlp tasks , however , we are confronted with new domains in which labeled data is scarce or non - existent . in such cases , we seek to adapt existing models from a resource - rich source domain to a resource - poor target domain . we introduce structural correspondence learning to automatically induce correspondences among features from different domains . we test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data , as well as improvements in target domain parsing accuracy using our improved tagger . 

behavioral analysis of transformer models on complex grammatical structures
state - of - the - art neural mt , e . g . transformer , yields quite promising translation accuracy . however , these models are easy to be interfered by noises , causing over - and undertranslation issues . this paper presents a behavioral analysis of transformer models in translating complex grammatical structures , i . e . multiple - word expressions and long - distance dependency . results consistently show that the more complex structures , the less translation accuracy the models yield . we imply that as phrase structures become more complex , the focus patterns learned by the attention mechanism may get erratically sporadic due to the issue of data sparseness . we suggest the use of locality penalty and the increase of attention heads to mitigate the issue , but their trade - offs should also be aware . 

multi - level mining and visualization of scientific text collections : exploring a bi - lingual scientific repository
we present a system to mine and visualize collections of scientific documents by semantically browsing information extracted from single publications or aggregated throughout corpora of articles . the text mining tool performs deep analysis of document collections allowing the extraction and interpretation of research paper ' s contents . in addition to the extraction and enrichment of documents with metadata ( titles , authors , affiliations , etc) , the deep analysis performed comprises semantic interpretation , rhetorical analysis of sentences , triple - based information extraction , and text summarization . the visualization components allow geographical - based exploration of collections , topic - evolution interpretation , and collaborative network analysis among others . the paper presents a case study of a bi - lingual collection in the field of natural language processing ( nlp) . 

a deep learning approach to machine transliteration
in this paper we present a novel transliteration technique which is based on deep belief networks . common approaches use finite state machines or other methods similar to conventional machine translation . instead of using conventional nlp techniques , the approach presented here builds on deep belief networks , a technique which was shown to work well for other machine learning problems . we show that deep belief networks have certain properties which are very interesting for transliteration and possibly also for translation and that a combination with conventional techniques leads to an improvement over both components on an arabic - english transliteration task . 

a content analysis system that supports sentiment analysis for subjectivity and polarity detection in online courses
given the current and increasing relevance of research aimed towards the optimization of teaching and learning experiences in online education , a plethora of studies regarding the application of different technologies to this purpose have been developed . specifically , natural language processing ( nlp ) has been used to detect potential sentiments and opinions in texts , enabling a broader scope for making inferences . at universidad aut ó noma of madrid , spain , we have designed and developed a tool for the application of nlp techniques to analyse the contents of online courses and the contributions of their learners ( video transcriptions , readings , questions and answers of the evaluation activities and learner ' s posts in forums , among others ) to improve the teaching material and the teaching - learning processes of these courses . this tool is called edx - cas ( “content analyser system for edx moocs” ) . in this paper , we provide a detailed description of the tool , its functionalities and its npl processes that support sentiment analysis for subjectivity and polarity detection . moreover , we present a review of current research in the field of application of nlp in the improvement of teaching and learning experiences in moocs . 

natural language requirements processing : from research to practice
automated manipulation of natural language requirements , for classification , tracing , defect detection , information extraction , and other tasks , has been pursued by requirements engineering ( re ) researchers for more than two decades . recent technological advancements in natural language processing ( nlp ) have made it possible to apply this research more widely within industrial settings . this technical briefing targets researchers and practitioners , and aims to give an overview of what nlp can do today for re problems , and what could do if specific research challenges , also emerging from practical experiences , are addressed . the talk will : survey current research on applications of nlp to re problems ; present representative industrially - ready techniques , with a focus on defect detection and information extraction problems ; present enabling technologies in nlp that can play a role in re research , including distributional semantics representations ; discuss criteria for evaluation of nlp techniques in the re context ; outline the main challenges for a systematic application of the techniques in industry . the crosscutting topics that will permeate the talk are the need for domain adaptation , and the essential role of the human - in - the - loop . 

identifying functional relations in web text
determining whether a textual phrase denotes a functional relation ( i . e . , a relation that maps each domain element to a unique range element ) is useful for numerous nlp tasks such as synonym resolution and contradiction detection . previous work on this problem has relied on either counting methods or lexico - syntactic patterns . however , determining whether a relation is functional , by analyzing mentions of the relation in a corpus , is challenging due to ambiguity , synonymy , anaphora , and other linguistic phenomena . we present the leibniz system that overcomes these challenges by exploiting the synergy between the web corpus and freely - available knowledge resources such as free - base . it first computes multiple typed functionality scores , representing functionality of the relation phrase when its arguments are constrained to specific types . it then aggregates these scores to predict the global functionality for the phrase . leibniz outperforms previous work , increasing area under the precision - recall curve from 0 . 61 to 0 . 88 . we utilize leibniz to generate the first public repository of automatically - identified functional relations . 

an automated learner for extracting new ontology relations
recently , the nlp community has shown a renewed interest in automatic recognition of semantic relations between pairs of words in text which called lexical semantics . this approach to semantics is concerned with psychological facts associated with the meaning of words . lexical semantics is an important task with many potential applications including but not limited to , information retrieval , information extraction , text summarization , and language modeling . as this task " automatic recognition of semantic relations between pairs of words in text " can be used in many nlp applications , its implementation are demanding and may include many potential methodologies . and as it includes semantic processing , the results produced still need enhancements and the outcome was always limited in terms of domain or coverage . in this research we developed a buffered system that handle the whole process of extracting causation relations in general domain ontologies . the main achievement of this work is the heavy analysis of statistical and semantic information of causation relation context to generate the learner . the system also builds relation resources that made it possible to learn from itself , were each time it runs the resources incremented with new relations information recording all the statistics of such relation , making its performance enhanced each time it runs . also we present a novel approach of learning based on the best lexical patterns extracted , besides two new algorithms the cia and ps that provide the final set of rules for mining causation to enrich ontologies . 

various approaches in sentiment analysis
in natural language processing ( nlp ) , sentiment analysis is flourishing rapidly as it aims at finding the emotional tone behind a piece of text and helps in understanding the attitudes , opinions , and emotions expressed within an online mention . the internet is flooded with large amounts of data such as texts , photos , audio , and video . with the help of sentiment analysis , we can find useful information from this data . this paper presents various approaches in sentiment analysis used by researchers and the major challenges faced in this field . by analyzing different papers , we came to know that the most widely used techniques in machine learning are na ï ve bayes and svm and in nlp , the lexicon - based approach is used . with the advancement in technology , sentiment analysis will be available for the public and smaller companies as well . 

using mobile technology for reading assessment
the enormous potential of information and communication technologies ( ict ) for addressing critical educational issues is generally acknowledged , but its use in the assessment of the complex skills of reading and understanding a text has been very limited to date . the paper contrasts traditional reading assessment protocols with readlet , an ict platform with a tablet front - end , designed to support online monitoring of silent and oral reading abilities in early graders . readlet makes use of cloud computing and mobile technology for large - scale data collection and allows the time alignment of the child ' s reading behaviour with texts tagged using natural language processing ( nlp ) tools . initial findings replicate established benchmarks from the psycholinguistic literature on reading in both typically and atypically developing children , making the application a new ground - breaking approach in the evaluation of reading skills . 

dyswebxia : a model to improve accessibility of the textual web for dyslexic users
the goal of this research is to make textual content in the web -- especially in spanish and english -- more accessible to people with dyslexia . the techniques that we will use to make the web more accessible are natural language processing ( nlp ) for its content ( text ) and web design guidelines for its layout . to find out which solutions tackle better our purpose we will test a diverse set of web pages examples . the main methodology to evaluate these examples will be eye tracking using regular and dyslexic students . in the case that our findings show that there are strategies that make the web more accessible for dyslexic users , we plan to develop and application which includes such results , transforming a regular web site into a dyslexic friendly web site . 

ontology - based wom extraction service from weblogs
in this paper , we introduce a web - based service that extracts reputations of a product from the internet . if a user inputs the product name , the service first collects articles reviewing the product from weblogs , bbs , and so on . also , it analyzes their contents using metadata and ontologies with conventional nlp techniques . then , it indicates the reputations ( positive or negative ) from the overall and several pre - defined aspects , and other related products that are the subject of much discussion in the articles . this paper illustrates two technical points regarding use of metadata and ontologies with nlp , and summarizes evaluations in a case that we applied it to a market research for a vehicle . 

using natural language processing ( nlp ) for designing socially intelligent robots
the design of natural interaction with social robots is highly complex process , given the huge design space of robots in terms of appearance and behavior and the challenges arising when using face detection and speech recognition in the wild . more natural and highly autonomous interaction is necessary to foster trust and engagement and hence establishing a long - term social relationship between users and robots . in this abstract , an adaptive and interactive dialogue system is designed to exchange a chat with a user using personal information stored in his / her user profile . nlp is used to extract user ' s basic information , hobbies and interests for building a rich user profile . the information from the user profile is used to customize and adapt subsequent dialogues in a way to build trust and initiate comfort between users themselves and the robot . the user profile is continuously updated whenever new information is extracted in subsequent dialogues . face detection ( fd ) is used to identify the user . an artificial neural network ( ann ) based fd system is used to increase the system ' s predictability . failure to recognize a user ' s face leads to creating a new user profile for the new unidentified user . nlp failure leads to storing the whole sentence and manual fixing thereafter . 

lshvec : a vector representation of dna sequences using locality sensitive hashing and fasttext word embeddings
drawing from the analogy between natural language and " genomic sequence language " , we explored the applicability of word embeddings in natural language processing ( nlp ) to represent dna reads in metagenomics studies . here , k - mer is the equivalent concept of word in nlp and it has been widely used in analyzing sequence data . however , directly replacing word embedding with k - mer embedding is problematic due to two reasons : first , the number of distinct k - mers is far more than the number of different words in our vocabulary , making the model too huge to be stored in memory . second , sequencing errors create lots of novel k - mers ( noise ) , which significantly degrade model performance . in this work , we introduce lshvec , a model that leverages locality sensitive hashing ( lsh ) for k - mer encoding to overcome these challenges . after k - mers are lsh encoded , we adopt the skip - gram with negative sampling to learn k - mer embeddings . experiments on metagenomic datasets with labels demonstrate that k - mer encoding using lsh can not only accelerate training time and reduce the memory requirements to store the model , but also achieve higher accuracy than using alternative encoding methods . we validate that lshvec is robust on reads with high sequencing error rates and works well with any sequencing technologies . in addition , the trained low - dimensional k - mer embeddings can be potentially used for accurate metagenomic read clustering and taxonomic classification . finally , we demonstrate the unprecedented capability of lshvec by participating in the second round of cami challenges and show that lshvec is able to handle metagenome datasets that exceed terabytes in size through distributed training across multiple nodes . 

mapping ordinances and tweets using smart city characteristics to aid opinion mining
this research focuses on mining ordinances ( local laws ) and public reactions to them expressed on social media . we place particular emphasis on ordinances and tweets relating to smart city characteristics ( sccs ) , since an important aim of our work is to assess how well a given region heads towards a smart city . we rely on sccs as a nexus between a seemingly infinite number of ordinances and tweets to be able to map them , and also to facilitate scc - based opinion mining later for providing feedback to urban agencies based on public reactions . common sense knowledge is harnessed in our approach to reflect human judgment in mapping . this paper presents our research in ordinance and tweet mapping with sccs , including the proposed mapping approach , our initial experiments , related discussion , and future work emerging therein . to the best of our knowledge , ours is among the first works to conduct mining on ordinances and tweets for smart cities . this work has a broader impact with a vision to enhance smart city growth . 

a machine learning approach for the classification of methamphetamine dealers on twitter in thailand . 
this research presents a method to classify messages from twitter ( tweet ) related to methamphetamine . the messages are classified into three classes : normal , seller , buyer . the models presented in this research are multinomial naive bayes , multi - class lstm , and hierarchical lstm . model training uses a balanced and imbalanced dataset . the text used for model training is tokenized from four tokenizers : tlex + , lexto + , attacut , and deepcut . to study the model performance ' s effect , we divide the data with a different dataset and tokenizer . the results showed that all models could classify the messages into the three classes . the most effective model built from a balanced dataset is the hierarchical lstm model using the lexto + tokenizer provides the highest accuracy , and the most effective model build from an imbalanced dataset is the multi - class lstm model using the lexto + tokenizer . this model gave the highest accuracy , but the fl - score of the hierarchical lstm model gave better accuracy in each class . the creation of a text classification model related to methamphetamine uses twitter messages . most of them are thai grammatical errors and has many slang usage . we found that lexto + is the best tokenizer to build a model . however , it is not much different from other tokenizers . on the other hand , the best dataset to build the model is a balanced dataset that significantly affects model performance . 

conditional collocation in japanese
analysis of collocation is targeted for natural language processing ( nlp ) . from a linguistic perspective , collocation provides us with a way to place words close together in a natural manner . by this approach , we can examine deep structure of semantics through words and their situation . although there have been some investigation based on co - occurrence , few discussion has been made about conditional collocation . in this investigation , we discuss a computational approach to extract conditional collocation by using data mining and statistical techniques . 

thai - english and english - thai translation performance of transformer machine translation
in this paper , the machine translation models were applied to the thai - english and english - thai machine translation task . we investigated three models of machine translation on thai and english sentence pairs . the translation performance of the transformer model is better than that of the recurrent neural network and the traditional statistical machine translation models . we found that the bleu scores of the transformer model were the highest in both thai - english ( 44 . 22 %) and english - thai ( 46 . 48 %) translations . besides , the results were also analysed linguistically . in comparison with the three models , the errors about detailed description and wrong word ordering were mostly found in the smt model , whereas wrong word choice and missing words were mostly found in the rnns model . although the transformer model could perform much better than others , three error categories - under - translation , over - translation , and incorrect lexical choice - were also found . 

model checking a rule - based parser
in the field of natural language processing , one key difficulty for rule - based systems is the debugging and tuning of rules . in this paper we suggest a novel method that use model checking tools to theoretically verify the rule system of a feature - based parser . we discuss the modeling of the system : the establishment of kripke structures , and most importantly , the method to compress the state space of the model . we use partial kripke structures as a main tool for space compression . we show that the state space can be reduced considerably while the compressed model still keeps similar behavior as the non - compressed one . initial examples of specifications are presented and certain restrictions on them are discussed . 

intrinsic or extrinsic evaluation : an overview of word embedding evaluation
compared with traditional methods , word em - bedding is an efficient language representation that can learn syntax and semantics by using neural networks . as the result , more and more promising experiments in natural language processing ( nlp ) get the state - of - the - art results by introducing word embedding . in principle , embedding representation learning embeds words to a low - dimensional vector space , there - fore vectors support initialization of nlp tasks such as text classification , sentiment analysis , language understanding , etc . however , polysemy is very common in many languages , which causes word ambiguation , further influences the accuracy of the system . additionally , language models based on distributed hypotheses mostly focused on word properties rather than morphology were our primary focus . this leads to unreasonable performance in different evaluations . at the same time , word embedding learning and measuring are two vital components of word representation . in this paper , we overviewed many language models including single sense and multiple sense word embedding , and many evaluated approaches including intrinsic and extrinsic evaluation . we found that there are obvious gaps between vectors and manual annotations in word similarity evaluation , and language models that achieved good performance in intrinsic evaluations could not produce similar results in extrinsic evaluations . to the best of our knowledge , there is no universal language model and embedding learning method for most nlp task , and each evaluations also hidden natural defects compared to human knowledge . more evaluated datasets are also investigated such as datasets used in intrinsic and extrinsic evaluations . we believe that an improved evaluation dataset and a more rational evaluation method would benefit from this overview . 

evaluating the competency of a first - order ontology
we report on the results of evaluating the competency of a first - order ontology for its use with automated theorem provers ( atps ) . the evaluation follows the adaptation of the methodology based on competency questions ( cqs ) [ 4 ] to the framework of first - order logic , which is presented in [ 2 ] , and is applied to adimen - sumo [ 1 ] . the set of cqs used for this evaluation has been automatically generated from a small set of semantic patterns and the mapping of wordnet to sumo . analysing the results , we can conclude that it is feasible to use atps for working with adimen - sumo v2 . 4 , enabling the resolution of goals by means of performing non - trivial inferences . 

a custom word embedding model for clustering of maintenance records
maintenance records of industrial equipment contain rich descriptive information in free - text format , such as involved parts , failure mechanisms , operating conditions , etc . our objective is to leverage this unstructured textual information to identify groups of similar maintenance jobs . in this article , we use a natural language based approach and propose a novel custom word embedding model , which utilizes two sources of information , first , maintenance records collected from in - field operations and second , industrial taxonomy , to effectively identify clusters . the advantages of our model include combined use of semantic and taxonomic sources of information for clustering , one step / simultaneous training , which enables knowledge sharing between the two information sources and reduces hyperparameters , and no dependence on third - party data . we demonstrate the efficacy of our model for cluster identification using a real - world dataset . the results show that simultaneous incorporation of semantic and taxonomic information enables accurate extraction of contextual insights for improving maintenance decision - making and equipment reliability . 

towards robustness to label noise in text classification via noise modeling
large datasets in nlp tend to suffer from noisy labels due to erroneous automatic and human annotation procedures . we study the problem of text classification with label noise , and aim to capture this noise through an auxiliary noise model over the classifier . we first assign a probability score to each training sample of having a clean or noisy label , using a two - component beta mixture model fitted on the training losses at an early epoch . using this , we jointly train the classifier and the noise model through a novel de - noising loss having two components : ( i ) cross - entropy of the noise model prediction with the input label , and ( ii ) cross - entropy of the classifier prediction with the input label , weighted by the probability of the sample having a clean label . our empirical evaluation on two text classification tasks and two types of label noise : random and input - conditional , shows that our approach can improve classification accuracy , and prevent over - fitting to the noise . 

using language resources in an intelligent tutoring system for french
this paper presents a project that investigates to what extent computational linguistic methods and tools used at geta for machine translation can be used to implement novel functionalities in intelligent computer assisted language learning . our intelligent tutoring system project is still in its early phases . the learner module is based on an empirical study of french as used by acadian elementary students living in new - brunswick , canada . additionally , we are studying the state of the art of systems using artificial intelligence techniques as well as nlp resources and / or methodologies for teaching language , especially for bilingual and minority groups . 

exploring the role of punctuation in parsing natural text
few , if any , current nlp systems make any significant use of punctuation . intuitively , a treatment of punctuation seems necessary to the analysis and production of text . whilst this has been suggested in the fields of discourse structure , it is still unclear whether punctuation can help in the syntactic field . this investigation attempts to answer this question by parsing some corpus - based material with two similar grammars - - - one including rules for punctuation , the other ignoring it . the punctuated grammar significantly out - performs the unpunctuated one , and so the conclusion is that punctuation can play a useful role in syntactic processing . 

feature engineering using shallow parsing in argument classification of persian verbs
identifying the verb ' s dependents and determining the semantic role for them is a natural pre - processing step in applications such as machine translation ( mt ) and question answering ( qa ) . in this paper , we present a feature set for assigning argument instances into thematic role classes such as “ agent ” and “ patient ” . this feature set contains mainly language specific features for syntactic segments ( chunks ) of persian sentences which can be categorized into three feature types including verb properties , chunk content and relation between the argument and verb of a sentence . we train an instance - based classifier on our manually annotated dataset to select the appropriate semantic role of each chunk . the classifier discriminates the best semantic role without considering the interaction between chunks in a sentence . the results show that our feature set discriminates the thematic roles of arguments in a considerable accuracy about 81 . 9 % which enhances the baseline accuracy about 18 . 8 % . our dataset is free release and available for the researchers . 

xplore word embedding using cbow model and skip - gram model
word embedding has gained a lot of attention in recent years . to learn the systematic relation between words from a large junk of unlabeled text data is quite popular these days while using in machine translation , question answering or even in text summarization . computer understand only 0 ’ s and 1 ’ s now can understand text by simply using word embedding and making vectors in corpus i . e . word vector to represent them in numerical form which are used to perform linear algebra on the words which are now in numerical form in vectors . word embedding is very effective tool in natural language processing ( nlp ) , both cbow ( continuous bag of words ) and skip gram have increased the efficiency of word embedding along with it both models have reduced the training time . this paper is divided into two parts . in the first part we have discussed what is word embedding along with two models to implement word embedding , these models are continuous bag of word and skip gram . our objective is to implement these two - model using shallow neural networks . in second part we have compared the efficiency of both the model while using different size of data sets . 

understanding and evaluating commonsense reasoning in transformer - based architectures
ascertaining the reason for and identifying the differences between sensical and nonsensical statements is a task that humans are naturally adept at . nlp methods , however , might struggle with such a task . this paper aims to evaluate various transformer - based architectures on common sense validation and explanation task , methods to improve their performance , as well as interpreting how fine - tuned language models perform such tasks , using the attention distribution of sentences at inference time . the tasks entail identifying the nonsensical statement from a given pair of similar statements ( validation ) , followed by selecting the correct reason for its nonsensical nature ( explanation ) . an accuracy of83 . 90 and 88 . 42 is achieved on the respective tasks , using roberta ( large ) language model fine - tuned on the data sets using the ulmfit training approach . 

language systems , inc .: muc - 4 test results and analysis
lsi ' s overall natural language processing ( nlp ) objective is the development of a broad coverage , reusable system which is readily transportable to additional domains , applications , and sublanguages in english , as well as providing a foundation for our multilingual work . our system , called dbg , for data base generator , is comprised of a set of nlp components which have been developed , extended , and rebuilt over a period of some years . the core of the system is an innovative principle - based parser , using ideas from [ 1] , which we began developing in the course of muc - 3 to replace our previous chart parser . our approach thus relies on the concept of powerful , robust parsing as the most crucial component in an nlp system . in applying our nlp system to text extraction , our ultimate objective is to develop a high quality text extraction system , where " high quality " is defined as scoring above 80 % - - a number well beyond any current muc scores . 

nlp - oriented study on the imperative sentence with interrogative mood
imperative sentence with interrogative mood ( isim ) is a transitional category between imperative sentence and interrogative sentence . it has both the form of interrogative sentence and the function to transfer information of imperative sentence . so , the classification and meaning of isim become argumentative in study and application in natural language processing . in this paper , the author claims three points in this paper : first , there are two parts of isim : imperative center which transfers imperative information and interrogative structure which makes the whole sentence more euphemistic , second , there are four types of isim in form : attached imperative sentence , imperative sentence of positive and negative , imperative sentence of right and wrong , rhetorical imperative sentence , third , the semantic and types of interrogative structure will affect the semantic of whole sentence . 

a rule - based approach to unknown word recognition in arabic
this paper describes a small experiment to test a rule - based approach to unknown word recognition in arabic . the morphological complexity of arabic presents its challenges to a variety of nlp applications , but it can also be viewed as an advantage , if we can tap into the complex linguistic knowledge associated with these complex forms . in particular , the derived forms of verbs can be analysed and an educated guess at the likely meaning of a derived form can be predicted , based on the meaning of a known form and the relationship between the known form and the unknown one . the performance of the approach is tested on the nemlar written arabic corpus . 

from word embedding to cyber - phrase embedding : comparison of processing cybersecurity texts
much of the vital information about emerging threats and the corresponding defensive measures are contained in large volumes of natural language texts online . capturing such actionable intelligence in real - time is critical to prevent large scale attacks automatically . the att & ck framework is a widely recognized standard to catalog technical details of cyber threats and deploy mitigating measures . a technique in att & ck specifies a set of adversary actions to achieve a particular goal , such as exfiltration over command and control channel . details of the technique include encrypted traffic and encoded data . a key challenge in identifying such cyber intelligence from natural language texts is that for a given action , such as encrypted traffic , many alternative expressions are possible ( e . g . , send using a self - signed certificate , send using https requests) . it is not practical to manually provide an exhaustive list of all such variants . we demonstrate that using cyber - phrase embedding on a cybersecurity text corpus is a promising approach to overcome such difficulties . our evaluation demonstrates that our model outperforms existing models . we have created an open - source project to make our tools and data available for the cybersecurity research community . 

fast methods for kernel - based text analysis
kernel - based learning ( e . g . , support vector machines ) has been successfully applied to many hard problems in natural language processing ( nlp ) . in nlp , although feature combinations are crucial to improving performance , they are heuristically selected . kernel methods change this situation . the merit of the kernel methods is that effective feature combination is implicitly expanded without loss of generality and increasing the computational costs . kernel - based text analysis shows an excellent performance in terms in accuracy ; however , these methods are usually too slow to apply to large - scale text analysis . in this paper , we extend a basket mining algorithm to convert a kernel - based classifier into a simple and fast linear classifier . experimental results on english basenp chunking , japanese word segmentation and japanese dependency parsing show that our new classifiers are about 30 to 300 times faster than the standard kernel - based classifiers . 

data - mining textual responses to uncover misconception patterns
an important , yet largely unstudied , problem in student data analysis is to detect misconceptions from students ' responses to open - response questions . misconception detection enables instructors to deliver more targeted feedback on the misconceptions exhibited by many students in their class , thus improving the quality of instruction . in this paper , we propose a new natural language processing ( nlp ) framework to detect the common misconceptions among students ' textual responses to open - response , short - answer questions . we introduce a probabilistic model for students ' textual responses involving misconceptions and experimentally validate it on a real - world student - response dataset . preliminary experimental results show that our proposed framework excels at classifying whether a response exhibits one or more misconceptions . more importantly , it can also automatically detect the common misconceptions exhibited across responses from multiple students to multiple questions ; this is especially important at large scale , since instructors will no longer need to manually specify all possible misconceptions that students might exhibit . 

gplsi : word coarse - grained disambiguation aided by basic level concepts
we present a corpus - based supervised learning system for coarse - grained sense disambiguation . in addition to usual features for training in word sense disambiguation , our system also uses base level concepts automatically obtained from wordnet . base level concepts are some synsets that generalize a hyponymy sub - hierarchy , and provides an extra level of abstraction as well as relevant information about the context of a word to be disambiguated . our experiments proved that using this type of features results on a significant improvement of precision . our system has achieved almost 0 . 8 f1 ( fifth place ) in the coarse - - grained english all - words task using a very simple set of features plus base level concepts annotation . 

enhancing the detection of criminal organizations in mexico using ml and nlp
this paper relies on machine learning ( ml ) and supervised natural language processing ( nlp ) to generate a geo - referenced database on the violent presence of mexican criminal organizations ( mcos ) between 2000 - 2018 . this application responds to the need for high - quality data on criminal groups to inform academic and policy analysis in a context of intense violence such as mexico . powered by ml and nlp tools , this computational social science application processes a vast collection of news stories written in spanish to track mcos ' violent presence . the unprecedented granularity of the data allows disaggregating daily - municipal information for 10 main mcos comprising more than 200 specific criminal cells . 

aspect - based personalized review ranking
many users of e - commerce websites are often used to making purchase decisions based on the product reviews . since there are too many reviews attached to a product , users may not have enough time and patience to read all the reviews . moreover , since each user has different requirements for different characteristics of the product , most reviews may have little help in making his purchase decision . to address this issue , we propose a personalized review ranking method to select useful reviews for users . to be specific , we try to assign a helpfulness score for each review , which measures the value of each review for a user ' s decision making . then , we sort reviews in descending order of helpfulness , so as to generate a subset of reviews for the user . our main idea is to find similar users ' reviews for a target user , and examine the number of aspects that similar users have reviewed . by considering the similarity between users and the number of aspects of that users have reviewed , we produce personalized review recommendation according to the helpfulness of the reviews . here , we define similar users as having similar sentimental tendencies towards the same aspects of the same products . we mainly look for similar users of similar products to improve the accuracy of review recommendation to similar users . 

generic nlp technologies : language , knowledge and information extraction
we have witnessed significant progress in nlp applications such as information extraction ( ie ) , summarization , machine translation , cross - lingual information retrieval ( clir ) , etc . the progress will be accelerated by advances in speech technology , which not only enables us to interact with systems via speech but also to store and retrieve texts input via speech . 

automatic knowledge extraction to build semantic web of things applications
the internet of things ( iot ) primary objective is to make a hyper - connected world for various application domains . however , iot suffers from a lack of interoperability leading to a substantial threat to the predicted economic value . schema . org provides semantic interoperability to structure heterogeneous data on the web . an extension of this vocabulary for the iot domain ( iot . schema . org ) is an ongoing research effort to address semantic interoperability for the web of things ( wot ) . to design this vocabulary , a central challenge is to identify the main topics ( concepts and properties ) automatically from existing knowledge in iot applications . we designed knowledge extraction for the wot ( ke4wot ) to automatically identify the most important topics from literature ontologies of three different iot application domains : 1 ) smart home ; 2 ) smart city ; and 3 ) smart weather - based on our corpus consisting of 4500 full - text conference and journal articles to utilize domain - specific knowledge encoded within iot publications . despite the importance of automatically identifying the relevant topics for iot . schema . org , up to know there is no study dealing with this issue . to evaluate the extracted topics , we compare the descriptiveness of these topics for the ten most popular ontologies in the three domains with empirical evaluations of 23 domain experts . the results illustrate that the identified main topics of iot ontologies can be used to sufficiently describe existing ontologies as keywords . 

machine learning for explaining and ranking the most influential matters of law
in this work , we propose a novel method in order to rank the most relevant legal principle citations in law - cases to support a certain motion . the first score relies on feature importance metrics , where each law article is a feature supplied to a classifier for the decision outcome . the second score is based on word embeddings text similarity . as a result , our method outperforms the baseline techniques based on feature importance selection and information retrieval methods in the ranking evaluation relevance criteria . 

asist : automated strategic partners ranking and recommendation system using nlp and security intelligence
strategic and technological environment is changing rapidly in cyberspace . a robust cybersecurity ecosystem is driven by identifying an ideal corporate partnership . performing research and analysis on a potential cybersecurity partnering company involves manual and labor - intensive processes without a standardized evaluation framework . adding on , the task of performing security evaluation on the company ' s products is often overlooked and neglected in the research and analysis process . the proposed solution - asist - fills this gap in the overall comprehensiveness of the analysis made on a company . firstly , the news crawler module was developed to automatically extract relevant cybersecurity news which is then passed to the natural language processing module to perform analysis and strategy roadmap score tabulation . concurrently , the common vulnerability scoring system ( cvss ) crawler module extracts the product security information of the company and performs tabulation of the product security score . at the final stage of program runtime , the companies are ranked by the reporting module , utilizing the two scores that were tabulated by the prior modules . the strategy roadmap score and product security score evaluated by asist are on an average within 5 % of the manually evaluated scores from professional industry analysts . 

human - in - the - loop based named entity recognition
named entity recognition ( ner ) is a challenging issue in natural language processing ( nlp ) tasks , and has drawn much attention from industry and academia . at present , with the incessant evolution of deep neural network ( dnn ) model , it has been widely used in ner tasks . however , dnn models heavily depend on a large amount of annotated training samples , and these models will show certain limitations when applied to domain - specific tasks . in this paper , a human - in - the - loop based ner ( h - ner ) approach is proposed from the perspective of human - machine collaboration . in particular , interactive operations allow users to quickly annotate samples and verify the accuracy of annotated samples , and ner model will be updated iteratively based on progressively increased training samples . experimental results show that this approach can effectively reduce the task load of annotation , and reach or even exceed the existing sample selection strategies in performance indicators such as f1 - score ( entity - level ) and accuracy ( sentence - level ) . furthermore , this approach will expand the serviceable range of ner and greatly improve its applicability . 

scsl : a linguistic specification language for mt
nowadays , mt systems grow to such a size that a first specification step is necessary if we want to be able to master their developement and maintenance , for the software part as well for the linguistic part (" lingwares ") . advocating for a clean separation between linguistic tasks and programming tasks , we first introduce a specification / implementation / validation framework for nlp then scsl , a language for the specification of analysis and generation modules . 

proposed myanmar word tokenizer based on lipidipikar treatise
natural language processing ( nlp ) based technologies are now becoming important and future intelligent systems will use more of these techniques as the technology is improving explosively . but asia becomes a dense area in nlp field because of linguistic diversity . many asian languages are inadequately supported on computers . myanmar language is an analytic language but it includes special character like killer , medial , etc . . in english or european languages , all of the syllables are formed by combining the alphabets that represent only consonants and vowels but myanmar language uses compound syllables that make more difficult to analyze . so we can face difficulties in word sorting . in our proposed system , the condensed form of myanmar ordinary scripts will be transformed into analyzable elaborated scripts based on lipidipikar treatise written by yaw min gyi u pho hlaing . these elaborated words can be easily sorted by using this treatise . in our proposed system , complexity of myanmar condensed words sorting compared with complexity of elaborated words sorting . 

deep natural language processing for search and recommendation
search and recommender systems process rich natural language text data such as user queries and documents . achieving high - quality search and recommendation results requires processing and understanding such information effectively and efficiently , where natural language processing ( nlp ) technologies are widely deployed . in recent years , the rapid development of deep learning technology has been proven successful for improving various nlp tasks , indicating their great potential of promoting search and recommender systems . developing deep learning models for nlp in search and recommender systems involves various fundamental components including query / document understanding , retrieval & ranking , and language generation . in this workshop , we propose to discuss deep neural network based nlp technologies and their applications in search and recommendation , with the goal of understanding ( 1 ) why deep nlp is helpful ; ( 2 ) what are the challenges to develop and productionize it ; ( 3 ) how to overcome the challenges ; ( 4 ) where deep nlp models produce the largest impact . 

improving answer extraction for bangali q / a system using anaphora - cataphora resolution
human computer interaction ( hci ) is a field of study to interact between humans ( user ) and computers on the design of computer technology . question answering ( qa ) system is one of the parts of hci and a process of information retrieval ( ir ) in natural language processing ( nlp ) . in this research , it is attempted for a bangla question answering system with simple sentences and experimented the system for both bangla and english language . and it is tried to perform with semantic and syntactical analysis . furthermore , for bangla , a word net is constructed to demonstrate the system process . our proposed method is a model in which it is easy for users to get most possible exact answer to their question easily and reduces the complexity of using noun instead of pronoun for the requested answer with respect to the given question queries for bangla . it improves better answer extraction than naive approach . 

natural language in computer human - interaction : a chi 99 special interest group
fifty - three people from accross the world participated in the chi 99 special interest group on natural language in computer human - interaction . the sig ' s main goal was to provide an opportunity for chi 99 attendees from two research communities , natural language processing ( nlp ) and human - computer interaction ( chi ) , to discuss issues of mutual interest . the sig embraced natural - language interfaces of all kinds , inlcuding text , spoken and multi - modal interaction . this report inlcudes the results of e - mail discussions following up on the sig itself . 

decoding the style and bias of song lyrics
the central idea of this paper is to gain a deeper understanding of song lyrics computationally . we focus on two aspects : style and biases of song lyrics . all prior works to understand these two aspects are limited to manual analysis of a small corpus of song lyrics . in contrast , we analyzed more than half a million songs spread over five decades . we characterize the lyrics style in terms of vocabulary , length , repetitiveness , speed , and readability . we have observed that the style of popular songs significantly differs from other songs . we have used distributed representation methods and weat test to measure various gender and racial biases in the song lyrics . we have observed that biases in song lyrics correlate with prior results on human subjects . this correlation indicates that song lyrics reflect the biases that exist in society . increasing consumption of music and the effect of lyrics on human emotions makes this analysis important . 

fast unsupervised dependency parsing with arc - standard transitions
unsupervised dependency parsing is one of the most challenging tasks in natural languages processing . the task involves finding the best possible dependency trees from raw sentences without getting any aid from annotated data . in this paper , we illustrate that by applying a supervised incremental parsing model to unsupervised parsing ; parsing with a linear time complexity will be faster than the other methods . with only 15 training iterations with linear time complexity , we gain results comparable to those of other state of the art methods . by employing two simple universal linguistic rules inspired from the classical dependency grammar , we improve the results in some languages and get the state of the art results . we also test our model on a part of the ongoing persian dependency treebank . this work is the first work done on the persian language . 

an unsupervised detection framework for chinese jargons in the darknet
with the continuous development of the darknet technology , the scale of darknet and have increased rapidly in recent years , leading to rampant crime in these anonymous trading markets . monitoring these markets can effectively combat the criminal forces that hide behind them . one of the difficulties in understanding the darknet is that criminals usually use jargons to disguise transactions and thus avoid surveillance . these jargons usually distort the original meaning of innocent - looking words in obscure ways , posing significant challenges for crime tracking . current research on chinese jargon detection mainly adopts the method of keyword filtering , however , such methods have little effect on the complex and ever - changing structure of darknet jargons . we propose a chinese jargon detection framework based on unsupervised learning . the main idea is to compare similarity with high - dimensional word embedding features from different corpus to find jargons . firstly , we collect data from six chinese tor websites to build a dark corpus dataset . afterwards , we build a word - based pre - training model called dc - bert , which can generate high - quality contextual word embeddings . finally , we construct a cross - corpus jargon detection framework based on similarity analysis , which can effectively detect chinese jargons in the darknet . the experimental results show that the proposed framework is both innovative and practical , reaching a detection accuracy of 91 . 5% . 

sentimental text processing tool for russian language based on machine learning algorithms
several studies have been published to analyze different approaches to traditional text classification methods . most of these studies cover the application of certain methods of semantic terminology to textual classification to a certain extent . however , they are not specifically aimed at the classification algorithms for semantic text and their advantages over the traditional text classification . as it is known , kazakhstan is a multinational country and russian is a transnational language . therefore , it was decided to develop a tool for processing the incoming text in different languages . actually , there are some existing tools that process text in english , but they do not support some foreign languages , especially that are in use in kazakhstan . the main goal is to analyze some machine learning ( ml ) algorithms and develop a sentimental text processing tool using those algorithms to make a web page for text processing through categorization and sentimental analysis . it was investigated that the best way to implement machine learning and natural language processing ( nlp ) algorithms on python is to create a web - page using django framework . the principle of work is to get the text as input , process it with ml algorithms and send result as a « . json » file . 

natural language processing for historical texts
book abstract : more and more historical texts are becoming available in digital form . digitization of paper documents is motivated by the aim of preserving cultural heritage and making it more accessible , both to laypeople and scholars . as digital images cannot be searched for text , digitization projects increasingly strive to create digital text , which can be searched and otherwise automatically processed , in addition to facsimiles . indeed , the emerging field of digital humanities heavily relies on the availability of digital text for its studies . together with the increasing availability of historical texts in digital form , there is a growing interest in applying natural language processing ( nlp ) methods and tools to historical texts . however , the specific linguistic properties of historical texts - - the lack of standardized orthography , in particular - - pose special challenges for nlp . this book aims to give an introduction to nlp for historical texts and an overview of the state of the art in this field . the book starts with an overview of methods for the acquisition of historical texts ( scanning and ocr ) , discusses text encoding and annotation schemes , and presents examples of corpora of historical texts in a variety of languages . the book then discusses specific methods , such as creating part - of - speech taggers for historical languages or handling spelling variation . a final chapter analyzes the relationship between nlp and the digital humanities . certain recently emerging textual genres , such as sms , social media , and chat messages , or newsgroup and forum postings share a number of properties with historical texts , for example , nonstandard orthography and grammar , and profuse use of abbreviations . the methods and techniques required for the effective processing of historical texts are thus also of interest for research in other domains . table of contents : introduction / nlp and digital humanities / spelling in historical texts / acquiring historical texts / text encoding and annotation schemes / handling spelling variation / nlp tools for historical languages / historical corpora / conclusion / bibliographyview less

automatic subject indexing of chinese documents
automatic subject indexing is a process to produce automatically a set of attributes that represent the content or topic of a document . in this paper , two approaches of automatic subject indexing based on vsm ( vector space model ) and subject words segmentation respectively are presented . the experimental results show that the first approach based on vsm is appropriate when the documents , which are indexed , are concentrative and the subject words available are less . the second approach based on subject words segmentation improves greatly efficiency of indexing and inter - indexer consistency . 

paraphrase identification based on resnext
the misuse of paraphrase generation technology has flooded the internet with low - quality , unclear manuscripts , deteriorating the user ' s reading experience . this paper mainly investigates into the paraphrase identification from the deep learning perspective , and proposes a paraphrase identification model based on deep resnext . our model captures the semantic and syntactic features of the given text with the help of deep neural network . specifically , we select multiple convolution kernels with different sizes to extract the features of word sequences of different lengths . at the same time , the residual block which helps build a paraphrase identification model suitable for various current platforms . the data preprocessing includes data cleaning , word segmentation , and word vectorization . experiments on benchmark dataset msrpc show that our model achieved a strong result of 85 . 4 % on accuracy rate . 

histogram based double gaussian feature normalization for robust language recognition
for automatic language recognition , performance can be seriously degraded due to the transfer characteristics of the communication channel . many methods are proposed to compensate the effect of the environment for better recognition results . in this paper , we propose a histogram based double gaussian feature normalization method for robust language recognition . compared with the baseline system , the proposed method achieves a relative error reduction of 17 . 4% , which shows advantages over other common feature normalization methods in language recognition systems . 

a reusable lexical database tool for machine translation
this paper describes the lexical database tool lola ( linguistic - oriented lexical database approach ) which has been developed for the construction and maintenance of lexicons for the machine translation system lmt . first , the requirements such a tool should meet are discussed , then lmt and the lexical information it requires , and some issues concerning vocabulary acquisition are presented . afterwards the architecture and the components of the lola system are described and it is shown how we tried to meet the requirements worked out earlier . although lola originally has been designed and implemented for the german - english lmt prototype , it aimed from the beginning at a representation of lexical data that can be reused for other lmt or mt prototypes or even other nlp applications . a special point of discussion will therefore be the adaptability of the tool and its components as well as the reusability of the lexical data stored in the database for the lexicon development for lmt or for other applications . 

jumping nlp curves : a review of natural language processing research [ review article ] 
natural language processing ( nlp ) is a theory - motivated range of computational techniques for the automatic analysis and representation of human language . nlp research has evolved from the era of punch cards and batch processing ( in which the analysis of a sentence could take up to 7 minutes ) to the era of google and the likes of it ( in which millions of webpages can be processed in less than a second ) . this review paper draws on recent developments in nlp research to look at the past , present , and future of nlp technology in a new light . borrowing the paradigm of ` jumping curves ' from the field of business management and marketing prediction , this survey article reinterprets the evolution of nlp research as the intersection of three overlapping curves - namely syntactics , semantics , and pragmatics curveswhich will eventually lead nlp research to evolve into natural language understanding . 

no noun phrase left behind : detecting and typing unlinkable entities
entity linking systems link noun - phrase mentions in text to their corresponding wikipedia articles . however , nlp applications would gain from the ability to detect and type all entities mentioned in text , including the long tail of entities not prominent enough to have their own wikipedia articles . in this paper we show that once the wikipedia entities mentioned in a corpus of textual assertions are linked , this can further enable the detection and fine - grained typing of the unlinkable entities . our proposed method for detecting unlinkable entities achieves 24 % greater accuracy than a named entity recognition baseline , and our method for fine - grained typing is able to propagate over 1 , 000 types from linked wikipedia entities to unlinkable entities . detection and typing of unlinkable entities can increase yield for nlp applications such as typed question answering . 

estimating the number of remaining links in traceability recovery ( journal - first abstract ) 
although very important in software engineering , establishing traceability links between software artifacts is extremely tedious , error - prone , and it requires significant effort . even when approaches for automated traceability recovery exist , these provide the requirements analyst with a , usually very long , ranked list of candidate links that needs to be manually inspected . in this paper we introduce an approach called estimation of the number of remaining links ( enrl ) which aims at estimating , via machine learning ( ml ) classifiers , the number of remaining positive links in a ranked list of candidate traceability links produced by a natural language processing techniques - based recovery approach . we have evaluated the accuracy of the enrl approach by considering several ml classifiers and nlp techniques on three datasets from industry and academia , and concerning traceability links among different kinds of software artifacts including requirements , use cases , design documents , source code , and test cases . results from our study indicate that : ( i ) specific estimation models are able to provide accurate estimates of the number of remaining positive links ; ( ii ) the estimation accuracy depends on the choice of the nlp technique , and ( iii ) univariate estimation models outperform multivariate ones . 

neural embeddings for metaphor detection in a corpus of greek texts
one of the major challenges that nlp faces is metaphor detection , especially by automatic means , a task that becomes even more difficult for languages lacking in linguistic resources and tools . our purpose is the automatic differentiation between literal and metaphorical meaning in authentic non - annotated phrases from the corpus of greek texts by means of computational methods of machine learning . for this purpose the theoretical background of distributional semantics is discussed and employed . distributional semantics theory develops concepts and methods for the quantification and classification of semantic similarities displayed by linguistic elements in large amounts of linguistic data according to their distributional properties . in accordance with this model , the approach followed in the current work takes into account the linguistic context for the computation of the distributional representation of phrases in geometrical space , as well as for their comparison with the distributional representations of other phrases , whose function in speech is already “ known ” with the objective to reach conclusions about their literal or metaphorical function in the specific linguistic context . this procedure aims at dealing with the lack of linguistic resources for the greek language , as the almost impossible up to now semantic comparison between “ phrases ” , takes the form of an arithmetical comparison of their distributional representations in geometrical space . 

toc generation in pdf document for smart automated compliance engine
portable document format ( pdf ) is a commonly used format for the scientific publication . currently , an input document is used to test the compliance and relevance of the document or text in automated compliance engines and natural language processing ( nlp ) based system . the whole document text is used for searching the compliance rules which is computationally expensive and slow process . for speeding up the compliance checking process and making it cost efficient , this paper purposes a method based on table of content ( toc ) data structure . this work proposed the pdfparser which performs data indexing , separate headings text , and non - heading text , create hierarchy of headings and generates toc to reduce the semantic - based string searching time and space . furthermore , in the nlp based system , mostly semantic - based string matching used . the proposed pdfparser uses the cosine similarity method for computing semantic based similarity . our purposed method performs 47 . 2 % better than the previous approach of searching in the non - indexed whole document and decreases the search time and space . in the worst - case scenario , where no string match found , our purposed method performs 20 . 5 % better . 

learning recognition of ambiguous proper names in hindi
an ambiguous proper name is a name which is also a valid dictionary word with a meaning of its own when used in the text . for example in english , the word ' bush ' in ' mr . bush ' is a proper name whereas in ' a dense bush ' it is a lexical entity . almost all proper names in hindi have a meaning and find an entry in the dictionary . recognition of named entities finds wide application in mt , ir and several other nlp tasks . while there have been a number of investigations on hindi ner in general , no work has been reported exclusively on ambiguous proper nouns which are more difficult to deal with . this paper presents a methodology for recognizing ambiguous proper names in hindi using hybridization of a rule - base and statistical crf based machine learning using morphological and context features . the methodology yields a f - score of 71 . 6% . 

deep learning to predict hospitalization at triage : integration of structured data and unstructured text
overcrowding in emergency departments ( ed ) is considered as an international issue , which could have adverse impacts on multiple care outcomes such as the length of stay for example . part of the solution could lie in the early prediction of the patient outcome as discharge or hospitalization . this study applies deep learning to this end . a large - scale dataset of about 260k ed records was provided by the amiens - picardy university hospital in france . in general , our approach is based on integrating structured data with unstructured textual notes recorded at the triage stage . the key idea is to apply a multi - input of mixed data for training a classification model to predict hospitalization . in a simultaneous manner , the model training utilizes the numeric features along with textual data . on one hand , a standard multi - layer perceptron ( mlp ) model is used with the standard set of features ( i . e . numeric and categorical ) . on the other hand , a convolutional neural network ( cnn ) is used to operate over the textual data . the two components of learning are conducted independently in parallel . the empirical results demonstrated that the classifier could achieve a very good accuracy with roc - auc ≈ 0 . 83 . the study is conceived to contribute to the mounting efforts of applying natural language processing in the healthcare domain . 

by using grey area relational grade combined with nlp method to optimize gm ( 1 , 1 ) model
in this paper , we suggest a new optimization method by using grey area relational grade combined with nonlinear programming ( abbreviated to nlp ) method for gm ( 1 , 1 ) model ’ s parameters , we introduce the grey area relational grade to establish a nlp optimization parameters model for a gm ( 1 , 1 ) model that has been established , by using mathematical software lingo 10 . 0 for its optimal solution . by a lot of data ’ s analysis , we conclude the new method is effective and feasible for the gm ( 1 , 1 ) models that have been established by using any other methods . 

dzongkha word segmentation using deep learning
natural language processing ( nlp ) has been applied to machine translation , chatbots , speech recognition , question and answer systems , document summarization and so on . the dzongkha language of bhutan , however , has not been considered in nlp systems , due , presumably , to the fact that the language is complex and written as a string of syllables without proper word boundaries . thus , dzongkha word segmentation is the essential first step in building the nlp applications . the novelty of our research is in applying deep learning to the task of dzongkha word segmentation , avoiding the need for manual feature engineering . the segmentation problem is formulated as a syllable tagging task . we also incorporate the windows approach where the tag of a syllable depends on its surrounding syllables . two sets of experiments were designed , with four models of varying context sizes in each set . we evaluated our models using the syllable - tagged - corpus prepared by dzongkha development commission . the model with context size 2 achieved the highest f - score of 94 . 40 % with 94 . 47 % precision and 94 . 35 % recall . 

smartmeeting : automatic meeting transcription and summarization for in - person conversations
meetings are a necessary part of the operations of any institution , whether they are held online or in - person . however , meeting transcription and summarization are always painful requirements since they involve tedious human effort . this drives the need for automatic meeting transcription and summarization ( amts ) systems . a successful amts system relies on systematic integration of multiple natural language processing ( nlp ) techniques , such as automatic speech recognition , speaker identification , and meeting summarization , which are traditionally developed separately and validated offline with standard datasets . in this demonstration , we provide a novel productive meeting tool named smartmeeting , which enables users to automatically record , transcribe , summarize , and manage the information in an in - person meeting . smartmeeting transcribes every word on the fly , enriches the transcript with speaker identification and voice separation , and extracts essential decisions and crucial insights automatically . in our demonstration , the audience can experience the great potential of the state - of - the - art nlp techniques in this real - life application . 

question paper generator and answer verifier
exams are getting digitized all over the world . basically , meaning that the traditional paper based tests are being replaced by the certain computer based tests which have proven to be both more consistent in allocating marks and faster than teacher correcting papers . the traditional exams usually consisted of subjective answers which were not the best way of grading the student ' s perception of the subject . hence , we are developing a computer based system that will generate objective based question that will be better suited to grade students academically . the system will generate a question paper and it will grade the student after he / she has solved the question paper . this system will save time as well as the efforts put in by the teachers , which they can spend on more productive activities . 

sinhala conversational interface for appointment management and medical advice
this paper proposes an intelligent conversational user interface to assist sinhala speaking users to make appointments with doctors and to obtain medical advices . this sinhala conversational interface for appointment management and medical advice ( sci - amma ) consists of speech recognition unit , query processing unit , dialog management unit , voice synthesizer unit , and user information management unit to handle user requests and maintain a meaningful dialogue . the sci - amma gets the users ' speech utterances and recognize the language content of it for further processing . language content is further processed using query processing unit to identify users ' intent . to fulfil the users ' intent , a reply is generated from dialogue management unit . this reply / answer will be delivered to the user by means of a voice synthesizer . the proposed system is successfully implemented using state of the art technology stack including flutter , python , prot é g é and firebase . performance of the system is demonstrated using several sample scenarios / dialogues . 

a parsing algorithm of natural language based on operator precedence
to deal with the problems in the current parsing method of natural language , a language model based on purely typed binary relations is proposed in this paper , which is quite different from classical context - free grammar , chomsky normal form and dependency grammar . the completeness of language structure coverage is easily captured using the proposed language model , so long as the binary relations can be recognized thoroughly . the parsing process is considered as the manipulation of binary operations of words , which is similar to the computation of arithmetic expressions in terms of the proposed language model , where operands are words and operators are the typed relations . and thereafter , a parsing algorithm based on operator precedence is proposed in the paper , with the algorithm formalized and the complexity analyzed . the proposed ideas have been proved valid and realistic through the application to parsing chinese sentences . further researches are needed to put it into the effective and practical usages in large scale to make it developed as a powerful theoretical way for parsing natural language . 

hadoopperceptron : a toolkit for distributed perceptron training and prediction with mapreduce
we propose a set of open - source software modules to perform structured perceptron training , prediction and evaluation within the hadoop framework . apache hadoop is a freely available environment for running distributed applications on a computer cluster . the software is designed within the map - reduce paradigm . thanks to distributed computing , the proposed software reduces substantially execution times while handling huge data - sets . the distributed perceptron training algorithm preserves convergence properties , thus guaranties same accuracy performances as the serial perceptron . the presented modules can be executed as stand - alone software or easily extended or integrated in complex systems . the execution of the modules applied to specific nlp tasks can be demonstrated and tested via an interactive web interface that allows the user to inspect the status and structure of the cluster and interact with the mapreduce jobs . 

on method and automatic construction theory of domain ontology based on depended text
based on analysis of relationship between ontology and conventional information organization and problems existing in ontology construction study and on authors ' experience of building ontology , the paper focuses on currently well accepted domain knowledge and domain experts ' effective domain which are two indispensable terms to quickly and successfully construct domain ontology . the paper also reconstructs and reuses existing accepted domain knowledge , establishes a set of depended texts ' ontology learning system , and finally realizes automatic construction of domain ontology concept description system through nlp theory and technology . this would further provide theory and technology support for domain ontology automatic constructionview less

comparative evaluation of two arabic speech corpora
the aim of this paper is to conduct a constructive and comparative evaluation between two important arabic corpora for two different arabic dialects , namely , saudi dialect corpus that was collected by king abdulaziz city for science and technology ( kacst) , and a levantine arabic dialect corpus . levantine dialect is spoken by ordinary lebanese , jordanian , syrian , and palestinian people . the later one was produced by the linguistic data consortium ( ldc) . advantages and disadvantages of these two corpora were presented and discussed . this discussion is aiming to help digital speech processing researchers to figure out the weakness and strength sides of these important corpora before considering them in their experiments . moreover , this paper can motivate in designing , maintaining , distributing , and upgrading arabic corpora to help arabic language speech research communities . 

a proposed approach for arabic language segmentation
this paper presents a research about natural language processing ( nlp ) . our area of interest is the process of arabic text segmentation . text segmentation is important step in any nlp . in this paper , we discuss several methods dealing mainly with cases of ambiguity of arabic text segmentation . several conclusions have been made . these conclusions lead to make a proposal of text segmentation . a vision based on connectors is developed . 

a cloud - hosted mapreduce architecture for syntactic parsing
syntactic parsing is a time - consuming task in natural language processing particularly where a large number of text files are being processed . parsing algorithms are conventionally designed to operate on a single machine in a sequential fashion and , as a consequence , fail to benefit from high performance and parallel computing resources available on the cloud . we designed and implemented a scalable cloud - based architecture supporting parallel and distributed syntactic parsing for large datasets . the main architecture consists of a syntactic parser ( constituency and dependency parsing ) and a mapreduce framework running on clusters of machines . the resulting cloud - based mapreduce parsing is able to build a map where syntactic trees of the same input file have the same key and collect into a single file containing sentences along with their corresponding trees . our experimental evaluation shows that the architecture scales well with regard to number or processing nodes and number of cores per node . in the fastest tested cloud - based setup , the proposed design performs 7 times faster when compared to a local setup . in summary , this study takes an important step toward providing and evaluating a cloud - hosted solution for efficient syntactic parsing of natural language data sets consisting of a large number of files . 

what was the question ? a systematization of information retrieval and nlp problems
in this paper we suggest a novel systematization of information retrieval and natural language processing problems . using this rather general description of problems we are able to discuss and proof the equivalence of some problems . we provide reformulations of well - known problems like named entity recognition using our novel description and discuss further research and the expected outcome . we will discuss the relation of two problems , cluster labeling and search query finding . with these results we are able to provide a novel optimization approach to both problems . this novel systematization approach provides a yet unknown view generating new classes of problems in nlp . it brings application and algorithmic approaches together and offers a better description with concepts of theoretical computer science . 

experience report : log mining using natural language processing and application to anomaly detection
event logging is a key source of information on a system state . reading logs provides insights on its activity , assess its correct state and allows to diagnose problems . however , reading does not scale : with the number of machines increasingly rising , and the complexification of systems , the task of auditing systems ' health based on logfiles is becoming overwhelming for system administrators . this observation led to many proposals automating the processing of logs . however , most of these proposal still require some human intervention , for instance by tagging logs , parsing the source files generating the logs , etc . in this work , we target minimal human intervention for logfile processing and propose a new approach that considers logs as regular text ( as opposed to related works that seek to exploit at best the little structure imposed by log formatting ) . this approach allows to leverage modern techniques from natural language processing . more specifically , we first apply a word embedding technique based on google ' s word2vec algorithm : logfiles ' words are mapped to a high dimensional metric space , that we then exploit as a feature space using standard classifiers . the resulting pipeline is very generic , computationally efficient , and requires very little intervention . we validate our approach by seeking stress patterns on an experimental platform . results show a strong predictive performance ( ≈ 90 % accuracy ) using three out - of - the - box classifiers . 

a novel chinese - english on translation method using mix - language web pages
in this paper , we propose a novel chinese - english organization name translation method with the assistance of mix - language web resources . firstly , all the implicit out - of - vocabulary terms in the input chinese organization name are recognized by a crfs model . then the input chinese organization name is translated without considering these recognized out - of - vocabulary terms . secondly , we construct some efficient queries to find the mix - language web pages that contain both the original input organization name and its correct translation . at last , a similarity matching and limited expansion based translation identification approach is proposed to identify the correct translation from the returned web pages . experimental results show that our method is effective for chinese organization name translation and can improve performance of chinese organization name translation significantly . 

measuring language development in early childhood education : a case study of grammar checking in child language transcripts
language sample analysis is an important technique used in measuring language development . at present , measures of grammatical complexity such as the index of productive syntax ( scarborough , 1990 ) are used to measure language development in early childhood . although these measures depict the overall competence in the usage of language , they do not provide for an analysis of the grammatical mistakes made by the child . in this paper , we explore the use of existing natural language processing ( nlp ) techniques to provide an insight into the processing of child language transcripts and challenges in automatic grammar checking . we explore the automatic detection of 6 types of verb related grammatical errors . we compare rule based systems to statistical systems and investigate the use of different features . we found the statistical systems performed better than the rule based systems for most of the error categories . 

malay tweets : discovering mental health situation during covid - 19 pandemic in malaysia
during the unprecedented of covid - 19 pandemic , numbers of research had been conducted on mental health in social media worldwide . past research has shown interest in twitter sentiment analysis by using keywords , geographical area , and range of ages . up to the authors ’ analysis , there is no research conducted on mental health using keyword in the case of malaysia . a malay tweet dataset was built for analysing mental health tweets during the first movement control order period using unique keywords . machine learning algorithms namely , na ï ve bayes classifier and support vector machine were used to predict the sentiment of tweets . the classifiers were evaluated using 10 - fold cross - validation , accuracy , precision , and f1 - score . the data then visualized in charts and wordcloud . the results shows that support vector machine performed better than na ï ve bayes classifier for both test set and 10 - fold cross - validation in terms of performances in n - gram tf - idf . the visualized data could provide insights to the authority pertaining the mental health issues , in which it relates to local news and situations during the periods . 

novel prototype for handling arabic natural language processing : smart morphological analyser
arabic stemming is a technique that finds the stem or lexical root for arabic words through the process of eliminating affixes attached to their roots . many studies have been conducted to generate the stem of arabic words according to the desirable level of analysis , i . e . , rootbased approach , stem - based approach and statistical approach . arabic language , which is the focus of this paper , is a semitic language which means that it is a derivational rather than a concatinative language . in this paper we implemented an arabic triliteral morphological analyser that is capable of analysing the classical and modern standard arabic ( msa ) effectively and able to analyse vowelised , semi - vowelised and nonvowelised text . the system is integratable with other applications . one shortcomming for the developed system is that the output obtained from the morphological analyser may contain several alternative solutions which leads to extraction ambiguity . 

beam - width prediction for efficient context - free parsing
efficient decoding for syntactic parsing has become a necessary research area as statistical grammars grow in accuracy and size and as more nlp applications leverage syntactic analyses . we review prior methods for pruning and then present a new framework that unifies their strengths into a single approach . using a log linear model , we learn the optimal beam - search pruning parameters for each cyk chart cell , effectively predicting the most promising areas of the model space to explore . we demonstrate that our method is faster than coarse - to - fine pruning , exemplified in both the charniak and berkeley parsers , by empirically comparing our parser to the berkeley parser using the same grammar and under identical operating conditions . 

study and analysis of emotion classification on textual data
emotion analysis plays a part in understanding the feelings of human beings . people ' s actions and speech express various feelings , behaviors and emotions which can have various impacts . emotion and sentiment analysis is a broad research area for finding emotion which helps getting useful insight through text and speech . in most of previous work , nearly all projects have focused on analyzing the expression based on positive , negative and neutral classification . this research work analyzes the proposed system by categorizing the text into emotion classes called joy , sadness , anger , fear , love and surprise . this work helps us to label text emotions into multiple classes and categorize the text for better accuracy . this research work represents the enhancement of a novel deep learning model scheme , long short - term memory and recurrent neural network which discuss various categories distribution on knowledgeable data . this will also summarize the previous works done on textual emotional classification based on various sentiment models and approach with comparative survey analysis . 

chinese semantic role labeling based on semantic knowledge
most of the semantic role labeling systems use syntactic analysis results to predict semantic roles . however , there are some problems that could not be well - done only by syntactic features . in this paper , lexical semantic features are extracted from some semantic dictionaries . two typical lexical semantic dictionaries are used , tongyici cilin and csd . cilin is built on convergent relationship and csd is based on syntagmatic relationship . according to both of the dictionaries , two labeling models are set up , cilin model and csd model . also , one pure syntactic model and one mixed model are built . the mixed model combines all of the syntactic and semantic features . the experimental results show that the application of different level of lexical semantic knowledge could help use some language inherent attributes and the knowledge could help to improve the performance of the system . 

an optimized customers sentiment analysis model using pastoralist optimization algorithm ( poa ) and deep learning
users usually express their sentiment online which influences purchased products and services . the computational study of people ' s feelings and thoughts on entities is known as sentiment analysis . the long short - term memory ( lstm ) model is one of the most common deep learning models for solving sentiment analysis problems . however , they possess some drawbacks such as longer training time , more memory for training , easily over fits , and sensitivity to randomly generated parameters . hence , there is a need to optimize the lstm parameters for enhanced sentiment analysis . this paper proposes an optimized lstm approach using a newly developed novel pastoralist optimization algorithm ( poa ) for enhanced sentiment analysis . the model was used to analyze sentiments of customers retrieved from amazon product reviews . the performance of the developed poa - lstm model shows an optimal accuracy , precision , recall and f1 measure of 77 . 36 % , 85 . 06 % , 76 . 29 % , and 80 . 44 % respectively , when compared with lstm model with 71 . 62 % , 78 . 26 % , 74 . 23 % , and 76 . 19 % respectively . it was also observed that poa with 20 pastoralist population size performs better than other models with 10 , 15 , 25 and 30 population size . 

gender bias and under - representation in natural language processing across human languages
natural language processing ( nlp ) systems are at the heart of many critical automated decision - making systems making crucial recommendations about our future world . however , these systems reflect a wide range of biases , from gender bias to a bias in which voices they represent . in this paper , a team including speakers of 9 languages - chinese , spanish , english , arabic , german , french , farsi , urdu , and wolof - reports and analyzes measurements of gender bias in the wikipedia corpora for these 9 languages . in the process , we also document how our work exposes crucial gaps in the nlp - pipeline for many languages . despite substantial investments in multilingual support , the modern nlp - pipeline still systematically and dramatically under - represents the majority of human voices in the nlp - guided decisions that are shaping our collective future . we develop extensions to profession - level and corpus - level gender bias metric calculations originally designed for english and apply them to 8 other languages , including languages like spanish , arabic , german , french and urdu that have grammatically gendered nouns including different feminine , masculine and neuter profession words . we compare these gender bias measurements across the wikipedia corpora in different languages as well as across some corpora of more traditional literature . 

a knowledge acquisition and management system for morphological dictionaries
a system for the acquisition and management of reusable morphological dictionaries is clearly a useful tool for nlp . as such , most currently popular finite - state morphology systems have a number of drawbacks . in the development of word manager , these problems have been taken into account . as a result , its knowledge acquisition component is well - developed , and its knowledge representation enables more flexible use than typical finite - state systems . 

an ontology for cmmi - acq model
this paper discusses an ontology developed to represent the cmmi - acq domain knowledge . cmmi introduces a collection of best practices that helps organizations improve their processes and cmmi - acq as one of its constellations is to provide organizations with a resource of effective and proven practices to support acquisition process improvement . this ontology has been developed based on sumo upper ontology . through the paper , cmmi and cmmi - acq will be illustrated . also the ontology , its development methodology , and its potential application will be described . 

graph - based methods for natural language processing and understanding — a survey and analysis
this survey and analysis presents the functional components , performance , and maturity of graph - based methods for natural language processing and natural language understanding and their potential for mature products . resulting capabilities from the methods surveyed include summarization , text entailment , redundancy reduction , similarity measure , word sense induction and disambiguation , semantic relatedness , labeling ( e . g . , word sense) , and novelty detection . estimated scores for accuracy , coverage , scalability , and performance are derived from each method . this survey and analysis , with tables and bar graphs , offers a unique abstraction of functional components and levels of maturity from this collection of graph - based methodologies . 

evaluating distributional semantic and feature selection for extracting relationships from biological text
the constant flow of biomolecular findings being published each day challenges our ability to develop methods to automatically extract the knowledge expressed in text to potentially influence new discoveries . finding relations between the biological entities ( e . g . proteins and genes ) in text is a challenging task . to facilitate the extraction process , a relation can be decomposed into a trigger and the complementary arguments ( e . g . theme , site ) . several approaches have been proposed based on machine learning which generally use a common set of features for all trigger types . here we evaluate the impact of applying a feature selection method for trigger classification . our proposed method uses a greedy feature selection algorithm to find an optimal set of attributes for each trigger type . we show that using the customized set of features can improve classification results significantly ( up to 53 . 96 % in f - measure ) . in addition , we evaluated different settings for including semantic features in the classifiers . we found that using semantic features can improve classification results and found the best setting for each trigger type . 

modelling field dependencies on structured documents with fuzzy logic
a new scenario has raised into the ir field with the increase in the use of mark - up languages . this new scenario has been defined as structured ir and is focused on documents with structure . the classic ir models have been extended in order to be applied to this document type . generally these adaptations have been carried out by weighting the fields that form the document structure , and making the assumption of statistics independence between fields . this assumption force to an estimation of the different weights applied to each field . in this paper a new ranking function for structured ir is proposed . this new function is based on fuzzy logic , and its main aim is to model through heuristics and expert knowledge the relations between fields within a document . 

nonlinear femtosecond laser radiation interaction with condensed matter and periodic structures formation with spatial scale less than half the wavelength
an analysis of known experimental results on the formation of periodic structures under ultrashort laser pulses interaction with condensed matter has been made . the examples of the structures with periods less than the diffraction limit in condensed matter have been interpreted based on the nonlinear mathematical model of unimodal logistical map which describes mutipled - one - half period changes with control parameter . 

a proposal for creating a chinese emotion thesaurus with tag of emotion intensity
in recent years , researchers have begun to pay more attention to the emotion recognition in natural language processing . in order to help this pursuit , this paper proposes a semi - automatic approach to create a chinese emotion thesaurus with tag of emotion intensity based on two kinds of language resources hownet and tongyici cilin . as a basic emotion resource , the emotion thesaurus should be used to help the classification and recognition of emotion in chinese . we present a new algorithm of the semantic similarity computation for aiding the emotion intensity computation between words . the experimental results on verbs prove that our approach is feasible and effective . 

a comparison of features extraction methods for arabic sentiment analysis
natural language processing ( nlp ) has built up so much importance in the past few years . with machine learning , nlp can detect a lot of unseen information from a huge volume of textual data , which can be helpful for sentiment analysis and text classification . for those tasks , features can be extracted , the operation bases on extracting an important subset of features from a data . however , identifying convenient features is very important for improving the nlp tasks . for arabic language the process of getting the related features is difficult due to many reasons , for example , this language has the most words compared to other languages . our contribution in this paper is to analyze the impact of feature extraction methods such as bag - of - words , tf - idf , and word2vec on the performance of sentiment analysis using arabic language . the extracted features are evaluated using many machine learning algorithms like logistic regression and support vector machine . 

extractive summarization for myanmar language
due to increasing availability of online information , tools and mechanisms for automatic summarization of documents is needed . text summarization is currently a major research topic in natural language processing . there are various approaches to generate text summary . among them , we proposed myanmar text summarization using latent semantic analysis ( lsa ) . latent semantic analysis ( lsa ) is a technique in natural language processing , and can analyze relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms . it is an unsupervised approach which does not need any traning or external knowledge . there is no lsa based sentence extraction in myanmar language . this is the first lsa based text summarizer in myanmar . this paper present generic , extractve and single - document myanmar text summarization using latent semantic analysis . this paper compare two sentence selection methods ( steinberger and jezek ' s approach and ozay approach ) of latent semantic analysis to extract important sentences . we summarize myanmar news from myanmar official websites such as 7day daily , iyarwaddy , etc . 

improve user experience on web for machine translation system using storm
a transfer based machine translation ( mt ) system is a large complex functional application where the job completion time is proportional to job size . when these applications are deployed on web with increasing translation load web user experience degrades . the end user has to wait unusually longer to get his first visible response . generic layer 3 load balancing techniques does not help to improve the response time as each job is assigned similar compute resources irrespective of job size . this paper presents an engineering approach to deploy mt system on cloud platform using storm , a distributed computing framework . this scheme , by utilizing the inherent parallelism of a functional application , not only reduces the job completion time considerably but it , also as a web application , gives very good user experience , viz . , the first visible response time within an acceptable time limit , and the subsequent responses well before the user finishes perusing the preceding response . using storm framework a translation job is split into multiple job partitions and is streamed into the storm cluster such that first job partitions of all jobs are processed before the second partitions , i . e . , all n th partitions of all jobs are processed before ( n + 1 ) th partitions . thus machine translation system is able to produce translation output as a continuous stream , sentence by sentence , as soon as each sentence gets translated . the system maintains flow rate of translated sentences stream high enough so that the next translated sentence is produced well before the end user finishes reading the previous sentence , thereby providing very good user experience . there is a class of natural language processing ( nlp ) applications , viz . , machine translation systems , text to speech systems , speech recognition systems , etc . , that are functional in nature , and this engineering approach would be equally applicable to them as well . 

an integrated architecture for shallow and deep processing
we present an architecture for the integration of shallow and deep nlp components which is aimed at flexible combination of different language technologies for a range of practical current and future applications . in particular , we describe the integration of a high - level hpsg parsing system with different high - performance shallow components , ranging from named entity recognition to chunk parsing and shallow clause recognition . the nlp components enrich a representation of natural language text with layers of new xml meta - information using a single shared data structure , called the text chart . we describe details of the integration methods , and show how information extraction and language checking applications for realworld german text benefit from a deep grammatical analysis . 

text summarization model of combining global gated unit and copy mechanism
text summarization is a common task in nlp . automatic text summarization aims to transform lengthy documents into shortened versions . recently , the neural networks based on seq2seq with attention are good at generating summarization . however , the accuracy of the summarization too difficult are to guarantee . in addition , the out - of - vocabulary ( oov ) problem is also an important factor affecting the quality of the generated summary . to solve these problems , we hybrid the advantages of the extractive and abstractive summarization systems to propose text summarization model of combining global gated unit and copy mechanism ( gguc ) . the experiment results demonstrate that the performance of the model is better than the other text summary system on lcsts datasets . 

a hybrid approach for arabic multi - word term extraction
building a domain model from a specialized corpus requires identifying candidate terms . it also includes identifying semantic relations between terms . once this model is constructed it can be used for many tasks of information retrieval . in this process , multi - word terms have a great importance . in the one hand they constitute domain relevant candidate terms . on the other hand syntactic relations that link their constituents can be used to infer semantic relations between terms . in this paper we propose to extract mutli - word terms from arabic specialized corpora . the proposed approach uses linguistic rules based on morphological features and pos ( part of speech ) tags to parse documents and retrieve candidate terms . statistical measures are used to deal with ambiguities generated by the linguistic tools and to rank candidate terms according to their relevance . we present experiments on a corpus from the environment domain . we report high quality results that are confirm the targets set for the precision metric . 

msr - nlp entry in bionlp shared task 2011
we describe the system from the natural language processing group at microsoft research for the bionlp 2011 shared task . the task focuses on event extraction , identifying structured and potentially nested events from unannotated text . our approach follows a pipeline , first decorating text with syntactic information , then identifying the trigger words of complex events , and finally identifying the arguments of those events . the resulting system depends heavily on lexical and syntactic features . therefore , we explored methods of maintaining ambiguities and improving the syntactic representations , making the lexical information less brittle through clustering , and of exploring novel feature combinations and feature reduction . the system ranked 4th in the genia task with an f - measure of 51 . 5% , and 3rd in the epi task with an f - measure of 64 . 9% . 

lexnet : a graphical environment for graph - based nlp
this interactive presentation describes lexnet , a graphical environment for graph - based nlp developed at the university of michigan . lexnet includes lexrank ( for text summarization ) , biased lexrank ( for passage retrieval ) , and tumbl ( for binary classification ) . all tools in the collection are based on random walks on lexical graphs , that is graphs where different nlp objects ( e . g . , sentences or phrases ) are represented as nodes linked by edges proportional to the lexical similarity between the two nodes . we will demonstrate these tools on a variety of nlp tasks including summarization , question answering , and prepositional phrase attachment . 

a first step towards nlp from digitized manuscripts : virtual restoration
digitization of the documental heritage conserved in libraries and archives is a common practice , in order to ensure the preservation and fruition of this extended part of the human cultural and historical patrimony . for the most precious , fragile and difficult to read and decipher manuscripts , specialized though portable digitization equipment , such as high resolution multispectral / hyperspectral cameras , is nowadays available . digitization made it possible the increasingly extensive use of digital image processing techniques , to perform a number of virtual restoration tasks , which constitute a first , often necessary step prior subsequent automatic analysis of the writing contents , with the ultimate goal to perform automatic transcription and / or natural language processing tasks . here we report our experience in this field , referring , as a case study , to the problem of removing one of the most frequent and impairing degradation affecting many ancient manuscripts , i . e . , the bleed - through distortion . in this case , virtual restoration gives also the immediate benefit to facilitate the work of philologists and paleographers interested in examining and transcribing the manuscript in a traditional way . 

application of the rk4ip method for the numerical study of noise - like pulses in supercontinuum generation
in this work , we focus on the numerical study of a nlp , generated by a figure - eight fiber laser ( f8l ) , propagating in single - mode fiber ( smf - 28 ) and photonic crystal fiber ( pcf ) through the rk4ip method . we have performed a numerical study in the evolution of supercontinuum spectra by varying the input power of nlp , using even lower input powers than those used with ultrashort pulses . our main purpose is to highlight the nlps are an excellent seed for wide , soft and in particular , flattened spectra . 

xiao - shih : the educational intelligent question answering bot on chinese - based moocs
in this study , the educational intelligent question answering bot named xiao - shih has been developed for solving learners ' questions as instructors and teaching assistants on moocs . experiments were conducted with xiao - shih in a paid course titled " python for data science " on " sharecourse " which is one of the largest chinese - based mooc platform in taiwan . over one thousand discussion threads posted in both english and chinese languages were retrieved to train the bot by natural language processing ( nlp ) and random forest ( rf ) in machine learning algorithms . this paper presents the implementation details on developing xiao - shih . first , we developed an initial version of xiao - shih with simply nlp techniques and text similarity approaches . however , xiao - shih only obtains 0 . 413 precision at best with different thresholds of the question similarity . therefore , features and labels of answering correctness have been collected for the next version of xiao - shih . trained by random forest with 70 % of the entire dataset , xiao - shih obtains 0 . 833 precision with test dataset . with this educational intelligent question answering bot , learners can solve their problems immediately in seconds rather than wait for humans ' response in hours even days . moreover , xiao - shih can also ease instructors ' and teaching assistants ' burden on answering questions . 

learning taxonomic relations from a set of text documents
this paper presents a methodology for learning taxonomic relations from a set of documents that each explain one of the concepts . three different feature extraction approaches with varying degree of language independence are compared in this study . the first feature extraction scheme is a language - independent approach based on statistical keyphrase extraction , and the second one is based on a combination of rule - based stemming and fuzzy logic - based feature weighting and selection . the third approach is the traditional tf - idf weighting scheme with commonly used rule - based stemming . the concept hierarchy is obtained by combining self - organizing map clustering with agglomerative hierarchical clustering . experiments are conducted for both english and finnish . the results show that concept hierarchies can be constructed automatically also by using statistical methods without heavy language - specific preprocessing . 

certain investigation on cause analysis of accuracy metrics in sentimental analysis on news articles
in the modern world , news is reaching the people through online mode . news readers are inclined to news based on interactivity and immediacy . the current technology facilitates all users a direct news on any event in real world . the sentimental analysis can be used to determine an emotional rating of text data sources from news articles such as times of india , india today etc . , tfidf methodology of word vectorization and word embedding of word vectorization is used to analyze the effectiveness in accuracy of models . the experiments have been performed on datasets using linear svm and gaussian naive bayes classifiers , in which consideration in decision making is based maximum scores acquired at certain class is ranked to be an important news to public . the accuracy for unigram by tfidf vectorization is 62 % , which is lesser than the accuracy of bigram by tfidf vectorization which is 65 % using svm classifier . for gaussian naive bayes classifier , the accuracy of unigram model is 61 % . even though the accuracy of gaussian naive byes classifier is less , independent feature considering nature of this classifier makes this model more effective in predicting the socially important news than other classifiers and also common occurrence of token of word between each class will be cause of affect the f1 score of models . 

a novel framework for neural machine translation of indian - english languages
the term ' machine translation ' ( mt ) refers to computer systems that performs translation of natural language from one language to another . machine translation is especially necessitated in the indian perspective because more than 50 % of the data generated online is in english which is known by only 12 % . many systems has been proposed in the indian perspective including rule - based , example - based , statistical based and a hybrid of these machine translation techniques . but , recent study has shown that neural machine translation provides better results . in recent times google and facebook have developed neural machine translation system . these systems are one - fits - all kind of systems which do not take into consideration the complexities in a language , like indian languages . so , this paper proposes a broad framework for implementing neural machine translation for indian - english languages . 

hierarchical n - gram algorithm for extracting arabic entities
entities extraction becomes very important for developing many applications of natural language processing ( nlp ) . in this paper , we present a new algorithm to extract entities from arabic text . the approach uses the semi - structured knowledge source : arabic wikipedia to predict the words that constitutes an arabic entity . our method is generic and can be applied directly to other languages to extract entities . the proposed method has been designed to analyze arabic text hierarchically with variable length n - gram . the experimental results have proven that the proposed system is very efficient in detecting entities from large set of arabic news . 

joint training and decoding using virtual nodes for cascaded segmentation and tagging tasks
many sequence labeling tasks in nlp require solving a cascade of segmentation and tagging subtasks , such as chinese pos tagging , named entity recognition , and so on . traditional pipeline approaches usually suffer from error propagation . joint training / decoding in the cross - product state space could cause too many parameters and high inference complexity . in this paper , we present a novel method which integrates graph structures of two sub - tasks into one using virtual nodes , and performs joint training and decoding in the factorized state space . experimental evaluations on conll 2000 shallow parsing data set and fourth sighan bakeoff ctb pos tagging data set demonstrate the superiority of our method over cross - product , pipeline and candidate reranking approaches . 

text mining strategy of power customer service work order based on natural language processing technology
in order to improve the level of power customer service , we need to start with natural language processing technology , and conduct in - depth text mining of power customer complaint work order . combined with the format and characteristics of power customer service work order , this paper proposed a text mining strategy of power customer service work order based on natural language processing technology which includes the following processes : work order data cleaning , text segmentation , information characterization , model training and model evaluation is proposed . the tf - idf algorithm and the model optimization method based on accuracy calculation are applied in this strategy , which greatly improves the level of power customer service . 

the framenet data and software
the framenet project has developed a lexical knowledge base providing a unique level of detail as to the the possible syntactic realizations of the specific semantic roles evoked by each predicator , for roughly 7 , 000 lexical units , on the basis of annotating more than 100 , 000 example sentences extracted from corpora . an interim version of the framenet data was released in october , 2002 and is being widely used . a new , more portable version of the framenet software is also being made available to researchers elsewhere , including the spanish framenet project . this demo and poster will briefly explain the principles of frame semantics and demonstrate the new unified tools for lexicon building and annotation and also framesql , a search tool for finding patterns in annotated sentences . we will discuss the content and format of the data releases and how the software and data can be used by other nlp researchers . 

thai vowels speech recognition using convolutional neural networks
the vowel is considered as the core of syllable in each word . this paper aims to present noisy thai vowels speech recognition by using convolutional neural network ( cnn ) . the noisy thai vowels dataset is the speech of thai vowels in real - world situations . the sound is collected in a real environment from several areas which consist of many types of noise at 30 - 40db snr ( signal to noise ratio ) . it constrains 16 khz speech data is recorded from a mobile phone . the vowel speeches are separated into 2 groups : male ' s voice and female ' s voice from 25 male and 25 female speakers . in this research , it constrains 18 classes ( 9 short vowels and 9 long vowels ) . mel frequency cepstral coefficients ( mfccs ) are used for feature extraction . the most accuracy rate of the cnn - thai simple vowel ( cnn - tsv ) model is 90 . 00 % and ss . s9 % on female and male voices respectively . the comparison results of cnn - tsv model with other models such as multilayer perceptron ( mlp ) and support vector machines ( svm ) show that the cnn - tsv model is the most effective for both female and male voices . this research can be used as one of an alternative model to apply for computer - assisted language learning ( call ) in the future development direction . 

an automated learning system for twitter trends
twitter is the social media platform for real - time broadcasting of information on world events . the microblogging site contains and continues to generate huge amounts of data along with the growing breadth of a geographically diverse user base . qualitative analysis of this enormous data will require substantial effort on information filtering to successfully drill down to relevant topics and events . this paper presents an automated learning system for trends in twitter to generate a recommendation system for users to understand the contexts in a particular trend . we have devised a framework using apriori algorithm and named entity recognition on learned twitter trends . the paper also presents schemes for visual representation of the results using concept hierarchies . 

hybrid modeling of an offline arabic handwriting recognition system ahrs
handwriting recognition hr is the old dream of all those who need to enter data into a computer . in this article , we present a state of the art in the field of handwriting recognition by focusing primarily on arabic handwritten script which we present an overview of the offline automatic arabic handwriting recognition systems ahrs and the techniques and strategies used . then , to try to resolve the problems inherent in arabic handwriting , within the general framework of arabic handwriting recognition and to meet the need to test a new method of learning , we model a new offline arabic handwriting recognition system ahrs which we detail the architecture of our system and the contributions offered at each stage : preprocessing , segmentation , features extraction , classification and post - processing . 

neural character - level syntactic parsing for chinese
in this work , we explore character - level neural syntactic parsing for chinese with two typical syntactic formalisms : the constituent formalism and a dependency formalism based on a newly released character - level dependency treebank . prior works in chinese parsing have struggled with whether to de   ne words when modeling character interactions . we choose to integrate full character - level syntactic dependency relationships using neural representations from character embeddings and richer linguistic syntactic information from human - annotated character - level parts - of - speech and dependency labels . this has the potential to better understand the deeper structure of chinese sentences and provides a better structural formalism for avoiding unnecessary structural ambiguities . specifically , we   first compare two different character - level syntax annotation styles : constituency and dependency . then , we discuss two key problems for character - level parsing : ( 1 ) how to combine constituent and dependency syntactic structure in full character - level trees and ( 2 ) how to convert from character - level to word - level for both constituent and dependency trees . in addition , we also explore several other key parsing aspects , including di   erent character - level dependency annotations and joint learning of parts - of - speech and syntactic parsing . finally , we evaluate our models on the chinese penn treebank ( ctb ) and our published shanghai jiao tong university chinese character dependency treebank ( scdt ) . the results show the e   effectiveness of our model on both constituent and dependency parsing . we further provide empirical analysis and suggest several directions for future study . 

electrically driven exciton - polariton lasers
exciton - polaritons are hybrid light - matter quasiparticles . they are composite and interacting bosons which can condense dynamically in a single - particle ground state via stimulated scattering . the formation of exciton - polaritons under electrical pumping has been demonstrated in quasi two dimensional systems [ 1 - 3] . in this work , we discuss the non - linear polariton emission caused by stimulated scattering in micropillar cavities under electrical pumping [ 4] . we distinguish our polariton laser from a conventional cavity mediated laser by probing it in a magnetic field . while a characteristic zeeman - splitting is present in the regime of polariton lasing [ 4 , 5] , it is fully absent when the device enters the weak coupling regime . 

on measuring the lexical quality of the web
in this paper we propose a measure for estimating the lexical quality of the web , that is , the representational aspect of the textual web content . our lexical quality measure is based in a small corpus of spelling errors and we apply it to english and spanish . we first compute the correlation of our measure with web popularity measures to show that gives independent information and then we apply it to different web segments , including social media . our results shed a light on the lexical quality of the web and show that authoritative websites have several orders of magnitude less misspellings than the overall web . we also present an analysis of the geographical distribution of lexical quality throughout english and spanish speaking countries as well as how this measure changes in about one year . 

transfer learning for bilingual content classification
linkedin groups provide a platform on which professionals with similar background , target and specialities can share content , take part in discussions and establish opinions on industry topics . as in most online social communities , spam content in linkedin groups poses great challenges to the user experience and could eventually lead to substantial loss of active users . building an intelligent and scalable spam detection system is highly desirable but faces difficulties such as lack of labeled training data , particularly for languages other than english . in this paper , we take the spam ( spanish ) job posting detection as the target problem and build a generic machine learning pipeline for multi - lingual spam detection . the main components are feature generation and knowledge migration via transfer learning . specifically , in the feature generation phase , a relatively large labeled data set is generated via machine translation . together with a large set of unlabeled human written spanish data , unigram features are generated based on the frequency . in the second phase , machine translated data are properly reweighted to capture the discrepancy from human written ones and classifiers can be built on top of them . to make effective use of a small portion of labeled data available in human written spanish , an adaptive transfer learning algorithm is proposed to further improve the performance . we evaluate the proposed method on linkedin ' s production data and the promising results verify the efficacy of our proposed algorithm . the pipeline is ready for production . 

classification of book review sentiment in bangla language using nlp , machine learning and lstm
books are said to be the best friend a person can have . book reading culture dates back to almost a couple of thousands of years . after ancient civilizations learned to write , they stored information in tablets or walls or stones are said to be the predecessors of books . the newest form of books is called e - books , digitalization or digital printing of paper - based books . in bangladesh , even a few years back , people had to go to the library in person to collect books but now online book stores are getting popular . with all its perks being easy , online book store comes with some penalty i . e . reader don ' t know about the books or the service of the book store itself . to avoid such , book readers tend to rely on reviews and ratings . our goal is to analyze bangla language reviews and give proper feedback about books and the online store so the book readers can buy desired books to read and get better services from online book stores . 6281 raw data are collected to train the machine . in this study , the machine learning and deep learning methods are used for the binary ( negative , positive ) analysis of sentiments . natural language processing ( nlp ) is used for text preprocessing . some of the popular algorithms like support vector machine ( svm ) , decision tree , knn , random forest , logistic regression and long short - term memory ( lstm ) are used to classify the sentiments . our highest accuracy is 97 . 49% , achieved by lstm . 

developing methodology for korean particle error detection
we further work on detecting errors in post - positional particle usage by learners of korean by improving the training data and developing a complete pipeline of particle selection . we improve the data by filtering non - korean data and sampling instances to better match the particle distribution . our evaluation shows that , while the data selection is effective , there is much work to be done with preprocessing and system optimization . 

caper : crawling and analysing facebook for intelligence purposes
organised crime uses information technology systems to communicate , work or expand its influence . the eu fp7 security research project caper ( collaborative information , acquisition , processing , exploitation and reporting for the prevention of organised crime ) , created in cooperation with european law enforcement agencies ( leas ) , aims to build a common collaborative and information sharing platform for the detection and prevention of organised crime , which exploits open source intelligence ( osint ) . leas are becoming more inclined to using osint tools , and particularly tools able to manage online social networks ( osns ) data . this paper presents the caper facebook crawling and analysis subsystem . heuristic algorithms have been implemented in order to extract specific properties of facebook ' s social graph , in particular user interactions . to support analysis tasks specifically , extensive effort has been spent on the analysis of textual user generated content and on the recognition of named - entities , in particular person names , locations and organisations . relationships between users and entities mentioned in posts and in related comments are created and merged into the users networks extracted from the social graph . all entity relationships are finally visualised in user - friendly network graphs . 

readability assessment for text simplification
we describe a readability assessment approach to support the process of text simplification for poor literacy readers . given an input text , the goal is to predict its readability level , which corresponds to the literacy level that is expected from the target reader : rudimentary , basic or advanced . we complement features traditionally used for readability assessment with a number of new features , and experiment with alternative ways to model this problem using machine learning methods , namely classification , regression and ranking . the best resulting model is embedded in an authoring tool for text simplification . 

estimating gender based on bengali conventional full name with various machine learning techniques
for finding patterns in data , machine learning models are being trained . gender relations psychology looks for social norms like inter dimensionality , beliefs , social experience and self - perception , and self - respect . training on gender based text nlp models unknowingly become acquainted with unusual patterns . in this paper , we represent gender recognition by using bengali conventional full names . we present a review and interpretation of gender classification based on individual names in this correspondence . these days , nlp has demonstrated excellent execution in identifying human gender . in the field of knowledge , gender classification is a demonstrative binary classification phenomenon . we ' ve used a total of seven algorithms in this research . we were added to the dataset with details regarding which features are currently used for prediction along with that it determines how these features are affected by data preprocessing model initialization and architecture selection . our research compares those classifiers , examines the impact of pretraining moreover , assesses the robustness of the alignment preprocessing through the confusion matrix . . the proposed neural network outperforms most approaches and is much more reliable than other models . this model has the best weighted precision of all the models , with such a 73 . 04 % accuracy score . 

proposed machine learning classifier algorithm for sentiment analysis
text mining has emerged as an active domain in the field of nlp ( natural language processing ) and due to availability of large data sets of reviews , it has become easy to do sentiment analysis and extract the result from it , but now - a - days the objectives are expressed in different ways making the data massive and difficult to understand for machines . in this research work , machines are first trained ( supervised learning ) with the help of the predefined data ( or more clearly reviews ) and then tested with the reviews available . this research work will show you the working of a system that uses the supervised training which classifies a product review as positive or negative using various classifier algorithms like knn , logistic regression and support vector machines . the model which will give the more accuracy will be considered as the best model . 

data collection for investigating speech variability in a specific speaker over long and short time periods
in this paper , we describe a japanese speech corpus collected for investigating the speech variability of a specific speaker over short and long time periods . although speakers use a speaker - dependent speech recognition system , it is known that speech recognition performance varies pending when the utterance was uttered . this is because speech varies even if the speaker utters a specific sentence . however , the relationship between intra - speaker speech variability and speech recognition performance is not clear . we have not seen a corpus of japanese speech data of a specific speaker over a long time period . hence , since 2002 , we have been collecting speech data for investigating the relationships between speech variability and speech recognition performance . in this paper , we introduce our speech corpus and conduct speech recognition experiments . experimental results show that the variability of recognition performance over different days is larger than variability of recognition performance within a day . 

health education based on natural language processing ( nlp ) for infectious disease outbreak
the purpose of this study is to test and use natural language processing ( nlp ) to analyze epidemic case reports to establish an effective health education system . a total of 100 cases were randomly selected from the epidemiological case report of feb 1 , 2021 to may 15 , 2021 published on the chinese public media website . the nlp techniques are used to help the assessment team identify and summarize relevant issues . infectious disease prediction system based on a small number of epidemic reports , in the shortest possible time to help the assessment team to summarize the relevant problems , for experts to make a judgment to provide a basis . we found that nlp technology can play a certain role in the analysis of epidemiological reports , which is based on mature languages of existing language libraries , and can effectively improve the analysis efficiency of experts . this preliminary study confirmed that nlp technology can be used to analyze the text of epidemic case reports and help experts quickly establish a health education system . 

bringing active learning to life
active learning has been applied to different nlp tasks , with the aim of limiting the amount of time and cost for human annotation . most studies on active learning have only simulated the annotation scenario , using prelabelled gold standard data . we present the first active learning experiment for word sense disambiguation with human annotators in a realistic environment , using fine - grained sense distinctions , and investigate whether al can reduce annotation cost and boost classifier performance when applied to a real - world task . 

natural language processing based rule based discourse analysis of marathi text
indian languages are verb final and morphologically rich . for the present work marathi language is explored which belongs to indo aryan family . the data is collected from the marathi balbharati book from standard 1st to 4th . this paper presents challenges of discourse analysis means two or more sentences are linked together which is required for many natural language based applications . rule based structure is framed to find out the discourse from the dataset . these rules are framed on the basis of the marathi grammar and world knowledge of the linguistic . with the help of pos tagger identify the tag of each word in the discourse is identified . the discourse anaphora is resolved in various foreign as well as indian languages but marathi is one of the languages where the discourse is not resolved uptil now . discourse means that the some relative sentences or the sub sentences are connected with each - other . an important step in understanding the connected discourse is skill to link the meaning of the successive sentence together . total 515 discourse sentences are considered here to resolving the anaphora from which 177 are the simple sentence and 338 are the discourse one . 

a3id : an automatic and interpretable implicit interference detection method for smart home via knowledge graph
the smart home brings together devices , the cloud , data , and people to make home living more comfortable and safer . trigger - action programming enables users to connect smart devices using if - this - then - that ( ifttt ) - style rules . with the increasing number of devices in smart home systems , multiple running rules that act on actuators in contradictory ways may cause unexpected and unpredictable interference problems , which can put residents and their belongings at risk . previous studies have considered explicit interference problems related to multiple rules targeting a single actuator , whereas implicit interference ( interference across different actuators ) detection is still challenging and not yet well studied owing to the effort - intensive and time - consuming annotation work of obtaining device information . the lack of knowledge about devices is a critical reason that affects the accuracy and efficiency in implicit interference detection . in this article , we propose a3id , an automatic detection method for implicit interference based on knowledge graphs . using natural language processing ( nlp ) techniques and a lexical database , a3id can extract knowledge of devices from a knowledge graph , including functionality , effect , and scope . then , it analyzes and detects interferences among the different devices semantically in three steps , without human intervention . furthermore , it provides user - friendly explanations in a well - designed structure to specify possible reasons for the implicit interference problems . our experiment on 11 859 ifttt - style rules shows that a3id outperforms state - of - the - art methods by more than 33 % in the f1 - score for the detection of implicit interference . moreover , evaluations on an extended data set for devices from conceptnet ( a knowledge graph ) and five smart home systems suggest that a3id also has favorable performance with other devices not limited to the smart home domain . 

solution of complex optimal control problem with time varying parameters and bound state constraints
in the work , a new integrated methodology for the solution of dynamic optimization problem with time varying parameters and bound state constraints was proposed . the dynamic optimization problem was fully discretized with simultaneous method through finite element collocation , and was transferred into nonlinear programing problem . then with the new predicted profile of time varying parameters and new monitored model structure coefficient in the left time horizon , the dynamic optimization problem was repeatedly solved in the whole time interval based on simulation technique , which was carried out one element by one element and one sub - interval by one sub - interval to obtain good initial guess for nlp solver . case study of heat pump heating system shows that the proposed methodology can solve this kind of dynamic problem smoothly ; better optimal results can be achieved and at the same time , the final state constraints can also be satisfied . 

p - swish : activation function with learnable parameters based on swish activation function in deep learning
in order to improve the performance of a deep neural network , the activation function is an important aspect that we must research continuously , that is why we have expanded the research in this direction . we introduced a novel p - swish activation function ( parametric swish) , which is able to bring performance improvements on object classification tasks using datasets such as cifar - 10 , cifar - 100 , but we will see that we also used datasets for natural language processing ( nlp) . to test it , we used several types of architectures , including lenet - 5 , network in network ( nin) , and resnet34 compared to popular activation functions such as sigmoid , relu , swish , and our proposals . in particular , the p - swish function facilitates fast network training , which makes it suitable for the transfer learning technique . 

using the wiktionary graph structure for synonym detection
this paper presents our work on using the graph structure of wiktionary for synonym detection . we implement semantic relatedness metrics using both a direct measure of information flow on the graph and a comparison of the list of vertices found to be " close " to a given vertex . our algorithms , evaluated on esl 50 , toefl 80 and rdwp 300 data sets , perform better than or comparable to existing semantic relatedness measures . 

integrating high precision rules with statistical sequence classifiers for accuracy and speed
integrating rules and statistical systems is a challenge often faced by natural language processing system builders . a common subclass is integrating high precision rules with a markov statistical sequence classifier . in this paper we suggest that using such rules to constrain the sequence classifier decoder results in superior accuracy and efficiency . in a case study of a named entity tagging system , we provide evidence that this method of combination does prove efficient than other methods . the accuracy was the same . 

dynamic optimal reactive power flow
an efficient method for minimization of energy loss over time is presented . the proposed method uses different loading conditions during a given future time interval instead of one single snapshot of the network . the method finds the optimal conditions during the given interval . the given interval is divided into several shorter periods . by increasing the number of periods or load profiles , the dimension of the problem will rise substantially . this problem is handled by using the generalized bender decomposition ( gbd ) technique . with this technique , the loading condition for each period are solved in a separate nlp subproblem . the results of the nlp subproblems are coordinated in a master problem . as shown in simulation results , the proposed method not only improves the voltage profile , but it also decreases the total energy loss over the given interval . 

chinese query expansion based on topic - relevant terms
in this paper we present a chinese query expansion model based on topic - relevant terms which were acquired from the google search engine automatically . in contrast to earlier methods , our queries are expanded by adding those terms that are most relevant to the concept of the query , rather than selecting terms that are relevant to the query terms . firstly , we use automatically extracted short terms from document sets to build indexes and use the short terms in both the query and documents to do initial retrieval . next , we acquire the topic - relevant terms of the short terms from the internet and the top 30 initial retrieval documents . finally , we use the topic - relevant terms to do query expansion . the experiments show that our query expansion model is more effective than the standard rocchio expansion . 

building effective queries in natural language information retrieval
in this paper we report on our natural language information retrieval ( nlir ) project as related to the recently concluded 5th text retrieval conference ( trec - 5 ) . the main thrust of this project is to use natural language processing techniques to enhance the effectiveness of full - text document retrieval . one of our goals was to demonstrate that robust if relatively shallow nlp can help to derive a better representation of text documents for statistical search . recently , we have turned our attention away from text representation issues and more towards query development problems . while our nlir system still performs extensive natural language processing in order to extract phrasal and other indexing terms , our focus has shifted to the problems of building effective search queries . specifically , we are interested in query construction that uses words , sentences , and entire passages to expand initial topic specifications in an attempt to cover their various angles , aspects and contexts . based on our earlier results indicating that nlp is more effective with long , descriptive queries , we allowed for long passages from related documents to be liberally imported into the queries . this method appears to have produced a dramatic improvement in the performance of two different statistical search engines that we tested ( cornell ' s smart and nist ' s prise ) boosting the average precision by at least 40% . in this paper we discuss both manual and automatic procedures for query expansion within a new stream - based information retrieval model . 

a preliminary study on fundamental thai nlp tasks for user - generated web content
existing literature on thai nlp often focuses on formally written texts with near - perfect spellings and boundaries between words or sentences . such assumptions , however , do not hold in real - world nlp tasks , especially when dealing with user - generated web content ( ugwc ) . so far , existing nlp research works on actual web data have been limited , making it unclear whether and how existing techniques can be applicable to ugwc . in this paper , several basic thai nlp algorithms ( word segmentation , sentence segmentation , word error detection , word variant detection , name entity recognition ) are re - investigated and benchmarked against real - world , practical ugwc data set . the difference in performance between our data set and others are compared as a guidance for future research . our baseline sentence segmentation on ugwc data set yields an average f - measure of 0 . 77 . for name entity recognition and word variant / error detection tasks , our system yields the accuracy of 0 . 93 and 0 . 53 , respectively . 

towards transforming user requirements to test cases using mde and nlp
the behavior , attributes and properties of a software system is represented in a set of requirements that are written in structured natural language and are usually ambiguous . in large development projects , different modeling techniques are used to create and manage these requirements which aid in the analysis of the problem domain . requirements are later used in the development process to create test cases , which is still mainly a manual process . to automate this process , we plan to use several of the techniques used in model - driven software development and natural language processing ( nlp) . the approach under consideration is to use a model - to - model transformation to convert requirements into test cases with the support of stanford corenlp techniques . key to this transformation process is the use of meta - modeling for requirements and test cases . in this paper we focus on creating a comprehensive meta - model for requirements that can represent both use cases and user stories and performing preliminary analysis of the requirements using nlp . in later work we will develop a set of transformation rules to convert requirements into partial test cases . to show the feasibility of our approach we develop a prototype that can accept a cross - section of requirements written as both use cases and user stories . 

linguistic fundamentals for natural language processing : 100 essentials from morphology and syntax
book abstract : many nlp tasks have at their core a subtask of extracting the dependencies — who did what to whom — from natural language sentences . this task can be understood as the inverse of the problem solved in different ways by diverse human languages , namely , how to indicate the relationship between different parts of a sentence . understanding how languages solve the problem can be extremely useful in both feature design and error analysis in the application of machine learning to nlp . likewise , understanding cross - linguistic variation can be important for the design of mt systems and other multilingual applications . the purpose of this book is to present in a succinct and accessible fashion information about the morphological and syntactic structure of human languages that can be useful in creating more linguistically sophisticated , more language - independent , and thus more successful nlp systems . table of contents : acknowledgments / introduction / motivation / morphology : introduction / morphophonology / morphosyntax / syntax : introduction / parts of speech / heads , arguments , and adjuncts / argument types and grammatical functions / mismatches between syntactic position and semantic roles / resources / bibliography / author ' s biography / general index / index of languagesview less

exploration of quantitative factors affecting the popularity of users in an online community
online communities for discussing different topics have been around since the dawn of the internet . whether an individual user is popular or not in the community is highly subjective and difficult to measure with a quantitative approach . in this paper we explore factors and usage patterns on a finnish football related online community futisforum2 for receiving votes in a yearly voting for forum user of the year . several quantitative factors are identified in order to calculate correlations between each factor and the yearly voting results . these factors include yearly message network centrality figures , number of message , user quote amounts , number of characters in messages , etc . although message amounts clearly correlate with the voting results , the strongest correlation was noticed when comparing eigenvector centrality to the voting results . the main outcome of the project was also generation of a database of 11 million messages for further research . 

fednlp : an interpretable nlp system to decode federal reserve communications
the federal reserve system ( the fed ) plays a significant role in affecting monetary policy and financial conditions worldwide . although it is important to analyse the fed ' s communications to extract useful information , it is generally long - form and complex due to the ambiguous and esoteric nature of content . in this paper , we present fednlp , an interpretable multi - component natural language processing ( nlp ) system to decode federal reserve communications . this system is designed for end - users to explore how nlp techniques can assist their holistic understanding of the fed ' s communications with no coding . behind the scenes , fednlp uses multiple nlp models from traditional machine learning algorithms to deep neural network architectures in each downstream task . the demonstration shows multiple results at once including sentiment analysis , summary of the document , prediction of the federal funds rate movement and visualization for interpreting the prediction model ' s result . our application system and demonstration are available at https :// fednlp . net . 

supervised bilingual word embeddings for low - resource language pairs : myanmar and thai
bilingual word embeddings ( bwes ) represent the lexicons of two different languages in a shared embedding space , which are useful for cross - lingual natural language processing ( nlp ) tasks . in particular , bilingual word embeddings are extremely useful for machine translation of low - resource languages due to the rare availability of parallel corpus for that languages . most of the researchers have already learned bilingual word embeddings for high - resource language pairs . to the best of our knowledge , there are no studies on bilingual word embeddings for low resource language pairs , myanmar - thai and myanmar - english . in this paper , we present and evaluate the bilingual word embeddings for myanmar - thai , myanmar - english , thai - english , and english - thai language pairs . to train bilingual word embeddings for each language pair , firstly , we used monolingual corpora for constructing monolingual word embeddings . a bilingual dictionary was also utilized to alleviate the problem of learning bilingual mappings as a supervised machine learning task , where a vector space is first learned independently on a monolingual corpus . then , a linear alignment strategy is used to map the monolingual embeddings to a common bilingual vector space . either word2vec or fasttext model was used to construct monolingual word embeddings . we used bilingual dictionary induction as the intrinsic testbed for evaluating the quality of cross - lingual mappings from our constructed bilingual word embeddings . for all low - resource language pairs , monolingual word2vec embedding models with the csls metric achieved the best coverage and accuracy . 

business decisions support using sentiment analysis in crm systems
this work tackles the problem of sentiment analysis in customer relationship management ( crm ) systems . we present two different approaches , one is based on natural language processing ( nlp ) algorithms , and the other approach is based on machine learning probabilistic classification . for experimental results , we used a dataset of online customers reviews on a product , to simulate a crm system . the machine learning model showed a better overall classification results than the nlp - based approach . but through the nlp - based approach we were able to extract the list of product ' s aspects . 

stopping criteria for active learning of named entity recognition
active learning is a proven method for reducing the cost of creating the training sets that are necessary for statistical nlp . however , there has been little work on stopping criteria for active learning . an operational stopping criterion is necessary to be able to use active learning in nlp applications . we investigate three different stopping criteria for active learning of named entity recognition ( ner ) and show that one of them , gradient - based stopping , ( i ) reliably stops active learning , ( ii ) achieves nearoptimal ner performance , ( iii ) and needs only about 20 % as much training data as exhaustive labeling . 

eager : extending automatically gazetteers for entity recognition
key to named entity recognition , the manual gazetteering of entity lists is a costly , errorprone process that often yields results that are incomplete and suffer from sampling bias . exploiting current sources of structured information , we propose a novel method for extending minimal seed lists into complete gazetteers . like previous approaches , we value wikipedia as a huge , well - curated , and relatively unbiased source of entities . however , in contrast to previous work , we exploit not only its content , but also its structure , as exposed in dbpedia . we extend gazetteers through wikipedia categories , carefully limiting the impact of noisy categorizations . the resulting gazetteers easily outperform previous approaches on named entity recognition . 

requirement analysis of the internal modules of natural language processing engines
this paper describesr the modules of natural language processing ( nlp ) engine which can be used with hungarian input . there are many standard nlp engines which have tokenization , part - of - speech ( pos ) tagging , named entity recognition , parsing modules . most of them work for universal languages like english . processing of hungarian language is a much more difficult and there cannot be found such a complete nlp system which satisfies all tasks for syntactic and semantic analysis of incoming inputs . this paper summarizes the existing solutions and techniques and gives a brief description of a proposed nlp engine using for inputs in hungarian language . 

3dof ascent phase trajectory optimization for aircraft based on adaptive gauss pseudospectral method
3dof ascent phase trajectory optimization problem for minimum fuel - to - climb is investigated in the research . the gauss pseudospectral method ( gpm ) and adaptive strategy is proposed to transcribe the trajectory optimization problem into a nonlinear program problem ( nlp ) . then , the sequential quadratic programming ( sqp ) integrated in a sparse nonlinear program solver named snopt is used to solve the resulting nlp problem with a proper initial guess . the optimality of the ascent trajectory is also checked via bellman ' s principle . the simulation results demonstrate that the method can generate a feasible 3dof ascent trajectory with all constraints satisfied . 

heterogeneous parsing via collaborative decoding
there often exist multiple corpora for the same natural language processing ( nlp ) tasks . however , such corpora are generally used independently due to distinctions in annotation standards . for the purpose of full use of readily available human annotations , it is significant to simultaneously utilize multiple corpora of different annotation standards . in this paper , we focus on the challenge of constituent syntactic parsing with treebanks of different annotations and propose a collaborative decoding ( or co - decoding ) approach to improve parsing accuracy by leveraging bracket structure consensus between multiple parsing decoders trained on individual treebanks . experimental results show the effectiveness of the proposed approach , which outperforms state - of - the - art baselines , especially on long sentences . 

bilstm - autoencoder architecture for stance prediction
the recent surge in the abundance of fake news appearing on social media and news websites poses a potential threat to high - quality journalism . misinformation hurts people , society , science , and democracy . this reason has led many researchers to develop techniques to identify fake news . in this paper , we discuss a stance prediction technique using the deep learning approach , which can be used as a factor to determine the authenticity of news articles . the fake news stance prediction is the process of automatically classifying the stance of a news article towards a target into one of the following classes : agree , disagree , discuss , unrelated . the stance prediction task ' s input is the news articles containing a pair : a headline as the target and a body as a claim . this paper proposes a deep learning architecture using bi - directional long short term memory and autoencoder for stance prediction . we illustrate , through empirical studies , that the method is reasonably accurate at predicting stance , achieving a classification accuracy as high as 94% . the proposed stance detection method would be useful for assessing the credibility of news articles . 

classification benchmarks for under - resourced bengali language based on multichannel convolutional - lstm network
exponential growths of social media and micro - blogging sites not only provide platforms for empowering freedom of expressions and individual voices , but also enables people to express anti - social behavior like online harassment , cyberbul - lying , and hate speech . numerous works have been proposed to utilize these data for social and anti - social behavior analysis , document characterization , and sentiment analysis by predicting the contexts mostly for highly resourced languages like english . however , some languages are under - resources , e . g . , south asian languages like bengali , tamil , assamese , malayalam , that lack of computational resources for natural language processing . in this paper 1 , we provide several classification benchmarks for bengali , an under - resourced language . we prepared three datasets of expressing hate , commonly used topics , and opinions for hate speech detection , document classification , and sentiment analysis . we built the largest bengali word embedding models to date based on 250 million articles , which we call bengfasttext . we perform three experiments , covering document classification , sentiment analysis , and hate speech detection . we incorporate word embeddings into a multichannel convolutional - lstm ( mc - lstm ) network for predicting different types of hate speech , document classification , and sentiment analysis . experiments demonstrate that bengfasttext can capture the semantics of words from respective contexts correctly . evaluations against several baseline embedding models , e . g . , word2vec and glove yield up to 92 . 30 % , 82 . 25 % , and 90 . 45 % f1 - scores in case of document classification , sentiment analysis , and hate speech detection , respectively during 5 - fold cross - validation tests . 

multimodal prediction of the audience ' s impression in political debates
debates are popular among politicians , journalists and scholars because they are a useful way to foster discussion and argumentation about relevant matters . in these discussions , people try to give a good impression ( the immediate effect produced in the mind after a stimulus ) by showcasing good skills in oratory and argumentation . we investigate this issue by using data gathered from an audience watching a national election debate and measuring the impression that politicians were giving during it . we then model a multimodal approach for automatically predicting their impression and analyze what modalities are the most important ones for this task . our results show that the vision modality brings the best results and that fusing modalities at the feature - level is beneficial depending on the setup . the dataset is made publicly available to the community for further research on this topic . 

sm6 : a 16nm system - on - chip for accurate and noise - robust attention - based nlp applications : the 33rd hot chips symposium – august 22 - 24 , 2021
in this work , we present sm6 , an soc architecture for real - time denoised speech and nlp pipelines , featuring ( 1 ) msse : an unsupervised probabilistic sound source separation accelerator , ( 2 ) flexnlp : a programmable inference accelerator for attention - based seq2seq dnns using adaptive floating - point datatypes for wide dynamic range computations , ( 3 ) a dual - core arm cortex a53 cpu cluster , which provides on - demand simd fft processing , and operating system support . in adverse acoustic conditions , msse allows flexnlp to store up to 6x smaller asr models obviating the very inefficient strategy of scaling up the dnn model to achieve noise robustness . msse and flexnlp produce efficiency ranges of 4 . 33 - 17 . 6 gsamples / s / w and 2 . 6 - 7 . 8tflops / w , respectively , with per - frame end - to - end latencies of 15 - 45ms . 

method of enriching queries by contextual information to approve of information retrieval system in arabic
in this paper , we propose a method is to improve the performance of information retrieval systems ( irs ) by increasing the selectivity of relevant documents on the web . indeed , a significant number of relevant documents on the web are not returned by an irs ( specifically a search engine ) , because of the richness of natural language arabics . for this purpose the search engine does not reach high performance and does not meet the needs of users . to remedy this problem , we propose a method of enrichment of the query . this method relies on many steps . first , identification of significant terms ( simple and composed ) present in the query . then , generation of a descriptive list and its assignment to each term that has been identified as significant in the query . a descriptive list is a set of linguistic knowledge of different types ( morphological , syntactic and semantic ) . in this paper we are interested in the statistical treatment , based on the similarity method . this method exploits the weighting functions of salton tf - idf and tf - ief on the list generated in the previous step . tf - idf function identifies relevant documents , while the tf - ief ' s role is to identify the relevant sentence . the terms of high weight ( which are terms which may be correlated to the context of the response ) are incorporated into the original query . the application of this method is based on a corpus of documents belonging to a closed domain . 

a smart campus system based on intention recognition and internet of things
improving the efficiency and experience of information query and teaching management through natural language processing ( nlp ) is a rising trend of the next - generation smart campus . to support campus information interaction and classroom management , the author developed the campus information interaction and classroom teaching support system based on nlp , face recognition , and internet of things ( iot ) . the author also innovatively developed the " intention recognition algorithm based on sentence structure and text similarity " , which realized accurate identification of user ' s intention in campus scenarios . in this system , nlp is combined with the campus - specific application situations . only one voice or text command can make the required information automatically and quickly pushed to the user ' s eyes . moreover , only one voice or text command is needed to enable the system to quickly and automatically complete the equipment management task and realize " what you say is what you get " . by improving the efficiency and experience of information query and teaching management in the campus scenarios , this system is a beneficial exploration on the development direction of the next generation smart campus . 

an experiment in unifying audio - visual and textual infrastructures for language processing research and development
this paper describes an experimental integration of two infrastructures ( eudico and gate ) which were developed independently of each other ; for different media ( video / speech vs . text ) and applications . the integration resulted into gaining an in - depth understanding of the functionality and operation of each of the two systems in isolation , and the benefits of their combined use . it also highlighted some issues ( e . g . , distributed access ) which need to be addressed in future work . the experiment also showed clearly the advantages of modularity and generality adopted in both systems . 

robust speech recognition technology program summary
the objective of this program is to develop and demonstrate robust , high - performance continuous speech recognizer ( csr ) techniques and systems focussed on application in spoken language systems ( sls ) . the techniques are based on a continuous - observation hidden markov model ( hmm ) approach , which has previously demonstrated high performance for normal speech and robustness for stressed speech . the motivation is that current state - of - the - art csr systems must be improved in performance and robustness for advanced sls environments , with variabilities including those due to spontaneous speech , noise , and task - induced stress . the focus of the robust csr techniques on sls applications is being facilitated by development and implementation of a well - structured interface between a csr and a natural language processor ( nlp ) , allowing collaboration with other groups developing nlps for sls applications . 

towards grammar checker development for persian language
with improvements in industry and information technology , large volumes of electronic texts such as newspapers , emails , weblogs , books and thesis are produced daily . producing electrical documents has considerable benefits such as easy organizing and data management . therefore , existence of automatic systems such as spell and grammar checker / corrector can help in reducing costs and increasing the electronic texts and it will improve the quality of electronic texts . you can input your text and the computer program will point out to you the spelling errors . it may also help with your grammar . grammatical errors are described as wrong relation between words like subject - verb disagreement or wrong sequence of words like using plural noun where a single noun is needed . grammar checking phase starts after spell checking is finished . this paper briefly describes the concepts and definition of grammar checkers in general followed by developing the first persian ( farsi ) grammar checker leading to an overview of the error types of persian language . the proposed system detects and corrects about 20 frequent persian grammar errors and tested on a sample dataset , retrieved about 70 % and 83 % accuracy respect to precision and recall metrics . 

conceptual schema extraction using pos annotations and weighted edit distance algorithm
database design process involves analysis of system requirements described in natural language for manual extraction of conceptual schema . this is a tedious process and prone to human error . earlier approaches of automation of this process had made use of either a finite set of rules , context free grammars ( cfg ) or semantic understanding . rule and cfg based approaches are not robust enough to cover all possible scenarios ; whereas semantic approaches are not generic and have domain dependencies . we have defined an approach where the sequence part - of - speech ( pos ) tags of a sentence are annotated to er components and a set of such annotated sentences serves as our corpus . use of pos tags instead of the actual terms in a sentence makes the approach more robust , generic and domain - independent . we have also defined our own algorithm , which takes an input sentence and uses an extension of edit distance technique , to find out similar matches for a pos sequence of an input sentence with an associated cost , if a perfect match is not found . the accuracy of our current system is 54% . the feedback provided by the user is used to update the underlying model making the approach more interactive and improving the accuracy of future predictions . 

sentiment analysis of amazon fine food reviews
the modernized world with recent inventions in technology seen nowadays has become more digitized . by making the products available online , e - commerce is taking advantage of this digitized world by making the customers get whatever they want without stepping out . the importance of the online review has become higher these days because the number of people depending on e - commerce websites for purchasing things have increased . as people believe in other opinions , going through lots of reviews before buying a product has become a common thing . for a better understanding of the product , in this busy world people don ' t have time to go through lots of , so , there is a need for developing a model which can polarize those reviews and generate an appropriate result . with the advancement in the area of machine learning and different technology , this task has become much more comfortable . for this project , machine learning algorithms are used on amazon fine food reviews dataset to analyze if the given review is a positive review or a negative review . 

quantifying the impact of complementary visual and textual cues under image captioning
describing an image with natural sentence without human involvement requires knowledge of both image processing and natural language processing ( nlp ) . most of the existing works are based on unimodal representations of the visual and textual contents using an encoder - decoder ( endec ) deep neural network ( dnn ) , where the input images are encoded using convolutional neural network ( cnn ) and the caption is generated by a recurrent neural network ( rnn ) . this paper dives into a basic image captioning model to quantify the impact of multimodal representation of the visual and textual cues . the multimodal representation is carried out via an early fusion of encoded visual cues from different cnns , along with combined textual features from different word embedding techniques . the resultant of the multimodal representation of the visual and textual cues are employed to train a long short - term memory ( lstm ) - based baseline caption generator to quantify the impact of various levels of complementary feature mutations . the ablation study involves two different cnn feature extractors and two types of textual feature extractors , shows that exploitation of the complementary information outperforms the unimodal representations significantly with endurable timing overhead . 

curation technologies for cultural heritage archives : analysing and transforming a heterogeneous data set into an interactive curation workbench
we present a platform that enables the semantic analysis , enrichment , visualisation and presentation of a document collection in a way that enables human users to intuitively interact and explore the collection , in short , a curation platform or workbench . the data set used is the result of a research project , carried out by scholars from south korea , in which official german government documents on the german re - unification were collected , intellectually curated , analysed , interpreted and published in multiple volumes . the documents we worked with are mostly in german , a small subset , mostly summaries , is in korean . this paper describes the original research project that generated the data set and focuses upon a description of the platform and natural language processing ( nlp ) pipeline adapted and extended for this project ( e . g . , ocr was added ) . our key objective is to develop an interactive curation workbench that enables users to interact with the data set in several different ways that go beyond the current version of the published document collection as a set of pdf documents that are available online . the paper concludes with suggestions regarding the improvement of the platform and future work . 

mamabot : a system based on ml and nlp for supporting women and families during pregnancy
artificial intelligence is transforming healthcare with a profound paradigm shift impacting diagnostic techniques , drug discovery , health analytics , interventions and much more . in this paper we focus on exploiting ai - based chatbot systems , mainly based on machine learning algorithms and natural language processing , to understand and respond to needs of patients and their families . in particular , we describe an application scenario for an ai - chatbot delivering support to pregnant women , mothers , and families with young children , by giving them help and instructions in relevant situations . 

the pause duration prediction for mandarin text - to - speech system
in this paper , we enter into detailed analysis on how the pause duration under different prosodic boundaries are affected by various contextual factors in natural speech . to get the correlation between them , the paper calculates the mean pause duration under different prosodic boundaries . the contextual factors investigated in this paper contains both linguistic features , such as boundary types , syllable tones of boundary sides , initial and final types etc , and acoustic features , such as pitch gap across the boundary . the paper makes experiments and discussion which reveals the influence of these factors on pause duration . based on that , the paper creates a pause duration prediction model for mandarin speech synthesis system . the model was proved to be able to generate high quality prosody output with the listening test . 

reinforced nmt for sentiment and content preservation in low - resource scenario
the preservation of domain knowledge from source to the target is crucial in any translation workflows . hence , translation service providers that use machine translation ( mt ) in production could reasonably expect that the translation process should transfer both the underlying pragmatics and the semantics of the source - side sentences into the target language . however , recent studies suggest that the mt systems often fail to preserve such crucial information ( e . g . , sentiment , emotion , gender traits ) embedded in the source text in the target . in this context , the raw automatic translations are often directly fed to other natural language processing ( nlp ) applications ( e . g . , sentiment classifier ) in a cross - lingual platform . hence , the loss of such crucial information during the translation could negatively affect the performance of such downstream nlp tasks that heavily rely on the output of the mt systems . in our current research , we carefully balance both the sides ( i . e . , sentiment and semantics ) during translation , by controlling a global - attention - based neural mt ( nmt ) , to generate translations that encode the underlying sentiment of a source sentence while preserving its non - opinionated semantic content . toward this , we use a state - of - the - art reinforcement learning method , namely , actor - critic , that includes a novel reward combination module , to fine - tune the nmt system so that it learns to generate translations that are best suited for a downstream task , viz . sentiment classification while ensuring the source - side semantics is intact in the process . experimental results for hindi – english language pair show that our proposed method significantly improves the performance of the sentiment classifier and alongside results in an improved nmt system . 

feeding owl : extracting and representing the content of pathology reports
this paper reports on an ongoing project that combines nlp with semantic web technologies to support a content - based storage and retrieval of medical pathology reports . we describe the nlp component of the project ( a robust parser ) and the background knowledge component ( a domain ontology represented in owl ) , and how they work together during extraction of domain specific information from natural language reports . the system provides a good example of how nlp techniques can be used to populate the semantic web . 

demonstration of the generic software framework for the integrated intelligent computer - assisted language learning ( iicall ) environment
the integrated intelligent computer - assisted language learning ( iicall ) environment enables language learning within common working environment like web browsers or e - mail clients . based on a generic data model , we developed a software framework for generic and transparent information exchange within those environments . the applicability of both is proven empirically by implementation of a corresponding prototype . the software framework constitutes a major improvement compared to current research . it enables a decoupled realization of individual systems within an iicall environment , while still ensuring interoperability during runtime . 

application of pre - training models in named entity recognition
named entity recognition ( ner ) is a fundamental natural language processing ( nlp ) task to extract entities from unstructured data . the previous methods for ner were based on machine learning or deep learning . recently , pre - training models have significantly improved performance on multiple nlp tasks . in this paper , firstly , we introduce the architecture and pre - training tasks of four common pre - training models : bert , ernie , ernie2 . 0 - tiny , and roberta . then , we apply these pre - training models to a ner task by fine - tuning , and compare the effects of the different model architecture and pre - training tasks on the ner task . the experiment results showed that roberta achieved state - of - the - art results on the msra - 2006 dataset . 

dynamic embedding projection - gated convolutional neural networks for text classification
text classification is a fundamental and important area of natural language processing for assigning a text into at least one predefined tag or category according to its content . most of the advanced systems are either too simple to get high accuracy or centered on using complex structures to capture the genuinely required category information , which requires long time to converge during their training stage . in order to address such challenging issues , we propose a dynamic embedding projection - gated convolutional neural network ( dep - cnn ) for multi - class and multi - label text classification . its dynamic embedding projection gate ( depg ) transforms and carries word information by using gating units and shortcut connections to control how much context information is incorporated into each specific position of a word - embedding matrix in a text . to our knowledge , we are the first to apply depg over a word - embedding matrix . the experimental results on four known benchmark datasets display that dep - cnn outperforms its recent peers . 

the evaluation of thai poem ' s content consistency using siamese network
many research describes textual entailment model for compare pair of the sentence but two sentences in term of the poem content consistency are not the same . the content consistency is very important for storytelling in thai poem composing . in this article , we propose the model and result of the evaluation of thai poem ' s content consistency using the siamese network 3 models comprise 1 ) merge vector model 2 ) siamese absolute different model and 3 ) siamese dot vector model compare with the basic cnn model . the training data is thai poem 14 , 173 pair ( batt ) and validation data is thai poem 3 , 544 pair . all models learn by apply one shot learning technic . the accuracy of siamese absolute different model near 100% . the macro average of f1 - score shows 99 . 27% . the area under curve shows 0 . 997 near the perfect value . 

implementation approach of indian language gujarati grammar ' s concept “ sandhi ” using the concepts of rule - based nlp
the term ` language ' in nlp has to be understood as natural languages like gujarati , hindi , english etc . , which we use in daily life to communicate . most of the nlp research has been centered on english & other european languages . nlp research concerning the indian language like gujarati is commenced in the last few years . the centre of attention of this paper is to demonstrate the road map of implementation of gujarati grammar ' s concept “ sandhi ” . in our words sandhi is a word segmentation process & it is present in most of the south asian language , such as devnagri , sanskrit , hindi , and gujarati & even in chinese & thai languages . ” sandhi leads to phonetic transformation at word boundaries of a written chunk ( small part ) , and the sounds at the end of word join together to form a single chunk of the character sequence . ” our main spotlight is on rule - based implementation of “ sandhi” . similar to every indian scripting language gujarati language ( grammar ) also has its own specified rules of composition for combining the consonants , vowels and modifiers . we have identified certain rules by which we accomplish the practical implementation of “ sandhi ” . there are many sandhi rules available , each denoting a unique combination of phonetic transformations , documented in the grammatical tradition of gujarati . the sandhi does not make any syntactic or semantic changes to the words implicated . sandhi is an elective operation that depends only on the alertness of the writer . 

comparative study for stylometric analysis techniques for authorship attribution
a text is a meaningful source of information . capturing the right patterns in written text gives metrics to measure and infer to what extent this text belongs or is relevant to a specific author . this research aims to introduce a new feature that goes more in deep in the language structure . the feature introduced is based on an attempt to differentiate stylistic changes among authors according to the different sentence structure each author uses . the study showed the effect of introducing this new feature to machine learning models to enhance their performance . it was found that the prediction of authors was enhanced by adding sentence structure as an additional feature as the f1 _ scores increased by 0 . 3 % and when normalizing the data and adding the feature it increased by 5 % . 

standardized gui framework using python for speech processing : nlp
speech processing embeds the recording of speech which is a huge container of private , confidential and business records , used for a wide variety of applications like health care , customer services and individual identification . research on speech processing mandates recording , storing , playing and analyzing a wide variety of spoken languages specifically for indian context . the recording may consist of single , multiple or cross languages . to obtain clear speech , noise removal and mass storage devices are necessary to store both original file and processed file . this paper describes a standardized graphical user interface framework for speech processing which can store and play speech of user specified length . the upper limit length of speech is restricted by the permanent storage facility . the method utilizes the operating system functionality to activate the microphone for recording , hard disk for storage and activation of a speaker for voice output . 

cognitive natural language search using calibrated quantum mesh
this paper describes the application of a search system for helping users find the most relevant answers to their questions from a set of documents . the system is developed based on a new algorithm for natural language understanding ( nlu ) called calibrated quantum mesh ( cqm ) . cqm finds the right answers instead of documents . it also has the potential to resolve confusing and ambiguous cases by mimicking the way a human brain functions . the method has been evaluated on a set of queries provided by users . the relevant answers given by the coseer search system have been judged by three human judges as well as compared to the answers given by a reliable answering system called askcfpb . coseer performed better in 57 . 0 % of cases , and worse in 16 . 5 % cases , while the results were comparable to askcfpb in 26 . 6 % of cases . the usefulness of a cognitive computing system over a microsoft - powered key - word based search system is discussed . this is a small step toward enabling artificial intelligence to interact with users in a natural manner like in an intelligent chatbot . 

neural models of text normalization for speech applications
machine learning , including neural network techniques , have been applied to virtually every domain in natural language processing . one problem that has been somewhat resistant to effective machine learning solutions is text normalization for speech applications such as text - to - speech synthesis tts . in this application , one must decide , for example , that 123 is verbalized as one hundred twenty three in 123 pages but as one twenty three in 123 king ave . for this task , state - of - the - art industrial systems depend heavily on hand - written language - specific grammars . we propose neural network models that treat text normalization for tts as a sequence - to - sequence problem , in which the input is a text token in context , and the output is the verbalization of that token . we find that the most effective model , in accuracy and efficiency , is one where the sentential context is computed once and the results of that computation are combined with the computation of each token in sequence to compute the verbalization . this model allows for a great deal of flexibility in terms of representing the context , and also allows us to integrate tagging and segmentation into the process . these models perform very well overall , but occasionally they will predict wildly inappropriate verbalizations , such as reading 3 cm as three kilometers . although rare , such verbalizations are a major issue for tts applications . we thus use finite - state covering grammars to guide the neural models , either during training and decoding , or just during decoding , away from such " unrecoverable " errors . such grammars can largely be learned from data . 

twitter as a multilingual source of comparable corpora
this article describes a new method to build comparable corpora from twitter . our strategy relies on the fact that twitter is one of the most popular online social microblog allowing large audiences to express their thoughts and reactions about specific events or breaking news in various languages . given two languages and a particular topic , we propose the exploitation of tweets in the two selected languages whose content is focused on the selected topic from the microblog twitter in order to construct a comparable corpus . 

chinese typed collocation extraction using corpus - based syntactic collocation patterns
collocations play significant role in many application and extraction them automatically is useful in nlp . syntactic - based phrase patterns used in collocation extraction have brought advantages due to the well - formedness of results and automatically classifying the candidates into syntactically congeneric categories . however , due to the language independency , the arbitrary choice of syntactic patterns for target collocations brings drawbacks for evaluation as well as adaptation for a new language . this work presents a corpus - driven framework to generate collocation templates for nouns and verbs phrase at first and then integrate them with statistical association measures for noun / verb phrase collocation extraction , namely typed collocation extraction . the experiment results show a higher average precision of 84 . 80 % and a so called local recall value of 55 . 99 % based on a randomly selected noun and verb headwords . 

energy - aware scheduling of periodic conditional task graphs on mpsocs
we investigate the problem of scheduling a set of periodic conditional task graphs of non - preemtible tasks on a mpsocs ( multi processor system - on - chip ) with shared memory such that the total expected processor energy consumption of the tasks in each scenario is minimized under two power models , namely dynamic and static power model , and propose a novel offline scheduling approach . our approach consists of a novel two - phase task scheduler that aims at minimizing total worst - case utilization of each processor and an optimal task execution speed selection algorithm using convex nlp ( non - linear programming ) . furthermore , we propose an o ( 1 ) time online dvs ( dynamic voltage scaling ) heuristic that assigns each task a speed online . our experimental results show that our two - phase scheduler achieves a 95 . 2 % success rate of constructing a feasible schedule , compared to a 42 % success rate of the state - of - the - art . for energy saving , our offline scheduling approach achieves an average improvement , a maximum improvement and a minimum improvement of 8 . 03 % , 14 . 08 % , and 4 . 8 % , respectively over our online dvs heuristic . 

verb - particle constructions and lexical resources
in this paper we investigate the phenomenon of verb - particle constructions , discussing their characteristics and their availability for use with nlp systems . we concentrate in particular on the coverage provided by some electronic resources . given the constantly growing number of verb - particle combinations , possible ways of extending the coverage of the available resources are investigated , taking into account regular patterns found in some productive combinations of verbs and particles . we discuss , in particular , the use of levin ' s ( 1993 ) classes of verbs as a means to obtain productive verb - particle constructions , and discuss the issues involved in adopting such an approach . 

robust machine learning systems : reliability and security for deep neural networks
machine learning is commonly being used in almost all the areas that involve advanced data analytics and intelligent control . from applications like natural language processing ( nlp ) to autonomous driving are based upon machine learning algorithms . an increasing trend is observed in the use of deep neural networks ( dnns ) for such applications . while the slight inaccuracy in applications like nlp does not have any severe consequences , it is not the same for other safety - critical applications , like autonomous driving and smart healthcare , where a small error can lead to catastrophic effects . apart from high - accuracy dnn algorithms , there is a significant need for robust machine learning systems and hardware architectures that can generate reliable and trustworthy results in the presence of hardware - level faults while also preserving security and privacy . this paper provides an overview of the challenges being faced in ensuring reliable and secure execution of dnns . to address the challenges , we present several techniques for analyzing and mitigating the reliability and security threats in machine learning systems . 

predicting customer churn in the telecommunication industry by analyzing phone call transcripts with convolutional neural networks
for telecommunication service providers , a principle method for reducing costs and generating revenue is to focus on retaining existing customers rather than acquiring new customers . to support this strategy , it is important to understand customer concerns as early as possible to prevent churn : the customer action of canceling a subscription and moving to a new provider . in this paper , we use actual customer phone call data and develop the convolutional neural network ( cnn ) - based predictive model to detect churn signals from transcript data of phone calls . experimental results show that when sufficient training data is provided with our text annotation method , our cnn - based predictive model generates state - of - the - art performance in churn prediction . 

anita : intelligent humanoid robot with self - learning capability using indonesian language
research and development on intelligent humanoid robot for general purpose focuses on speech recognition systems to be able to interact with people naturally . in this research , we would like to propose humanoid robot with the self - learning capability based on indonesian language and small vocabularies for accepting and giving response from people based on natural language processing ( nlp) . this kind of robot can be used widely in schools , universities and public services . the humanoid robot should consider the style of questions and conclude the answer through conversation under unreliable automatic speech in noisy environment . in our scenario , robot will detect user ' s face and accept commands from the user to do an action , where the answer from user will be processed to the google translator and the result will be compared with vocabularies defined on the system . we describe how this formulation establish a promising framework by empirical results using subjects randomly . the comparative experiments with samples from user were presented . 

a text analysis of the 2020 u . s . presidential election campaign speeches
campaign speeches provide significant insight into how candidates communicate their message and highlight their priorities to various audiences . this study explores the campaign speeches of donald trump , joseph biden , michael pence , and kamala harris during the 2020 us presidential election using natural language processing ( nlp ) techniques and a novel data pipeline of unstructured automated video captions . the intent of this effort is to evaluate the stylistic elements of the candidate speeches through elements such as formality , repetitiveness , topic variance , sentiment , and vocabulary choice / range to establish how candidates differ in their approaches and what effectively resonates with the voters . the nlp methods used include unsupervised similarity and clustering algorithms . through this work , the results uncovered large stylistic differences amongst the candidates overall ; however , more notably also indicate stark differences between the top and bottom of the republican ticket compared to the democratic ticket . the findings support the idea that the candidate pairs were selected strategically to cover the largest bloc of voters possible as part of the election process . 

universal networking language : a framework for emerging nlp applications
presently there has been a surge for representing information in a common language for use cases like question - answering system , machine translation , text summarization etc . unl has been a centre of attraction for researchers in the past two decades and many have tried to harness its power . this paper throws a glimpse into the strides made by such researchers . as unl provides a language independent platform it eases out the decision making by accessing the valuable and meaningful information , which is otherwise a challenging errand . thus it captures information and finds its major application in natural language processing ( nlp ) domain . the paper is written with the intent to introduce its readers to the unl framework and explain how it is being used to solve some real world problems . 

extracting information on pneumonia in infants using natural language processing of radiology reports
natural language processing ( nlp ) is critical for improvement of the healthcare process because it has the potential to encode the vast amount of clinical data in textual patient reports . many clinical applications require coded data to function appropriately , such as decision support and quality assurance applications . however , in order to be applicable in the clinical domain , performance of the nlp systems must be adequate . a valuable clinical application is the detection of infectious diseases , such as surveillance of healthcare - associated pneumonia in newborns ( e . g . neonates ) because it produces significant rates of morbidity and mortality , and manual surveillance of respiratory infection in these patients is a challenge . studies have already demonstrated that automated surveillance using nlp tools is a useful adjunct to manual clinical management , and is an effective tool for infection control practitioners . this paper presents a study aimed at evaluating the feasibility of an nlp - based electronic clinical monitoring system to identify healthcare - associated pneumonia in neonates . we estimated sensitivity , specificity , and positive predictive value by comparing the detection with clinicians ' judgments and our results demonstrated that the automated method was indeed feasible . sensitivity ( recall ) was 87 . 5% , and specificity ( true negative rates ) was 94 . 1% . 

facts devices allocation via sparse optimization
although there are vast potential locations to install facts devices in a power system , the actual installation number is very limited due to economical consideration . therefore the allocation strategy exhibits strong sparsity . this paper formulates facts device allocation problem as a general sparsity constrained opf problem and employs lq ( 0 <; q <; 1 ) norms to enforce sparsity on facts devices setting values to achieve solutions with desirable device numbers and sites . an algorithm based on alternating direction method of multipliers is proposed to solve the sparsity - constrained opf problem . the algorithm exploits the separability structure and decomposes the original problem into an nlp subproblem , an lq regularization subproblem , and a simple dual variable update step . the nlp subproblem is solved by the interior point method . the lq regularization subproblem has a closed - form solution expressed by shrinkagethreholding operators . the convergence of the proposed method is theoretically analyzed and discussed . the proposed method is successfully tested on allocation of svc , tcsc and tcps on ieee 30 - , 118 - and 300 - bus systems . case studies are presented and discussed for both single - type and multiple - type facts devices allocation problems , which demonstrates the effectiveness and efficiency of the proposed formulation and algorithm . 

a framework to predict social crime through twitter tweets by using machine learning
an increasing amount of data and information coming from social networks that can be used to generate a variety of data patterns for different types of investigation such as human social behavior , system security , criminology etc . a framework is developed to predict major types of social media crimes ( cyber stalking , cyber bullying , cyber hacking , cyber harassment , and cyber scam ) using the data obtained from social media website . the proposed framework is consist of three modules ; data ( tweet ) pre - processing , classifying model builder and prediction . to build the prediction model multinomial na ï ve bayes ( mnb ) , k - nearest neighbors ( knn ) and support vector machine ( svm ) is used that classify given data into different classes of crime . further n - gram language model is used with these machine learning algorithms to identify the best value of n and measure the accuracy of the system at different levels such as unigram , bigram , trigram , and 4 - gram . results shows that all three algorithm attain the precision , recall and f - measure above than 0 . 9 however support vector machine performed slightly better . the proposed system produced better accuracy result as compared to existing network - based feature selection approach . 

research on pre - training of tibetan natural language processing
in the field of natural language processing , pre - training can effectively improve the performance of downstream tasks . in recent years , pre - training has been continuously developed in tibetan nlp . we built three pre - trained models of tibetan word2vec , tibetan elmo , and tibetan albert , and applied them to the two downstream tasks of tibetan text classification and tibetan part - of - speech tagging . comparing them with the baseline models of these two downstream tasks , it is found that the performance of the downstream tasks using the pre - training is significantly better than the baseline model . the three pre - trained models have also brought a gradual improvement in performance for tibetan downstream tasks . 

a clustering retrieval system of chinese information
with tremendous and ever - growing amounts of electronic documents from world wide web and digital libraries , it becomes more and more difficult to get information that people really want . in order to predigest search process , people use clustering method to browse through search results . however traditional chinese information clustering techniques are inadequate since they don ' t generate clusters with highly readable themes . this paper reformats the clustering problem as a salient phrase ranking problem . given a query and its related ranked list of documents ( typically a list of titles and snippets ) returned from a certain web search engine , this method first extracts and ranks salient phrases as candidate cluster theme , based on regression model of svr ( support vector regression ) learned from human labeled training data . the documents are assigned to relevant salient phrases to form candidate clusters , and the final clusters are generated by merging these candidate clusters . this paper also searches for a reasonable format to display the final themes of clusters , in order to help users to find the interesting documents easily . experiment results verified our method feasible and effective . 

white paper on natural language processing
we take the ultimate goal of natural language processing ( nlp ) to be the ability to use natural languages as effectively as humans do . natural language , whether spoken , written , or typed , is the most natural means of communication between humans , and the mode of expression of choice for most of the documents they produce . as computers play a larger role in the preparation , acquisition , transmission , monitoring , storage , analysis , and transformation of information , endowing them with the ability to understand and generate information expressed in natural languages becomes more and more necessary . some tasks currently performed by humans cannot be automated without endowing computers with natural language processing capabilities , and these provide two major challenges to nlp systems : 1 . reading and writing text , applied to tasks such as message routing , abstracting , monitoring , summarizing , and entering information in databases , with applications , in such areas as intelligence , logistics , office automation , and libraries . computers should be able to assimilate and compose extended communications . 2 . translation , of documents or spoken language , with applications , in such areas as in science , diplomacy , multinational commerce , and intelligence . computers should be able to understand input in more than one language , provide output in more than one language , and translate between languages . 

graph - based text representation model and its realization
in this paper , on the foundation of summarizing several common used text representation models , such as boolean model , probability model , vector space model and so on , mainly according to the defects of the vector space model , the word semantic space is proposed . and in the word semantic space , a graph - based text representation model is designed . some properties of this type of text representation model have been given , and this model can describe the words semantic constraints in the text . at the same time , this model can also solve the defects of vector space model , such as the order or words , the boundary between sentences and phrases , etc . and at last the method of computing the text similarity is put forward . 

collecting conceptualized relations from terabytes of web texts for understanding unknown terms
this paper describes our attempt to extract various relations between super ordinate concepts from terabytes of web corpus for human - like speculation of the meaning of unknown terms . in order to discover various conceptualized relations , we focus on web - scale text corpora and introduce a simple string - matching method to process them . to derive relations between concepts , our method first extracts relations between terms and next replaces each term by appropriate concepts using wikipedia , word net , and yago knowledge . we extracted over 10 million relations between concepts in a day from more than 10tb of web texts using 100 machines . experimental results revealed that extracted relations by our method contained much more meaningless relations than those by nlp - based methods . nevertheless , they were useful in an application of speculating the meaning of unknown terms , improving the recall by more than 0 . 06 points and decreasing the accuracy by only 0 . 04 points ( the improvement of the f1 - measure was 0 . 03 points ) . we found from the results that the coverage of conceptualized relations is important to improve the precision in the application . this is because the lack of knowledge ( conceptualized relations ) leads to misunderstanding of the meaning of unknown terms , as we humans misunderstand things with our insufficient knowledge . 

using nlp and ontologies for notary document management systems
in this paper we describe the use of nlp techniques and ontologies as the core for building novel e - gov based information systems , and in particular we define the main characteristics of a document management system in the legal domain , that manages a variety of paper documents , automatically transforming them into rdf statements , for suitable indexing , retrieval and long term preservation . although we describe a general architecture that can be used for several application domains , our system is particularly suitable for the italian notary realm . 

analysis of weak signals for detecting lone wolf terrorists
lone wolf terrorists pose a large threat to modern society . the current ability to identify and stop these kind of terrorists before they commit a terror act is limited since they are very hard to detect using traditional methods . however , these individuals often make use of internet to spread their beliefs and opinions , and to obtain information and knowledge to plan an attack . therefore , there is a good possibility that they leave digital traces in the form of weak signals that can be gathered , fused , and analyzed . in this work we present an analysis method that can be used to analyze extremist forums to profile possible lone wolf terrorists . this method is conceptually demonstrated using the foi impactorium fusion platform . we also present a number of different technologies that can be used to harvest and analyze information from internet , serving as weak digital traces that can be fused using the suggested analysis method , in order to discover possible lone wolf terrorists . 

speech , voice , text , and meaning : a multidisciplinary approach to interview data through the use of digital tools
interview data is multimodal data : it consists of speech sound , facial expression and gestures , captured in a particular situation , and containing textual information and emotion . this workshop shows how a multidisciplinary approach may exploit the full potential of interview data . the workshop first gives a systematic overview of the research fields working with interview data . it then presents the speech technology currently available to support transcribing and annotating interview data , such as automatic speech recognition , speaker diarization , and emotion detection . finally , scholars who work with interview data and tools may present their work and discover how to make use of existing technology . 

high - performance high - volume layered corpora annotation
nlp systems that deal with large collections of text require significant computational resources , both in terms of space and processing time . moreover , these systems typically add new layers of linguistic information with references to another layer . the spreading of these layered annotations across different files makes them more difficult to process and access the data . as the amount of input increases , so does the difficulty to process it . one approach is to use distributed parallel computing for solving these larger problems and save time . we propose a framework that simplifies the integration of independently existing nlp tools to build language - independent nlp systems capable of creating layered annotations . moreover , it allows the development of scalable nlp systems , that executes nlp tools in parallel , while offering an easy - to - use programming environment and a transparent handling of distributed computing problems . with this framework the execution time was decreased to 40 times less than the original one on a cluster with 80 cores . 

layoutlm : pre - training of text and layout for document image understanding
pre - training techniques have been verified successfully in a variety of nlp tasks in recent years . despite the widespread use of pre - training models for nlp applications , they almost exclusively focus on text - level manipulation , while neglecting layout and style information that is vital for document image understanding . in this paper , we propose the layoutlm to jointly model interactions between text and layout information across scanned document images , which is beneficial for a great number of real - world document image understanding tasks such as information extraction from scanned documents . furthermore , we also leverage image features to incorporate words ' visual information into layoutlm . to the best of our knowledge , this is the first time that text and layout are jointly learned in a single framework for document - level pre - training . it achieves new state - of - the - art results in several downstream tasks , including form understanding ( from 70 . 72 to 79 . 27 ) , receipt understanding ( from 94 . 02 to 95 . 24 ) and document image classification ( from 93 . 07 to 94 . 42 ) . the code and pre - trained layoutlm models are publicly available at https : / / aka . ms / layoutlm . 

optimal target aiming maneuver for an advanced aircraft via timescale pseudospectral method
a two - timescale chebyshev pseudospectral optimal control method , which can reduce the converted nonlinear programming ( nlp ) significantly , is proposed . the optimal aiming maneuver problem of an advanced aircraft , which is a six - degree - of - freedom trajectory problem , is solved by this two - timescale method . it is the first time that the six - degree - of - freedom equations are used to solve optimization of the flight trajectory problems . 

identification and prioritization of urban issues from smart city data
government should be able to publish and interact better for offering e - government services effectively to their netizens . in this context publish refers to delivering information through web pages and interact refers to allowing citizens to provide feedback and suggestions . developing a model for understanding and prioritizing urban governance issues from these user - generated contents is an important research issue . this paper proposes a four - step solution approach to prioritizing urban governance issues from user - generated contents using aspect based sentiment analysis ( absa ) . first , a novel lexicon - based approach is used for selecting trigrams as aspect phrase and pruning the list using cross - domain stop words . second , the opinionated sentences are extracted using the identified aspects . third , polarity and subjectivity of the aspects and corresponding comments along with the bag - of - words for the aspects are considered as the features for learning the category of the urban issue . fourth , the sentiment score of a specific category is used to prioritize the urban issues . the proposed methodology is applied to a real user - generated content extracted from a discussion forum on smart cities from an e - governance portal in india . our research demonstrates that aspect based sentiment analysis ( absa ) with necessary modifications can be utilized to extract issues , suggestions , and ideas from crowd - sourced citizen - generated content ( social data ) . sentiment analysis can be used to prioritize them as well . analysis of the data for the candidate city shows that while the citizens are happy about educational facilities in the city they are highly concerned about health issues . a deeper investigation revealed the health issues are due to high degree of carbon emission , lack of solid waste management system and lack of sanitation among the others . 

ebl : an approach to automatic lexical acquisition
a method for automatic lexical acquisition is outlined . an existing lexicon that , in addition to ordinary lexical entries , contains prototypical entries for various non - exclusive paradigms of open - class words , is extended by inferring new lexical entries from texts containing unknown words . this is done by comparing the constraints placed on the unknown words by the natural language system ' s grammar with the prototypes and a number of hand - coded phrase templates specific for each paradigm . once a sufficient number of observations of the word in different contexts have been made , a lexical entry is constructed for the word by assigning it to one or several paradigm ( s) . parsing sentences with unknown words is normally very time - consuming due to the large number of grammatically possible analyses . to circumvent this problem , other phrase templates are extracted automatically from the grammar and domain - specific texts using an explanation - based learning method . these templates represent grammatically correct sentence patterns . when a sentence matches a template , the original parsing component can be bypassed , reducing parsing times dramatically . 

safloor : smart fall detection system for the elderly
the main reason for hospital admission of elderly people worldwide is falls . the longer one has to stay on the floor due to inability to get up after falling , the more severe the injury can become . existing fall detection devices that need to be worn on the body is not very useful because the elderly simply forget or do not want to wear them . from this , we have created a fall detection system named “ safloor ” - a soft mat with force sensors embedded inside . safloor can be placed in fall - prone areas , such as by the bedside , bathroom , at the bottom of stairs , etc . it can distinguish between a real fall and other impacts such as walking and dropped objects , and send out a notification to a family member or a care giver via line message when a fall is detected . the experiment consisting of 14 participants with different weights and heights shows that safloor has a successful fall detection rate of 88% . 

chinese terminology extraction using bilingual web resources
automatic terminology extraction requires termhood verification for extracted terms in a specific domain . chinese terminology extraction suffers from insufficient domain corpora for verification even though there is abundance of information in other languages . this paper presents a novel approach to overcome this problem by using word translations and bilingual web resources to improve both coverage and precision . the proposed approach incorporates bilingual information from within candidate terms themselves and from existing domain knowledge to conduct termhood calculation . in contrast to previous researches , this method is not confined to only pre - determined corpora . preliminary experiments show a 14 . 8 % improvement in coverage and 26 . 3 % improvement in precision , respectively . 

segmenting a sentence into morphemes using statistic information between words
this paper is on dividing non - separated language sentences ( whose words are not separated from each other with a space or other separaters ) into morphemes using statistical information , not grammatical information which is often used in nlp . in this paper we describe our method and experimental result on japanese and chinese sentences . as will be seen in the body of this paper , the result shows that this system is efficient for most of the sentences . 

an arabic question - answering system for factoid questions
in this paper , we propose an arabic question - answering ( q - a ) system called qasal ( question - answering system for arabic language ) . qasal accepts as an input a natural language question written in modern standard arabic ( msa ) and generates as an output the most efficient and appropriate answer . the proposed system is composed of three modules : a question analysis module , a passage retrieval module and an answer extraction module . to process these three modules we use the nooj platform which represents a linguistic development environment . 

automatic formation , termination & correction of assamese word using predictive & syntactic nlp
automatic formation , termination & correction of assamese words is a method in which a user will get the relevant suggestions for the word which he / she is intended to write . in the formation and termination method user will type a letter and the system will display the most probable words related to that letter and if the user finds required word he / she will select the word , if the required word does not appear in the suggestion list then the user has to type more letters along with the previous letter to purify the suggestion list . in the correction method , if a user type a word which is not in the corpus then the system will show one warning for that word which prevents the occurrence of the errors . 

hatesense : tackling ambiguity in hate speech detection
hate speech propagated online has been a long - trailing issue which induces several negative effects on society . the current efforts for the automated detection of hate speech online have utilized machine learning techniques in order to try and solve the issue as a classification problem . however , the significant drawback that has been identified in existing literature is that the inability of existing systems to tackle the ambiguity when it comes to hate speech detection , more specifically differentiating between hateful and offensive content . this research aims to tackle this issue of ambiguity in hopes of improving hate speech detection in general . the proposed system will utilize human reasoning techniques such as ontologies and fuzzy logics along with sentiment analysis in order to detect hate speech and deconstruct the ambiguity present . the results of the proposed approach show that the system can perform well when it comes to differentiating between hateful and offensive content and it is able to outperform existing systems in crucial factors . yet , the deconstruction of ambiguity becomes difficult when there are a smaller number of hateful keywords present although the fuzzy control system was able to compensate in most cases . thereby this research stresses the need for considering the disambiguation between hateful and offensive content when it comes to hate speech detection and utilization of human reasoning techniques to further facilitate this process . 

constructing parse forests that include exactly the n - best pcfg trees
this paper describes and compares two algorithms that take as input a shared pcfg parse forest and produce shared forests that contain exactly the n most likely trees of the initial forest . such forests are suitable for subsequent processing , such as ( some types of ) reranking or lfg f - structure computation , that can be performed ontop of a shared forest , but that may have a high ( e . g . , exponential ) complexity w . r . t . the number of trees contained in the forest . we evaluate the performances of both algorithms on real - scale nlp forests generated with a pcfg extracted from the penn treebank . 

trajectory optimization of two cooperative aircrafts based on rhc
trajectory optimization problem of two cooperative aircrafts in cooperative tactical engagement maneuvering is studied . the optimization problem of air combat for two aircrafts is established based on the analysis of vertical tactical engagement and three different performance measures and terminal constraints are proposed . the receding horizon control ( rhc ) model and the numerical solution based on simpson - direct - collocation is put forward . in order to improve the online performance , a bp neural network based approximation of the performance measures is proposed . finally , a simulation shows that this method is feasible for trajectory optimization in cooperative air combat . 

chinese person name recognition with semantic radical
radical is the primary component of chinese character , but its function in chinese information processing has not yet been well studied . with both theoretical analysis and corpus statistics , this paper presents the correlation between semantic radicals and chinese characters used in person names . a person name recognition method that uses semantic radicals as computational resources in maximum entropy framework is proposed . this method was contrasted with a baseline method that doesn ' t take radicals into consideration . the experiments results show that semantic radicals can be quite useful in chinese text processing . 

an efficiency - accuracy tradeoff for idss in vanets with markov - based reputation scheme
vehicle ad hoc networks ( vanets ) are considered to be a next big thing that will remarkably change our lives , since this kind of technology is able to make our lives and roads safer . intrusion detection systems ( ids ) is an important technology that can mitigate both inner and outer threats for the vulnerable networks like vanets . however , it is difficult to adopt the same idss that have been appropriately used in wired networks , due to the fast moving and highly dynamic nature of vanets . thus , in this paper , an efficient ids with a markov - based reputation scheme is proposed , called eids - mrs . in eids - mrs , the non - linear programming ( nlp ) - optimization is designed as the efficient mechanism to reduce the execution time of idss in vanets . moreover , considering the security risks of nlp - optimization , a reputation scheme based on the hidden generalized mixture transition distribution ( hgmtd ) model , namely rs - hgmtd , is proposed for each vehicle in vanets to evaluate the creditworthiness of their neighbors . experiments show that the eids - mrs has better performance than other available idss in terms of detection rate , detection time and overhead . 

code generator based on voice command for multiple programming language
in this modern era , several new programming languages are introduced among the programmers and are the benediction for them but there are some challenging issues . the major problem of a programmer to memorize the syntaxes of different programming languages at the same time . it is very common among the programmers to mix up the syntaxes of different programming languages during coding . moreover , the people who are suffering with parkinson ' s disease cannot do programming because of their shaky hands . in additional , hand typing programming takes enough time to perform the coding . this study discusses how to reduce typing problems by hands , syntax memorization problems and time consuming problems during programming . however , the proposed system named vcl ( voice command language ) is a very easy and time saving software . this software is capable of coding on multiple programming languages ( such as python , java and c ) by using same voice command against the syntax for all languages . voice command is recognized by the google speech api and then software generate the code against the command . this software will help the programmers to focus more on logics rather than memorizing the syntaxes . the study will also help the programmers to speed up their programming performance and will create a new window for the hand injured people . 

combining graph connectivity and genetic clustering to improve biomedical summarization
automatic summarization is emerging as a feasible instrument to help biomedical researchers to access online literature and face information overload . the natural language processing community is actively working toward the development of effective summarization applications ; however , automatic summaries are sometimes less informative than the user needs . in this work , our aim is to improve a summarization graph - based process combining genetic clustering with graph connectivity information . in this way , while genetic clustering allows us to identify the different topics that are dealt with in a document , connectivity information ( in particular , degree centrality ) allows us to asses and exploit the relevance of the different topics . our automatic summaries are compared with others produced by commercial and research applications , to demonstrate the appropriateness of using this combination of techniques for automatic summarization . 

need for computational and psycho - linguistics models in natural language processing for web documents
natural language processing ( nlp ) is an argument that provokes territory of figural manner for motorized scrutiny and depiction of humane language . research in it unfolded from the term of punch cards till the age of intelligent examination engines which takes less than a second to figure out millions of web pages and digital documents . in the web , pieces of evidence and end - users produced content to have formerly stimulated text . the obligation of advantageous and astute content driving is decisive forthwith . consequently , the web is being treated as a foremost source of evidence brought about the extensive extent of a probe to perceive the compass of the machine . considering the democratization of online content formulation , it has skeptically influenced the information resurgence and eradication . this paper pulls contemporary developments in the nlp probe to perceive the scope of machine learning in a modern era to avoid ramification of adverse instructions . 

sense : a student performance quantifier using sentiment analysis
academic feedback is essential in secondary schools to keep a rapport between students , teachers , and parents and guardians . there are three main factors that contribute towards a student ' s progress : attitude , attendance and aptitude . monitoring their progress is key to a student ' s development in school and allows both teachers and parents or guardians to support them to a greater extent . annual reports are sent to a student ' s home to summarise their performance over the academic year , following set criterion from the government . one aspect of a student ' s report is the teacher ' s written comment , providing more details on a student ' s attitude towards their learning . however , families whose primary language is not english may struggle to interpret this information . working in schools has demonstrated the diversity of students and their wide range of backgrounds , including - but not limited to - language barriers . this work proposes a system called sense ( student performance quantifier using sentiment analysis ) for improving the information conveyed in secondary school reports through means of natural language processing . by combining the three key features which contribute towards a student ' s progress , a numerical representation is produced for an easier interpretation . this reduces the likelihood of a tarnished relationship between home and schools through better means of conveying information and maintains communication between students , teachers and parents or guardians . 

towards execution time prediction for manual test cases from test specification
knowing the execution time of test cases is important to perform test scheduling , prioritization and progress monitoring . this work in progress paper presents a novel approach for predicting the execution time of test cases based on test specifications and available historical data on previously executed test cases . our approach works by extracting timing information ( measured and maximum execution time ) for various steps in manual test cases . this information is then used to estimate the maximum time for test steps that have not previously been executed , but for which textual specifications exist . as part of our approach , natural language parsing of the specifications is performed to identify word combinations to check whether existing timing information on various test activities is already available or not . finally , linear regression is used to predict the actual execution time for test cases . a proof - of - concept use case at bombardier transportation serves to evaluate the proposed approach . 

discrete light bullet vortices
we discuss the observation of vortex light bullets in arrays of waveguides . these are among the most complex spatiotemporal solitary waves observed in an experiment . they are characterized by a complex interplay of angular orbital momentum and nonlinear , spatiotemporal effects . we show that vortex light bullets exhibit a new mode of stability and show that they are robust under realistic experimental conditions . we demonstrate that they can be excited with the help of a discrete phase plate and give proof of their existence using an ultrafast spatiotemporal cross - correlation technique and rigorous numerical analysis . 

a survey on social media analytics for smart city
urbanization offers opportunities in terms of economic and social life but as the population increases , urban issues like traffic jam , pollution , overcrowding , resource management like water , energy are also increased . continuous monitoring of city ' s infrastructure and automated collection of daily incidents are required to improve quality of life of citizens . every day , vast no . of citizens are taking part in shaping the smart city using social network by expressing their thoughts , views , feelings & experiences . although physical sensors planted across the city help to keep eye on the city , it will only answer the question " what happened " but the question " why " remains unanswered sometimes . in this scenario " social media " proves to be the goldmine of the information . so this paper presents a comprehensive literature survey where social media analytic have been used for smart city implementation . some of the interesting areas include improving quality of life of citizens , smart mobility and transparent and inclusive governance . it further suggests some opportunities which can be used for future research . 

semantic change detection with gaussian word embeddings
diachronic study of the evolution of languages is of importance in natural language processing ( nlp ) . recent years have witnessed a surge of computational approaches for the detection and characterization of lexical semantic change ( lsc ) due to the availability of diachronic corpora and advancing word representation techniques . we propose a gaussian word embedding ( w2g ) - based method and present a comprehensive study for the lsc detection . w2g is a probabilistic distribution - based word embedding model and represents words as gaussian mixture models using covariance information along with the existing mean ( word vector ) . we also extensively study several aspects of w2g - based lsc detection under the semeval - 2020 task 1 evaluation framework as well as using google n - gram corpus . in the sub - task 1 ( lsc binary classification ) of the semeval - 2020 task 1 , we report the highest overall ranking as well as the highest ranks for the two ( german and swedish ) of the four languages ( english , swedish , german and latin ) . we also report the highest spearman correlation in the sub - task 2 ( lsc ranking ) for swedish . our overall rankings in the lsc classification and ranking sub - tasks are 1 $^ { \ rm { st }}$ and 7 $^ { \ rm { th }}$ , respectively . qualitative analysis has also been presented . 

feeling voice management system and feeling voice database for voice feeling measurement
this paper , proposes a model that applies the idea of colorimetry , which handles the feeling of color quantitatively , to the feeling recognition from the human voice . specifically , " feelings voice management system " and " standard voice feelings evaluator " corresponding to the " color management system " and " standard observer " in a color field are described . also , we will describe the construction and characteristics of the voice feelings database for making a standard voice feelings evaluator . 

clutter removal algorithm based on grid density with a recursive approach for rockfall detection in 3d point clouds from a terrestrial lidar scanner
in analyzing 3d point clouds obtained from a terrestrial lidar scanner for rockfall detection , a widely - used clutter removal algorithm is nearest neighbor clutter removal ( nncr ) . however , there is a critical problem regarding computational complexity of nncr . subsequently , we presented a new algorithm for clutter removal based on grid density as a solution to this problem . nevertheless , the previously proposed method showed that data points were lost . this study proposes a multi - scale grid - density - based method , assuming that the clutter is normally distributed . outcomes from the experiment indicate that a proposed method could retrieve data points lost in the previous method . the balanced accuracies , recalls , and f - scores of the proposed method were improved by approximately 13 , 33 , and 17 percent , respectively , compared with the previously proposed method . also , the proposed method is about 19 times faster than nncr . 

automatic measurement of syntactic development in child language
to facilitate the use of syntactic information in the study of child language acquisition , a coding scheme for grammatical relations ( grs ) in transcripts of parent - child dialogs has been proposed by sagae , macwhinney and lavie ( 2004 ) . we discuss the use of current nlp techniques to produce the grs in this annotation scheme . by using a statistical parser ( charniak , 2000 ) and memory - based learning tools for classification ( daelemans et al . , 2004 ) , we obtain high precision and recall of several grs . we demonstrate the usefulness of this approach by performing automatic measurements of syntactic development with the index of productive syntax ( scarborough , 1990 ) at similar levels to what child language researchers compute manually . 

proliv : a tool for teaching by viewing computational linguistics
proliv - animated process - modeler of complex ( computational ) linguistic methods and theories - is a fully modular , flexible , xml - based stand - alone java application , used for computer - assisted learning in natural language processing ( nlp ) or computational linguistics ( cl ) . having a flexible and extendible architecture , the system presents the students , by means of text , of visual elements ( such as pictures and animations ) and of interactive parameter set - up , the following topics : latent semantics analysis ( lsa ) , ( computational ) lexicons , question modeling , hidden - markov - models ( hmm ) , and topic - focus . these topics are addressed to first - year students in computer science and / or linguistics . 

optimum coordination of directional overcurrent relays using the hybrid ga - nlp approach
the time of operation of overcurrent relays ( ocrs ) can be reduced , and at the same time , the coordination can be maintained , by selecting the optimum values of time multiplier setting ( tms ) and plug setting ( ps ) of ocrs . this paper presents hybrid genetic algorithm ( ga ) - nonlinear programming ( nlp ) approach for determination of optimum values of tms and ps of ocrs . ga has a drawback of , sometimes , converging to the values which may not be optimum , and nlp methods have a drawback of converging to local optimum values , if the initial choice is nearer to local optimum . this paper proposes a hybrid method to overcome the drawback of ga and nlp method , and determine the optimum settings of ocrs . the main contributions of this paper are - 1 ) systematic method for formulation of problem of determining optimum values of tms and ps of ocrs in power distribution network as a constrained nonlinear optimization problem , 2 ) determining initial values of tms and ps using ga technique and finding final ( global optimum ) values using nlp method , thus making use of the advantages of both methods ( and at the same time overcoming the drawbacks of the methods ) . 

answer generation for chinese cuisine qa system
in this paper , we propose an approach on answer generation for cooking question answering system . we first review previous work of question analysis . then , we give annotation scheme for knowledge database . finally , we present the answer planning based approach for generate an exact answer in natural language . an evaluation has been conducted on natural language questions and the result shows that the system can satisfy user ' s demand . 

approximate scalable bounded space sketch for large data nlp
we exploit sketch techniques , especially the count - min sketch , a memory , and time efficient framework which approximates the frequency of a word pair in the corpus without explicitly storing the word pair itself . these methods use hashing to deal with massive amounts of streaming text . we apply count - min sketch to approximate word pair counts and exhibit their effectiveness on three important nlp tasks . our experiments demonstrate that on all of the three tasks , we get performance comparable to exact word pair counts setting and state - of - the - art system . our method scales to 49 gb of unzipped web data using bounded space of 2 billion counters ( 8 gb memory) . 

boydcut : bidirectional lstm - cnn model for thai sentence segmenter
sentence is imperative in order to build the top level of nlp ( natural language processing ) applications such as information retrieval , news summarization , knowledge graph . in thai language , neither each word token is not separated by using just only space like english language , nor the sentence is verified its boundary by using full stop symbols . this paper proposes boydcut , an nlp framework for identifying sentence boundaries based on bidirectional lstm - cnn model . we develop this framework by utilizing the combination of character , word , and part of speech features . with bidirectional lstm , it can learn sequent of word - level in sentences and learn extracted features from character level . with the benefit of the combination bidirectional lstm - cnn model , we do not need feature engineering for feature extraction that can be saved a lot of cost and time in order to build a sentence segmentation model . we also simply design the experiments in a different bucket of features extracted from a deep sequential model . the result empirically shows well perform in internal and external data , as well as help a lot in order to build several useful on the top level of nlp applications . 

linking user requests , developer responses and code changes : android os case study
since software systems are designed to satisfy customers ' needs , developers have an obligation to address users ' requirements and demands logged via issue trackers and other forums . having to respond to a large number of requests while developing and perfecting systems presents prioritization challenges , however . android operating system ( os ) developers have largely overcome this obstacle by responding to specific user requests , which may be traced back to actual software code changes , providing lessons for the software engineering community . this study applies text and data mining techniques to investigate the android community as an ecosystem , exploring how developers responded to issues raised by the community over several versions of the os . results show a strong relationship between issues raised by the community and developer responses to these issues . this relationship also extended to actual source code changes made by developers . furthermore , the findings show a correlation between user requests and developer responses enacted via code changes across specific android versions and important functionalities . this evidence suggests that developers have invested in the android platform to guarantee its survival and overall success , largely through addressing user demands . we outline implications for software engineering professionals and software systems success . 

extracting historical terms based on aligned chinese - english parallel corpora
this paper examines the feasibility of implementing statistic - oriented term extraction and evaluation methods in extracting historical terms from aligned parallel corpora of chinese historical classics and their translations . it proposes to take transliteration as anchor points to establish sentence - level alignment . it also investigates the approach to extract term translation pairs based on 4000 parallel sentences or segments of sentences from the corpora of the chinese historical classic shi ji ( records of the historian ) and its english translations by two well - known translators . the experimental results indicate that the statistically sound algorithm can successfully extract those terms whose english translations are consistent throughout the corpus and those transliterated pairs , but fails to extract the translations of those terms that are translated differently by the two translators although the translations may be equally qualified in terms of their usage in the english language . the algorithm also fails to extract the top frequency terms which are ambiguous in meaning due to changes of its part of speech . therefore , this paper suggests insights gained from the linguistic and translation studies perspectives can be integrated with the statistic measurements to improve the extraction and validating results . 

vector representation of internet domain names using a word embedding technique
word embeddings is a well known set of techniques widely used in natural language processing ( nlp ) , and word2vec is a computationally - efficient predictive model to learn such embeddings . this paper explores the use of word embeddings in a new scenario . we create a vector representation of internet domain names ( dns ) by taking the core ideas from nlp techniques and applying them to real anonymized dns log queries from a large internet service provider ( isp ) . our main objective is to find semantically similar domains only using information of dns queries without any other previous knowledge about the content of those domains . we use the word2vec unsupervised learning algorithm with a skip - gram model to create the embeddings . and we validate the quality of our results by expert visual inspection of similarities , and by comparing them with a third party source , namely , similar sites service offered by alexa internet , inc . 

individualised nlp - enhanced feedback for distance language learning
the purpose of this paper is to present a strategy for the provision of language courses to learners of spanish for specific purposes with intelligent feedback . as a byproduct , students can be recommended to proceed through a slightly different learning path in order to overcome their shortcomings . the paper is structured in 5 sections : section 1 is an introduction to this research work ; section 2 describes the didactic assumptions and the learning environment in which the strategy is applied ; section 3 presents the tools for both linguistic analysis and error detection , and describes how the system generates feedback ; and section 4 concludes with some remarks and presents future work . 

