A Cloud - Hosted MapReduce Architecture for Syntactic Parsing
Syntactic parsing is a time - consuming task in natural language processing particularly where a large number of text files are being processed . Parsing algorithms are conventionally designed to operate on a single machine in a sequential fashion and , as a consequence , fail to benefit from high performance and parallel computing resources available on the cloud . We designed and implemented a scalable cloud - based architecture supporting parallel and distributed syntactic parsing for large datasets . The main architecture consists of a syntactic parser ( constituency and dependency parsing ) and a MapReduce framework running on clusters of machines . The resulting cloud - based MapReduce parsing is able to build a map where syntactic trees of the same input file have the same key and collect into a single file containing sentences along with their corresponding trees . Our experimental evaluation shows that the architecture scales well with regard to number or processing nodes and number of cores per node . In the fastest tested cloud - based setup , the proposed design performs 7 times faster when compared to a local setup . In summary , this study takes an important step toward providing and evaluating a cloud - hosted solution for efficient syntactic parsing of natural language data sets consisting of a large number of files . 

Requirement Engineering of Software Product Lines : Extracting Variability Using NLP
The engineering of software product lines begins with the identification of the possible variation points . To this aim , natural language ( NL ) requirement documents can be used as a source from which variability - relevant information can be elicited . In this paper , we propose to identify variability issues as a subset of the ambiguity defects found in NL requirement documents . To validate the proposal , we single out ambiguities using an available NL analysis tool , QuARS , and we classify the ambiguities returned by the tool by distinguishing among false positives , real ambiguities , and variation points , by independent analysis and successive agreement phase . We consider three different sets of requirements and collect the data that come from the analysis performed . 

Automated Question Answering From Lecture Videos : NLP vs . Pattern Matching
This paper explores the feasibility of automated question answering from lecture video materials used in conjunction with PowerPoint slides . Two popular approaches to question answering are discussed , each separately tested on the text extracted from videotaped lectures : 1 ) the approach based on Natural Language Processing ( NLP ) and 2 ) a self - learning probabilistic pattern matching approach . The results of the comparison and our qualitative observations are presented . The advantages and shortcomings of each approach are discussed in the context of video applications for e - learning or knowledge management . 

Creating software models with semantic annotation
Requirements engineering is a big part of software engineering and consumes a lot of time . We propose a novel approach of automatically creating software domain models from textual requirements specifications using semantic annotation . Natural language processing ( NLP ) has progressed much in the last years and the usage of NLP tools for automatic annotation shows promising results . We use Fillmore ' s thematic roles to explicitly denote the semantic relations in a sentence . Semantic annotations also maintain the connection between textual artifacts and their corresponding model elements . Therefore changes in the domain model can be fed back to the textual specification . Additionally , changes in the textual specification can be analyzed and their impact towards the software model can be assessed . 

Using Different RNN Variants to Generate Realist Reviews
This project aims to implement LSTM and GRU to generate realistic text . First , some basic background information will be introduced . After that , the design of models will be presented , then the performance of different RNN variants will be compared . In addition , the time consumption also as a factor of performance to compare . At last , the reason for the difference in performance will be discussed . 

Looking at word meaning : an interactive visualization of semantic vector spaces for Dutch synsets
In statistical NLP , Semantic Vector Spaces ( SVS ) are the standard technique for the automatic modeling of lexical semantics . However , it is largely unclear how these black - box techniques exactly capture word meaning . To explore the way an SVS structures the individual occurrences of words , we use a non - parametric MDS solution of a token - by - token similarity matrix . The MDS solution is visualized in an interactive plot with the Google Chart Tools . As a case study , we look at the occurrences of 476 Dutch nouns grouped in 214 synsets . 

An English part - of - speech tagger for machine translation in business domain
Part - of - speech tagging is a crucial preprocessing step for machine translation . Current studies mainly focus on the methods , linguistic , statistic , machine learning or hybrid . But so far not many serious attempts have been performed to test the reported accuracy of taggers on different , perhaps domain - specific , corpora . Therefore , this paper presents an English POS tagger for English - Chinese machine translation in business domain , demonstrating how a present tagger can be adapted to learn from a small amount of data and handle unknown words for the purpose of machine translation . A small size of 998k English annotated corpus in business domain is built semi - automatically based on a new tagset , the maximum entropy model is adopted and rule - based approach is used in post - processing . Experiments show that our tagger achieves an accuracy of 99 . 08 % in closed test and 98 . 14 % in open test , which is a quite satisfactory result , compared with the reported best open test result of 97 . 18 % of Stanford English tagger . 

Morphological analyzer and generator for Tulu language : a novel approach
Developing a complete and well specified natural language processing ( NLP ) models for any language with limited electronic resources is always a challenging and demanding task . Morphological Analyzer and Generator ( MAG ) is an indispensable tool used extensively in natural language processing applications such as Machine Translation ( MT ) , spell checker , Information Retrieval ( IR ) , lexicography etc . A Morphological Analyser describes the root word and its various morphemes ordering structure of the word form in use . While a Morphological Generator processes the given root word and its morpheme structure to derive the required word form . This paper presents the development of a Morphological Analyzer and Generator for an agglutinative Dravidian language called Tulu . Even though such a Morphological Analyzer and Generator tool is mandatory for many Natural Language Processing ( NLP ) tasks but is not yet available for Tulu language . The proposed rule based Morphological Analyzer and Generator system is implemented using AT & T Open FST ( Finite state transducer ) . From the experiment it is observed that , the performance of the proposed system is encouraging and can be substantially improved by increasing the number of rules . 

Generating example contexts to illustrate a target word sense
Learning a vocabulary word requires seeing it in multiple informative contexts . We describe a system to generate such contexts for a given word sense . Rather than attempt to do word sense disambiguation on example contexts already generated or selected from a corpus , we compile information about the word sense into the context generation process . To evaluate the sense - appropriateness of the generated contexts compared to WordNet examples , three human judges chose which word sense ( s ) fit each example , blind to its source and intended sense . On average , one judge rated the generated examples as sense - appropriate , compared to two judges for the WordNet examples . Although the system ' s precision was only half of WordNet ' s , its recall was actually higher than WordNet ' s , thanks to covering many senses for which WordNet lacks examples . 

Bengali VADER : A Sentiment Analysis Approach Using Modified VADER
Sentiment analysis is an essential field of natural language processing ( NLP ) that classifies the opinion expressed in a text according to its polarity ( e . g . , positive , negative or neutral ) . Bengali NLP research is lagging behind English NLP , where there are very few works on Bengali sentiment analysis . In this paper , we approach this issue by modifying a popular English tool VADER to support Bengali sentiment polarity identification . We have compiled a Bengali polarity lexicon from the English polarity lexicon of VADER . Furthermore , we have modified the functionalities of English VADER , so that it can directly classify Bengali text sentiments without the requirement of Bengali to English translation using tools such as Google Translator , MyMemory Translator , etc . Our experiments demonstrate that the modified Bengali VADER significantly improves the sentiment analysis result of Bengali text over the current model . 

Parsing model for answer extraction in Chinese question answering system
This paper presents a novel approach to answer extraction in Chinese question answering ( QA ) system . An extended lexicalized context - free grammar parser is used for question and answer understanding , the exact answer is extracted with the clue of named entity ( NE ) and question classification ( QC ) . It shows that the - performance of QA will be greatly improved with more comprehensive language understanding technologies . 

 [ iSAI - NLP 2020 Keynote Speakers - 3 abstracts ] 
Provides an abstract for each of the three keynote presentations and may include a brief professional biography of each presenter . The complete presentations were not made available for publication as part of the conference proceedings . 

An Analyze Movement Path of Employees in Fire Drill by Indoor Location System Using Bluetooth
Fire drill is a practice of the emergency procedure to be used in case of fire . The key success factor of this practice is evacuation time which start since the fire alarm rang until the last person left the building . Another factor is the evacuation path . The safety regulation requires person inside building to go to the nearest fire exit during the emergency . However , during practice we never know the real evacuation path . Therefore , in this paper , we present indoor location system using Bluetooth to track movement path of employees during fire drill . This system records the path of employee during evacuation and analyzes each employee . The experiment found that there are 40 % of all employees who do not go to nearest fire exit according to safety regulation . In addition , 92 % of employee who did not got to nearest fire exit are the employee who the starting point is at the back of the building . One observation is that the assembly point is in the front of the building . 

Reference Resolution Supporting Lexical Disambiguation
This paper describes ongoing work in carrying out the semantic analysis of texts and reference resolution in a control structure that permits each process to inform the other , rather than in a more traditional , unidirectional fashion ( semantics followed by reference resolution ) . We concentrate on situations in which a polysemous predicate cannot be lexically disambiguated until the meaning of one of its arguments has been specified , and that can only be accomplished with the help of reference resolution procedures . As a sidebar , we briefly introduce our “ feature value bundling ” approach to configuring reference resolution engines without the need for large annotated corpora . 

SemEval - 2010 task 10 : linking events and their participants in discourse
In this paper , we describe the SemEval - 2010 shared task on " Linking Events and Their Participants in Discourse " . This task is a variant of the classical semantic role labelling task . The novel aspect is that we focus on linking local semantic argument structures across sentence boundaries . Specifically , the task aims at linking locally uninstantiated roles to their co - referents in the wider discourse context ( if such co - referents exist) . This task is potentially beneficial for a number of NLP applications and we hope that it will not only attract researchers from the semantic role labelling community but also from co - reference resolution and information extraction . 

Classification of Facemarks Using N - gram
In this paper , we present an approach for the classification of facemarks into some facial expression categories . Facemarks are some of the expressions that are often used in text - based communication . Facemarks express human facial expression or action and they help us to understand what writers imply . However , there are some problems in the proccessing of the facemarks with computer ; facemarks are numerous and users make new facemarks . Therefore , we propose to use the characters in facemarks to classify them . Though the facemarks are various , only some of signs and letters are used for the characters that compose the them . Moreover , there is a feature of the expressions of facemarks in the characters . We present an approach for facemarks classification using N - gram and evaluate this method . 

Trajectory optimization of a cruise missile using the Gauss Pseudospectral Method
Compared with the conventional optimization approaches , Gauss Pseudospectral Method ( GPM ) can deal better with optimal problems with complex constraints and is less time consuming . As a numerical technique , GPM transforms the optimal control problem to a nonlinear programming problem ( NLP ) by using discrete approximation . This paper considers a cruise missile ' s mission which is to hit a fixed target while minimizing the exposure to anti - air defenses . The objective function can be set as integrated altitude along the trajectory . The Gpops tool kit is used to demonstrate GPM on this problem . The case study ' s results show that the method converges fast and precisely . 

Adopting New Rules in Rule - Based Machine Translation
This paper examines the agreement and word - order tackling in Arabic MT systems . The paper represents an English to Arabic approach for translating well - structured English sentences into well - structured Arabic sentences using a new rules to handle the problems of ordering and agreement . The methodology is flexible and scalable , the main advantages are : first , it is a rule - based approach , and second , it can be applied on some other languages with minor modifications . The database has been designed to be flexible where most of the rules are defined in tables in order to generalize the code . Validation rules have been applied in both the database design and the programming code in order to ensure the integrity of data . A major design goal of this system is that it will be used as a stand - alone tool and can be very well integrated with a general machine translation system for English sentence . 

The user non - acceptance paradigm : INFOSEC ' s dirty little secret
This panel will address users ' perceptions and misperceptions of the risk / benefit and benefit / nuisance ratios associated with information security products , and will grope for a solution , based on the psychology of personality trait - factoring results , among other multidisciplinary approaches , to the problem of user non - acceptance of information security products . This problem has acquired a much more scientific guise when amalgamated with the psychology of personality and reinforced by reflections from the field on patterns of user behavior . A gross simplification of the main thrust of the panel is this thesis : if we start profiling the defenders rather than the offenders and do it on the basis of real science rather than very crude personality tests , then we will , at the very least , understand what is happening and possibly create a desirable profile for sysadmins , CIOs , and perhaps even CFOs . This swept - under - the - rug problem is information security ' s " dirty little secret . " No other forum is designed to address this , and it may well become yet another major conceptual and paradigmatic shift in the field , of the type initiated in the NSPWs over the last decade . We know that the panel will generate an assured considerable interest among the participants . 

Keyword Extraction Performance Analysis
This paper presents a survey - cum - evaluation of methods for the comprehensive comparison of the task of keyword extraction using datasets of various sizes , forms , and genre . We use four different datasets which includes Amazon product data - Automotive , SemEval 2010 , TMDB and Stack Exchange . Moreover , a subset of 100 Amazon product reviews is annotated and utilized for evaluation in this paper , to our knowledge , for the first time . Datasets are evaluated by five Natural Language Processing approaches ( 3 unsupervised and 2 supervised) , which include TF - IDF , RAKE , TextRank , LDA and Shallow Neural Network . We use a ten - fold cross - validation scheme and evaluate the performance of the aforementioned approaches using recall , precision and F - score . Our analysis and results provide guidelines on the proper approaches to use for different types of datasets . Furthermore , our results indicate that certain approaches achieve improved performance with certain datasets due to inherent characteristics of the data . 

Automatic lexicon enhancement by means of corpus tagging
Using specialised text corpus to automatically enhance a general lexicon is the aim of this study . Indeed , having lexicons which offer maximal cover on a specific topic is an important benefit in many applications of Automatic Speech and Natural Language Processing . The enhancement of these lexicons can be made automatic as big corpora of specialised texts are available . A syntactic tagging process , based on 3 - class and 3 - gram language models , allows us to automatically allocate possible syntactic categories to the Out - Of - Vocabulary ( OOV ) words which are found in the corpus processed . These OOV words generally occur several times in the corpus , and a number of these occurrences can be important . By taking into account all the occurrences of an OOV word in a given text as a whole , we propose here a method for automatically extracting a specialised lexicon from a text corpus which is representative of a specific topic . 

Research on ethnic minority language text tools and corpora of China
The increasing interest in the use of large - scale textual resources for NLP research has led to the rapid proliferation of both massive amounts of textual data and text - handling tools . Most of them are dedicated to a widely - used language ( such as English , French or Chinese) . However , huge efforts are required to develop the corresponding resource for other ethnic minority languages . In this paper , we analyze the requirements of ethnic minority language corpus , discuss some surrounding issues on the development of corpus - handling tools for ethnic minority language , and present the initial results of above work . 

Automatic rules extraction from medical texts
The majority of existing knowledge is encoded in unstructured texts and is not linked to formalized knowledge , like ontologies and rules . The potential solution to this problem is to acquire this knowledge through natural language processing ( NLP ) tools and text mining techniques . Prior work has focused on the automatic extraction of ontologies from texts , but the acquired knowledge is generally limited to simple hierarchies of terms . This paper presents a polyvalent framework for acquiring more complex relationships from texts and codes them in the form of rules . Our approach starts with existing domain knowledge represented as OWL ontology and SWRL " Semantic Web Rule Language " rules by applying NLP tools and text matching techniques to deduce different atoms as classes , properties etc . This is to capture the deductive knowledge in the form of new rules . We evaluate our approach thereafter by applying it on medical field more precisely Gynecology specialty , showing that this approach can generate automatically and accurately SWRL rules for the representation of more formal knowledge necessary for reasoning . 

Multilingual computational semantic lexicons in action : the WYSINNWYG approach to NLP
Much effort has been put into computational lexicons over the years , and most systems give much room to ( lexical ) semantic data . However , in these systems , the effort put on the study and representation of lexical items to express the underlying continuum existing in 1 ) language vagueness and polysemy , and 2 ) language gaps and mismatches , has remained embryonic . A sense enumeration approach fails from a theoretical point of view to capture the core meaning of words , let alone relate word meanings to one another , and complicates the task of NLP by multiplying ambiguities in analysis and choices in generation . In this paper , I study computational semantic lexicon representation from a multilingual point of view , reconciling different approaches to lexicon representation : i ) vagueness for lexemes which have a more or less finer grained semantics with respect to other languages ; ii ) underspecification for lexemes which have multiple related facets ; and , iii ) lexical rules to relate systematic polysemy to systematic ambiguity . I build on a What You See Is Not Necessarily What You Get ( WYSINNWYG ) approach to provide the NLP system with the " right " lexical data already tuned towards a particular task . In order to do so , I argue for a lexical semantic approach to lexicon representation . I exemplify my study through a cross - linguistic investigation on spatially - based expressions . 

A Comparative Study of Opinion Summarization Techniques
In the Web 3 . 0 platforms , enormous amount of information is shared whereby individuals express their thoughts and opinions and learn from others ' experiences . Many e - commerce websites provide service of posting opinionated reviews to allow consumers post their opinions using free text . Examples of these e - commerce websites include eBay , Amazon , and Yahoo shopping . Summarizing text is taken as an interesting task of Natural Language Processing ( NLP ) . The proposed work presents a comparative study of different techniques used for opinion summarization . It covers both abstractive and extractive approaches where summary of sentences is achieved by considering aspects . This article highlights the gaps in the previous study by proposing a novel graph - based technique for generating abstractive summary of duplicate sentences . The method discusses the details by constructing graphs , ensuring the sentence correctness using some constraints , and finally scoring the sentences individually by fusing sentiments using SentiWordNet . Extractive approach uses the principle of principal component analysis ( PCA ) . The work includes the application of PCA in summarization of text by reducing the number of dimensions in data ( aspects ) and relatively finding the summary of the reviews on ranking the most relevant ones , according to the prime aspects without any loss of information respective of a particular domain . The analysis is conducted on the standard Opinosis data set and comparison is made between both of the techniques to discuss which method generates more coherent and complete summary . 

Grammar - driven versus data - driven : which parsing system is more affected by domain shifts ? 
In the past decade several parsing systems for natural language have emerged , which use different methods and formalisms . For instance , systems that employ a handcrafted grammar and a statistical disambiguation component versus purely statistical data - driven systems . What they have in common is the lack of portability to new domains : their performance might decrease substantially as the distance between test and training domain increases . Yet , to which degree do they suffer from this problem , i . e . which kind of parsing system is more affected by domain shifts ? Intuitively , grammar - driven systems should be less affected by domain changes . To investigate this hypothesis , an empirical investigation on Dutch is carried out . The performance variation of a grammar - driven versus two data - driven systems across domains is evaluated , and a simple measure to quantify domain sensitivity proposed . This will give an estimate of which parsing system is more affected by domain shifts , and thus more in need for adaptation techniques . 

Exploiting latent information to predict diffusions of novel topics on social networks
This paper brings a marriage of two seemly unrelated topics , natural language processing ( NLP ) and social network analysis ( SNA ) . We propose a new task in SNA which is to predict the diffusion of a new topic , and design a learning - based framework to solve this problem . We exploit the latent semantic information among users , topics , and social connections as features for prediction . Our framework is evaluated on real data collected from public domain . The experiments show 16 % AUC improvement over baseline methods . The source code and dataset are available at http : / / www . csie . ntu . edu . tw / ~ d97944007 / diffusion / 

Implementation approaches for various categories of question answering system
Search engines can return ranked documents as a result for any query from which the user struggle to navigate and search the correct answer . This process wastes user ' s navigation time and due to this the need for automated question answering systems becomes more urgent . We need such a system which is capable of replying the exact and concise answer to the question posed in natural language . The best way to address this problem is use of Question answering systems ( QAS ) . The basic aim of QAS is to provide short and correct answer to the user saving his / her navigation time . The concept of Natural Language Processing plays an important role in developing any QAS . This paper provides an implementation approaches for various categories of QAS such as Closed Domain based QAS , Open Domain based QAS , WEBBASED QAS , Information Retrieval or Information Extraction ( IR / IE ) based QAS , and Rule based QAS which will be helpful for new directions of research in this area . 

Text normalization algorithm for facebook chats in Hausa language
The rapid increase in using non - standard words ( NSWs ) in communication through the social media is causing difficulties in understanding contents of the text messages . In addition , it affects the performance of several natural language processing ( NLP ) task such as machine translation , information retrievals , summarization and etc . In this study , we present an automatic text normalization system on Facebook chatting based on Hausa language . The proposed algorithm manually developed dictionary that employ normalization of each non - standard word with its equivalent standard word . This is accomplished through modification of the technique employed by [ 1 ] to fit Hausa NSWs ' formation . It was found that our proposed algorithm was able to normalized Hausa NSWs with an accuracy of 100 % The results of this research can facilitate comprehensive communication via Facebook using Hausa language . 

Natural language interface to a corporate cash management system
The development is described of a natural language interface to the corporate cash management software of a major bank . The author presents an overview of natural language processing technology , discusses the project ' s advantages and limitations , and suggests other applications for NLP in banking systems . < > View less

BE : a search engine for NLP research
Many modern natural language - processing applications utilize search engines to locate large numbers of Web documents or to compute statistics over the Web corpus . Yet Web search engines are designed and optimized for simple human queries - - - they are not well suited to support such applications . As a result , these applications are forced to issue millions of successive queries resulting in unnecessary search engine load and in slow applications with limited scalability . 

On dual decomposition and linear programming relaxations for natural language processing
This paper introduces dual decomposition as a framework for deriving inference algorithms for NLP problems . The approach relies on standard dynamic - programming algorithms as oracle solvers for sub - problems , together with a simple method for forcing agreement between the different oracles . The approach provably solves a linear programming ( LP ) relaxation of the global inference problem . It leads to algorithms that are simple , in that they use existing decoding algorithms ; efficient , that they avoid exact algorithms for the full model ; and often exact , in that empirically they often recover the correct solution in spite of using an LP relaxation . We give experimental results on two problems : 1 ) the combination of two lexicalized parsing models ; and 2 ) the combination of a lexicalized parsing model and a trigram part - of - speech tagger . 

Applying POMDP to moving target optimization
Diversity maintains security by making the computing environment less standard and less predictable . Recent studies show that many randomization techniques , e . g . address space layout randomization ( ASLR ) significantly enhance system security simply through reducing the number of return to libc exploits [ 14] . However , " diversity " may incur significant overhead on the computing platforms . We study the problem of implementing diversity to trade off security performance with diversity implementation costs . We address this problem by formulating it as a partially observable Markov decision process ( POMDP ) . An optimal solution considering a fixed amount of history can be obtained by transforming the POMDP optimization problem into a nonlinear programming ( NLP ) problem . Simulation results for a set of benchmark problems illustrate the effectiveness of the proposed method . 

Lexical classes based stop words categorization for Gujarati language
Stop words elimination is important pre - processing step in Natural Language Processing ( NLP ) and text mining applications . Stop words removal improves the performance and quality of classifications system . In the context of classification task it is possible to reduce number of dimensions in the term of space by removing most common words which has less significant meaning and irrelevant . But it does not mean stop words removal can improve the performance of all types of applications in the area of NLP , Artificial Intelligence , Text Mining and Machine Translation . In context of Machine Translation ( MT ) stop words elimination process will lead to loss of accuracy , because each token has specific meaning which will be converted into target language . As on date there is no unique stop words list is available for Gujarati language with its lexical classes ( Part - of - Speech Tags ) to improve the performance of MT system . This paper present construction and categorization of stop words list for Gujarati language based on its lexical classes ( nouns , verbs , adjectives , adverbs , etc . ) of Part - of - Speech family . We have prepared 126 raw text documents written in Gujarati language in which each document contained more than 260 tokens . After tokenization process , we got list of 32840 tokens . From the total number of tokens we created list of 1125 unique stop words with its lexical classes by manual inspection and help of linguistic experts . The stop words list and specifically categorization thereof is released herewith for NLP applications , particularly MT systems , in Gujarati language by the research community . 

Event - event relation identification : A CRF based approach
Temporal information extraction is a popular and interesting research field in the area of Natural Language Processing ( NLP ) . The main tasks involve the identification of event - time , event - document creation time and event - event relations in a text . In this paper , we take up Task C that involves identification of relations between the events in adjacent sentences under the TimeML framework . We use a supervised machine learning technique , namely Conditional Random Field ( CRF ) . Initially , a baseline system is developed by considering the most frequent temporal relation in the task ' s training data . For CRF , we consider only those features that are already available in the TempEval - 2007 training set . Evaluation results on the Task C test set yield precision , recall and F - score values of 55 . 1 % , 55 . 1 % and 55 . 1 % , respectively under the strict evaluation scheme and 56 . 9 % , 56 . 9 and 56 . 9 % , respectively under the relaxed evaluation scheme . Results also show that the proposed system performs better than the baseline system . 

Exploiting lexical expansions and Boolean compositions for web querying
This paper describes an experiment aiming at evaluating the role of NLP based optimizations ( i . e . morphological derivation and synonymy expansion ) in web search strategies . Keywords and their expansions are composed in two different Boolean expressions ( i . e . expansion insertion and Cartesian combination ) and then compared with a keyword conjunctive composition , considered as the baseline . Results confirm the hypothesis that linguistic optimizations significantly improve the search engine performances . 

Nonlinear programming problem solving based on winner take all emotional neural network for tensegrity structure design
In this paper , a tensegrity structure ( TS ) design is formulated as a nonlinear programming ( NLP ) problem , and a winner - take - all artificial emotional neural network ( WTA - ENN ) is proposed to solve the resulting NLP . The main feature of proposed WTA - ENN is related to low number of learning weights and simplicity of its learning rules that make it a suitable model for complicated TS design problems . Numerical results indicate that WTA - ENN can effectively solve NLP problem obtained from basic module of a typical TS Tower . The proposed method can be effectively used in architectural , structural and robotics design . 

Applied AI matters : AI4Code : applying artificial intelligence to source code
The marriage of Artificial Intelligence ( AI ) techniques to problems surrounding the generation , maintenance , and use of source code has come to the fore in recent years as an important AI application area1 . A large chunk of this recent attention can be attributed to contemporaneous advancements in Natural Language Processing ( NLP ) techniques and sub - fields . The naturalness hypothesis , which states that " software is a form of human communication " and that code exhibits patterns that are similar to ( human ) natural languages ( Devanbu , 2015 ; Hindle , Barr , Gabel , Su , & Devanbu , 2016 ) , has allowed for the application of many of these NLP advances to code - centric usecases . This development has contributed to a spate of work in the community - - - much of it captured in a survey by Allamanis , Barr , Devanbu , and Sutton ( 2018 ) that focuses on classifying these approaches by the type of probabilistic model applied to source code . This increase in the variety of AI techniques applied to source code has found various manifestations in the industry at large . Code and software form the backbone that underpins almost all modern technical advancements : it is thus natural that breakthroughs in this area should reflect in the emergence of real world deployments . 

Introduction to Chinese Natural Language Processing
Book Abstract : This book introduces Chinese language - processing issues and techniques to readers who already have a basic background in natural language processing ( NLP ) . Since the major difference between Chinese and Western languages is at the word level , the book primarily focuses on Chinese morphological analysis and introduces the concept , structure , and interword semantics of Chinese words . The following topics are covered : a general introduction to Chinese NLP ; Chinese characters , morphemes , and words and the characteristics of Chinese words that have to be considered in NLP applications ; Chinese word segmentation ; unknown word detection ; word meaning and Chinese linguistic resources ; interword semantics based on word collocation and NLP techniques for collocation extraction . Table of Contents : Introduction / Words in Chinese / Challenges in Chinese Morphological Processing / Chinese Word Segmentation / Unknown Word Identification / Word Meaning / Chinese Collocations / Automatic Chinese Collocation Extraction / Appendix / References / Author BiographiesView less

A Verb - Centric Approach for Relationship Extraction in Biomedical Text
Advances in biomedical technology and research have resulted in a large number of research findings , which are primarily published in unstructured text such as journal articles . Text mining techniques have been thus employed to extract knowledge from such data . In this article we focus on the task of identifying and extracting relations between bio - entities such as green tea and breast cancer . Unlike previous work that employs heuristics such as co - occurrence patterns and handcrafted syntactic rules , we propose a verb - centric algorithm . This algorithm identifies and extracts the main verb ( s ) in a sentence , therefore , it does not require the usage of predefined rules or patterns . Using the main verb ( s ) it then extracts the two involved entities of a relationship . The biomedical entities are identified using a dependence parse tree by applying syntactic and linguistic features such as preposition phrases and semantic role analysis . The proposed verb - centric approach can effectively handle complex sentence structures such as clauses and conjunctive sentences . We evaluate the algorithm on several data sets and achieve an average F - score of 0 . 905 , which is significantly higher than that of previous work . 

HyperEmbed : Tradeoffs Between Resources and Performance in NLP Tasks with Hyperdimensional Computing Enabled Embedding of n - gram Statistics
Recent advances in Deep Learning have led to a significant performance increase on several NLP tasks , however , the models become more and more computationally demanding . Therefore , this paper tackles the domain of computationally efficient algorithms for NLP tasks . In particular , it investigates distributed representations of $ n $ - gram statistics of texts . The representations are formed using hyperdimensional computing enabled embedding . These representations then serve as features , which are used as input to standard classifiers . We investigate the applicability of the embedding on one large and three small standard datasets for classification tasks using nine classifiers . The embedding achieved on par $ F _{ 1} $ scores while decreasing the time and memory requirements by several times compared to the conventional $ n $ - gram statistics , e . g . , for one of the classifiers on a small dataset , the memory reduction was 6 . 18 times ; while train and test speed - ups were 4 . 62 and 3 . 84 times , respectively . For many classifiers on the large dataset , memory reduction was ca . 100 times and train and test speed - ups were over 100 times . Importantly , the usage of distributed representations formed via hyperdimensional computing allows dissecting strict dependency between the dimensionality of the representation and n - gram size , thus , opening a room for tradeoffs . 

Complex adaptive reasoning : knowledge emergence in the revelator game
We introduce the game of complex adaptive reasoning ( CAR ) , as a self - correcting methodology for knowledge emergence in dialogue and narrative contexts of collaboration . Inspired by Holland ' s complex adaptive systems research ( CAS ) , Revelator ' s CAR methodology is designed to model the complex logical relations among conjectures ( represented in " If & then " rule form ) that players articulate in game plays . The game ' s agent - based representation of these rule - form plays gives them adaptive capability , making it possible for " argument colonies " to emerge as possible knowledge to be tested . We are experimenting with natural language processing ( NLP ) query support for controlled natural language ( CNL ) formulation of game plays , to reveal knowledge formation in the conversational contexts of news blogs and research wikis . In Revelator ' s framework , knowledge is represented as continually emerging , not simply to be captured but to advance in a recursive process that improves human reasoning skills with appropriate support from technology . 

Architecture of Knowledge Extraction System based on NLP
Knowledge extraction is to extract useful structured text information from messy free text . Under the current massive information background , it has attracted extensive attention . This paper analyzes the concept of NLP and the application process of NLP algorithm , discusses web information retrieval system , information extraction based on natural language processing and text relationship extraction , and tests the pipeline performance . The results show that the pipeline time is not linearly correlated with the size of the novel , but positively correlated . 

Towards applying OCR and Semantic Web to achieve optimal learning experience
As more and more learners are opting for online learning , e - learning industry is working on improving learning experience of online user by providing relevant content and lot of additional references . Since online learners mostly prefer video tutorials , identifying major topics and subtopics covered in video tutorial is a big challenge . Recently , for efficient knowledge sharing and interoperability over web lot of attention is given to semantic web . In this paper , we propose a semantic web - based framework for automatic topic identification from video tutorials in order to identify the concepts and their associated semantically relevant resources . Our framework identifies relevant topic using disambiguation in e - learning resource which helps learners in more focused study . 

Text sentiment polarity classification based on TextCNN - SVM combination model
In the face of a large amount of information , managing , filtering , and screening information has become effectively one of the focuses in information science , especially in the accurate processing of text information . In this paper , a TextCNN - SVM combination model for text sentiment polarity classification was proposed and implemented in the TensorFlow framework . Combined with the ability of TextCNN to extract text eigenvalues well and the classification function of Support Vector Machine ( SVM ) to find the maximum segmentation hyperplane , we improve the shortcoming of the SoftMax layer , which is weaker than the machine learning algorithm in terms of classification ability and generalization ability . Compared with the traditional TextCNN model , the accuracy of the combined model has been improved by about 12 . 37% . 

Analyzing social media marketing in the high - end fashion industry using named entity recognition
We study the marketing strategies of high - end fashion brands in social media . In particular , we focus on the informational content of brands ' posts in Instagram . Using Named Entity Recognition ( NER ) in Natural Language Processing ( NLP ) , we develop a novel procedure to classify posts according to their information content . In addition , we apply NER to department store listings and expert runway reviews to obtain measures of brand leadership and brand similarity . Regression analyses show that , while follower brands respond to brand similarity and competitive pressure by relying on informational posts , the informational content of leaders presents a U - shaped relation with brand similarity . We interpret this finding using two theories from the marketing literature : the tradeoff between new and existing customers , and marketing life cycle of industries . 

IoT Device Auto - Tagging Using Transformers
The IoT platform can identify a device matching the data only when securing the information on a large amount of IoT devices in advance . However , in a situation where a lot of companies release a variety type of IoT , it is not easy to retain information about all IoT devices . That is , it is a difficult situation where the current IoT platform has difficulty in analyzing unstandardized tagging information . Therefore , this paper provides a technique of ` Tagging ' that automatically identifies the device based on information collected from IoT devices or sensor devices . This technique was developed based on the natural language processing algorithm based on the attention mechanism among the machine learning models . 

Hate Speech & Offensive Language Detection Using ML & NLP
To restore peace and harmony in this cross - cultural Internet era , it is of utmost importance for every citizen to behave and spread brotherhood . Under the given circumstances of 5G evolution citizens have taken their role onto the internet very seriously thereby most of the netizens spend their time condemning , judging , and trolling other netizens , public figures for that matter . Because of the consequences in an unprejudiced society involving race , gender , or religion , the challenge of automatically detecting hate speech and objectionable language in social media material is critical . However , existing research in this field is mostly focused on several languages , which limits its relevance to certain groups . The use of harsh language on social media platforms , as well as the consequences that this has , has become a serious problem in modern culture . Automatic ways to recognize and deal with this sort of content are necessary due to the large volume of content produced every day . Machine Learning & Natural Language processing has cutting - edge algorithms and classifiers that have benefitted mankind in impossible ways . Hence , our effort in this project is to make use of this impeccable technology to create an efficient system that automatically detects hate speech and offensive language from the Twitter dataset . 

Leveraging hidden dialogue state to select tutorial moves
A central challenge for tutorial dialogue systems is selecting an appropriate move given the dialogue context . Corpus - based approaches to creating tutorial dialogue management models may facilitate more flexible and rapid development of tutorial dialogue systems and may increase the effectiveness of these systems by allowing data - driven adaptation to learning contexts and to individual learners . This paper presents a family of models , including first - order Markov , hidden Markov , and hierarchical hidden Markov models , for predicting tutor dialogue acts within a corpus . This work takes a step toward fully data - driven tutorial dialogue management models , and the results highlight important directions for future work in unsupervised dialogue modeling . 

Introducing inference - driven OWL ABox enrichment
Publically available text - based documents ( e . g . news , meeting transcripts ) are a very important source of knowledge for organizations and individuals . These documents refer domain entities such as persons , places , professional positions , decisions , actions , etc . Querying these documents ( instead of browsing , searching and finding ) is a very relevant task for any person in general , and particularly for professionals dealing with intensive knowledge tasks . Querying text - based documents ' data , however , is not supported by common technology . For that , such documents ' content has to be explicitly and formally captured into knowledge base facts . Making use of automatic NLP processes for capturing such facts is a common approach , but their relatively low precision and recall give rise to data quality problems . Further , facts existing in the documents are often insufficient to answer complex queries and , therefore , it is often necessary to enrich the captured facts with facts from third - party repositories ( e . g . public LOD , private IS databases ) . This paper describes the adopted process to identify what data is currently missing from the knowledge base repository and which is desirable to collect from external repositories . The proposed process aims to foster and is driven by OWL DL inference - based instance ( ABox ) classification , which is supported by the constraints of the TBox . 

An MILP - NLP decomposition approach applied to a refinery scheduling problem
A classic refinery scheduling problem is commonly presented as a mixed - integer nonlinear programming ( MINLP ) problem with bilinear terms , resulting in poor solution quality and time - consuming computational issue . This work proposes a decomposition strategy for the integrated model , where the discretized and relaxed problems are broken down into two stages . The master problem is addressed first , which is a linear relaxation of the original MINLP that makes available a global minimum of the cost . The sub - problem is solved as a second step and is a smaller NLP with a fixed set of discrete variables . Then , an iterative algorithm is proposed through variable transfer to avoid the composition discrepancies and impracticable solutions . The proposed method for MINLP is compared with standard MINLP solvers . The results in several cases from a real refinery demonstrate the effectiveness of the proposed method . 

Encoding - Decoding Methods for Neural Machine Translation
Deep Learning techniques have significant progress in the last few years . Different designs and methods of the model have experimented to enhance the performance in various domains of Natural Language Processing ( NLP) . Machine Translation is a domain of NLP . In this paper , we have reviewed different Encoding - Decoding methods for Neural Machine Translation ( NMT) . We also , compare these various methods and put forward a past , present and future of Neural Machine Translation . 

Book recommended formulation based on multiple bibliographic information
We proposed the formulation for discovering the candidate bibliographic records for the primary resource by integrating multiple bibliographic parameters including call number , subject heading , and title ' s keyword . To imitate the searching procedure of librarians , we can customize weights for assigning different priority for different bibliographic parameters . The precision and recall of the retrieval lists were compared to the performance of librarians . The results show that the quality of the recommended list formulated by our proposed system is comparable to the list from librarian . However , the performance of the proposed system can be improved by integrating NLP algorithm , and more bibliographic information . 

SpaML : a Bimodal Ensemble Learning Spam Detector based on NLP Techniques
In this paper , we put forward a new tool , called SpaML , for spam detection using a set of supervised and unsupervised classifiers , and two techniques imbued with Natural Language Processing ( NLP ) , namely Bag of Words ( BoW ) and Term Frequency - Inverse Document Frequency ( TF - IDF ) . We first present the NLP techniques used . Then , we present our classifiers and their performance on each of these techniques . Then , we present our overall Ensemble Learning classifier and the strategy we are using to combine them . Finally , we present the interesting results shown by SpaML in terms of accuracy and precision . 

Lattice LSTM for Chinese Sentence Representation
Words provide a useful source of information for Chinese NLP , and word segmentation has been taken as a pre - processing step for most downstream tasks . For many NLP tasks , however , word segmentation can introduce noise and lead to error propagation . The rise of neural representation learning models allows sentence - level semantic information to be collected from characters directly . As a result , it is an empirical question whether a fully character - based model should be used instead of first performing word segmentation . We investigate a neural representation that simultaneously encodes character and word information without the need for segmentation . In particular , candidate words are found in a sentence by matching with a pre - defined lexicon . A lattice structured LSTM is used to encode the resulting word - character lattice , where gate vectors are used to control information flow through words , so that the more useful words can be automatically identified by end - to - end training . We compare the performance of the resulting lattice LSTM and baseline sequence LSTM structures over both character sequences and automatically segmented word sequences . Results on NER show that the character - word lattice model can significantly improve the performance . In addition , as a general sentence representation architecture , character - word lattice LSTM can also be used for learning contextualized representations . To this end , we compare lattice LSTM structure with its sequential LSTM counterpart , namely ELMo . Results show that our lattice version of ELMo gives better language modeling performances . On Chinese POS - tagging , chunking and syntactic parsing tasks , the resulting contextualized Chinese embeddings also give better performance than ELMo trained on the same data . 

Localized waves in annular system of tunnel - coupled waveguides with defects
We report on the problem of light propagation in tunnel - coupled waveguides . The theory of switching and forming of localized and delocalized waves in the annular system of coupled waveguides was developed . We first studied the localization of waves in nonlinear defective waveguide . It was shown that the degree of localization increases with increasing nonlinearity coefficient or intensity of input emission in defective waveguide . An effect of defective waveguide screening from neighboring zero - defect waveguides was found . Conditions for excitation of the rotatory switching in coupled waveguides with linear defect were found . 

Deep Recurrent Architecture based Scene Description Generator for Visually Impaired
Vision is the most essential sense for human beings . But today , more than 2 . 2 billion people worldwide suffer from some form of vision impairment . This paper presents an end - to - end human - centric model for aiding the visually impaired by employing the deep recurrent architecture of the start - of - the - art image captioning models . A VGG - 16 net convolutional neural network ( CNN ) is used to extract feature vectors from real - time video ( image frames ) and an long short - term memory ( LSTM ) network is employed to generate captions from these feature vectors . The model is tested on the Flickr 8K Dataset , one of the most popularly used image captioning dataset which contains over 8000 images . On real - time videos , the model generates rich descriptive captions which are converted to audio for a visually impaired person to listen . Comprehensively the model generates promising results which has great potential to enhance the lives of the visually impaired people by assisting them to get a better understanding of their surroundings . 

Natural language processing complexity and parallelism
This paper reviews the processes involved in Natural Language Processing ( NLP ) . It then demonstrates the various kinds of choices that need be taken during the execution of the word morphology , the syntactic text analysis , or text generation components . It compares the time complexity of traditional serial algorithms and examines the possible expected gain in some corresponding parallel counterparts . 

Integrate statistical model and lexical knowledge for Chinese multiword chunking
Multiword chunking is designed as a shallow parsing technique to recognize external constituent and internal relation tags of a chunk in sentence . In this paper , we propose a new solution to deal with this problem . We design a new relation tagging scheme to represent different intra - chunk relations and make several experiments of feature engineering to select a best baseline statistical model . We also apply outside knowledge from a large - scale lexical relationship knowledge base to improve parsing performance . By integrating all above techniques , we develop a new Chinese MWC parser . Experimental results show its parsing performance can greatly exceed the rule - based parser trained and tested in the same data set . 

An experience in developing the Nepali sense tagged corpus
A key resource that aids in several NLP tasks is WordNet . Wordnet is used as the sense inventory for sense tagging of corpus . Sense tagging is the task of tagging each word in the sentence with the correct sense of the word in the given context . Sense tagging activity helps in validation of WordNet and improvement of Wordnet quality . Sense tagging is one of the toughest annotation works and this paper discusses about the Sense Tagging tool , procedures involved in sense tagging the Nepali corpus and the challenges involved in sense tagging . Nepali WordNet is used as the sense inventory for sense tagging of Nepali corpus . For accurately sense tagging voluminous data , a standard and definitive lexicon is required . In this work the corpus in Nepali language is taken from newspaper domain . 

A Deep Learning – based Approach for Emotions Classification in Big Corpus of Imbalanced Tweets
Emotions detection in natural languages is very effective in analyzing the user ' s mood about a concerned product , news , topic , and so on . However , it is really a challenging task to extract important features from a burst of raw social text , as emotions are subjective with limited fuzzy boundaries . These subjective features can be conveyed in various perceptions and terminologies . In this article , we proposed an IoT - based framework for emotions classification of tweets using a hybrid approach of Term Frequency Inverse Document Frequency ( TFIDF ) and deep learning model . First , the raw tweets are filtered using the tokenization method for capturing useful features without noisy information . Second , the TFIDF statistical technique is applied to estimate the importance of features locally as well as globally . Third , the Adaptive Synthetic ( ADASYN ) class balancing technique is applied to solve the imbalance class issue among different classes of emotions . Finally , a deep learning model is designed to predict the emotions with dynamic epoch curves . The proposed methodology is analyzed on two different Twitter emotions datasets . The dynamic epoch curves are shown to show the behavior of test and train data points . It is proved that this methodology outperformed the popular state - of - the - art methods . 

Smart Talents Recruiter - Resume Ranking and Recommendation System
During the previous decade , the augmentation of automatic e - recruitment has led to the enlargement of web channels that are devoted to candidate dissemination . In an economic and strategic context where cost - effective is basal , the recommendation of the candidates for the given job requirements has become mandatory . The purpose of this work is to acquaint the actual results that we have achieved on a new recommendation system named Smart Applicant Ranker which is a candidate recommendation tool designed to supervise recruiters while they input their job requirements into the system . This system is designed using Ontology where we can compare the resume models with the given job requirements to match the best comparable candidates . Two ranking algorithms are underlined in this system which will be invoked to assign a ranking point to the recommended candidates against the other candidates on the recommendation pool . The Smart Applicant Ranker system will be kept in a Semantic Web approach that provides IT recruitment firms to seek experts in an efficient way . 

A niche at the nexus : situating an NLP curriculum interdisciplinarily
This paper discusses the establishment and implementation of a curriculum for teaching NLP . At the core are two classes which involve some theoretical background , extensive hands - on experience with state - of - the - art technologies , and practical application in the form of an intensive programming project . Issues involving interdisciplinary coordination , curriculum design , and challenges in teaching this discipline are discussed . 

Enhancing and Evaluating an Impact of OCR and Ontology on Financial Document Checking Process
This research objective is to increase the efficiency of financial document auditing process for reimbursement by using Optical Character Recognition ( OCR ) and Ontology . On the researched system , user can use the system for checking completeness of document completeness , in accordance with the disbursement accounting standards , with digital photo after paid immediately . In the past it took more than a week to submit the evidence and make an account disbursement document . This research can reduce the time since the occurrence of the program . ( Transaction ) up to the creation of documents with more than 50 % disbursement . 

A Modeling Framework for the Moroccan sociolect recognition used on the social media
In this paper , we propose a new modeling methodology for Moroccan sociolect recognition used on the social media . It is based on detecting the language of each word in the text : classical Arabic , Tamazight , French or English , determination of the dominant language and processing the words belonging to the Moroccan sociolect . The Interest in this area comes from the huge and simultaneous use of , numbers , Latin script or figures and / or emoticons to speak in Arabic in Morocco which is the result of the country ' s history . 

Computer - aided generation of multiple - choice tests
Summary form only given . The paper describes a novel automatic procedure for the generation of multiple - choice tests from electronic documents . In addition to employing various NLP techniques including term extraction and shallow parsing , the system makes use of language resources such as corpora and ontologies . The system operates in a fully automatic mode and also a semiautomatic environment where the user is offered the option to post - edit the generated test items . The results from the conducted evaluation suggest that the new procedure is very effective saving time and labour considerably and that the test items produced with the help of the program are not of inferior quality to those produced manually . 

Optimum Coordination of Directional Overcurrent Relays Using the Hybrid GA - NLP Approach
The time of operation of overcurrent relays ( OCRs ) can be reduced , and at the same time , the coordination can be maintained , by selecting the optimum values of time multiplier setting ( TMS ) and plug setting ( PS ) of OCRs . This paper presents hybrid genetic algorithm ( GA ) - nonlinear programming ( NLP ) approach for determination of optimum values of TMS and PS of OCRs . GA has a drawback of , sometimes , converging to the values which may not be optimum , and NLP methods have a drawback of converging to local optimum values , if the initial choice is nearer to local optimum . This paper proposes a hybrid method to overcome the drawback of GA and NLP method , and determine the optimum settings of OCRs . The main contributions of this paper are - 1 ) systematic method for formulation of problem of determining optimum values of TMS and PS of OCRs in power distribution network as a constrained nonlinear optimization problem , 2 ) determining initial values of TMS and PS using GA technique and finding final ( global optimum ) values using NLP method , thus making use of the advantages of both methods ( and at the same time overcoming the drawbacks of the methods ) . 

Smart Sentimental Agent Analysis through Live Streaming Data
The new branch of science which aims on getting the computers process data and become more learned without the use of explicit programming is termed as Machine Learning . The most premier task of Natural Language processing ( NLP ) is Sentiment analysis or opinion mining . The need for Sentimental Analysis has gained much popularity over the recent years . Through the paper , we target to deal with the problem of Review System , an utmost important part of any organizational CRM . . Data inflow in this project is through the twitter API supplying live stream of tweets . . Finally we would set a stage to provide insights into our future work on sentiment analysis and using this Smart Agent Analysis on existing CRM in order to improve their existing Feedback structure . Answers to these questions are provided by statistical analysis on keyword . 

Analytic thinking of patients ' viewpoints pertain to spa treatment
In the increasingly digitized and connected world , advantages also bring equally important challenges that often require new thinking and approach . One of the primary challenges is data that is being generated in huge quantity at enormous pace and in variety of forms . Social medium has blow up as a sort of online discussion where people create and share the information at a substantial rate . The outlook of social media user is now noticeably used for taking the decisions . Evaluating sentiments , opinions and emotions by a group of people in the form of reviews pertaining to a certain event can carve out the niche of providing insights into the review text analytics , especially when text data is large . On one hand sentiments shows the conformity , disparity or objectivity among the masses whereas on the other hand emotions coming out from text clusters the group feedback . In this paper , the authors analyses the sentiments of patients pertaining to ayurvedic spa treatment . User reviews from the top rated websites are extracted to obtain the sentiments . The authors explore the possible ways to analysis the user sentiments using NLTK and Python programming libraries . 

BETO Emotion Analysis of Facebook Users Reacting to Major Media Outlets in Costa Rica
An emotion analysis service , based on Transfer Learning , was built using BETO , and applied to Facebook posts written in Spanish . The system was tested using comments related to the anniversary of major media outlets in Costa Rica , the comments on these posts provide insight into the public opinion towards the mentioned media outlet . After collecting the comments and processing them into the emotion analyzer , the most common feelings for each media where : Amelia Rueda ( joy ) , La Naci ó n ( anger and sadness ) and Teletica . com ( joy and anger ) . 

Harmonic Means between TF - IDF and Angle of Similarity to Identify Prospective Applicants in a Recruitment Setting
Recruitment industry is better and bigger than ever . There is no denying that technology plays a major role in helping recruiters evolve and adopt with the pace of recruitment on a global scale . With the increasing population , the demand for manpower has been relative to the growth and challenging needs of recruiters ; be it online or traditional way of outsourcing . In this study , we propose a combination of angle or similarity and term frequency – inverse document frequency to easily classify prospective job applicants . The results show that the two models are relative to each other , value - wise and harmonic means . Their values are synchronized to a certain extent based on our query . This is helpful because recruiters may save a lot of time in classifying prospective applicants . It can also be concluded that harmonic similarity is viable in combining the two models . As a future work , it is possible to develop a full featured application to be deployed in a production setting . 

Text Sentiment Analysis Based on ResGCNN
Sentiment analysis of text is a significant task in Natural Language Processing ( NLP ) , and Recurrent Neural Network ( RNN ) and Convolutional Neural Network ( CNN ) are two commonly used deep learning models of NLP . RNN ' s variant Long Short - Term Memory ( LSTM ) and Gated Recurrent Unit ( GRU ) can solve the long - term dependence of traditional RNN . Their variant Bi - directional LSTM ( BiLSTM ) , Bi - directional GRU ( BiGRU ) solve the problem that unidirectional LSTM and GRU can only access the above but not the below . However , the ability of the cyclic neural network to extract key features is not strong . CNN can extract local features of text vectors effectively , but ignores the meaning of text contexts . TextCNN can extract text location features by using convolution cores of multiple different windows . As the number of layers increases , deep learning can obtain more complex features , but it will cause the problem of gradient disappearance , and errors cannot be effectively back - propagated , and the residual networks can solve it . This paper proposes a ResGCNN network combining residual network , multilayer BiGRU , and TextCNN . The excellent classification performance is obtained on multiple English and Chinese data sets . 

An Automated Topology Synthesis Framework for Analog Integrated Circuits
This article presents an analog integrated circuit automated topology synthesis framework , where circuit topology synthesis can be efficiently realized by encoding circuit topology generation process as tree structure construction . Then the tree structures are decoded into circuit topologies . Our proposed method can not only handle large circuit designs but also generate creative topologies . To ensure only unique circuit topologies to be generated , two levels of isomorphism checks are performed at both tree structure level and circuit topology level . Then the generated un - sized circuit topologies are efficiently evaluated through a new method , which integrates topological symbolic analysis with g m / I D methodology and curve - fitting technique . Along with the small - signal analysis , both linear and nonlinear programming techniques are utilized for topology feasibility checking . With only a small number of circuit topologies through the fast evaluation stage toward the subsequent detailed sizing and further evaluation , the efficiency of the whole circuit synthesis process can be significantly improved . The experimental results demonstrate high efficiency , strong reliability , and wide applicability of our proposed methods . 

ANNIS : complex multilevel annotations in a linguistic database
We present ANNIS , a linguistic database that aims at facilitating the process of exploiting richly annotated language data by naive users . We describe the role of the database in our research project and the project requirements , with a special focus on aspects of multilevel annotation . We then illustrate the usability of the database by illustrative examples . We also address current challenges and next steps . 

Korean - Thai Lexicon for Natural Language Processing
This paper presents Korean - Thai lexicon . This research aims to study and collect necessary features to construct the Korean - Thai lexicon for natural language processing ( NLP ) and speech processing researches . The research method used for study was that of ( 1 ) creating Korean - Thai lexicon consisting of 7 parts : Korean words , Korean Revised Romanization , part of speech , sub part of speech , special characteristic , Thai meaning and description of meaning ( 2 ) Korean transcription . According to lack of useful tools for the Korean - Thai machine translation , therefore we have a proposal for creating Korean - Thai lexicon for machine translation . The Korean - Thai lexicon consists of 36 , 000 Korean words . As it would take a lot of time and effort to gather enough Korean words to cover all domains , Korean Revised Romanization was applied for some words such as terminology , names and places . 

Detecting phishing attacks from URL by using NLP techniques
Nowadays , cyber attacks affect many institutions and individuals , and they result in a serious financial loss for them . Phishing Attack is one of the most common types of cyber attacks which is aimed at exploiting people ' s weaknesses to obtain confidential information about them . This type of cyber attack threats almost all internet users and institutions . To reduce the financial loss caused by this type of attacks , there is a need for awareness of the users as well as applications with the ability to detect them . In the last quarter of 2016 , Turkey appears to be second behind China with an impact rate of approximately 43 % in the Phishing Attack Analysis report between 45 countries . In this study , firstly , the characteristics of this type of attack are explained , and then a machine learning based system is proposed to detect them . In the proposed system , some features were extracted by using Natural Language Processing ( NLP ) techniques . The system was implemented by examining URLs used in Phishing Attacks before opening them with using some extracted features . Many tests have been applied to the created system , and it is seen that the best algorithm among the tested ones is the Random Forest algorithm with a success rate of 89 . 9 % . 

Normalizing Spontaneous Reports Into MedDRA : Some Experiments With \ mathsf { MagiCoder } \ mathsf { MagiCoder } 
Text normalization into medical dictionaries is useful to support clinical tasks . A typical setting is pharmacovigilance ( PV ) . The manual detection of suspected adverse drug reactions ( ADRs ) in narrative reports is time consuming and natural language processing ( NLP ) provides a concrete help to PV experts . In this paper , we carry out experiments for testing performances of MagiCoder , an NLP application designed to extract MedDRA terms from narrative clinical text . Given a narrative description , MagiCoder proposes an automatic encoding . The pharmacologist reviews , ( possibly ) corrects , and then , validates the solution . This drastically reduces the time needed for the validation of reports with respect to a completely manual encoding . In previous work , we mainly tested MagiCoder performances on Italian written spontaneous reports . In this paper , we include some new features , change the experiment design , and carry on more tests about MagiCoder . Moreover , we do a change of language , moving to English documents . In particular , we tested MagiCoder on the CADEC dataset , a corpus of manually annotated posts about ADRs collected from the social media . 

Review of Real - word Error Detection and Correction Methods in Text Documents
Spell checking is one of the most widely used tasks of NLP . It has broad range of uses like information retrieval , proofreading , email client etc . Today in many applications of NLP spell checker is being used . It is a language tool which breaks down the text for spelling errors . It flags when there exists any misspelled / unintended word in the given text . The typographic errors are categorized in ` non - word errors ' and ` real - word errors ' . There is enough work done in order to tackle the farmer error but it still remains the challenge for the researchers to tackle the later one . This paper focuses on later one and analyses the methods which are being used worldwide to detect and correct such errors . Paper also focuses on the challenges faced by researchers while processing the real - word errors . 

A graph - based approach for semantic similar word retrieval
Semantic relatedness or semantic similarity between words is an important basic issue for many Natural Language Processing ( NLP ) applications , such as sentence retrieval , word sense disambiguation , question answering , and so on . This research issue attracts many researchers , but most of studies focus on improving the effectiveness , i . e . , applying kinds of techniques to improve precision ( effectiveness ) but not efficiency . To tackle the problem , we propose to address the efficiency issue , that how to efficiently find top - k most semantic similar words to the query for a given dataset . This issue is very important for real applications especially for current big data . Efficient graph - based approaches on searching top - k semantic similar words are proposed in this paper . The results demonstrate that the proposed model can perform significantly better than baseline method . 

Serving the readers of scholarly documents : A grand challenge for the introspective digital library
The scholarly literature produced by human civilization will soon be considered small data , able to be portably conveyed by the network and carried on personal machines . This semi - structured text centric knowledge base is a focus of attention for scholars , as the wealth of facts , facets and connections in scholarly documents are large . Such machine analysis can derive insights that can inform policy makers , academic and industrial management , as well as scholars as authors themselves . There is another underserved community of scholarly document users that has been overlooked : the readers themselves . I call for the community to put more efforts towards supporting our own scholars ( especially beginning scholars , new to the research process ) with automation from information retrieval and natural language processing . Techniques that mine information from within the full text of a document could be used to introspect a digital library ' s materials , inferring better search metadata , improving scholarly document recommendation , and aiding the understanding of the text , figures , presentations and citations of our scholarly literature . Such an introspective digital library will enable scholars to assemble an understanding of other scholars ' work more efficiently , and provide downstream machine reading applications with input for their analytics . 

Introduction to the special issue on finite - state methods in NLP
No abstract available . 

BEN collaborative poster
Summary form only given . In 1999 , the American Association for the Advancement of Science ( AAAS ) Directorate for Education and Human Resources ( EHR ) Programs and Science ' s Signal Transduction Knowledge Environment ( STKE ) - with 11 other professional societies and coalitions for biological sciences - established the BiosciEdNet ( BEN ) Collaborative . Since its inception , BEN has grown from its original 11 to 24 Collaborators . Currently , the digital library collections of BEN Collaborators provide a rich array of materials for undergraduate biological sciences educators , including ones that prepare K - 12 teachers . BEN Collaborators are building digital collections that are inclusive of all educators and students . In summary , BEN has already developed tools and services that can be shared to leverage technologies across societies , coalitions , and collections affiliates . BEN has a documentation site - http : / / www . biosciednet . org / project _ site / that includes all its technical standards and specifications . For long - term sustainability the BEN Collaborative goal is to design and develop digital library collections that are valued by the members of professional societies , thereby eventually ensuring inclusion in the ongoing operating budgets of societies . Some societies are already providing in - kind support for staff and dollars for developmentView less

Named Entity Recognition using Machine learning techniques for Telugu language
In this paper , we depict hybrid approach , i . e . , combination of rule based approach and machine learning techniques , i . e Conditional Random Fields ( CRF ) for Named Entity Recognition ( NER ) . The main objective of Named Entity Recognition is to categorize all Named Entities ( NE ) in a document into predefined classes like Person name , Location name , Organization name . This paper first outlines the Named Entity Recognizer using rule based approach . In this approach we prepared Gazette lists for names of persons , locations and organizations , some suffix and prefix features and dictionary consist of 200000 words to recognize the category of names entities . Further , we used Machine learning technique , i . e . , CRF in order to improve the accuracy of the system . 

Machine Learning Classifications of Coronary Artery Disease
Coronary Artery Disease ( CAD ) is one of the leading causes of death worldwide , and so it is very important to correctly diagnose patients with the disease . For medical diagnosis , machine learning is a useful tool ; however features and algorithms must be carefully selected to get accurate classification . To this effect , three feature selection methods have been used on 13 input features from the Cleveland dataset with 297 entries , and 7 were selected . The selected features were used to train three different classifiers , which are SVM , Na ï ve Bayes and KNN using 10 - fold cross - validation . The resulting models evaluated using Accuracy , Recall , Specificity and Precision . It is found that the Na ï ve Bayes classifier performs the best on this dataset and features , outperforming or matching SVM and KNN in all the four evaluation parameters used and achieving an accuracy of 84% . 

Semantic web based machine translation
This paper describes the experimental combination of traditional Natural Language Processing ( NLP ) technology with the Semantic Web building stack in order to extend the expert knowledge required for a Machine Translation ( MT ) task . Therefore , we first give a short introduction in the state of the art of MT and the Semantic Web and discuss the problem of disambiguation being one of the common challenges in MT which can only be solved using world knowledge during the disambiguation process . In the following , we construct a sample sentence which demonstrates the need for world knowledge and design a prototypical program as a successful solution for the outlined translation problem . We conclude with a critical view on the developed approach . 

A case study on experimental - data validation for natural language processing
Text data randomly extracted from a particular corpus are usually employed by NLP ( Natural language processing ) systems as experimental data . However , it is hard to determine whether the experimental data is appropriate without a reasonable validation process . We in this paper describe a data validation approach for a NLP - based e - learning system we have built before . 

New resources trigger new technologies
Two decades ago large - scale corpora , as new language resources brought forth a new paradigm shift marked by the revival of empiricism . However , now some researchers including the beginner of the revival began to rethink : “ what should they ( next generation students ) do when most of the low hanging fruit has been pretty much picked over ? ” or to predict that the weird state of computational linguistics without general linguistics should be brought to an end . The author anticipates a newly adjusting of paradigm is approaching . Again new language resources will trigger new NLP technologies . What are the new language resources like ? The resources like HowNet will soon be brought into full play . Corpora helped us achieve shallow practice . Instead , HowNet will take us deeper and thus may help us reach the high - hanging fruit . After a brief overview of HowNet , the author will give an overall demonstration of three HowNet - based application tools that are all closely related to some immediate potential demands . They are : ( 1 ) Text - CT ( vs . Text - X - ray ) , which can show all the senses of each word and expression of a text , rather than merely word strings or at most its POS ; ( 2 ) Sense - Colony - Tester , which works on the basis of an sense colony activator and is able to measure the sense colony testing value of each sense in the text ; ( 3 ) Morphological Decomposer , which can be used to deal with various types of OOVs by decomposing the morphological formation in both English and Chinese and extracting their meanings . 

Fabrication of micro - Bragg reflectors by guided beams in photorefractive Cu : H : LiNbO3 : Mg waveguides
High photorefractive response is obtained in the photorefractive LiNbO 3 waveguides fabricated by the combined proton and copper exchange . It has been shown , that holographic recording with guided beams can be more effectively used , comparatively to recording with external beams , for fabrication of micro - Bragg reflectors in the photorefactive LiNbO 3 waveguides . 

Hadith Authenticity Prediction using Sentiment Analysis and Machine Learning
Starting around 815AD / 200AH scholars have put immense effort towards gathering and sifting authentic hadiths , which are prophetic traditions of the Muslim community . The authenticity of a hadith solely depends on the reliability of its reporters and narrators . Till now scholars have had to do this task manually by precisely anatomizing each hadith ' s chain of narrators or the list of people related to the transmission of a particular hadith . The evolution of modern computer science techniques has enabled new methods and introduced a potential paradigm shift in the science of hadith authentication . Focusing on the chain of narrators ( also known as " Isnad " ) of a hadith , we have used a technique called ` Sentiment Analysis ' from Natural Language Processing ( NLP ) to build a text classifier which tries to predict the authenticity of a hadith . It learns from our custom - made dataset of Isnads and predicts an unknown hadith to be either authentic or fabricated based upon its Isnad . Our classifier was 86 % accurate when tested on the test hadith dataset . 

Enhancing the Detection of Criminal Organizations in Mexico using ML and NLP
This paper relies on Machine Learning ( ML ) and supervised Natural Language Processing ( NLP ) to generate a geo - referenced database on the violent presence of Mexican Criminal Organizations ( MCOs ) between 2000 - 2018 . This application responds to the need for high - quality data on criminal groups to inform academic and policy analysis in a context of intense violence such as Mexico . Powered by ML and NLP tools , this computational social science application processes a vast collection of news stories written in Spanish to track MCOs ' violent presence . The unprecedented granularity of the data allows disaggregating daily - municipal information for 10 main MCOs comprising more than 200 specific criminal cells . 

Tools for extracting and structuring knowledge from texts
We demonstrate an approach and an accompanying UNIX toolbox for performing various kinds of Knowledge Extractions and Structuring . The goal is to " practically " enhance the productivity while constructing resources for NLP systems on the basis of large corpora of technical texts . , Users are lexicon / grammar builders , terminologists and knowledge engineers . We stay open to already explored methods in this or neighbouring activities but put a greater stress on the use of linguistic knowledge . The originality of the work presented here lies in the scope of applications addressed and in the degree of use of linguistic knowledge . 

A Study on Machine Learning and Deep Learning Methods Using Feature Extraction for Bengali News Document Classification
News is newly received remarkable facts about current phenomenon . Miscellaneous facts are constantly happening in this world . Mass media helps to reach these facts to the common folks widely . As we are pushed forward to modern world , getting a convenient environment , Bengali mass media are also leaning towards digital platforms . In this article , some supervised machine learning approaches and deep learning approaches have been proposed for classifying Bengali news documents . We have used an open dataset for our work which contains more than three hundred thousand ( 3 , 76 , 211 ) Bengali text documents . Removing stop - words , dropping duplicate data , tokenizing , stemming etc have been commonly done as preprocessing steps . Bag - of - Words with TF - IDF and some Word Embedding approaches - Average Word2Vec , Glove & fastText have been used for feature extraction . We have trained our text corpus using supervised machine learning method and Deep learning method . Significantly , among these models , Support Vector Machine with average Word2Vec has achieved 97 % accuracy and Bidirectional LSTM has achieved 96 % accuracy . 

Ontology as the basic framework for knowledge processing
A new way of organizing knowledge to assure the best use of digital library is presented . Terms serve as the basic elements for constructing the multi - dimensional knowledge network using both ontological and generic concept relations , within which entity hierarchy acts as the skeleton of the ontology with various modules added to take care of the properties and activities of the entities . It is flexible and adaptable , and the multiple pathways thus made available for searching contribute to its efficiency . 

Aspect - Based Personalized Review Ranking
Many users of e - commerce websites are often used to making purchase decisions based on the product reviews . Since there are too many reviews attached to a product , users may not have enough time and patience to read all the reviews . Moreover , since each user has different requirements for different characteristics of the product , most reviews may have little help in making his purchase decision . To address this issue , we propose a personalized review ranking method to select useful reviews for users . To be specific , we try to assign a helpfulness score for each review , which measures the value of each review for a user ' s decision making . Then , we sort reviews in descending order of helpfulness , so as to generate a subset of reviews for the user . Our main idea is to find similar users ' reviews for a target user , and examine the number of aspects that similar users have reviewed . By considering the similarity between users and the number of aspects of that users have reviewed , we produce personalized review recommendation according to the helpfulness of the reviews . Here , we define similar users as having similar sentimental tendencies towards the same aspects of the same products . We mainly look for similar users of similar products to improve the accuracy of review recommendation to similar users . 

Practical world modeling for NLP applications
No abstract available . 

Using Social Media for Mental Health Surveillance : A Review
Data on social media contain a wealth of user information . Big data research of social media data may also support standard surveillance approaches and provide decision - makers with usable information . These data can be analyzed using Natural Language Processing ( NLP ) and Machine Learning ( ML ) techniques to detect signs of mental disorders that need attention , such as depression and suicide ideation . This article presents the recent trends and tools that are used in this field , the different means for data collection , and the current applications of ML and NLP in the surveillance of public mental health . We highlight the best practices and the challenges . Furthermore , we discuss the current gaps that need to be addressed and resolved . 

Using Intelligent Agents for Social Sensing across Disadvantaged Networks
Users who retrieve information across disadvantaged networks need to do so in such a way as to minimize network performance impact while maximizing the usefulness and quality of information ( QOI ) received . Taking advantage of features from all three network genres ( telecommunication , information , and social ) will enable this balancing act . These users also need to interact using unstructured , ad - hoc queries to decrease the cognitive overload of specialized training or the necessity of learning a constrained language . High QOI can be maintained if an intelligent agent on the network can use social sensing to capture the intent of the query and identify the implied task . Knowing the task will allow other agents that service the requests to filter , summarize , or transcode data prior to responding , lessening the network footprint . This paper describes an approach that uses natural language processing ( NLP ) techniques , multi - valued logic based inferencing , network status checking , and task - relevant metrics for information retrieval . This research effort has resulted in a low - level NLP approach that can be used to capture intent from unstructured text , a quality metric formed from intrinsic and extrinsic attributes of the objects and the tasks under consideration , and a simple inferencing approach to allow intelligent agents to make quality assessments , delivering the appropriate form of the information that will lessen the impact on a disadvantaged network . 

Wide - coverage semantic representations from a CCG parser
This paper shows how to construct semantic representations from the derivations produced by a wide - coverage CCG parser . Unlike the dependency structures returned by the parser itself , these can be used directly for semantic interpretation . We demonstrate that well - formed semantic representations can be produced for over 97 % of the sentences in unseen WSJ text . We believe this is a major step towards widecoverage semantic interpretation , one of the key objectives of the field of NLP . 

NU : BRIEF – A Privacy - aware Newsletter Personalization Engine for Publishers
Newsletters have ( re - ) emerged as a powerful tool for publishers to engage with their readers directly and more effectively . Despite the diversity in their audiences , publishers ’ newsletters remain largely a one - size - fits - all offering , which is suboptimal . In this paper , we present NU : BRIEF , a web application for publishers that enables them to personalize their newsletters without harvesting personal data . Personalized newsletters build a habit and become a great conversion tool for publishers , providing an alternative readers - generated revenue model to a declining ad / clickbait - centered business model . Demo :  https : / / demo . nubrief . com / md03PaAJSwXMegL5BbKpQlArK3elb3hDUglcHodx4gE = / Explainer video :  https : / / www . youtube . com / watch ? v = AUZGuyPJYH4

No mining , no meaning : relating documents across repositories with ontology - driven information extraction
Far from eliminating documents as some expected , the Internet has lead to a proliferation of digital documents , without a centralized control or indexing . Thus , identifying relevant documents becomes simultaneously more important and much harder , since what users require may be dispersed across many documents and many repositories . This paper describes Ontologic Anchoring , a technique to relate documents in domain ontologies , using named entity recognition ( a natural - language processing approach ) and semantic annotation to relate individual documents to elements in ontologies . This approach allows document retrieval using domain - level inferences , and integration of repositories with heterogeneous media , languages and structure . Ontological anchoring is a two - way street : ontologies allow semantic indexing of documents , and simultaneously new documents enrich ontologies . The approach is illustrated with an initial deployment for heritage documents in Spanish . 

Methods and Applications for Relation Detection Potential and Limitations of Automatic Learning in IE
The detection of relation instances is a central functionality for the extraction of structured information from unstructured textual data and for gradually turning texts into semi - structured information . Experience from many years of shared - task efforts in the MUC and ACE frameworks has led to promising initial results but also to frustrating barriers . But the systematic collective efforts have also yielded valuable insights into the complexity of the task and the limitations of existing approaches . An entire research area has emerged from the numerous efforts to increase the sophistication of the various approaches to relation extraction and from the obtained empirical results . In the meantime , it has become clear that there cannot be a single best method for relation extraction since there are many types of relations differing in complexity and in their reflection in the vocabulary of the language . The scale of complexity ranges from simple binary relations of frequent entity types all the way to complex embeddings of relations of various arity . Types of opinions and complex events are just special types of relations . For some relations the language provides prepositions , verbs or other lexemes that allow a concise and compact encoding . Others have to be described by a combination of different words and constructions . Relation extraction tasks do not only differ in the complexity and the linguistic inventory associated with the relevant relations . They also differ with respect to the size and nature of the available data for training and application . With respect to the applied mathematical methods , we find discrete ( or symbolic ) and non - discrete approaches . The latter are usually statistical methods . We also witness a growing tendency to combine different methods . With respect to the acquisition of the classifiers or detection grammars , the existing approaches fall in three large categories : i . detection by classifiers / grammars acquired through intellectual human labor ii . detection by classifiers / grammars acquired through supervised learning iii . detection by classifiers / grammars acquired through unsupervised or minimally supervised learning In the talk we will provide examples for the classes of approaches and summarize their respective advantages and disadvantages . We will argue that different relation detection tasks require different methods or even different combinations of methods . 

Effect of gaussian densities and amount of training data on grapheme - based acoustic modeling for Arabic
Grapheme - based acoustic modeling for Arabic is a demanding research area since high phonetic transcription accuracy is not yet solved completely . In this paper , we are studying the use of a pure grapheme - based approach using Gaussian mixture model to implicitly model missing diacritics and investigating the effect of Gaussian densities and amount of training data on speech recognition accuracy . Two transcription systems were built : a phoneme - based system and a grapheme - based system . Several acoustic models were created with each system by changing the number of Gaussian densities and the amount of training data . Results show that by increasing the number of Gaussian densities or the amount of training data , the improvement rate in the grapheme - based approach was found to be faster than in the phoneme - based approach . Hence the accuracy gap between the two approaches can be compensated by increasing either the number of Gaussian densities or the amount of training data . 

Graph - based service quality evaluation through mining Web reviews
This work tries to find a possible solution to the basic research problem : how to conduct service quality evaluation through mining Web reviews ? To address this problem , this work proposes a novel approach to service quality evaluation which has two essential subtasks : 1 ) finding the most important service aspects , and 2 ) measuring service quality using ranked service aspects . We propose three graph - based ranking models to rank service aspects and a simple linear method of measuring service quality . Empirical experimental results show all our three methods outperform the approach of Noun Frequency . We also show the effectiveness of our service quality evaluation method by conducting intensive regression experiments . 

Ambiguity solution of pinyin segmentation in continuous Pinyin - to - Character conversion
Chinese Pinyin - to - character conversion is a key technology in Chinese Pinyin input system . In sentence based Pinyin - to - character conversion , segmentation of Pinyin string has important influence on performance of Pinyin - to - character conversion . There are lots of ambiguities in segmentation of Pinyin string . This paper classifies them into overlap and combinational ambiguities , and proposes disambiguation algorithms for them respectively . We then combine ambiguity resolution with several different language model to implement Pinyin - to - character conversion task , experiments show a good performance brought by proposed algorithms . 

ChEMBL Bot - A Chat Bot for ChEMBL database
ChEMBL is a chemical database that provides curated bioactivity data along with extensive annotations about compounds ( Ex . biological relevance , medicinal uses , pharmacology ) . It is of general interest to experts from diverse areas . This makes it imperative that multiple access end - points be made available so it reaches a diverse set of users . ChEMBL already produces various ways to access the resources including web URL , REST APIs , MySQL / XML downloads , etc . This paper introduces a chat bot called “ ChEMBL Bot” , which seeks to complement the ways in which ChEMBL may be accessed . The chat bot is an extensible tool that responds to queries around ChEMBL compounds using casual parlance to address questions pertaining to the compounds ' associations with targets , diseases , mechanisms of action , phase of approval ( FDA ) and patents . It is a conversational agent that uses semantics deduced from ChEMBL data and relies on natural language processing tools via Dialogflow . 

Natural language parsing using Fuzzy Simple LR ( FSLR ) parser
In daily life the language used for communication can be termed as Natural Language ( NL ) and it evolves from generation to generation . NL is the most powerful tool that humans possess for conveying information . At the core of Natural Language Processing ( NLP ) task there is an important issue of Natural Language Understanding ( NLU ) . NLP is computer manipulation of NL . In this paper we propose a fuzzy parser which is a form of syntax analyzer that performs analysis of a complete source input . The Bottom up LR ( left to right ) syntax analysis [ 1 ] method is a useful and versatile technique for parsing deterministic Fuzzy context - free languages . Here we have proposed a Fuzzy Simple LR parser ( FSLR ) for parsing English sentences which uses Fuzzy Context Free Grammar ( FCFG ) . LR parsers are a family of efficient , bottom - up shift - reduce parsers that can be used to parse a large class of context - free languages . The system is intended to rank the large number of syntactic analyses produced by NL grammars according to the frequency of occurrence of the individual rules deployed in each analysis . This paper discusses a procedure for constructing an LR parse table from Fuzzy context free grammar and using this table the input sentence is tested for syntactic correctness . 

Codebook - Based Training Beam Sequence Design for Millimeter - Wave Tracking Systems
In this paper , we propose a codebook - based beam tracking strategy for mobile millimeter - wave ( mmWave ) systems , where the temporal variation of the angle of departure ( AoD ) is considered . A closed - form upper bound of the average tracking error probability ( ATEP ) is derived and further optimized . We first consider a slow - varying scenario where narrow training beams implemented by single radio - frequency ( RF ) chain are employed . We show that the ATEP can be reduced by optimizing the power allocation strategy over these training beams , which is formulated and transformed into a second - order cone programming . The fast - varying scenario is further considered where the use of narrow training beams becomes inefficient due to the rapid variations of AoD . In order to reduce the training time , multiple RF chains generating wide beams are employed to track the AoD ' s variations , and the associated beam pattern design problem is shown to be a 0 - 1 nonlinear optimization problem ( NLP ) . A sequential quadratic programming method is used to solve this binary NLP . To reduce the complexity , a progressive edge - growth algorithm is further introduced by associating the binary NLP with a bipartite graph . Numerical results demonstrate significant gains of the proposed beam tracking strategy over existing benchmarks for both scenarios . 

An efficient Parallel Substrate for Typed Feature Structures on shared memory parallel machines
This paper describes an efficient parallel system for processing Typed Feature Structures ( TFSs ) on shared - memory parallel machines . We call the system Parallel Substrate for TFS ( PSTFS ) . PSTFS is designed for parallel computing environments where a large number of agents are working and communicating with each other . Such agents use PSTFS as their low - level module for solving constraints on TFSs and sending / receiving TFSs to / from other agents in an efficient manner . From a programmers ' point of view , PSTFS provides a simple and unified mechanism for building high - level parallel NLP systems . The performance and the flexibility of our PSTFS are shown through the experiments on two different types of parallel HPSG parsers . The speed - up was more than 10 times on both parsers . 

Improving the Selection Error Recognition in a Chinese Grammar Error Detection System
In this paper we report how we improve our Chinese grammar error detection system . We focus on the recognition of word selection error , which is the hardest error type defined in 2015 NLP - TEA shared task CGED . Four major error types was defined in CGED shared task , including redundant word error , missing word error , word selection error and word disorder error . Based on the conditional random fields ( CRF ) model , our system trains a linear tagger that can be used to recognize the errors in learners ' essays . In 2015 CGED shared task , we have found that our system can achieve high precision and accuracy . Our previous system used the features in the sentence only , however , it is not sufficient to recognize the word selection error . In this paper , we propose that by integrating the collocation feature into the CRF model , our system can improve the performance on the recognition of the selection errors . 

An application of latent semantic analysis to word sense discrimination for words with related and unrelated meanings
We present an application of Latent Semantic Analysis to word sense discrimination within a tutor for English vocabulary learning . We attempt to match the meaning of a word in a document with the meaning of the same word in a fill - in - the - blank question . We compare the performance of the Lesk algorithm to Latent Semantic Analysis . We also compare the performance of Latent Semantic Analysis on a set of words with several unrelated meanings and on a set of words having both related and unrelated meanings . 

Uncovering Machine Learning - Ready Data from Public Clinical Trial Resources : A case - study on normalization across Aggregate Content of ClinicalTrials . gov
The state of clinical data is a barrier to the development of machine learning models to improve healthcare . Uncontrolled clinical freetext is common in both the patient and clinical trials : the resulting spelling , grammatical errors , phrasing variation , and other resulting variability results in difficult - to - leverage data . As part of our effort to harmonize the Aggregate Analysis of ClinicalTrials . gov ( AACT ) drop - withdrawal reasons to a controlled vocabulary , we explored two solutions . Elastic ' s fuzzy matching capability matched entries in the AACT Drop - Withdrawal table to a list of user - specified terms ( 74 . 6 % coverage ) . The second approach was a custom pipeline employing NLP preprocessing , Levenshtein Distance ( Fuzzy Matching ) , and semantic similarity mapping using a pre - trained FastText Model ( 98 % coverage ) . Although manual oversight is still required , the amount of effort to harmonize with a controlled vocabulary is notably reduced . This work enables the rapid harmonization of clinical databases , allowing them to be leveraged for machine learning and analytics . 

Experimental Comparison of Machine Learning Techniques for Analysing the Facial Expression
Emoticons are miniature pictures that are customarily used in internet community Communications in the 21st century . The fusion of textual and imagery contained in the same message develops today ' s modern way of conversation . In spite of being universally utilized in online media , Emoticons basic interpretation has received very little observation from a “ Natural Language Processing ” point of view . In this paper , we investigate the relation between facial expressions and emoticons , studying the novel task of predicting which emojis are evoked by the user ' s facial expressions . We experimented with variants of word embedding techniques , and train various models based on MNBs and LSTMs in this task respectively . The experimental results show that our model can predict reasonable emoticons from emotions . 

Study on Key Technologies of Generator of Q / A System
Automatic question - answer system is a very hot research in natural language processing realm . But it also exist some problems . The developing period of the Q / A system in different fields is too long , and the recycle rate is so low . To deal with it , the paper researches the generator of automatic Q / A system . First , the paper introduces the answer shape measure and the module of Q / A system . It brings elaborately forward the main idea of generator . Second , it puts forward the method of answer extraction and the translation method of the heterogeneous data in the automatic Q / A system . Automatic Q / A system make the corresponding answer by the principle of the maximal weigh . The sentences are regard as the basic unit of the result matching . It considerably enhances the intelligence degree of Q / A system . Finally , it gives an example of QA system . On the base of above theory and algorithms , we implement a generator of Q / A system . 

Automatic Checking of Conformance to Requirement Boilerplates via Text Chunking : An Industrial Case Study
Context . Boilerplates have long been used in Requirements Engineering ( RE ) to increase the precision of natural language requirements and to avoid ambiguity problems caused by unrestricted natural language . When boilerplates are used , an important quality assurance task is to verify that the requirements indeed conform to the boilerplates . Objective . If done manually , checking conformance to boilerplates is laborious , presenting a particular challenge when the task has to be repeated multiple times in response to requirements changes . Our objective is to provide automation for checking conformance to boilerplates using a Natural Language Processing ( NLP ) technique , called Text Chunking , and to empirically validate the effectiveness of the automation . Method . We use an exploratory case study , conducted in an industrial setting , as the basis for our empirical investigation . Results . We present a generalizable and tool - supported approach for boilerplate conformance checking . We report on the application of our approach to the requirements document for a major software component in the satellite domain . We compare alternative text chunking solutions and argue about their effectiveness for boilerplate conformance checking . Conclusion . Our results indicate that : ( 1 ) text chunking provides a robust and accurate basis for checking conformance to boilerplates , and ( 2 ) the effectiveness of boilerplate conformance checking based on text chunking is not compromised even when the requirements glossary terms are unknown . This makes our work particularly relevant to practice , as many industrial requirements documents have incomplete glossaries . 

Combating Domestic Violence during COVID - 19 Pandemic in Bangladesh : Using a Mobile Application integrated with an Effective Solution
Domestic violence ( DV ) is not new . Yet , researchers and human rights experts are reporting an alarming rise in DV against women since countries began locking down areas to stop the virus from spreading . They are now calling for a way to assist victims without risking infection of the virus . This paper proposes a mobile application that addresses DV in the context of the COVID - 19 pandemic . It consists of two interconnected components - the Women ’ s Support Division ( WSD ) and Conversational Interactive Response ( CIR ) . The CIR module is a Natural Language Processing ( NLP ) chatbot for answering questions . One of the sub - modules , the Action Key module sends notifications with GPS location to acquaintances . It was found to have a performance time of 8 . 64 milliseconds which ensures the proposed system transmits data faster than other systems . The system usability scale ( SUS ) result of our proposed application has an average 66 . 71 % score which indicates the system is fit for use . Finally , data for DV in Bangladesh is analyzed from 2014 to April 2020 . Most applications addressed violence against women who are outdoors . Few have focused on DV , which is increasing indoors during the COVID - 19 pandemic . 

RCSUM : To build a summarization system directly generating summaries with evaluation metrics
Several automated metrics have been adopted by document understanding conference ( DUC ) in recent years as they offered great advantages in the area of summarization . However , there ' re still no evaluation metrics which can be directly used to generate Summaries , mainly because that human reference summaries are indispensable for these metrics . Here we report that our group first discovered and developed RCSUM , a summarization system that can generate summaries with evaluation metrics . RCSUM was developed from ROUGE - C , a new fully automated evaluation metrics recently developed in our group , which can use ROUGE in an absolutely manual - independent way , and reserved capabilities of distinguishing good or bad summaries . Experiments conducted on the 2001 to 2005 DUC data showed that , evaluated by ROUGE - 2 as well as ROUGE - SU4 , our system performed very well . 

Bangla word clustering based on N - gram language model
In this paper , we describe a method for producing Bangla word clusters based on semantic and contextual similarity . Word clustering is important for parts of speech ( POS ) tagging , word sense disambiguation , text classification , recommender system , spell checker , grammar checker , knowledge discover and for many others Natural Language Processing ( NLP ) applications . Computerization of Bangla language processing has been started a long ago , but still it is in neophyte stage and suffers from resource scarcity . We propose an unsupervised machine learning technique to develop Bangla word clusters based on their semantic and contextual similarity using N - gram language model . According to N - gram model , a word can be predicted based on its previous and next words sequence . N - gram model is applied successfully for word clustering in English and some other languages . As word clustering in Bangla is a new dimension in Bangla language processing research , so we think this process is good way to start and our assumption is true as our result is quite decent . We produced 456 clusters using a locally available large Bangla corpus . Subjective score derived from the clusters reveal strong similarity of the words in the same cluster . 

Constraint Soup
To facilitate mass customization in the building industry , an automated method is needed to check the validity of user - created designs . This check requires that numerous complex building codes and regulations , as well as architects ' demands are formalized and captured by non - programming domain experts . This can be done via a natural language interface , as it reduces the required amount of training of users . In this paper we describe an algorithm for interpreting architectural constraints in such a system . 

Impact of In - domain Vector Representations on the Classification of Disease - related Tweets : Avian Influenza Case Study
A number of methods have been proposed for the construction of vector representations for natural language processing ( NLP ) tasks . These methods have been applied to various domains and each has its own pros and cons . Despite their effectiveness , the proposed approaches usually ignore the sentiment information concerning specific tasks . In this paper , we examined various types of word vectors and their impact on the performance of a sentiment classification problem in the area of infectious diseases . Vectors were used in the embedding layer of a word - based convolutional neural network ( CNN ) to identify tweets pertaining to avian influenza . We proposed a new approach to build effective word embeddings for the sentiment analysis task . Furthermore , the performance of the language model was compared in terms of using various corpus sizes and vector dimensions . Our experiments indicated that initializing the sentiment learning network with domain - specific word embeddings outperforms general domain embeddings . We found that the proposed method leads to a considerable improvement in the classification performance . 

Sketching techniques for large scale NLP
In this paper , we address the challenges posed by large amounts of text data by exploiting the power of hashing in the context of streaming data . We explore sketch techniques , especially the Count - Min Sketch , which approximates the frequency of a word pair in the corpus without explicitly storing the word pairs themselves . We use the idea of a conservative update with the Count - Min Sketch to reduce the average relative error of its approximate counts by a factor of two . We show that it is possible to store all words and word pairs counts computed from 37 GB of web data in just 2 billion counters ( 8 GB RAM) . The number of these counters is up to 30 times less than the stream size which is a big memory and space gain . In Semantic Orientation experiments , the PMI scores computed from 2 billion counters are as effective as exact PMI scores . 

A novel kernel for sequences classification
In this paper , a novel kernel , called position weight subsequences kernel ( PWSK) , is introduced for identifying gene sequences . String subsequences kernel ( SSK) , which is based on string alignment , performs well for text categorization problems . For gene sequences identification , not only the comprised subsequences but also the positions of them are important . To integrate the position information , the decay factor of match position in SSK was replaced by position weight in PWSK . By doing this , PWSK can integrate both the content and position information of subsequences . This kernel was used for splice site identification and the experimental results demonstrated its efficiency . The sensitivities for donor sites and acceptor sites are 94 % and 95 % , respectively , and the specificities for them are 96 % and 96 % . The performance is better than that of SSK . The reason is that the content of sequence alone is not enough to interpret splicing , and it is necessary to include the position information . Compared with the existing approaches , PWSK achieves better sensitivities for both the donor sites and the acceptor sites , and the specificities of them are comparable . 

An independent - domain natural language interface for relational database : Case Arabic language
Making information stored in database accessible for non expert users , has become one of the problems of great interest for the research community of database querying system . Hence for overriding the complexity of using database language such as Structured Query Language ( SQL) , the using of natural language can be a very important and simple method . But without helps computer cannot understand this language . For that its necessary to develop an interface able to translate natural language query into database query language . In this paper we present the architecture of generic interface for querying database using Arabic language . This interface functions independently of database domain and has the capacity to improve through experience its knowledge base . 

A Memetic Algorithm for Tour Trip Design Problem
to design a tour plan which provide a maximum satisfaction , before have any experiences with the destination can be hard and time consuming process . The goal of this study is to create an algorithm that efficiently generate a tour plan with high or maximum satisfaction within a reasonable processing time . The memetic algorithm which is a combination of genetics algorithm and local search algorithm would be created to solve this problem . This study used real data gathered from trusted tourist community in Thailand such as TripAdvisor . com , Wongnai . com , etc . The result of this study shown Memetic Algorithm ( MA ) approach could solve tour trip design problem efficiently since both saving in computation time and % gap are in a good shape and well - balanced . 

Dynamic optimization of a batch transesterification process for biodiesel production
With the advancement of studies in the field of renewable fuels , biodiesel production has received a lot of attention over the years . Biodiesel production is based on transesterification reaction between a vegetable oil containing triglycerides and a shorter chain alcohol e . g . methanol . A batch transesterification process needs tighter control on reaction temperature for achieving higher triglyceride conversion . Here , open loop control called optimal control has been employed on the transesterification kinetic model of a batch process . A multi - objective optimization problem is formulated with two objectives namely , maximization of methyl ester concentration at batch end time and minimization of final batch time . An e - constraint approach is adopted for solving the multi - objective optimization problem . Orthogonal collocation on finite elements scheme is employed to generate the optimal temperature trajectory . Under the framework of collocation method , state and control profiles have been discretized using piecewise linear Lagrange ' s polynomials and the control problem has got transformed into an NLP problem . fmincon solver is utilized in solving the NLP in MATLAB® . Maximum methyl ester concentration at final batch time and the minimum batch end time have been determined to be 0 . 8315 mol / l and 67 . 4 min , respectively . Optimal results have been found with two finite elements and five interior collocation points . 

On Online Hate Speech Detection . Effects of Negated Data Construction
In the era of social media and mobile internet , the design of automatic tools for online detection of hate speech and / or abusive language becomes crucial for society and community empowerment . Nowadays of current technology in this respect is still limited and many service providers are still relying on the manual check . This paper aims to advance in this topic by leveraging novel natural language processing , machine learning , and feature engineering techniques . The proposed approach advocates a classification - like technique that makes use of a special data design procedure . The latter enforces a balanced training scheme by exploring the negativity of the original dataset . This generates new transfer learning paradigms , Two classification schemes using convolution neural network and LSTN architecture that use FastText embeddings as input features are contrasted with baseline models constituted of Logistic regression and Naives ' Bayes classifiers . Wikipedia Comment dataset constituted of Personal Attack , Aggression and Toxicity data are employed to test the validity and usefulness of the proposal . 

Person name extraction from Modern Standard Arabic or Colloquial text
Person Name extraction from Arabic text is a challenging task . While most existing Arabic texts are written in Modern Standard Arabic Text ( MSA ) the volume of Arabic Colloquial text is increasing progressively with the wide spread use of social media examples of which are Facebook , Google Moderator and Twitter . Previous work addressed extracting persons ' names from MSA text only and especially from news articles . Previous work also relied on a lot of resources such as gazetteers for places , organizations , verbs , and person names . In this paper we introduce a system for extracting persons ' names from any type of Arabic text whether it is MSA or Colloquial using very few resources . In our system , Natural Language Processing ( NLP ) is integrated with a limited set of dictionaries to extract a person ' s name from Arabic text . The paper also presents the results of evaluating the system on two datasets , one for MSA and the other for Colloquial Arabic . The results achieved were found to be satisfactory in terms of precision , recall and f - measure . 

Automatic Event Coding Framework for Spanish Political News Articles
Today , Spanish speaking countries face widespread political crisis . These political conflicts are published in a large volume of Spanish news articles from Spanish agencies . Our goal is to create a fully functioning system that parses realtime Spanish texts and generates scalable event code . Rather than translating Spanish text into English text and using English event coders , we aim to create a tool that uses raw Spanish text and Spanish event coders for better flexibility , coverage , and cost . To accommodate the processing of a large number of Spanish articles , we adapt a distributed framework based on Apache Spark . We highlight how to extend the existing ontology to provide support for the automated coding process for Spanish texts . We also present experimental data to provide insight into the data collection process with filtering unrelated articles , scaling the framework , and gathering basic statistics on the dataset . 

The Architecture of Speech - to - Speech Translator for Mobile Conversation
With competencies and the results of the engineering of natural language processing technology owned by BPPT since 1987 , BPPT develops an English - Bahasa Indonesia speech - to - speech translation system ( S2ST ) . In this paper , we propose an architecture of speech - to - speech translation system for Android - based mobile conversation using separate mobile devices for each language . This architecture applies three leading technologies , namely : WebSocket , REST , and JSON . The system utilizes a two - way communication protocol between two users and a simple voice activation detector that can detect a boundary of user ' s utterance . 

Why human translators still sleep in peace ?: ( four engineering and linguistic gaps in NLP ) 
Because they will keep their job quite for a few . This paper has been inspired by a recent editorial on the Financial Times , that gives a discouraging overview of commercial natural language processing systems ( ' the computer that can sustain a natural language conversation . . . is unlikely to exist for several decades ' ) . Computational linguists are not so much concerned with applications but computer scientists have the ultimate objective to build systems that can ' increase the acceptability of computers in everyday situations . ' Eventually , linguists as well would profit by a significant break - through in natural language processing . This paper is a brief dissertation on four engineering and linguistic issues we believe critical for a more striking success of NLP : extensive acquisition of the semantic lexicon , formal performance evaluation methods to evaluate systems , development of shell systems for rapid prototyping and customization , and finally a more linguistically motivated approach to word categorization . 

Applying Monte Carlo simulation to biomedical literature to approximate genetic network
Biologists often need to know the set of genes associated with a given set of genes or a given disease . We propose in this paper a classifier system called Monte Carlo for Genetic Network ( MCforGN ) that can construct genetic networks , identify functionally related genes , and predict gene - disease associations . MCforGN identifies functionally related genes based on their co - occurrences in the abstracts of biomedical literature . For a given gene g , the system first extracts the set of genes found within the abstracts of biomedical literature associated with g . It then ranks these genes to determine the ones with high co - occurrences with g . It overcomes the limitations of current approaches that employ analytical deterministic algorithms by applying Monte Carlo Simulation to approximate genetic networks . It does so by conducting repeated random sampling to obtain numerical results and to optimize these results . Moreover , it analyzes results to obtain the probabilities of different genes ' co - occurrences using series of statistical tests . MCforGN can detect gene - disease associations by employing a combination of centrality measures ( to identify the central genes in disease - specific genetic networks ) and Monte Carlo Simulation . MCforGN aims at enhancing state - of - the - art biological text mining by applying novel extraction techniques . We evaluated MCforGN by comparing it experimentally with nine approaches . Results showed marked improvement . 

Optimal Routing of Energy - Aware Vehicles in Transportation Networks With Inhomogeneous Charging Nodes
We study the problem of routing for energy - aware battery - powered vehicles ( BPVs ) in networks with charging nodes . The objective is to minimize the total elapsed time , including travel and recharging time at charging stations , so that the vehicle reaches its destination without running out of energy . Relaxing the homogeneity of charging stations , and here , we investigate the routing problem for BPVs through a network of “ inhomogeneous ” charging nodes . We study two versions of the problem : the single - vehicle ( user - centric ) routing problem and the multiple - vehicle ( system - centric ) routing problem . For the former , we formulate a mixed - integer nonlinear programming ( NLP ) problem for obtaining an optimal path and charging policy simultaneously . We then reduce its computational complexity by decomposing it into two linear programming problems . For the latter , we use a similar approach by grouping vehicles into “ subflows ” and formulating the problem at a subflow - level with the inclusion of traffic congestion effects . We also propose an alternative NLP formulation obtaining near - optimal solutions with orders of magnitude reduction in the computation time . We have applied our optimal routing approach to a subnetwork of the eastern Massachusetts transportation network using actual traffic data provided by the Boston Region Metropolitan Planning Organization . Using these data , we estimate cost ( congestion ) functions and investigate the optimal solutions obtained under different charging station and energy - aware vehicle loads . 

Automatic semantic sequence extraction from unrestricted non - tagged texts
Mophological processing , syntactic parsing and other useful tools have been proposed in the field of natural language processing ( NLP ) . Many of those NLP tools take dictionary - based approaches . Thus these tools are often not very efficient with texts written in casual wordings or texts which contain many domain - specific terms , because of the lack of vocabulary . In this paper we propose a simple method to obtain domain - specific sequences from unrestricted texts using statistical information only . This method is language - independent . We had experiments on sequence extraction on email texts in Japanese , and succeeded in extracting significant semantic sequences in the test corpus . We tried morphological parsing on the test corpus with ChaSen , a Japanese dictionary - based morphological parser , and examined our system ' s efficiency in extraction of semantic sequences which were not recognized with ChaSen . Our system detected 69 . 06 % of the unknown words correctly . 

Real - Time Crisis Mapping of Natural Disasters Using Social Media
The proposed social media crisis mapping platform for natural disasters uses locations from gazetteer , street map , and volunteered geographic information ( VGI ) sources for areas at risk of disaster and matches them to geoparsed real - time tweet data streams . The authors use statistical analysis to generate real - time crisis maps . Geoparsing results are benchmarked against existing published work and evaluated across multilingual datasets . Two case studies compare five - day tweet crisis maps to official post - event impact assessment from the US National Geospatial Agency ( NGA ) , compiled from verified satellite and aerial imagery sources . 

WRS : A Novel Word - embedding Method for Real - time Sentiment with Integrated LSTM - CNN Model
Artificial Intelligence ( AI ) is a research - focused technology in which Natural Language Processing ( NLP ) is a core technology in AI . Sentiment Analysis ( SA ) aims to extract and classify the people ' s opinions by NLP . The Machine Learning ( ML ) and lexicon dictionaries have limited competency to efficiently analyze massive live media data . Recently , deep learning methods significantly enrich the accuracy of recent sentiment models . However , the existing methods provide the aspect - based extraction that reduces individual word accuracy if a sentence does not follow the aspect information in real - time . Therefore , this paper proposes a novel word embedding method for the real - time sentiment ( WRS ) for word representation . The WRS ' s novelty is a novel word embedding method , namely , Word - to - Word Graph ( W2WG ) embedding that utilizes the Word2Vec approach . The WRS method assembles the different lexicon resources to employ the W2WG embedding method to achieve the word feature vector . Robust neural networks leverage these features by integrating LSTM and CNN to improve sentiment classification performance . LSTM is utilized to store the word sequence information for the effective real - time SA , and CNN is applied to extract the leading text features for sentiment classification . The experiments are conducted on Twitter and IMDB datasets . The results demonstrate our proposed method ' s effectiveness for real - time sentiment classification . 

Optimal point - to - point motion planning of flexible parallel manipulator with adaptive Gauss pseudo - spectral method
Optimal point - to - point motion planning of flexible parallel manipulator was investigated in this paper . First , the dynamic model of a 3RRR planar parallel manipulator was derived through Kane equation and the improved curvature - based finite element ( ICFE ) method . Secondly , an optimal point - to - point motion planning problem was constructed with consideration of the flexible dynamic model . Then , an adaptive Gauss Pseudo - spectral Method ( GPM ) was proposed to transcribe the trajectory optimization problem into Nonlinear Programming ( NLP ) problem , and the SQP was adopted to solve the NLP problem . At last , the simulation was carried out on a 3RRR flexible parallel manipulator , the result showed residue vibration of the flexible links can be diminished and the predefined accuracy can be satisfied by adding collocation points or refining the mesh interval through the proposed adaptive algorithm . 

Improved estimation for unsupervised part - of - speech tagging
We demonstrate that a simple hidden Markov model can achieve state of the art performance in unsupervised part - of - speech tagging , by improving aspects of standard Baum - Welch ( EM ) estimation . One improvement uses word similarities to smooth the lexical tag / spl rarr / word probability estimates , which avoids over - fitting the lexical model . Another improvement constrains the model to preserve a specified marginal distribution over the hidden tags , which avoids over - fitting the tag / spl rarr / tag transition model . Although using more contextual information than an HMM remains desirable , improving basic estimation still leads to significant improvements and remains a prerequisite for training more complex models . 

Application of neurolinguistic techniques to knowledge acquisition
Good communication between the domain expert and the knowledge engineer is a must for successful knowledge engineering . However , this is not an easily achieved goal because much of the process of knowledge engineering is still a relatively ad hoc endeavor , pursued by individuals from a wide range of backgrounds with limited training . Consequently , individual knowledge engineers are frequently faced with numerous difficulties including experts who have difficulty verbalizing their knowledge and reasoning processes , who feel uncomfortable discussing their work , or who are unwilling to reveal their knowledge . In these situations , a primary responsibility of the knowledge engineer is to establish an environment in which the expert can be more able , comfortable , and willing to communicate his knowledge . The authors argue that the techniques used in psychological interviewing can be used equally well in knowledge engineering . They discuss some techniques derived from neurolinguistic programming ( NLP ) ( Grinder & amp ; Bandler , 1976 ) and show how they can address possible trouble points in setting up a comfortable interviewing environment . 

Using language resources in an intelligent tutoring system for French
This paper presents a project that investigates to what extent computational linguistic methods and tools used at GETA for machine translation can be used to implement novel functionalities in intelligent computer assisted language learning . Our intelligent tutoring system project is still in its early phases . The learner module is based on an empirical study of French as used by Acadian elementary students living in New - Brunswick , Canada . Additionally , we are studying the state of the art of systems using Artificial Intelligence techniques as well as NLP resources and / or methodologies for teaching language , especially for bilingual and minority groups . 

Categorizing sentence structures for phrase level morphological analyzer for English to Hindi RBMT
Algorithms for morphological analyzers have evolved majorly around words . Since writing styles are changing due to impact of languages on each other , higher version of morphological analyzers are desired for various NLP systems such as Machine Translation , Knowledge Extraction , Information Retrieval , etc . Often word level morphological analyzers adhere to language grammars and knowledge set pertaining to GNP and dictionary . Some algorithms use phrasal dictionaries also . But , impact of languages on each other leads to changes in GNP , grammatical and phrasal usage of words . General morph algorithms cannot deal with impact of such usage of words or phrases . Therefore new generation of morph analyzers are desired to handle cross lingual impact . In this paper , methodology for English language morphological analyzer is proposed for interpretation of phrases and group of words to derive knowledge in Hindi for tourism domain . The methodology , although general , is oriented towards Machine Translation . Proposed methodology is based on creation of knowledge base for morph analyzers using formulations of FST and RTN . Using this methodology , ten categories of phrasal structures in sentences have been identified which when used in MA of RBMT would improve the functional efficiency of MT in producing correct translation . 

EEQuest : An Event Extraction and Query System
We present EEQuest , an application that extracts events from text using natural language processing ( NLP ) and supervised machine - learning techniques , and provides a system to query events extracted from a text corpus . We provide a use case for the application wherein we extract business - related events from news articles . The extracted events are then categorized based on the business organization / company that they are related to . Finally , the events are added to a knowledge base using which a query system is built . The system can be used to display events related to a particular organization or a group of organizations . Although we are using the system to extract business - related events , the event extraction mechanism can be used in a more general sense with any available textual data , to extract any kind of events that have a structure that can answer the question : Who did what , when and where ? 

By using grey area relational grade combined with NLP method to optimize GM ( 1 , 1 ) model
In this paper , we suggest a new optimization method by using grey area relational grade combined with nonlinear programming ( abbreviated to NLP ) method for GM ( 1 , 1 ) model ’ s parameters , we introduce the grey area relational grade to establish a NLP optimization parameters model for a GM ( 1 , 1 ) model that has been established , by using mathematical software LINGO 10 . 0 for its optimal solution . By a lot of data ’ s analysis , we conclude the new method is effective and feasible for the GM ( 1 , 1 ) models that have been established by using any other methods . 

Gamified Rehabilitation for Pain Distraction in Total - Knee - Replacement Patients
Total - Knee - Replacement ( TKR ) is becoming a prevalent procedure for the treatment of knee osteoarthritis among older adults worldwide . A key to the success of TKR is an effective post - surgical rehabilitation . However , for TKR patients , pain is a major factor that hinders them from taking up the rehabilitation exercise fully and leads to unsatisfactory functional recovery rate . To help TKR patients cope with pain and adhere more to rehabilitation exercises , we design a gamified rehabilitation tool , Fun - Knee , for effective pain distraction based on the " Peak - End Effect " theory . We have developed the alpha version of Fun - Knee and conducted an acceptability and usability test of its hardware and software . The results from a focus group show the general acceptance of end users on Fun - Knee . 

Health Claims Unpacked : A toolkit to Enhance the Communication of Health Claims for Food
Health claims are sentences on the food product packages to claim the nutrition and the benefits of the nutrition . Consumers in different European contexts often have difficulties understanding health claims , leading to increased confusion about and decreased trust in the food they buy . Focusing on this problem , we develop a toolkit for improving the communication of health claims for consumers . The toolkit provides ( 1 ) interactive activities to disseminate knowledge about health claims to the public , and ( 2 ) an NLP - based analysis and prediction engine that food manufacturers can use to estimate how consumers like the health claims that the manufacturers created . By using the AI - powered toolkit , consumers , manufacturers , and food safety regulators are engaged in determining the different linguistic and cultural barriers to the effective communication of health claims and formulating solutions that can be implemented on multiple levels , including regulation , enforcement , marketing , and consumer education . 

Towards an Understanding of Arab Women Researchers Contribution in Arabic NLP
Arabic language is spoken by around 400 million people living in the Middle East , North Africa , and the Horn of Africa . Like other languages , literary Arabic continues to evolve ; this suggests a large potential audience for Arabic Natural language processing ( NLP ) and the need for a fair contribution from both men and women Arabic NLP researchers . Our paper will explore Arab women researchers ’ contributions in the field of Arabic Natural Language processing research ( ANLP ) by answering 9 research questions . We answer these questions through examining research metadata from Digital Bibliography & Library Project ( DBLP ) and Google Scholar . We found that the proportion of Arab women researchers in ANLP compared to men is similar to the international level . We also observed their publication , collaboration and citation patterns . Our results point to the need for gender equity initiatives to increase the number of Arab female researchers in ANLP . 

Bottom up : Exploring word emotions for Chinese sentence chief sentiment classification
In this paper we demonstrate the effectiveness of employing basic sentiment components for analyzing the chief sentiment of Chinese sentence among nine categories of sentiments ( including “ No emotion ” ) . Compared to traditional lexicon based methods , our research explores emotion intensities of words and phrases in an eight dimensional sentiment space as features . An emotion matrix kernel is designed to evaluate inner product of these sentiment features for SVM classification with O ( n ) time complexity . Experimental result shows our method significantly improves performance of sentiment classification . 

Siamese Multiplicative LSTM for Semantic Text Similarity
Learning the Semantic Textual Similarity ( STS ) is a critical issue for many NLP tasks such as question answering , document summarization and etc . . In this paper , we combine the Multiplicative LSTM structure with a Siamese architecture which learn to project word embeddings of each sentence into a fixed - dimensional embedding space to represent this sentence . Then these sentence embeddings can be used to evaluate the STS task . We compare with several similar architectures and the proposed method has achieved better results and is competitive with the best state - of - the - art siamese neural network architecture . 

Sentiment analysis of a document using deep learning approach and decision trees
The given paper describes modern approach to the task of sentiment analysis of movie reviews by using deep learning recurrent neural networks and decision trees . These methods are based on statistical models , which are in a nutshell of machine learning algorithms . The fertile area of research is the application of Google ' s algorithm Word2Vec presented by Tomas Mikolov , Kai Chen , Greg Corrado and Jeffrey Dean in 2013 . The main idea of Word2Vec is the representations of words with the help of vectors in such manner that semantic relationships between words preserved as basic linear algebra operations . The extra advantage of the mentioned algorithm above the alternatives is computational efficiency . This paper focuses on using Word2Vec model for text classification by their sentiment type . 

A Deep Learning Based Autonomous Mobile Robotic Assistive Care Giver
Assistive robotic technology is increasingly employed in many industries including health care . One of the most important features of this assistive technology is its autonomous verbal communication skill . We propose a new theory for autonomous agent based on the five human senses . Then we proceed to address one of the five senses , the speech . Our approach to address and develop an autonomous verbal communication is to apply deep learning to learn about different topics in healthcare . We developed a novel approach where we created a set of question - answer dataset from articles and interviews with physician specialist from U . S . National Public Radio ( NPR) . We trained a deep learning model which is able to listen to conversations between a patient and a physician to answer to the questions when the physician is not able to answer or it might not answer the question completely . We discuss the corpus on Health Science which shows what NPR can teach to machine . We share the usage of the Corpus to train a deep learning model to be used by pepper which is a humanoid robot that can be implemented in helping provide elderly care in individuals diagnosed with early stages of dementia . 

An advanced NLP framework for high - quality Text - to - Speech synthesis
In order to build a TTS ( Text - to - Speech ) synthesis system one must provide two key components : a NLP ( Natural Language Processing ) stage , which essentially operates on the input text , and a speech generation stage to produce the desired output . These two distinct levels must exchange both data and commands to produce intelligible and natural speech . As the complete TTS task relies on many distinct scientific areas , any achievement toward standardization can minimize the effort and increase the dynamic of the results . This paper gives an overview of the NLP stage in the TTS system for Romanian language built by our collective , and describes the integration into the system of SSML ( Speech Synthesis Markup Language ) , as a nowadays well recognized standard for TTS document authoring and inter - modules communication . 

DLS Magician : Promoting Early - Stage Collaboration by Automating UI Design Process in an E & P Environment
In this work , we present a prototype of an intelligent system that can automate the UI design process via converting text descriptions into interactive design prototypes . We conducted user research in an international oilfield services company , and found that product owners prefer to validate their hypotheses via visual mockups rather than text descriptions ; however , many of them need assistance from designers to produce the visual mockups . Based on this finding and after exploring multiple possibilities using design thinking , we chose a solution that uses natural language processing ( NLP ) to automate the visual design process . To validate the answer , we conducted user tests via means and iterated the solution . In the future , we expect the work can be fully deployed in a working environment to help product owners initiate their projects faster . 

Natural Language Contents Evaluation System for Detecting Fake News using Deep Learning
This Recently , a lot of information is spreading rapidly on SNS . Inaccurate communication of news media includes fears about unreliable sources and fake news that lacks confirmation of facts . Fake news is spread through SNS , causing social confusion and further economic loss . The purpose of the news is accurate information transmission . In this regard , it is very important to judge the discrepancies in the contents of the text and the distorted reports . We try to solve the problem of judging whether the sentence to be verified is correct after collecting the facts . This paper defines the problem of extracting the related sentences from the input sentence in Fact Data Corpus which is assumed to be fact and judging whether the extracted sentence and the input sentence are true or false . In the various NLP tasks , we create a Korean - specific pre - training model using state - of - the - art BERT . Using this model , fine - tuning is performed to match the data set detected by Korean fake news . The AUROC score of 83 . 8 % is derived from the test set generated using the fine - tuned model . 

Unsupervised Singleton Expansion from Free Text
Set Expansion is the problem of automatically extending a given set of seed elements with objects of the same class , for instance { red , green , white } → { red , green , white , gray , yellow , . . . } . In this paper we address the problem in the challenging scenario of extending singletons , that is , sets with only one seed element . Differently from existing work , we do not assume the presence of markup , such as html lists , nor whatsoever ontology , indeed relying only on free ( unstructured , plain ) text . Despite the challenging problem , we show that the singleton expansion can be accomplished unsupervisedly by means of nearest neighbor search ( NNS ) over word embeddings . We further propose an algorithm that significantly improve the performance of NNS both for small and large ( long tail ) expansions , while maintaining the important quality of being language independent . 

Evaluating a statistical CCG parser on Wikipedia
The vast majority of parser evaluation is conducted on the 1984 Wall Street Journal ( WSJ ) . In - domain evaluation of this kind is important for system development , but gives little indication about how the parser will perform on many practical problems . Wikipedia is an interesting domain for parsing that has so far been under - explored . We present statistical parsing results that for the first time provide information about what sort of performance a user parsing Wikipedia text can expect . We find that the C & C parser ' s standard model is 4 . 3 % less accurate on Wikipedia text , but that a simple self - training exercise reduces the gap to 3 . 8 % . The self - training also speeds up the parser on newswire text by 20 % . 

A Preliminary Study on Fundamental Thai NLP Tasks for User - generated Web Content
Existing literature on Thai NLP often focuses on formally written texts with near - perfect spellings and boundaries between words or sentences . Such assumptions , however , do not hold in real - world NLP tasks , especially when dealing with User - generated web content ( UGWC ) . So far , existing NLP research works on actual web data have been limited , making it unclear whether and how existing techniques can be applicable to UGWC . In this paper , several basic Thai NLP algorithms ( word segmentation , sentence segmentation , word error detection , word variant detection , name entity recognition ) are re - investigated and benchmarked against real - world , practical UGWC data set . The difference in performance between our data set and others are compared as a guidance for future research . Our baseline sentence segmentation on UGWC data set yields an average F - measure of 0 . 77 . For name entity recognition and word variant / error detection tasks , our system yields the accuracy of 0 . 93 and 0 . 53 , respectively . 

A Formal Account of Effectiveness Evaluation and Ranking Fusion
This paper proposes a theoretical framework which models the information provided by retrieval systems in terms of Information Theory . The proposed framework allows to formalize : ( i ) system effectiveness as an information theoretic similarity between system outputs and human assessments , and ( ii ) ranking fusion as an information quantity measure . As a result , the proposed effectiveness metric improves popular metrics in terms of formal constraints . In addition , our empirical experiments suggest that it captures quality aspects from traditional metrics , while the reverse is not true . Our work also advances the understanding of theoretical foundations of the empirically known phenomenon of effectiveness increase when combining retrieval system outputs in an unsupervised manner . 

Detecting compositionality in multi - word expressions
Identifying whether a multi - word expression ( MWE ) is compositional or not is important for numerous NLP applications . Sense induction can partition the context of MWEs into semantic uses and therefore aid in deciding compositionality . We propose an unsupervised system to explore this hypothesis on compound nominals , proper names and adjective - noun constructions , and evaluate the contribution of sense induction . The evaluation set is derived from WordNet in a semisupervised way . Graph connectivity measures are employed for unsupervised parameter tuning . 

Intrinsic or Extrinsic Evaluation : An Overview of Word Embedding Evaluation
Compared with traditional methods , word em - bedding is an efficient language representation that can learn syntax and semantics by using neural networks . As the result , more and more promising experiments in natural language processing ( NLP ) get the state - of - the - art results by introducing word embedding . In principle , embedding representation learning embeds words to a low - dimensional vector space , there - fore vectors support initialization of NLP tasks such as text classification , sentiment analysis , language understanding , etc . However , polysemy is very common in many languages , which causes word ambiguation , further influences the accuracy of the system . Additionally , language models based on distributed hypotheses mostly focused on word properties rather than morphology were our primary focus . This leads to unreasonable performance in different evaluations . At the same time , word embedding learning and measuring are two vital components of word representation . In this paper , we overviewed many language models including single sense and multiple sense word embedding , and many evaluated approaches including intrinsic and extrinsic evaluation . We found that there are obvious gaps between vectors and manual annotations in word similarity evaluation , and language models that achieved good performance in intrinsic evaluations could not produce similar results in extrinsic evaluations . To the best of our knowledge , there is no universal language model and embedding learning method for most NLP task , and each evaluations also hidden natural defects compared to human knowledge . More evaluated datasets are also investigated such as datasets used in intrinsic and extrinsic evaluations . We believe that an improved evaluation dataset and a more rational evaluation method would benefit from this overview . 

Evaluating the effects of treebank size in a practical application for parsing
Natural language processing modules such as part - of - speech taggers , named - entity recognizers and syntactic parsers are commonly evaluated in isolation , under the assumption that artificial evaluation metrics for individual parts are predictive of practical performance of more complex language technology systems that perform practical tasks . Although this is an important issue in the design and engineering of systems that use natural language input , it is often unclear how the accuracy of an end - user application is affected by parameters that affect individual NLP modules . We explore this issue in the context of a specific task by examining the relationship between the accuracy of a syntactic parser and the overall performance of an information extraction system for biomedical text that includes the parser as one of its components . We present an empirical investigation of the relationship between factors that affect the accuracy of syntactic analysis , and how the difference in parse accuracy affects the overall system . 

AI Based Chat - Bot Using Azure Cognitive Services
Letters ruled the earlier era in communication . Then with emergence of Telephones and subsequently mobile phones , voice conversations ruled the communication . However , currently , with the emergence of Internet and lots of social media , chat conversations are ruling the world . Think of your closest friend and ask yourself , have you talked more or chatted more ? So , with popularity of chat in today ' s world , many technologists envisioned that chat couldn ' t just be a mode of communication between humans but also between a human and a computer . That ' s what chat - bot is . In some cases it is powered by machine learning ( the more you interact with the chat - bot the smarter it gets ) . Or , more commonly , it is driven using intelligent rules ( i . e . if a person says this , respond with that ) . A chat - bot can be useful in providing services in a variety of scenarios . These services include life - saving health messages , it may also include weather forecast or to purchase a new laptop , smartphone , and anything else in between . Many of the big companies like Google ( Google Assistant ) , Amazon ( Alexa ) , Microsoft ( Cortana ) and Oracle are spending good amount of energy and money for research on personal assistants . The following subjects would be touched upon for the development of chat - bot : · Using Azure Bot Architecture · Using NLP for Language Understanding from the user and for the Language Generation · Using Custom Vision services for the image recognition . 

A comparison of greedy and optimal assessment of natural language student input using word - to - word similarity metrics
We present in this paper a novel , optimal semantic similarity approach based on word - to - word similarity metrics to solve the important task of assessing natural language student input in dialogue - based intelligent tutoring systems . The optimal matching is guaranteed using the sailor assignment problem , also known as the job assignment problem , a well - known combinatorial optimization problem . We compare the optimal matching method with a greedy method as well as with a baseline method on data sets from two intelligent tutoring systems , AutoTutor and iSTART . 

Improving Patient Cohort Identification using Neural Word Embedding with Structured Analysis
Patient cohort is similar symptoms of group of patients over a time period . It is important to correctly identify patient cohort for observational study or interventional study . To identify patient cohort , we can easily retrieve information from large structured and unstructured data tables , but this information may not fulfill our interest . We need to obtain knowledge from the unstructured medical data for further detailed inspection . In this work , we have improved structured extraction by presenting context on a health record dataset for identifying cohort of diabetic patients . Results shows that traditional structured query - based data extraction methods accurately identified 97 . 14 % positive patients to our question of interest where adding natural language processing supported technique have retrieved 98 . 37 % precisely . 

Product Categorization by Title Using Deep Neural Networks as Feature Extractor
Natural Language Processing ( NLP ) has been receiving increasing attention in the past few years . In part , this is related to the huge flow of data being made available everyday on the internet , which increased the need for automatic tools capable of analyzing and extracting relevant information , especially from the text . In this context , text classification became one of the most studied tasks on the NLP domain . The objective is to assign predefined categories or labels to text or sentences . Important applications include sentence classification , sentiment analysis , spam detection , among many others . This work proposes an automatic system for product categorization using only their titles . The proposed system employs a state - of - the - art deep neural network as a tool to extract features from the titles to be used as input in different machine learning models . The system is evaluated in the large - scale Mercado Libre dataset , which has the common characteristics of real - world problems such as imbalanced classes , unreliable labels , besides having a large number of samples : 20 , 000 , 000 in total . The results showed that the proposed system was able to correctly categorize the products with a balanced accuracy of 86 . 57 % on the local test split of the Mercado Libre dataset . It also surpassed the fourth place on the public rank of the MeLi Data Challenge with 91 . 19 % of balanced accuracy , which represents less than 1 % of the difference to the winner . 

Taming Pretrained Transformers for Extreme Multi - label Text Classification
We consider the extreme multi - label text classification ( XMC ) problem : given an input text , return the most relevant labels from a large label collection . For example , the input text could be a product description on Amazon . com and the labels could be product categories . XMC is an important yet challenging problem in the NLP community . Recently , deep pretrained transformer models have achieved state - of - the - art performance on many NLP tasks including sentence classification , albeit with small label sets . However , naively applying deep transformer models to the XMC problem leads to sub - optimal performance due to the large output space and the label sparsity issue . In this paper , we propose X - Transformer , the first scalable approach to fine - tuning deep transformer models for the XMC problem . The proposed method achieves new state - of - the - art results on four XMC benchmark datasets . In particular , on a Wiki dataset with around 0 . 5 million labels , the prec @ 1 of X - Transformer is 77 . 28 % , a substantial improvement over state - of - the - art XMC approaches Parabel ( linear ) and AttentionXML ( neural ) , which achieve 68 . 70 % and 76 . 95 % precision @ 1 , respectively . We further apply X - Transformer to a product2query dataset from Amazon and gained 10 . 7 % relative improvement on prec @ 1 over Parabel . 

NCI Evalution : Assessment of Higher Order Thinking Skills via Short Free Text Answer
NCI a hybrid approach of Natural Language Processing ( NLP ) , Convolutional Neural Network ( CNN ) and Information Theory are applied to assess higher order thinking skills via short free text answer . Data of students ' examination scripts are collected from 3 domains namely Information Technology , Engineering and Management with a total of 479 answer scripts . All data are of tertiary level examinations . These answer text are firstly , processed by applying NLP . Next , the pattern of known - information is generated by applying CNN - seq concept and finally based on the pattern the amount of known - information is computed by applying information theory approach . Result shows that the proposed techniques achieved an excellent achievement of 0 . 89 and above for all domains and all level of higher order thinking skills . LSA based testing achieved from 0 . 47 to 0 . 83 agreement . This result supports that the proposed hybrid technique can improve automated short free text assessment in a given scope of domain and competency level . 

BaSa : A Technique to Identify Context based Common Tokens for Hindi Verses and Proses
Activating multilingual options on websites in this digital era has enabled the opportunity to generate text in different languages . Hindi has turned to be the significant language in different domains and mainly in the government domain . Proses ( proses ) and verses ( verses ) are generally used as an input data source to perform different operations like classification , clustering and so on . Carrying out different natural language processing ( NLP ) steps is a prerequisite for implementing machine learning algorithms . In this paper , the BaSa technique is proposed to identify context based common tokens from given corpora . Common tokens present at different levels of Natural language processing ( NLP ) are computed . It will give guidelines in the form for researchers working in Hindi NLP . Corpus of 820 proses and 710 verses is processed . It is observed that percentage of number of tokens reduced at each stage of NLP is almost same for proses and verses . 

A Topic Detection and Tracking Method Combining NLP with Suffix Tree Clustering
A topic detection and tracking method combining semantic analysis with Suffix Tree Clustering ( STC ) algorithm is presented . A feature selection using NLP algorithm was introduced to select the noun , verb and name entity as the input of STC . Focusing on the topic drifting , we formed the VSM of cluster by the key words extracted from the nodes of suffix tree by mutual information algorithm . After the similarity computing of clusters and topic detection and tracking , a semantic analysis was introduced to filter the words with same meaning and analyze the semantic structure of words in label of cluster . Finally a content - relevant description was generated for each topic . The experiments showed that this method can detect and track the topics from the news articles effectively . 

Improving data driven wordclass tagging by system combination
In this paper we examine how the differences in modelling between different data driven systems performing the same NLP task can be exploited to yield a higher accuracy than the best individual system . We do this by means of an experiment involving the task of morpho - syntactic wordclass tagging . Four well - known tagger generator ( Hidden Markov Model , Memory - Based , Transformation Rules and Maximum Entropy ) are trained on the same corpus data . After comparison , their outputs are combined using several voting strategies and second stage classifiers . All combination taggers outperform their best component , with the best combination showing a 19 . 1 % lower error rate than the best indvidual tagger . 

Automated Mapping of Environmental Higher Education Ranking Systems Indicators to SDGs Indicators using Natural Language Processing and Document Similarity
To evaluate the ESHERSs and determine their efficiency to measure environmental sustainability , we tackle this problem as a classification assignment . This study benchmark three ESHERSs : UI GreenMetric , Times Higher Education Impact ranking , and STARS ( Sustainability Tracking , Assessment Rating System ) by AASHE ( the association for the advancement of sustainability in higher education ) . Next , we recruited a group of experts who mapped the ESHERS indicators to the SDGs indicators . Then , we use NLP techniques to classify ( map ) the ESHERS indicators to the SDGs indicators . Since most of the ESHERS indicators and the SDGs indicators are in the form of short text , we use the query expansion technique to make the NLP techniques more effective . Each ESHERS indicator and its expanded text represents a document . And , each SDG indicator and its expanded text represents a document . We took the expanded text from the description of the ESHERS indicators and the description of SDG indicators , forming the corpus for our study . Then , we used document similarity to find the similarity between every pair of the corpus documents . We used different similarity measures to see the similarity between the forms . Then , we used a voting system to map the ESHERSs indicators to the SDGs indicators . The proposed system was able to automatically map the underlying ranking systems indicators to the UN SDGs with 99 % accuracy compared to the experts mapping . 

Mining Electronic Health Records : Challenges and Impact
Big data applications in the Healthcare Sector can provide a high potential for improving the overall efficiency and quality of care delivery . In the health care sector though , big data analytics has still to address several technical requirements , being unstructured data analysis one of them . Unstructured data represents a powerful untapped resource - one that has the potential to provide deeper insights into data and ultimately help drive competitive advantage . In this talk some of the most common challenges of processing such data to extract useful knowledge will be analyzed . In particular , we will deal with the following challenges : i ) clinical narratives preprocessing using NLP , ii ) name entity recognition , iii ) semantic enrichment , iv ) integration of the results . We will focus on the real use cases in which we are working in the frame of a European H2020 project called IASIS . In fact , we will analyze the challenges of analyzing reports and notes of patients suffering from Alzheimer ' s disease disease and lung cancer to extract patterns ( survival , treatment , antecedents , . . . ) that can help physicians to get insights for better management of the disease . 

UZurich in the BioNLP 2009 shared task
We describe a biological event detection method implemented for the BioNLP 2009 Shared Task 1 . The method relies entirely on the chunk and syntactic dependency relations provided by a general NLP pipeline which was not adapted in any way for the purposes of the shared task . The method maps the syntactic relations to event structures while being guided by the probabilities of the syntactic features of events which were automatically learned from the training data . Our method achieved a recall of 26 % and a precision of 44 % in the official test run , under " strict equality " of events . 

Exploring NLP web APIs for building Arabic systems
Natural language processing ( NLP ) is the branch of Artificial Intelligence that is concerned with enabling computers understand human languages . Implementing new NLP tools that effectively and efficiently process Arabic is not an easy task , usually such tools face challenges related to NLP various tasks . However , with the movement of many NLP companies to provide their NLP services via Web APIs , building NLP systems that can benefit from such APIs is becoming a reality . This paper will explore the available NLP Web APIs that supports Arabic language . It will also discuss their strengths and weaknesses and provide suggestion for future use . 

Towards a Simplification of Medical Documents
In this paper , we propose a solution to simplify reading and understanding medical documents via the automatic demystification of complex medical terms found in web pages . The suggested approach detects those terms using a combination of NLP and heuristics . It computes the probability that a word is a medical and complex term through text processing and analysis . It then makes use of an ad hoc dictionary to simplify the complex terms . Finally , it exploits the Microsoft Bing cognitive API to retrieve images and videos related to the complex medical terms detected and presents them to the users so that they may have a better understanding of the document being read . 

A Multimodal Mixed Reality Data Exploration Framework for Tactical Decision Making
In a data - driven , open - source information space , automatization tools and techniques are indispensable for analyzing the large amount of data and gaining knowledge from it . Often however , methods focus solely on the data acquisition methods without providing an appropriate visualization and interactive exploration tool or framework that presents the information directly to the user in a descriptive manner . Especially in crisis areas , where it is crucial to keep communication chains short and simple , mixed reality methods can help to present and explore important and relevant records that directly add value to the user in near real - time . This paper presents a novel approach towards data exploration in mixed reality environments , with the aim to enhance tactical decision - making processes and shorten unnecessarily long communication chains . 

Research on Dependency Parsing based on Optimized Neural Networks
In view of the complex structure of Chinese sentence patterns , it is difficult to accurately extract the feature information of dependency structure in dependency analysis , which directly affects the accuracy of correlation analysis . In this paper , based on stanfordnlp , the sentence dependency structure cannot be accurately extracted in Chinese dependency analysis . BiLSTM ( Bi - directional Long short - term Memory ) is used to solve the problem of Long distance dependent feature extraction , and the context feature information is further extracted by combining conditional random field ( CRF ) . Then , a BiLSTM - CRF optimized neural network for Chinese dependency analysis is proposed , and the experimental results are verified . The experimental results show that it is effective to improve the accuracy of Chinese dependency analysis . 

Explainable Natural Language Processing
Book Abstract : This book presents a taxonomy framework and survey of methods relevant to explaining the decisions and analyzing the inner workings of Natural Language Processing ( NLP ) models . The book is intended to provide a snapshot of Explainable NLP , though the field continues to rapidly grow . The book is intended to be both readable by first - year M . Sc . students and interesting to an expert audience . The book opens by motivating a focus on providing a consistent taxonomy , pointing out inconsistencies and redundancies in previous taxonomies . It goes on to present ( i ) a taxonomy or framework for thinking about how approaches to explainable NLP relate to one another ; ( ii ) brief surveys of each of the classes in the taxonomy , with a focus on methods that are relevant for NLP ; and ( iii ) a discussion of the inherent limitations of some classes of methods , as well as how to best evaluate them . Finally , the book closes by providing a list of resources for further research on explainability . 

The formalization of ‘ temporal adverbials + ZHE imperfective ’ sentences
dasiaZHEpsila imperfective has long time been a burning problem with linguistic research . However , up till now , few studies on the semantic meanings of dasiatemporal adverbials + ZHE imperfectivepsila sentences and their formalizations have been done . This paper addresses the formalization of dasiaZHEpsila imperfective and its combinations with temporal adverbial by an automatic parsing using CTT ( Copenhagen Tree Tracer) . This paper first advances that dasiatemporal adverbials + ZHE imperfectivepsila structure has three types of viewpoints by a corpus - based study . Then , it reveals their formalized structures . Finally , it presents the formalizations of dasiaZHEpsila imperfective and its combinations with temporal adverbials . 

Twitter catches the flu : detecting influenza epidemics using Twitter
With the recent rise in popularity and scale of social media , a growing need exists for systems that can extract useful information from huge amounts of data . We address the issue of detecting influenza epidemics . First , the proposed system extracts influenza related tweets using Twitter API . Then , only tweets that mention actual influenza patients are extracted by the support vector machine ( SVM ) based classifier . The experiment results demonstrate the feasibility of the proposed approach ( 0 . 89 correlation to the gold standard ) . Especially at the outbreak and early spread ( early epidemic stage ) , the proposed method shows high correlation ( 0 . 97 correlation ) , which outperforms the state - of - the - art methods . This paper describes that Twitter texts reflect the real world , and that NLP techniques can be applied to extract only tweets that contain useful information . 

Named entity recognition in tweets : an experimental study
People tweet more than 100 Million times daily , yielding a noisy , informal , but sometimes informative corpus of 140 - character messages that mirrors the zeitgeist in an unprecedented manner . The performance of standard NLP tools is severely degraded on tweets . This paper addresses this issue by re - building the NLP pipeline beginning with part - of - speech tagging , through chunking , to named - entity recognition . Our novel T - ner system doubles F1 score compared with the Stanford NER system . T - ner leverages the redundancy inherent in tweets to achieve this performance , using LabeledLDA to exploit Freebase dictionaries as a source of distant supervision . LabeledLDA outperforms co - training , increasing F1 by 25 % over ten common entity types . Our NLP tools are available at : http : / / github . com / aritter / twitter _ nlp

Language Distance using Common N - Grams Approach
Within the framework of historical linguistics , the prevailing view of the linguist experts is that language similarity cannot be measured . However , there has been significant research to attempt to quantify this phenomenon . We extend experiments conducted by Gamallo et al . from 2017 to compare forty - one European languages . We use a distance measure that was proven to work well in authorship attribution tasks . We build a distance network where the nodes represent the languages , and the weighted edges represent distances . We additionally provide statistical analysis that confirms the stability of our method across different experimental setups . 

Generating english - persian parallel corpus using an automatic anchor finding sentence aligner
The more we can enlarge a parallel bilingual corpus , the more we have made it effective and powerful . Providing such corpora demands special efforts both in seeking for as much already translated texts as possible and also in designing appropriate sentence alignment algorithms with as less time complexity as possible . In this paper , we propose algorithms for sentence aligning of two Persian - English texts in linear time complexity and with a surprisingly high accuracy . This linear time - complexity is achieved through our new language - independent anchor finding algorithm which enables us to align as a big parallel text as a whole book in a single attempt and with a high accuracy . As far as we know , this project is the first automatic construction of an English - Persian parallel sentence - level corpus . 

Data Readiness Level For Unstructured Data With A Focus On Unindexed Text Data
As we entered into the so called big data era , the amount of data organizations are dealing with are greater than ever before . When time or computational resources is a constraint , dealing with large amount of data can be painful for many organizations . In this paper , we proposed a new concept called Data Readiness Level ( DRL) . It will measures the readiness of a file very quickly . We define readiness as ready for immediate analytical purposes . DRL is pair - wised measure , one is PKSS value , another is PVI value . One can see PKSS as the degree of similarity to the objective . PVI exhibits how much valuable information the file has . In the real word , not every data contains valuable information . Even if it is , it is not guaranteed that it will be relevant the objective . For example , an atmospheric data from NOAA often times contain many valuable information . However , if our objective is to search psychology literatures , then the atmospheric data is not what we really want . With the aid of DRL , users can simply analyze those data with higher DRL values when time is a constraint . We collected 40 , 000 PDF files by hand from IEEE Xplore digital library . Overall , the experimental result was quite remarkable . 

Learning Context Using Segment - Level LSTM for Neural Sequence Labeling
This article introduces an approach that learns segment - level context for sequence labeling in natural language processing ( NLP ) . Previous approaches limit their basic unit to a word for feature extraction because sequence labeling is a token - level task in which labels are annotated word - by - word . However , the text segment is an ultimate unit for labeling , and we are easily able to obtain segment information from annotated labels in a IOB / IOBES format . Most neural sequence labeling models expand their learning capacity by employing additional layers , such as a character - level layer , or jointly training NLP tasks with common knowledge . The architecture of our model is based on the charLSTM - BiLSTM - CRF model , and we extend the model with an additional segment - level layer called segLSTM . We therefore suggest a sequence labeling algorithm called charLSTM - BiLSTM - CRF - segLSTM < inline - formula > < tex - math notation =" LaTeX " > $^{ sLM}$ < / tex - math > < / inline - formula > which employs an additional segment - level long short - term memory ( LSTM ) that trains features by learning adjacent context in a segment . We demonstrate the performance of our model on four sequence labeling datasets , namely , Peen Tree Bank , CoNLL 2000 , CoNLL 2003 , and OntoNotes 5 . 0 . Experimental results show that our model performs better than state - of - the - art variants of BiLSTM - CRF . In particular , the proposed model enhances the performance of tasks for finding appropriate labels of multiple token segments . 

Robust extraction of subcategorization data from spoken language
Subcategorization data has been crucial for various NLP tasks . Current method for automatic SCF acquisition usually proceeds in two steps : first , generate all SCF cues from a corpus using a parser , and then filter out spurious SCF cues with statistical tests . Previous studies on SCF acquisition have worked mainly with written texts ; spoken corpora have received little attention . Transcripts of spoken language pose two challenges absent in written texts : uncertainty about utterance segmentation and disfluency . 

Siamese LSTM with Convolutional Similarity for Similar Question Retrieval
In this paper , we model the similar question retrieval task as a binary classification problem . We propose a novel approach of “ 1D - Siamese LSTM for cQA ( 1D - SLcQA ) ” to find the semantic similarity between a new question and existing question ( s ) . In 1D - SLcQA , we use a combination of twin LSTM networks and a contrastive loss function to effectively memorize the long term dependencies i . e . , capture semantic similarity even when the length of the answers / questions is very large ( 200 words ) . The similarity of the questions is modeled using a single network with ( 1D ) ( feature ) convolution between feature vectors learned from twin LSTM layers . Experiments on large scale real world Yahoo Answers dataset show that 1D - SLcQA outperform the state of the art approach of Siamese cQA approach ( SCQA ) . 

Low delay nearend speech detector for acoustic echo cancellation
Double talk detector ( DTD ) also called nearend signal detector ( NSD ) is an important control unit of the Acoustic echo cancellers ( AEC ) for full duplex communications system . Usually , the standard NSDs have delay in detection of onset of nearend signal . To reduce this detection delay , new triggering logic is proposed in this paper . This triggering logic can work with any normalized NSD algorithm which has delay in detection of onset of the nearend signal . This detection logic also improves the double talk detection for low or weak nearend signal . The proposed logic is implemented with Angle based Double Talk Detector ( ABM ) [ 1 ] and tested for many real time signals captured on different hardware platforms . The test results prove the significance of proposed NSD and verify the expected performance . The additional complexity required for the proposed triggering logic is marginal . 

Identifying Provider Counseling Practices Using Natural Language Processing : Gout Example
National guidelines for a number of health conditions recommend that practitioners assess and reinforce patient ' s adherence to specific diet and lifestyle modifications . Counseling intervention has shown to have a long - term positive effect on patient adherence but the extent to which physicians comply is unknown . Evidence of counseling provided by practitioner is recorded only as free text in electronic medical records . To identify physicians ' counseling practices we developed a natural language processing system to detect text documentation of dietary counseling in gout patients . 

An Adaptive Partial Sensitivity Updating Scheme for Fast Nonlinear Model Predictive Control
In recent years , efficient optimization algorithms for nonlinear model predictive control ( NMPC ) have been proposed , that significantly reduce the online computational time . In particular , the direct multiple shooting and the sequential quadratic programming ( SQP ) are used to efficiently solve nonlinear programming ( NLP ) problems arising from continuous - time NMPC applications . One of the computationally demanding steps for the online optimization is the computation of sensitivities of the nonlinear dynamics at every sampling instant , especially for systems of large dimensions , strong stiffness , and when using long prediction horizons . In this paper , within the algorithmic framework of the real - time iteration scheme based on multiple shooting , an inexact sensitivity updating scheme is proposed , that performs a partial update of the Jacobian of the constraints in the NLP . Such update is triggered by using a curvature - like measure of nonlinearity , so that only sensitivities exhibiting highly nonlinear behavior are updated , thus adapting to system operating conditions and possibly reducing the computational burden . An advanced tuning strategy for the updating scheme is provided to automatically determine the number of sensitivities being updated , with a guaranteed bounded error on the quadratic programming solution . Numerical and control performance of the scheme is evaluated by means of two simulation examples performed on a dedicated implementation . Local convergence analysis is also presented and a tunable convergence rate is proven , when applied to the SQP method . 

Spam - Detection with Comparative Analysis and Spamming Words Extractions
Communication through email plays an essential part especially in every sector of our day - to - day life . Considering its significance , it is important to filter spam emails from emails . Spam email , also known as junk email , is unwanted messages that are sent by the electronic medium in large quantities . Most of the spam emails are commercial in nature that is not only irritating but also harmful due to malicious scams or malware hosting sites or use viruses attached to the message . In this paper , we identify spam emails and expose how spam emails can be distinguished from legitimate / normal emails . We deployed four machine learning models and two deep learning models over the datasets including the combined dataset . Besides , we also try to find the important keywords that are found repeatedly from spam emails repository . This type of knowledge will enable us to detect spam emails for our personnel and community security purpose . 

NLP serving the cause of language learning
E - learning paves the way to a new type of course , more student centred , granulized , on demand , and highly interactive . Natural Language Processing ( NLP ) technologies associated with other multimedia technologies can help to address the major issues raised by this new type of courses : interaction , personalization and reliable information access . This paper presents Exills , a true e - learning solution which integrates natural language processing tools and virtual reality . Exills is unique in that unlike most of the language learning systems , it focuses on improving learners ' performance rather than learners ' competence . 

Language model adaptation based on the classification of a trigram ' s language style feature
In this paper , an adaptation method of the language style of a language model is proposed based on the differences between spoken and written language . Several interpolation methods based on trigram counts are used for adaptation . An interpolation method considering Katz smoothing computes weights according to the confidence score of a trigram . An adaptation method based on the classification of a trigram ' s style feature computes weights dynamically according to the trigram ' s language style tendency , and several weight generation functions are proposed . Experiments for spoken language on the Chinese corpora show that these methods , especially the method considering both a trigram ' s confidence and style tendency , can achieve a reduction in the Chinese character error rate for pinyin - to - character conversion . 

Multi - Class Sentiment Analysis from Turkish Tweets with RNN
Twitter is a social media platform where users can post their messages called ‘ tweets ’ . Comment on a product , person , or event on Twitter ; It takes reading and interpreting thousands of tweets to find out what emotion it represents . With sentiment analysis , it is possible to perform this process automatically in a short time . In this study ; A data set consisting of Turkish tweets divided into 5 different emotion categories was used . Sentiment analysis was carried out using RNN architecture , which is a deep learning method . In the dataset , there are equal numbers of tweets for each of the emotions “ angry” , “ fear” , “ happy” , “ surprise” , “ sad” . The success of the models established by performing multi - class sentiment analysis with LSTM , BiLSTM and GRU based on RNN architecture was compared . Highest accuracy ; It has been in the model established with bidirectional LSTM , that is , BiLSTM , which is very successful in past and future word contexts . 

Towards Natural Language Processing ( NLP ) based tool design for technical debt reduction on an agile project
This work presents the first approach towards a design of a tool to support agile projects documentation process based on Natural Language Processing ( NLP ) . It is presented a proposed design of a tool as a result of the analysis of the NLP potential to improve agile project documentation processes . The tool has three different approaches : text - diagram , code - diagram , and a full version , which includes the above approaches being text - code - diagram . Finally , the potential advantages and disadvantages of each approach are discussed . In addition , we present the following steps on the future work towards the development of a tool from the proposed design . 

An architecture to support intelligent user interfaces for Wikis by means of Natural Language Processing
We present an architecture for integrating a set of Natural Language Processing ( NLP ) techniques with a wiki platform . This entails support for adding , organizing , and finding content in the wiki . We perform a comprehensive analysis of how NLP techniques can support the user interaction with the wiki , using an intelligent interface to provide suggestions . The architecture is designed to be deployed with any existing wiki platform , especially those used in corporate environments . We implemented a prototype integrating the NLP techniques keyphrase extraction and text segmentation , as well as an improved search engine . The prototype is integrated with two widely used wiki platforms : Media - Wiki and TWiki . 

Word mover ' s distance for affect detection
Emotion detection from text has been an important task in Natural language processing ( NLP ) for many years . Many approaches were based on the emotional words or lexicon in order to detect emotions . While the word embedding vectors like Word2Vec were successfully employed in many NLP approaches . Also , word mover ' s distance ( WMD ) is a method introduced recently to calculate the distance between two documents based on the embedded words . However , this paper is investigating the ability to detect or classify emotions in sentences using word vectorization and distance measures . Our results confirm the novelty of using Word2Vec and WMD in predicting the emotions in text . 

Comparative Study of Various Image Captioning Models
This paper briefly tells about the different models used for image captioning . It also discusses how the progress in the areas of machine translation and object recognition which has eventually enhanced the performance of image captioning model greatly in current time . Implementation of models are also being discussed in this paper . In the end , standard evaluation matrices have been used to evaluate the performance of the models . 

An information arrangement technique for a text classification and summarization based on a summarization frame
In this paper , the purpose is to arrange information to understand at one view . The proposed summarization frame technology is a system to hierarchically arrange and classify information by targeting content and level of importance in sentences . Moreover , the technique in which the Concept Base , the Degree of Association Algorithm , the Time Judgment system and the Place judgment system are used to understand content of sentences is proposed . The Concept Base generates semantics from a certain word , and the Degree of Association Algorithm uses the results of the semantics expansion to express the relationship between one word and another as a numeric value . Only needed information like the number of strokes limitation etc . can be easily extracted by hierarchically arranging information in the document summary . Moreover , the speed - up of the retrieval can be expected by narrowing the retrieval object in information retrieval . An answer matched to TPO can be expected to be achieved in a QA system . Sentences are classified according to the content . Each classification is classified into a more detailed field . Important keywords are extracted from the sentences classified into the field . Moreover , the extracted keywords are classified into common and peculiar word for the sentences in the field . In addition , sentences of each field hierarchize sentences to three stages according to the importance of the content . In addition , the sentences of each field are hierarchized at three levels according to the importance of the content . 

Recent Advances and Challenges of Arabic Why Question Answering Systems
Arabic Question Answering Systems have gaining a remarkable significance through the increasing amount of the Arabic contents in the Internet and the growing demand for precise answers which cannot be offered by the regular information retrieval techniques . As the amount of research in Arabic Question Answering system is behind when it is compared to other languages , and handling non - factoid QASs is the not - trivial task in Natural Language Processing ( NLP ) , it ' s not surprising that few researches built Arabic why question answering ( whyQA ) systems . This paper addresses the main challenges and gaps analysis for Arabic whyQA systems , and some future trends have been mentioned for the guidance of the new research in that area . 

Isolated Gujarati Handwritten Character Recognition ( HCR ) using Deep Learning ( LSTM ) 
With the swiftly escalating the paperless and automated offices and governance we required to convert paper into digital form . HCR is the form of optical character recognition to recognition the printed or handwritten text into digital text . The writing style of a person , size and thickness of the characters are different from person to person hence HCR is more challenging for automated system . As concern with Gujarati HCR there is more requirement to develop such an automated system that can be used for Gujarati Character Recognition with accents . [ 1] . In this study author have focused to develop LSTM model for offline Gujarati Character Recognition . Along with this an attempt to improve the rate of Gujarati character recognition using the LSTM model with the help of teaching and learning process of model with the given dataset of almost 58 , 000 images . The novelty of this proposed system is to identify a complete set of Guajarati characters that are available with the Unicode dataset . Author have used LSTM model in this study and achieved ~ 97 % success rate of each character class . 

Data selection for statistical machine translation
The bilingual language corpus has a great effect on the performance of a statistical machine translation system . More data will lead to better performance . However , more data also increase the computational load . In this paper , we propose methods to estimate the sentence weight and select more informative sentences from the training corpus and the development corpus based on the sentence weight . The translation system is built and tuned on the compact corpus . The experimental results show that we can obtain a competitive performance with much less data . 

Benefits of modularity in an automated essay scoring system
E - rater is an operational automated essay scoring application . The system combines several NLP tools that identify linguistic features in essays for the purpose of evaluating the quality of essay text . The application currently identifies a variety of syntactic , discourse , and topical analysis features . We have maintained two clear visions of e - rater ' s development . First , new linguistically - based features would be added to strengthen connections between human scoring guide criteria and e - rater scores . Secondly , e - rater would be adapted to automatically provide explanatory feedback about writing quality . This paper provides two examples of the flexibility of e - rater ' s modular architecture for continued application development toward these goals . Specifically , we discuss a ) how additional features from rhetorical parse trees were integrated into e - rater , and b ) how the salience of automatically generated discourse - based essay summaries was evaluated for use as instructional feedback through the re - use of e - rater ' s topical analysis module . 

Using genetic algorithm for Persian grammar induction
Most of efficient computational approaches in NLP tasks are supervised methods which need annotated corpora . But the lack of supervised data in Persian encourages researchers to increase their interests and efforts on unsupervised and semi - supervised approaches . This paper presents a novel semi - supervised approach which called Genetic - based inside - outside ( GIO ) , for Persian grammar inference for inducing a grammar model in a PCFG formalism . GIO is an extension of the inside - outside algorithm enriched by some notions of genetic algorithm . In pure genetic algorithm for grammar induction , randomly generated initial population make it computationally expensive , so we used inside - outside algorithm to generate initial population . Our experiments show that our approach ' s result is better than other applied methods for Persian grammar induction . 

NLP - based enhancement of information security in ITO : a diffusion of innovation theory perspective
Information technology outsourcing ( ITO ) has grown significantly in recent decades and is now over a USD trillion - dollar industry . Service provider organisations are striving to improve the efficiencies of their service deliveries . Natural language processing ( NLP ) provides an opportunity to bring efficiencies through automation in understanding and processing information . Since information security risk management ( ISRM ) in ITO is a growing concern of both , client and service provider organisations , they are adopting to improve ISRM in ITO using NLP . This paper explores those ISRM improvement scenarios . It also investigates the information security risks ( ISRs ) that result from the use of NLP in ITO and proposes strategies to manage those ISRs . To gain insights into the problem , a qualitative research approach is followed using the case study method . Six semi - structured interviews were conducted from participants in three organisations in the ICT industry , engaged in an ITO relationship . To the best of our knowledge , it is the first study to investigate the use of NLP for enhancing ISRM in ITO . 

AMRITA _ CEN @ FIRE - 2014 : Named Entity Recognition for Indian Languages using Rich Features
This paper aims at implementing Named Entity Recognition ( NER ) for four languages such as English , Tamil , Hindi and Malayalam . The results obtained from this work are submitted to a research evaluation workshop Forum for Information Retrieval and Evaluation ( FIRE 2014 ) . This system detects three levels of named entity tags which are referred as nested named entities . It is a multi - label problem solved using chain classifier method . In this work , Conditional Random Field ( CRF ) and Support Vector Machine ( SVM ) are used for implementing NER system . In FIRE 2014 , we developed a English NER system using CRF and other NER system for Tamil , Hindi and Malayalam are based on SVM . The FIRE estimated the average precision for all the four languages as 41 . 93 for outermost level and 33 . 25 for inner level . In order to improve the performance of Indian languages , we implemented CRF based NER system for the same corpus in Tamil , Hindi and Malayalam . The average precision measure for these mentioned languages are 42 . 87 for outer level and 36 . 31 for inner level . The overall performance of the NER system improved by 2 . 24 % for outer level and 9 . 20 % for inner level . 

Using Natural Language Processing techniques and fuzzy - semantic similarity for automatic external plagiarism detection
Plagiarism is one of the most serious crimes in academia and research fields . In this modern era , where access to information has become much easier , the act of plagiarism is rapidly increasing . This paper aligns on external plagiarism detection method , where the source collection of documents is available against which the suspicious documents are compared . Primary focus is to detect intelligent plagiarism cases where semantics and linguistic variations play an important role . The paper explores the different preprocessing methods based on Natural Language Processing ( NLP ) techniques . It further explores fuzzy - semantic similarity measures for document comparisons . The system is finally evaluated using PAN 2012 1 data set and performances of different methods are compared . 

BERTina : A BERT - powered HIV / AIDS Question Answering Tool
The HIV / AIDS ( Human Immunodeficiency Virus / Acquired Immuno - Deficiency Syndrome ) pandemic is still prominent in the global South . However , the proper messaging concerning this pandemic is still a challenge ; especially in resource - limited countries . Fortunately , the use of Information and Communication Technology ( ICT ) can assist in resolving this problem . In this paper , we propose a Natural Language Processing ( NLP ) tool called BERTina , which can be used as an HIV / AIDS question answering tool . The utility of the BERTina tool is underlined by the robustness of this tool against adversarial questions . 

Compression of Deep Learning Models for Text : A Survey
In recent years , the fields of natural language processing ( NLP ) and information retrieval ( IR ) have made tremendous progress thanks to deep learning models like Recurrent Neural Networks ( RNNs ) , Gated Recurrent Units ( GRUs ) and Long Short - Term Memory ( LSTMs ) networks , and Transformer  [ 121 ] based models like Bidirectional Encoder Representations from Transformers ( BERT )  [ 24 ] , Generative Pre - training Transformer ( GPT - 2 )  [ 95 ] , Multi - task Deep Neural Network ( MT - DNN )  [ 74 ] , Extra - Long Network ( XLNet )  [ 135 ] , Text - to - text transfer transformer ( T5 )  [ 96 ] , T - NLG  [ 99 ] , and GShard  [ 64 ] . But these models are humongous in size . On the other hand , real - world applications demand small model size , low response times , and low computational power wattage . In this survey , we discuss six different types of methods ( Pruning , Quantization , Knowledge Distillation ( KD ) , Parameter Sharing , Tensor Decomposition , and Sub - quadratic Transformer - based methods ) for compression of such models to enable their deployment in real industry NLP projects . Given the critical need of building applications with efficient and small models , and the large amount of recently published work in this area , we believe that this survey organizes the plethora of work done by the “ deep learning for NLP ” community in the past few years and presents it as a coherent story . 

Incorporating multi - task learning in conditional random fields for chunking in semantic role labeling
This paper presents a novel application of incorporating Alternating Structure Optimization ( ASO ) to conduct the task of text chunking of Semantic Role Labeling ( SRL ) in Chinese texts . ASO is a competent linear algorithm based on the theory of multi - task learning . In this paper , by constructing several SRL tasks to constitute a multi - task , we are able to encode the inference obtained by ASO algorithm as additional feature to further boost the performance of the target task employing Conditional Random Fields ( CRFs ) . To our knowledge , our method is the first that incorporates multi - task learning into a statistical model in SRL for Chinese texts . We evaluate our approach on Penn Treebank data sets and obtain encouraging result . 

Towards the design of a Conceptual Framework for the operation of Intensive Care Units based on Big Data Analysis
The development of Big Data Analytics ( BDA ) technology and the maturity of the Machine Learning ( ML ) sector offer great opportunities for applications in Intensive Care Units ( ICUs ) . This paper describes a Conceptual Framework and proposes its use in designing architectures and big data applications in ICUs . The Conceptual Framework is based on BDA , MLNatural Language Processing ( NLP ) and consists of the following subsystems : The " Big Data Integration and ICUs " module , the " ICUs and critical care services " module , the " Use of standards and ICUs " module , the " Machine Learning and ICUs " module , and the “ NLP and ICUs ” module . The framework is developed using Soft System Methodology ( SSM ) and Design Science Research Methodology ( DSRM ) . 

Simultaneous trajectory optimization framework for lunar ascent with terrain
This paper proposes a simultaneous trajectory optimization framework to determine the optimal trajectory of lunar ascent with terrain . By considering the influence of the rotation of the moon , a three - dimensional kinematics and dynamics model for the lunar ascent process is given . To ensure flight safety , the terrain model is considered the collision - free path constraints of this trajectory optimization problem . Moreover , the terminal position and velocity are transformed into orbit elements as terminal insertion constraints . The maximum angular rate of the pitch and yaw angle is also constrained . Given that lunar ascent is subject to the tradeoff between fuel and payload , as well as to conserve fuel for possible contingencies , minimum fuel use is used as the performance index . The formulated trajectory optimization problem is discretized into a nonlinear programming ( NLP ) problem by the hp - adaptive pseudospectral method , which is used to deal with the complex terrain constraints . The initial values for solving the NLP problem are generated by the homotopy - based backtracking strategy . Four scenarios are designed to test the capability of the proposed simultaneous trajectory optimization framework for lunar ascent with terrain . Simulation results indicate that the proposed simultaneous trajectory optimization framework has enough adaptability to efficiently address the lunar ascent trajectory optimization problem with complex terrain constraints . The results of this study will benefit future autonomous lunar ascent missions . 

VOVALEAD : a scalable video search engine based on content
Most news organizations provide immediate access to topical news broadcasts through RSS streams or podcasts . Until recently , applications have not permitted a user to perform content based search within a longer spoken broadcast to find the segment that might interest them . Recent progress in both automatic speech recognition ( ASR ) and natural language processing ( NLP ) has produced robust tools that allow us to now provide users with quicker and more focused access to relevant segments of news broadcast videos . Our public online demonstrator of the Voxalead application currently indexes daily broadcast news content from 50 sources in English , French , Chinese , Arabic , Spanish , Dutch , Italian and Russian . 

A Novel Quadratically Constrained Quadratic Programming Method for Optimal Coordination of Directional Overcurrent Relays
The coordination of directional overcurrent relays ( DOCRs ) in meshed power grids with multiple sources is a constrained optimization problem , which has been stated in recent literature as linear ( LP ) , nonlinear ( NLP ) , and mixed - integer nonlinear programming ( MINLP ) problem . In this paper , the DOCR coordination NLP problem is reformulated as an equivalent quadratically constrained quadratic programming ( QCQP ) model , leading to significant reduction of problem complexity . Another contribution of this work is the systematic problem statement using graph theory concepts . The proposed method is applied to three different meshed power systems , employing state - of - the - art optimization software . Simulation results demonstrate the efficacy and superiority of the proposed QCQP model over the prevailing NLP approach . 

Shared - task evaluations in HLT : lessons for NLG
While natural language generation ( NLG ) has a strong evaluation tradition , in particular in userbased and task - oriented evaluation , it has never evaluated different approaches and techniques by comparing their performance on the same tasks ( shared - task evaluation , STE ) . NLG is characterised by a lack of consolidation of results , and by isolation from the rest of NLP where STE is now standard . It is , moreover , a shrinking field ( state - of - the - art MT and summarisation no longer perform generation as a subtask ) which lacks the kind of funding and participation that natural language understanding ( NLU ) has attracted . 

A search tool in Turkish using contextual vectors
Natural Language Processing ( NLP ) field experienced considerable improvement in the year 2018 . With the help of the transfer learning approach BERT ( Bidirectional Encoder Representations ) introduced by Google and approaches following it , state - of - the - art results were achieved in many NLP tasks . It became easier to research in this direction for Turkish with the help of published Turkish pre - trained BERT models in February , 2020 . A new opportunity has arisen on research and development of contextual embeddings - based document retrieval instead of keyword - based for Turkish . This work introduces a BERT - contextual - vector - based Turkish text search tool . Sample queries and responses are presented to demonstrate the benefits it brings . 

Sentiment Analysis and Product Review Classification in E - commerce Platform
Online shopping is becoming one of the most de - manding everyday needs , nowadays . These days people are feeling comfortable shopping online . The number of its customers is increasing day by day as well as raising some problems . The major problem is that the customers can not choose the quality - full product by reading every review of an online product . Besides , the product reviews are helpful to improve the services of an e - commerce site but required huge manpower and time . We have focused on Bangla text and aimed to solve these problems by the application of Deep Neural Network ( DNN ) and Natural Language Processing ( NLP ) . In this study , we have proposed two deep learning NLP models : one is for sentiment analysis and the other one is for Product Review Classification intended to improve both the quality and services . Significantly , our proposed models result in high accuracy : 0 . 84 and 0 . 69 for both Sentiment Analysis and Product Review Classification , respectively . Undoubtedly , these models can help the customers to choose the right product and the service provider to improve their services . 

A framework for analyzing semantic change of words across time
Recently , large amounts of historical texts have been digitized and made accessible to the public . Thanks to this , for the first time , it became possible to analyze evolution of language through the use of automatic approaches . In this paper , we show the results of an exploratory analysis aiming to investigate methods for studying and visualizing changes in word meaning over time . In particular , we propose a framework for exploring semantic change at the lexical level , at the contrastive - pair level , and at the sentiment orientation level . We demonstrate several kinds of NLP approaches that altogether give users deeper understanding of word evolution . We use two diachronic corpora that are currently the largest available historical language corpora . Our results indicate that the task is feasible and satisfactory outcomes can be already achieved by using simple approaches . 

Research on Semi - Automatic Construction of Domain Ontology Based on Machine Learning and Clustering Technique
In this paper we take the approach that constructed the ontology automatically , which attempted to take a method that extremely beneficial for the knowledge acquisition task was the integration of knowledge acquisition with machine learning techniques to increase the ontology construction effect , including domain concepts acquisition , taxonomy relation recognition , non - taxonomy relation recognition and ontology formalization description . This paper adopted an approach of non - dictionary Chinese word Segmentation techniques based on N - Gram to acquire domain candidate concepts , take the method based of NLP in the recognition of domain concept property relation , extracted subject , predicate and object of sentences . This triangle data can be treated as the triplet of data and object type property . 

An Android Application for Clinical Diagnosis Using NLP and Fuzzy Logic
The application of natural language processing ( NLP ) methods to designing conversational frameworks for health diagnosis improves patients ’ access to medical information . An Android application based on fuzzy logic rules and fuzzy inference was created in this research . In Ghana , the service assesses the symptoms of diseases . The android application is built with the Support Vector Machine learning technique , with the aim of improving the model ’ s accuracy and performance . Natural Language Processing is often used by the machine to achieve the conversational style of asking the users for their symptoms . People can spend less time in hospitals and get low - cost or free care by using this technique , which is mainly used in Ghana ’ s rural areas . 

An ungreedy Chinese deterministic dependency parser considering long - distance dependency
This paper presents a two - step dependency parser to parse Chinese deterministically . By dividing a sentence into two parts and parsing them separately , the error accumulation can be avoided effectively . Previous works on shift - reduce dependency parser may guarantee the greedy characteristic of deterministic parsing less . This paper improves on a kind of deterministic dependency parsing method to weaken the greedy characteristic of it . During parsing , both forward and backward parsing directions are chosen to decrease the unparsed rate . Support vector machines are utilized to determine the word dependency relations and in order to solve the problem of long distance dependency , a group of combined global features are presented in this paper . The proposed parser achieved significant improvement on dependency accuracy and root accuracy . 

InaNLP : Indonesia natural language processing toolkit , case study : Complaint tweet classification
This research discusses how natural language processing ( NLP ) toolkit for Indonesia formal text and social media text , named as InaNLP , has been developed . Several NLP modules were integrated into InaNLP to make people easier in building an NLP system for Indonesia language . The toolkit contains several NLP modules such as sentence splitter , tokenization , Part of Speech ( POS ) tagger , phrase chunker , named entity ( NE ) tagger , syntactic parser , semantic analyzer , and word normalization . Several NLP modules were built using rule based approach , whereas several others implemented statistical based approach . Here , the accuracy of several modules such as the POS tagger , NE tagger , syntactic parser and semantic analyzer are shown . In the NE tagger , five ( 5 ) word windows with features of POS , orthography , and word list are used . In the NE tagger experiment for evaluating the features , using SMO algorithm and 1500 sentences , for 15 NE classes , token classification accuracy of 93 . 419 % , which outperform related work , could be achieved . For the POS tagger , using 12 , 000 token as the training data and 3 , 000 token as the testing data , the accuracy of 96 . 50 % was achieved . For the syntactic parser , using CYK algorithm and 100 sentences as the training data and 36 sentences as the testing data , the experiment achieved the accuracy of 47 . 22 % . For the semantic analyzer , using 200 sentences as the training data , the experiment achieved the accuracy of 62 . 50 % . This research also shows an example in building an Indonesia NLP system using InaNLP for complaint tweet classification . In the experiment for the complaint classification , using 7440 data , the experiment achieved 0 . 892 of average F - measure score . 

Early results for Chinese named entity recognition using conditional random fields model , HMM and maximum entropy
Entity recognition ( NER ) is an important step for many natural language applications , such as information extraction , text summarization , and question answering . Chinese NER has some special characteristics that make this task difficult . In this paper , we present some NER experiments on the corpora used for Chinese 863 NER task in 2004 based on three models : maximum entropy , hidden Markov model ( HMM ) and the more recent conditional random fields ( CRFs ) . The results show that CRFs model outperforms the other two models in the sense of best results and average performance , and model scalability among data sizes . In our experiments , CRFs model approach can achieve an overall Fl measure around 84 . 39 / 80 . 68 in simple / traditional Chinese NER respectively , with a gain of 2 . 01 / 10 . 50 over the best system in 863 competitions . 

NLP as an essential ingredient of effective OSINT frameworks
The immense amount of information that can be collected from open sources have become increasingly important to the academic and business communities over the past two decades . Also national and global security is becoming more and more reliant on rapidly making sense of and managing the relevant intelligence data . The enormous size and complexity of the gathered data require advanced and unique data storage , management , analysis , and visualization technologies . The technologies and applications currently adopted typically involve elements like text mining , natural language processing , and stochastic - based algorithms . Natural language processing , in particular , has been a prominent research topic for many years now . We have conceptualized an analysis framework with a strong focus on various techniques of natural language processing to aggregate , manipulate , and analyze intelligence information . 

Keyword extraction using machine learning approaches
Data Mining is the procedure to separate concealed prescient data from database and change it into a reasonable structure for sometime later . The varying areas in information mining are Web Mining , Text Mining , Sequence Mining , graph Mining , temporal Data Mining , Spatial Data Mining , Distributed Data Mining and Multimedia Mining . A portion of the utilizations of information mining , it is utilized for the budgetary information examination , retail and media transmission ventures , science and designing and interruption location and counteractive action . In this exploration paper a calculation is proposed for ensembling and catchphrase extraction in content mining . In the proposition the above methods are upgraded which in turns , enhance the exactness , review , f - measure and accuracy . 

Is machine translation ripe for cross - lingual sentiment classification ? 
Recent advances in Machine Translation ( MT ) have brought forth a new paradigm for building NLP applications in low - resource scenarios . To build a sentiment classifier for a language with no labeled resources , one can translate labeled data from another language , then train a classifier on the translated text . This can be viewed as a domain adaptation problem , where labeled translations and test data have some mismatch . Various prior work have achieved positive results using this approach . In this opinion piece , we take a step back and make some general statements about cross - lingual adaptation problems . First , we claim that domain mismatch is not caused by MT errors , and accuracy degradation will occur even in the case of perfect MT . Second , we argue that the cross - lingual adaptation problem is qualitatively different from other ( monolingual ) adaptation problems in NLP ; thus new adaptation algorithms ought to be considered . This paper will describe a series of carefully - designed experiments that led us to these conclusions . 

Quality Classification of ASEAN Wikipedia Articles using Statistical Features
The quality of Wikipedia articles is still the main concerned in all languages . Wikipedia relies mostly on human editors and administrators to provide the quality of content . But the magnitude of Wikipedia content makes locating all instances of article very time consuming . Therefore , we need the automatic quality detection that can help users to evaluate the quality of articles . In this paper , we propose the feature set to applied for the ASEAN language Wikipedia articles . We investigate the statistical features such as # of link , # of infobox , length of article , # of headings , # of files , # of contributors , # of viewer , # of written articles found in other languages , and # of templates applied in the article . The experiments are perform using Na ï ve Bayes and Decision tree algorithm . We found that the accuracy of Decision tree ( 96 . 34 %) outperform Na ï ve Bayes ( 86 . 47 %) . Moreover , we found that the statistical features play an important role in quality classification of Vietnamese , Indonesian , Malaysian , Thai , and Tagalog / Philippines Wikipedia articles . 

Suffix Based Automated Parts of Speech Tagging for Bangla Language
Natural language processing ( NLP ) is the technique by which we process the human language with the computer . Parts - of - Speech ( POS ) tagging is one of the fundamental requirements for some NLP applications . It is considered as a solved problem for some foreign languages , such as English , Chinese , due to higher accuracy ( 97% ) , where it is still an unsolved problem for Bangla because of its ambiguity . Although making a POS tagger for Bangla is not a new work , but each one of available POS taggers has different kinds of limitations . We choose to develop an unsupervised system rather than a supervised system , because a supervised system needs a huge data resource for training purpose and available resources in Bangla is really poor . Here we develop a POS tagger mainly based on Bangla grammar especially suffixes . Because Bangla is a very inflectional language , where a single word has many variants based on their suffixes . In this POS tagger , we assign 8 base POS tags , where some rules , based on Bangla grammar and suffix , are applied to identify POS tags with the cooperation of verb root dataset . To handle non - suffix words , a dataset of almost 14500 Bangla words , with having their default POS tags , is added with the system , which helps to increase the efficiency of this POS tagger . A modified version of previously used algorithm for suffix analysis is applied , which result in a satisfactory level of about 94 . 2% . 

Tracing Contacts With Mobile Phones to Curb the Pandemic : Topics and Stances in People ’ s Online Comments About the Official German Contact - Tracing App
The Covid - 19 pandemic has led to a health crisis with 90 million infections and two million deaths by the end of January 2021 . To prevent an overload of medical capacities , quickly identifying potentially infected persons is vital to stop the spread of the virus . Mobile apps for tracing people ’ s contacts seem effective , but raise public concerns , e .  g . , about privacy . Hence , they are contested in public discourse . We report a large - scale NLP - supported analysis of people ’ s comments about the German contact - tracing app on news websites , social media and app stores . We identified prevalent topics , stances , and how commenting developed over time . We found privacy to be among the most debated topics discussed from various perspectives . Commenting peaked at one point in time , when public discourse centered on the potential tracing protocols and their privacy protection . We encourage further research on the link between the public discussions and actual adoption rates of the app . 

ProPOSEL : a human - oriented prosody and PoS English lexicon for machine learning and NLP
ProPOSEL is a prosody and PoS English lexicon , purpose - built to integrate and leverage domain knowledge from several well - established lexical resources for machine learning and NLP applications . The lexicon of 104049 separate entries is in accessible text file format , is human and machine - readable , and is intended for open source distribution with the Natural Language ToolKit . It is therefore supported by Python software tools which transform ProPOSEL into a Python dictionary or associative array of linguistic concepts mapped to compound lookup keys . Users can also conduct searches on a subset of the lexicon and access entries by word class , phonetic transcription , syllable count and lexical stress pattern . ProPOSEL caters for a range of different cognitive aspects of the lexicon© . 

Approach for multiword expression recognition & annotation in urdu corpora
Multiword Expression ( MWE ) is an open challenging task in NLP because of its compositional behavior . In this paper , we have presented the classification of multiword expressions and also designed some MWE tags for annotation of Urdu corpora . If MWE annotation is proper then it can be accurately processed through parsing phase and can improve the accuracy of machine translator . For annotation of MWEs , initially extracted the compound type MWE on the basis of automatic approach and also extracted different type of MWEs using semiautomatic and manual approach . These extracted MWEs are pre - processed and after that got the correct multiword expressions . Using these MWEs , we have annotated the Urdu corpora of 50 , 000 sentences . 

Industrial Requirements Classification for Redundancy and Inconsistency Detection in SEMIOS
Requirements are usually " hand - written " and suffers from several problems like redundancy and inconsistency . The problems of redundancy and inconsistency between requirements or sets of requirements impact negatively the success of final products . Manually processing these issues requires too much time and it is very costly . The main contribution of this paper is the use of k - means algorithm for a redundancy and inconsistency detection in a new context , which is Requirements Engineering context . Also , we introduce a filtering approach to eliminate " noisy " requirements and a preprocessing step based on the Natural Language Processing ( NLP ) technique to see the impact of this latter on the k - means results . We use Part - Of - Speech ( POS ) tagging and noun chunking to detect technical business terms associated to the requirements documents that we analyze . We experiment this approach on real industrial datasets . The results show the efficiency of the k - means clustering algorithm , especially with the filtering and preprocessing steps . Our approach is using the software SEMIOS and will be integrated as a new functionality . 

Large scale behavioral targeting using course management system
The world behaves in a manner showing similarity in responses to various actions , this similarity in behavior needs to be tapped . This phenomenon is called Collective Behavior . Collective behavior is the like or similar response of the members of a society to a given stimulus or suggestion . The study of collective behavior can also be applied for the college campus environment . The system developed taps this collective behavior of students on Moodle . dbit . in - a community based course management system . This will help to track and classify student interests and their relative strengths using Forums and the same will be analyzed to help students match their interests and strengths in their future endeavours . 

Parsing Thai Social Data : A New Challenge for Thai NLP
Dependency parsing ( DP ) is a task that analyzes text for syntactic structure and relationship between words . DP is widely used to improve natural language processing ( NLP ) applications in many languages such as English . Previous works on DP are generally applicable to formally written languages . However , they do not apply to informal languages such as the ones used in social networks . Therefore , DP has to be researched and explored with such social network data . In this paper , we explore and identify a DP model that is suitable for Thai social network data . After that , we will identify the appropriate linguistic unit as an input . The result showed that , the transition based model called , improve Elkared dependency parser outperform the others at UAS of 81 . 42% . 

TagRuler : Interactive Tool for Span - Level Data Programming by Demonstration
Despite rapid developments in the field of machine learning research , collecting high quality labels for supervised learning remains a bottleneck for many applications . This difficulty is exacerbated by the fact that state - of - the art models for NLP tasks are becoming deeper and more complex , often increasing the amount of training data required even for fine - tuning . Weak supervision methods , including data programming , address this problem and reduce the cost of label collection by using noisy label sources for supervision . However until recently , data programming was only accessible to users who knew how to program . In order to bridge this gap , the Data Programming by Demonstration framework was proposed to facilitate the automatic creation of labeling functions based on a few examples labeled by a domain expert . This framework has proven successful for generating high accuracy labeling models for document classification . In this work , we extend the DPBD framework to span - level annotation tasks , arguably one of the most time consuming NLP labeling tasks . We built a novel tool , TagRuler , that makes it easy for annotators to build span - level labeling functions without programming and encourages them to explore trade - offs between different labeling models and active learning strategies . We empirically demonstrated that an annotator could achieve a higher F1 score using the proposed tool compared to manual labeling for different span - level annotation tasks . 

A parsing algorithm of natural language based on operator precedence
To deal with the problems in the current parsing method of natural language , a language model based on purely typed binary relations is proposed in this paper , which is quite different from classical context - free grammar , Chomsky normal form and dependency grammar . The completeness of language structure coverage is easily captured using the proposed language model , so long as the binary relations can be recognized thoroughly . The parsing process is considered as the manipulation of binary operations of words , which is similar to the computation of arithmetic expressions in terms of the proposed language model , where operands are words and operators are the typed relations . And thereafter , a parsing algorithm based on operator precedence is proposed in the paper , with the algorithm formalized and the complexity analyzed . The proposed ideas have been proved valid and realistic through the application to parsing Chinese sentences . Further researches are needed to put it into the effective and practical usages in large scale to make it developed as a powerful theoretical way for parsing natural language . 

Semantic Error Detection and Correction in Bangla Sentence
Detection and correction of errors in Bengali text is essential . In general , Bengali text error can be classified into non - word error and semantic error ( also known as context sensitive error) . Till date , auto - correction for semantic error in Bengali sentence is challenging since there is no significant research works on this very topic . In this paper , we bring out the concept of Semantic Error detection and correction . We have developed a method that can detect and correct this kind of errors . Semantic error includes typographical error , grammatical errors , homophone errors , homonym error etc . Our goal to this study is to develop an approach to handle multiple semantic errors in a sentence . We have used our own built confused word list by edit distance and apply Na ï ve Bayes Classifier to detect and correct typographical and homophone error . For a candidate word from a sentence , we pick out a set of words which is a collection of confused words . We use all other neighbor words as features for each word from confusion set . Then we apply na ï ve theorem to calculate the probability and decide whether a target word is error or not . We have used 28 , 057 sentences to evaluate our model and we have achieved more than 90 % accuracy . All data corpora used to evaluate the model are built by us . We strongly believe that the problem we have solved may shed light on the advancement of Bengali language processing significantly . 

Semantic keywords - based duplicated web pages removing
Because of many duplicated web pages existing on the web , search engines need to find and remove them , not only for saving process time and hardware resource , but also for ensuring that users can get the result information without many replicas . In this paper , we propose a method to find and remove duplicated Chinese Web pages for search engine . First we describe a scheme based on semantic keywords combined with sentence overlapping , and then show an implemented prototype , with the experimental results that suggest the prototype work well under a proper setting . 

Voice - based Road Navigation System Using Natural Language Processing ( NLP ) 
In a highly technological era , voice - based navigation systems play a major role in bridging the gap between man and machine . To overcome the difficulty in understanding the user ' s voice commands and natural language simulations , process the path with the user ' s turn by turn directions with the mention of key entities such as street names , landmarks , points of interest , connections and path mapping in an interactive interface , we propose a user - centric roadmap navigation mobile application called “ Direct Me” . To generate the user ' s preferred path , the system will first convert audio streams to text through ASR using the Pocket Sphinx library , followed by Natural Language Processing ( NLP ) by taking advantage of Stanford CoreNLP Framework to retrieve navigation - related information and handle the path in the map using the Google Map API at the user ' s request . This system is used to provide an effective approach to translating natural language commands into a format that can be fully understood by machine and will benefit in the development of human - machine - oriented interface . 

Testing challenges for NLP - intensive bots
The popularity of bots is on the rise , with many bots able to interact with users via a chat or voice interface thanks to the embedding of a Natural Language Processing ( NLP ) component . Still , companies often express concerns about the quality of such bots , as their malfunctioning could have a severe impact on the company revenue or image . Unfortunately , the field of testing NLP - intensive bots is still in its infancy . This paper aims to characterize the testing properties and techniques ( and their adaptation ) relevant to this type of bots . We believe this will be helpful as a reference framework to compare and evaluate future bot testing research initiatives . 

Application of Chinese sentiment categorization to digital products reviews
Sentiment categorization have been widely explored in many fields , such as government policy , information monitoring , product tracking , etc . This paper adopts k - NN , Naive Bayes and SVM classifiers to categorize sentiments contained in on - line Chinese reviews on digital products . Our experimental results show that combining the words and phrases with sentiment orientation as hybrid features , SWM classifier achieves an accuracy of 96 , 47% , which is words of all parts of speech as features . 

Blexisma2 : A distributed agent framework for constructing a semantic lexical database based on conceptual vectors
In the framework of meaning representation in Natural Language Processing ( NLP ) , we aim to develop a system that can be used for heterogeneous applications such as Machine Translation , Information Retrieval or Lexical Access . This system is based on six hypotheses which concern meaning representation and acquisition . In this paper , we discuss the related hypotheses that motivate the construction of a such system and how these hypotheses , together with NLP software engineering concerns , led us to conceive a distributed multi - agent system for our goals . We present Blexisma2 , a distributed multi - agent system for NLP , its conceptual properties , and an example of inter - agent collaboration . The system is currently being tested on a Grid computing environment . 

Measuring Orthogonal Mechanics in Linguistic Annotation Games
Gamification has been recently growing in popularity among researchers investigating Information and Communication Technologies . Scholars have been trying to take advantage of this approach in the field of natural language processing ( NLP ) , developing Games With A Purpose ( GWAPs ) for corpus annotation that have obtained encouraging results both in annotation quality and overall cost . However , GWAPs implement gamification in different ways and to different degrees . We propose a new framework to investigate the mechanics employed in the gamification process and their magnitude in terms of complexity . This framework is based on an analysis of some of the most important contributions in the field of NLP - related gamified applications and GWAP theory . Its primary purpose is to provide a first step towards classifying mechanics that mimic mainstream video games and may require skills that are not relevant to the annotation task , defined as orthogonal mechanics . In order to test our framework , we develop and evaluate Spacewords , a linguistic space game for synonymy annotation . 

Hybrid systems for information extraction and question answering
Information Extraction , Summarization and Question Answering all manipulate natural language texts and should benefit from the use of NLP techniques . Statistical techniques have till now outperformed symbolic processing of unrestricted text . However , Information Extraction and Question Answering require by far more accurate results of what is currently produced by Bag - Of - Words approaches . Besides , we see that such tasks as Semantic Evaluation of Text Entailment or Similarity - - as required by the RTE Challenge , impose a much stricter performance in semantic terms to tell true from false pairs . We will speak in favour of a hybrid system , a combination of statistical and symbolic processing with reference to a specific problem , that of Anaphora Resolution which looms large and deep in text processing . 

Nonlinear programming applied to thermal and fluid design optimization
Thermal engineers are now commonly responsible for sizing and selecting active cooling components such as fans and heat sinks , and increasingly single and two - phase coolant loops . . Meanwhile , heat transfer and fluid flow design analysis software has matured , growing both in ease of use and in phenomenological modeling prowess . Unfortunately , most software retains a focus on point - design simulations and needs to do a better job of helping thermal engineers not only evaluate designs , but also investigate alternatives and even automate the search for optimal designs . This paper shows how readily available nonlinear programming ( NLP ) techniques can be successfully applied to automating design synthesis activities , allowing the thermal engineer to approach the problem from a higher level of automation . This paper briefly introduces NLP concepts , and then demonstrates their application both to a simplified fin ( extended surface ) as well as a more realistic case : a finned heat sink . 

Overview of the Electronics Redesign of the multi - Needle Langmuir Probe System
A single chip ASIC solution is being developed for the University of Oslo ’ s multi - Needle Langmuir Probe system , which currently uses a multi - chip architecture . It has been in development since 2017 , and is presently being characterized and tested in an electronics lab as well as a plasma chamber . The units under test consist of two generations of ASIC chips that both use high voltage XFAB technology alongside ADC and DAC chips provided by well established manufacturers that are used as reference . All components are on a single PCB that is used for each testing condition . An overview is given of the current status of the project , along with future testing procedures and outlook . 

The Development of Intelligent Models for Health Classification
This paper focuses on development intelligent health classification models for individuals with C4 . 5 techniques , which collected physical fitness data of those who exercise in Ratchaburi province . A total of dataset were used for creating models 376 records , and 19 attributes of personal information consists of age , sex , weight , height , pulse , upper blood , lower blood , weight bike , hand womenorce , leg stretch , triceps , biceps , suprailiac , subscapular , leg , grip , womenlex , lung capacity , O 2 and two classes . The classification is divided into five classes , including the best health , good , normal , low and very low . In our experiment , the researcher divided the data set into two groups : training and testing and designed the test using 10 - fold crossvalidation method . The accuracy rate of C4 . 5 shown 100 % . 

Using dependency parsing and probabilistic inference to extract relationships between genes , proteins and malignancies implicit among multiple biomedical research abstracts
We describe BioLiterate , a prototype software system which infers relationships involving relationships between genes , proteins and malignancies from research abstracts , and has initially been tested in the domain of the molecular genetics of oncology . The architecture uses a natural language processing module to extract entities , dependencies and simple semantic relationships from texts , and then feeds these features into a probabilistic reasoning module which combines the semantic relationships extracted by the NLP module to form new semantic relationships . One application of this system is the discovery of relationships that are not contained in any individual abstract but are implicit in the combined knowledge contained in two or more abstracts . 

OntoGene in BioCreative II . 5
We describe a system for the detection of mentions of protein - protein interactions in the biomedical scientific literature . The original system was developed as a part of the OntoGene project , which focuses on using advanced computational linguistic techniques for text mining applications in the biomedical domain . In this paper , we focus in particular on the participation to the BioCreative II . 5 challenge , where the OntoGene system achieved best - ranked results . Additionally , we describe a feature - analysis experiment performed after the challenge , which shows the unexpected result that one single feature alone performs better than the combination of features used in the challenge . 

NLP Tools for Knowledge Extraction from Italian Archaeological Free Text
This paper deals with the development of advanced tools and technologies for creating relevant information and suitable metadata out of textual documentation produced by Italian archaeological research . A set of Natural Language Processing tools were developed to recognize and annotate various archaeological entities in Italian language textual reports . The CIDOC CRM is the ontology chosen for encoding resulting output , allowing for a maximum degree of standardisation of the produced metadata to guarantee interoperability with archaeological information already existing in other semantically enabled digital archives . The work took place as part of the development for the TEXTCROWD platform for the European Open Science Cloud for Research Pilot Project . 

BioCreative / OHNLP Challenge 2018
The application of Natural Language Processing ( NLP ) methods and resources to clinical and biomedical text has received growing attention over the past years , but progress has been limited by difficulties to access shared tools and resources , partially caused by patient privacy and data confidentiality constraints . Efforts to increase sharing and interoperability of the few existing resources are needed to facilitate the progress observed in the general NLP domain . Leveraging our research in corpus analysis and de - identification research , we have created multiple synthetic data sets for a couple of NLP tasks based on real clinical sentences . We are organizing a challenge workshop to promote community efforts towards the advancement in clinical NLP . The challenge workshop will have two tasks : 1 ) Family History Information Extraction ; and 2 ) Clinical Semantic Textual Similarity . 

On the Naturalness and Localness of Software Logs
Logs are an essential part of the development and maintenance of large and complex software systems as they contain rich information pertaining to the dynamic content and state of the system . As such , developers and practitioners rely heavily on the logs to monitor their systems . In parallel , the increasing volume and scale of the logs , due to the growing complexity of modern software systems , renders the traditional way of manual log inspection insurmountable . Consequently , to handle large volumes of logs efficiently and effectively , various prior research aims to automate the analysis of log files . Thus , in this paper , we begin with the hypothesis that log files are natural and local and these attributes can be applied for automating log analysis tasks . We guide our research with six research questions with regards to the naturalness and localness of the log files , and present a case study on anomaly detection and introduce a tool for anomaly detection , called ANALOG , to demonstrate how our new findings facilitate the automated analysis of logs . 

A Deep Learning Approach for Part - of - Speech Tagging in Nepali Language
Part of Speech ( POS ) tagging is the most fundamental task in various natural language processing ( NLP ) applications such as speech recognition , information extraction and retrieval and so on . POS tagging involves annotation of appropriate tag for each token in the corpus based on its context and the syntax of the language . In computational linguistics , optimal POS tagger is of paramount importance since tagging errors can critically affect the performance of the complex NLP systems . Developing an efficient POS tagger for morphologically rich languages like Nepali is a challenging task . In this paper , a deep learning based POS tagger for Nepali text is proposed which is built using Recurrent Neural Network ( RNN ) , Long Short - Term Memory Networks ( LSTM ) , Gated Recurrent Unit ( GRU ) and their bidirectional variants . Performance metrics such as accuracy , precision , recall and F1 - score were chosen for the model evaluation . It is observed from the results that our model shows significant improvement and outperforms the state - of - art POS taggers with more than 99 % accuracy . 

Generating a Scene from Text for Smart Education
Students learn best when they can see the material being taught . So , instead of a traditional approach where teachers just write a sentence and it is up to students to visualize the sentence which is written , a new approach has been proposed . The system that has been proposed is , whenever a user scans the text , it will be detected and processed using Natural Language Processing ( NLP) . An animated scene will be created which will describe the text as appropriate as possible . This system will have a good impact on primary education . Students will be able to visualize the sentence and facts that are being taught by the teacher . The system consists of two steps . The first step is scanning the written text and converting it into actual text using OCR and then processing that text using NLP . Second step is to generate a scene which best describes the text . 

A Systematic Mapping Study of Language Features Identification from Large Text Collection
Natural Language Processing 11 Henceforth : NLP is an emerging research area in today ' s era . The NLP resources are quite useful when it comes to building a machine capable of translating between linguistic pairs - a solution that strives to resolve the language barrier problems . Based on this premise , we are focusing our research on feature identification from large text collections of Albanian language . ` Rule - based ' or statistical Part - of - Speech 22 Henceforth : POS ( POS ) taggers are sought to be utilized that would either need considerable time for rule development or a sufficient amount of manually labelled data . In light of this , the impact of this research is based on exploring numerous cases that are conducive to progress and further development of this field . One of the goals of this paper is to conduct a systematic review study ; to explore and analyze existing research that seek to target low resources language such as is the case of the Albanian language . According to prior observation of published research conducted since 2015 , we are focusing our research on studies that have been published in areas that are relevant to Natural Language Processing . Based on considerable load of related research on this field , it is essential to conduct a review and provide an outline of the research situation as well as current developments in this specific but important field of research . 

Robust speech recognition technology program summary
The major objective of this program is to develop and demonstrate robust , high - performance continuous speech recognizer ( CSR ) techniques and systems focused on application in spoken language systems ( SLS ) . A key supporting objective is to develop techniques for integration of CSR and natural language processing ( NLP ) systems in SLS applications . The CSR techniques are based on a continuous - observation Hidden Markov Model ( HMM ) approach , which has previously demonstrated high performance for normal speech and robustness for stressed speech . The motivation is that current state - of - the - art CSR systems must be improved in performance and robustness for advanced SLS environments , with variabilities including those due to spontaneous speech , noise , and task - induced stress . The effort in CSR / NLP integration is focused on development of a structured CSR / NLP interface , which will allow effective collaboration with and between other groups developing NLP and / or CSR systems . 

Characterizing Depressive Related Speech with MFCC
The experimental results from comparative study of acoustical properties in speech as emotional indicator based on spectral characteristics of speech signal have formerly been studied and reported for its quantitative information in association with the emotional states in persons suffering depression . This symptom affects speech production system of speaker , which modulates in spoken sound . MFCC has been reported for its characteristic change corresponding to severity of depression . The sixteenth MFCCs from remitted , depressed and suicidal patient groups were extracted , statistically tested and classified in pairwise fashion by using ML , LS and LMS classifiers . The best score of classification can be obtained at 0 . 2487 in error based on ML classifier with 80 % of MFCC samples in testing phase . Results suggest the dominant property of MFCC in separation between suicidal and recovering speakers from depression . 

SBERT - WK : A Sentence Embedding Method by Dissecting BERT - Based Word Models
Sentence embedding is an important research topic in natural language processing ( NLP ) since it can transfer knowledge to downstream tasks . Meanwhile , a contextualized word representation , called BERT , achieves the state - of - the - art performance in quite a few NLP tasks . Yet , it is an open problem to generate a high quality sentence representation from BERT - based word models . It was shown in previous study that different layers of BERT capture different linguistic properties . This allows us to fuse information across layers to find better sentence representations . In this work , we study the layer - wise pattern of the word representation of deep contextualized models . Then , we propose a new sentence embedding method by dissecting BERT - based word models through geometric analysis of the space spanned by the word representation . It is called the SBERT - WK method . No further training is required in SBERT - WK . We evaluate SBERT - WK on semantic textual similarity and downstream supervised tasks . Furthermore , ten sentence - level probing tasks are presented for detailed linguistic analysis . Experiments show that SBERT - WK achieves the state - of - the - art performance . Our codes are publicly available . 

Toward a noun morphological analyser of standard Amazigh
Despite its position as second official language of Morocco , Amazigh language still suffers from the lack of studies from computational point of view . To achieve this goal , and given that the noun classification system in Amazigh is computationally interesting and constitutes a major word category which plays a major role in syntactic analysis , we have undertaken to develop a component of a noun morphological analysis for standard Amazigh of Morocco using finite state technology , within the linguistic developmental environment NooJ . 

HOO 2012 : a report on the preposition and determiner error correction shared task
Incorrect usage of prepositions and determiners constitute the most common types of errors made by non - native speakers of English . It is not surprising , then , that there has been a significant amount of work directed towards the automated detection and correction of such errors . However , to date , the use of different data sets and different task definitions has made it difficult to compare work on the topic . This paper reports on the HOO 2012 shared task on error detection and correction in the use of prepositions and determiners , where systems developed by 14 teams from around the world were evaluated on the same previously unseen errorful text . 

Finite state tools for natural language processing
We describe a set of tools using deterministic , acyclic , finite - state automata for natural language processing applications . The core of the tool set consists of two programs constructing finite - state automata ( using two different , but related algorithms) . Other programs from the set interpret the contents of those automata . Preprocessing scripts and user interfaces complete the set . The tools are available for research purposes in source form in the Internet . 

MACAON : an NLP tool suite for processing word lattices
MACAON is a tool suite for standard NLP tasks developed for French . MACAON has been designed to process both human - produced text and highly ambiguous word - lattices produced by NLP tools . MACAON is made of several native modules for common tasks such as a tokenization , a part - of - speech tagging or syntactic parsing , all communicating with each other through XML files . In addition , exchange protocols with external tools are easily definable . MACAON is a fast , modular and open tool , distributed under GNU Public License . 

Session details : Why has theoretical NLP made so little progress ? 
No abstract available . 

Evaluating Distributional Semantic and Feature Selection for Extracting Relationships from Biological Text
The constant flow of biomolecular findings being published each day challenges our ability to develop methods to automatically extract the knowledge expressed in text to potentially influence new discoveries . Finding relations between the biological entities ( e . g . proteins and genes ) in text is a challenging task . To facilitate the extraction process , a relation can be decomposed into a trigger and the complementary arguments ( e . g . theme , site ) . Several approaches have been proposed based on machine learning which generally use a common set of features for all trigger types . Here we evaluate the impact of applying a feature selection method for trigger classification . Our proposed method uses a greedy feature selection algorithm to find an optimal set of attributes for each trigger type . We show that using the customized set of features can improve classification results significantly ( up to 53 . 96 % in f - measure ) . In addition , we evaluated different settings for including semantic features in the classifiers . We found that using semantic features can improve classification results and found the best setting for each trigger type . 

Dialogue tagsets in oncology
Dialogue analysis is widely used in oncology for training health professionals in communication skills . Parameters and tagsets have been developed independently of work in natural language processing . In relation to emergent standards in NLP , syntactic tagging is minimal , semantics is domain - specific , pragmatics is comparable , and the analysis of cognitive affect is richly developed . We suggest productive directions for convergence . 

Towards bounded model checking using nonlinear programming solver
Due to their complexity , currently available bounded model checking techniques based on Boolean Satisfiability and Satisfiability Modulo Theories inadequately handle non - linear floating - point and integer arithmetic . Using a numerical approach , we reduce a bounded model checking problem to a constraint satisfaction problem . Currently available techniques attempt to solve the constraint problem but can guarantee neither global convergence nor correctness . Using the IPOPT and ANTIGONE non - linear programming ( NLP ) solvers , we transform the original constraint satisfaction problem from one having disjunctions of constraints into one having conjunctions of constraints with a few introduced auxiliary variables . The transformation lowers the computing cost and preserves the Boolean structure of the original problem while complying with limits of NLP solvers . 

Beam - width prediction for efficient context - free parsing
Efficient decoding for syntactic parsing has become a necessary research area as statistical grammars grow in accuracy and size and as more NLP applications leverage syntactic analyses . We review prior methods for pruning and then present a new framework that unifies their strengths into a single approach . Using a log linear model , we learn the optimal beam - search pruning parameters for each CYK chart cell , effectively predicting the most promising areas of the model space to explore . We demonstrate that our method is faster than coarse - to - fine pruning , exemplified in both the Charniak and Berkeley parsers , by empirically comparing our parser to the Berkeley parser using the same grammar and under identical operating conditions . 

Analysis of Weak Signals for Detecting Lone Wolf Terrorists
Lone wolf terrorists pose a large threat to modern society . The current ability to identify and stop these kind of terrorists before they commit a terror act is limited since they are very hard to detect using traditional methods . However , these individuals often make use of Internet to spread their beliefs and opinions , and to obtain information and knowledge to plan an attack . Therefore , there is a good possibility that they leave digital traces in the form of weak signals that can be gathered , fused , and analyzed . In this work we present an analysis method that can be used to analyze extremist forums to profile possible lone wolf terrorists . This method is conceptually demonstrated using the FOI Impactorium fusion platform . We also present a number of different technologies that can be used to harvest and analyze information from Internet , serving as weak digital traces that can be fused using the suggested analysis method , in order to discover possible lone wolf terrorists . 

Designing Whisper : a semiautonomous system for database encoding
Whisper is a measurably effective user interface for extracting formatted textural data from unformatted text . Quantifiable objectives for user productivity , data accuracy and learnability were achieved by a design that integrates direct - manipulation techniques with natural language processing ( NLP ) capabilities . It provides a consistent and extensible user interface that will facilitate the gradual insertion of more powerful NLP as it becomes available , without disrupting established encoding productivity . The authors describe the application problem , the design approach and the empirical evaluation of Whisper . < > View less

Applying Restrained Likelihood and Floating TMR to Multi - Speaker Identification for Co - channel Speech
In this paper , a multi - speaker identification system for co - channel speech is proposed . By using constrained likelihood and floating TMR method , this system can identify two speakers on co - channel speech with high accuracy . 

A deeply annotated testbed for geographical text analysis : The Corpus of Lake District Writing
This paper describes the development of an annotated corpus which forms a challenging testbed for geographical text analysis methods . This dataset , the Corpus of Lake District Writing ( CLDW) , consists of 80 manually digitised and annotated texts ( comprising over 1 . 5 million word tokens) . These texts were originally composed between 1622 and 1900 , and they represent a range of different genres and authors . Collectively , the texts in the CLDW constitute an indicative sample of writing about the English Lake District during the early seventeenth century and the early twentieth century . The corpus is annotated more deeply than is currently possible with vanilla Named Entity Recognition , Disambiguation and geoparsing . This is especially true of the geographical information the corpus contains , since we have undertaken not only to link different historical and spelling variants of place - names , but also to identify and to differentiate geographical features such as waterfalls , woodlands , farms or inns . In addition , we illustrate the potential of the corpus as a gold standard by evaluating the results of three different NLP libraries and geoparsers on its contents . In the evaluation , the standard NER processing of the text by the different NLP libraries produces many false positive and false negative results , showing the strength of the gold standard . 

Semantic similarity measure for Thai language
Assessing the semantic similarity of texts is a fundamental concept which has many applications in natural language processing and related fields . This work presents both word and sentence semantic similarity measures specifically for Thai language . The word similarity measure is based on word embedding vectors , WordNet database and an edit - distance measure . The sentence similarity measure relies on the word similarity measure as a baseline . The proposed measures are compared with existing methods on benchmark datasets . 

Questions Analysis of Confucius Analects Knowledge System
In this paper we explore the method based on keywords to analyze the questions for Confucius Analects Knowledge System . After analyzing the questions we can find the answers from the knowledge database of Confucius Analects . Confucius Analects Knowledge System is regard as a specific domain Question - Answer system . Taking a reference to the chapters ' structure of Confucius Analects and the literature on Confucius Analects the chapters are classified into 8 categories . One of questions analysis ' work is deciding the categories of questions . We use How Net to get all meanings of keywords then select congruent meanings for each category . Questions ' categories depend on the keywords . All the questions collected are clustered by k - medoids algorithm . We find some rules from the clusters so that all the questions are classified into several types . The other work of questions analysis ' is deciding the questions ' types . Then we can find answers from the knowledge base . 

MedLingMap : a growing resource mapping the Bio - Medical NLP field
The application of natural language processing ( NLP ) in the biology and medical domain crosses many fields from Healthcare Information to Bioinformatics to NLP itself . In order to make sense of how these fields relate and intersect , we have created " MedLingMap " ( www . medlingmap . org ) which is a compilation of references with a multi - faceted index . The initial focus has been creating the infrastructure and populating it with references annotated with facets such as topic , resources used ( ontologies , tools , corpora ) , and organizations . Simultaneously we are applying NLP techniques to the text to find clusters , key terms and other relationships . The goal for this paper is to introduce MedLingMap to the community and show how it can be a powerful tool for research and exploration in the field . 

Seeking Nonsense , Looking for Trouble : Efficient Promotional - Infection Detection through Semantic Inconsistency Search
Promotional infection is an attack in which the adversary exploits a website ' s weakness to inject illicit advertising content . Detection of such an infection is challenging due to its similarity to legitimate advertising activities . An interesting observation we make in our research is that such an attack almost always incurs a great semantic gap between the infected domain ( e . g . , a university site ) and the content it promotes ( e . g . , selling cheap viagra ) . Exploiting this gap , we developed a semantic - based technique , called Semantic Inconsistency Search ( SEISE ) , for efficient and accurate detection of the promotional injections on sponsored top - level domains ( sTLD ) with explicit semantic meanings . Our approach utilizes Natural Language Processing ( NLP ) to identify the bad terms ( those related to illicit activities like fake drug selling , etc . ) most irrelevant to an sTLD ' s semantics . These terms , which we call irrelevant bad terms ( IBTs ) , are used to query search engines under the sTLD for suspicious domains . Through a semantic analysis on the results page returned by the search engines , SEISE is able to detect those truly infected sites and automatically collect new IBTs from the titles / URLs / snippets of their search result items for finding new infections . Running on 403 sTLDs with an initial 30 seed IBTs , SEISE analyzed 100K fully qualified domain names ( FQDN ) , and along the way automatically gathered nearly 600 IBTs . In the end , our approach detected 11K infected FQDN with a false detection rate of 1 . 5 % and over 90 % coverage . Our study shows that by effective detection of infected sTLDs , the bar to promotion infections can be substantially raised , since other non - sTLD vulnerable domains typically have much lower Alexa ranks and are therefore much less attractive for underground advertising . Our findings further bring to light the stunning impacts of such promotional attacks , which compromise FQDNs under 3 % of . edu , . gov domains and over one thousand gov . cn domains , including those of leading universities such as stanford . edu , mit . edu , princeton . edu , havard . edu and government institutes such as nsf . gov and nih . gov . We further demonstrate the potential to extend our current technique to protect generic domains such as . com and . org . 

Learning job representation using directed graph embedding
In recent years , embedding technologies have gained popularity in many areas of machine learning , such as NLP , computer vision , information retrieval , etc . . In this paper , we propose a latent representation of job positions consisting of job title and company pairs , which can capture not only similarity relations but also ordering relations among job positions . We first construct a directed graph of job positions from the user ' s job transition history in the resume data , then we train the job position embedding using an asymmetric relation preserving graph embedding algorithm . Experimental results on a career move prediction task using real - world data set demonstrated that the proposed embedding solution can outperform state - of - the - art embedding methods . 

Bayesian Analysis in Natural Language Processing
Book Abstract : Natural language processing ( NLP ) went through a profound transformation in the mid - 1980s when it shifted to make heavy use of corpora and data - driven techniques to analyze language . Since then , the use of statistical techniques in NLP has evolved in several ways . One such example of evolution took place in the late 1990s or early 2000s , when full - fledged Bayesian machinery was introduced to NLP . This Bayesian approach to NLP has come to accommodate for various shortcomings in the frequentist approach and to enrich it , especially in the unsupervised setting , where statistical learning is done without target prediction examples . We cover the methods and algorithms that are needed to fluently read Bayesian learning papers in NLP and to do research in the area . These methods and algorithms are partially borrowed from both machine learning and statistics and are partially developed " in - house " in NLP . We cover inference techniques such as Markov chain Monte Carlo sampling and variational inference , Bayesian estimation , and nonparametric modeling . We also cover fundamental concepts in Bayesian statistics such as prior distributions , conjugacy , and generative modeling . Finally , we cover some of the fundamental modeling techniques in NLP , such as grammar modeling and their use with Bayesian analysis . 

A fast LP approach for enhanced utilization of variable impedance based FACTS devices
Transmission systems are under stress and need to be upgraded . Better utilization of the existing grid provides a fast and cheap alternative to building new transmission . One way to improve the utilization of the transmission network is power flow control via flexible ac transmission system ( FACTS ) devices . While FACTS devices are used today , the utilization of these devices is limited ; traditional dispatch models ( e . g . , security constrained economic dispatch ) assume a fixed , static transmission grid even though it is rather flexible . The primary barrier is the complexity that is added to the power flow problem . The mathematical representation of the DC optimal power flow , with the added modeling of FACTS devices , is a nonlinear program ( NLP ) . This paper presents a method to convert this NLP into a mixed - integer linear program ( MILP ) . The MILP is reformulated as a two - stage linear program , which enforces the same sign for the voltage angle differences for the lines equipped with FACTS . While this approximation does not guarantee optimality , more than 98 % of the presented empirical results , based on the IEEE 118 - bus and Polish systems , achieved global optimality . In the case of suboptimal solutions , the savings were still significant and the solution time was dramatically reduced . 

Converting Natural Language Text to ROS - Compatible Instruction Base
Natural Language processing is a growing field . Although it is difficult to create a natural language system that can robustly react to and handle every situation , it is quite possible to design the system to react to specific instructions or scenario . The contributions of this work are ( 1 ) to design a set of instruction types that can allow for conditional statements within natural language instructions , ( 2 ) to create a modular system using Robot Operating System ( ROS ) in order to allow for more robust communication and integration , and ( 3 ) to allow for an interconnection between the written text and derived instructions that will make the sentence construction more seamless and natural for the user . As the results will show , this system can be run on a diverse set of sentence structures , allowing for robust paragraphs . This system must also then be carefully set to fit the exact parameters that the user is looking for , trying to strike the balance between how much the user needs to learn and how accurate to the instruction the system needs to run . 

Predicting Frequently Asked Questions ( FAQs ) on the COVID - 19 Chatbot using the DIET Classifier
A popular dialogue system in the field of natural language processing ( NLP ) is the chatbot . Chatbots aim to create conversations between humans and machines . COVID - 19 is a member of the Coronavidae ( CoV ) family of the Corona viirinae family which causes the respiratory system to become severe in humans . This paper predicts chatbot answers to questions about COVID - 19 with the RASA framework and uses the DIET Classifier pipeline for 300 training data . The test results with the DIET Classifier model on rasa . core . test and rasa . nlu . test provided confidence values of F1 - Score , precision , and accuracy for the correct answer to the question about COVID - 19 , namely 1 . 0 with a percentage of around 85% . 

Analyzing Tweets on New Norm : Work from Home during COVID - 19 Outbreak
The COVID - 19 pandemic triggered a large - scale work - from - home trend globally in recent months . In this paper , we study the phenomenon of “ work - from - home ” ( WFH ) by performing social listening . We propose an analytics pipeline designed to crawl social media data and perform text mining analyzes on textual data from tweets scrapped based on hashtags related to WFH in COVID - 19 situation . We apply text mining and NLP techniques to analyze the tweets for extracting the WFH themes and sentiments ( positive and negative ) . Our Twitter theme analysis adds further value by summarizing the common key topics , allowing employers to gain more insights on areas of employee concerns due to pandemic . 

Session summary
Collectively , the papers in this session are mainly concerned with parsing , semantic interpretation , and inference . Interest in these processes can be motivated if we recognize that the overall goal of the field of NLP is the manipulation of natural language in ways that depend on meaning . Parsing , the recovery of the linguistic or grammatical structure of a natural - language utterance , is of concern to NLP because , in general , the meaning of a natural - language utterance depends on its structure . An example that illustrates this point is the sentence The man with the umbrella opened the door . If we tried to process this sentence without paying attention to its grammatical structure , we could easily be misled by the fact that it contains the substring the umbrella opened . But this sentence has nothing to do with umbrellas opening . Because of the structure of the sentence , the umbrella must be grouped with with and not with opened . 

The Automatic Text Classification Method Based on BERT and Feature Union
For the traditional model based on the deep learning method most used CNN ( convolutional neural networks ) or RNN ( Recurrent neural Network ) model and is based on the dynamic character - level embedding or word - level embedding as input , so there is a problem that the text feature extraction is not comprehensive . In the development environment of the Internet of Things , A method of Automatic text classification based on BERT ( Bidirectional Encoder Representations from Transformers ) and Feature Fusion was proposed in this paper . Firstly , the text - to - dynamic character - level embedding is transformed by the BERT model , and the BiLSTM ( Bi - directional Long - Short Term Memory ) and CNN output features are combined and merged to make full use of CNN to extract the advantages of local features and to use BiLSTM to have the advantage of memory to link the extracted context features to better represent the text , so as to improve the accuracy of text classification task . A comparative study with state - of - the - art approaches manifests the proposed method outperforms the state - of - the - art methods in accuracy . It can effectively improve the accuracy of tag prediction for text data with sequence features and obvious local features . 

Improving Performances of Log Mining for Anomaly Prediction Through NLP - Based Log Parsing
Failure prediction of industrial systems is a promising application domain for data mining approaches and should naturally rely on log messages which are a prime source of data as they are generated by many systems . However , before extracting relevant information of such log messages , another critical step is to parse the logs , that is to say to transform a raw unstructured text from the log messages into a suitable input for data mining . These two problems ( log parsing then log mining ) are often studied separately while they are directly related in the context of failure prediction ; moreover , few performance benchmarks are publicly available . In this paper , we focus on the impact of log parsing techniques via natural language processing on the performances of log mining on two datasets . The first one is a log of an industrial aeronautical system comprising over 4 , 500 , 000 messages collected over one year of operation ; the second one is a public benchmark set from an HDFS cluster . On the latter , we show that it is possible to raise the F - score from 96 % to 99 . 2 % while using simpler and more robust log parsing techniques that require less parameter tuning provided that they are correctly combined with log mining techniques . 

Evolution of communities on Twitter and the role of their leaders during emergencies
Twitter is presently utilized as a channel of communication and information dissemination . At present , government and non - government emergency management organizations utilize Twitter to disseminate emergency relevant information . However , these organizations have limited ability to evaluate the Twitter communication in order to discover communication patterns , key players , and messages that are being propagated through Twitter regarding the event . More importantly there is a general lack of knowledge of who are the individuals or organizations that disseminate warning information , provide confirmations of an event and associated actions , and urge others to take action . This paper presents a methodology that shows how Natural Language Processing ( NLP ) and Social Network Analysis ( SNA ) can aid in addressing these issues . The methodology , in addition to qualitative data collected during on - site interviews and publicly available information , was successfully applied to a Twitter data set collected during 2011 Japan tsunami . NLP techniques were applied to extract actionable messages . Based on the messages extracted by NLP , SNA was used to construct a network of actionable messages . While SNA discovered communities and extracted the community leaders , NLP was used to determine the behavior of the community members and the role of the community leaders . Therefore , the proposed methodology automatically finds communities , evaluates its members ' behaviors , and authenticates cohesive behaviors of the community members during emergencies . Moreover , the methodology efficiently finds the leaders of the communities , while also identifying their role in communities . 

A Novel Sentiment Classification of Product Reviews using Levenshtein Distance
E - commerce platform has brought about a revolutionary change in the way people conduct shopping . In India , the number of digital buyers in 2016 was 130 . 4 million and by 2021 the number is estimated to increase to around 400 million . In this paper we propose a Levenshtein distance based sentiment classification engine that reads the reviews across different websites for a given product , eventually providing a metric that would help the user in making an informed choice of product . 

FAST : an automatic generation system for grammar tests
This paper introduces a method for the semi - automatic generation of grammar test items by applying Natural Language Processing ( NLP ) techniques . Based on manually - designed patterns , sentences gathered from the Web are transformed into tests on grammaticality . The method involves representing test writing knowledge as test patterns , acquiring authentic sentences on the Web , and applying generation strategies to transform sentences into items . At runtime , sentences are converted into two types of TOEFL - style question : multiple - choice and error detection . We also describe a prototype system FAST ( Free Assessment of Structural Tests ) . Evaluation on a set of generated questions indicates that the proposed method performs satisfactory quality . Our methodology provides a promising approach and offers significant potential for computer assisted language learning and assessment . 

Analyzing Quality of Software Requirements ; A Comparison Study on NLP Tools
For analyzing the quality of software requirements and performing natural language ( NL ) processing , only few of many tools are currently adopted by the industry . Most of these tools are no longer available and are obsoleted due to their complexity , less accuracy and high cost . The only few remaining tools exhibit either optimum functionality or at sub - optimal level . Some tools are limited to processing NL whereas the rest check for grammatical mistakes . First , this paper performs comparison on five existing tools named as QVscribe , QuOD , Innoslate , RAT and RQA in which we compare features , functionality and quality indicators addressed by these tools . Second , we propose a new tool named as Requirement Assessment Tool ( RAT ) for analyzing quality of individual requirements and specification document . Finally , check the performance of these tools for automatically detecting ambiguity , deviations and quality affecting defects on the basis of real - world natural language requirements set i . e . golden standard data . The results of these tools concluded that RAT gives more accurate results for detecting defects in all categories except looking for negative sentences . 

Deep Neural Networks for Social Media Word Segmentation of Asian Languages
Information extraction today faces new challenges with noisy , short , unstructured data . This is especially the case for social media messages , such as tweets , in which language can be erroneous or cryptic , and contains references to a great number of new entities . Traditional NLP systems are challenged and need to develop new strategies to handle with these data . With the emergence of the neural network - based approach , the research about the word segmentation has benefited from large - scale raw texts by leveraging them for pretrained character and word embeddings . To this end , we experimented the use of both character and word embeddings to provide extra features to input layer of our neural network - based system architecture . This system has been tested on both Chinese and Japanese social media datasets . With the help of rich pretrained embeddings , our model achieved the promising results both on Chinese and Japanese social media word segmentation task by comparing with the state - of - the - art NLP tools . 

Exploiting online human knowledge in Requirements Engineering
Data - driven Natural Language Processing ( NLP ) methods have noticeably advanced in the past few years . These advances can be tied to the drastic growth of the quality of collaborative knowledge bases ( KB ) available on the World Wide Web . Such KBs contain vast amounts of up - to - date structured human knowledge and common sense data that can be exploited by NLP methods to discover otherwise - unseen semantic dimensions in text , aiding in tasks related to natural language understanding , classification , and retrieval . Motivated by these observations , we describe our research agenda for exploiting online human knowledge in Requirements Engineering ( RE ) . The underlying assumption is that requirements are a product of the human domain knowledge that is expressed mainly in natural language . In particular , our research is focused on methods that exploit the online encyclopedia Wikipedia as a textual corpus . Wikipedia provides access to a massive number of real - world concepts organized in hierarchical semantic structures . Such knowledge can be analyzed to provide automated support for several exhaustive RE activities including requirements elicitation , understanding , modeling , traceability , and reuse , across multiple application domains . This paper describes our preliminary findings in this domain , current state of research , and prospects of our future work . 

sylbreak4all : Regular Expressions for Syllable Breaking of Nine Major Ethnic Languages of Myanmar
Unlike many other western languages , the Myanmar language uses a syllabic writing system and no space between words . Syllable segmentation is the necessary preprocess for natural language processing ( NLP ) tasks such as grapheme - to - phoneme ( g2p ) conversion , machine translation , romanization , and so on . In this study , sylbreak4all , a syllable segmentation tool , was developed for nine major ethnic languages of Myanmar , and they are Burmese , Shan , Pa ’ o , Pwo Kayin , S ’ gaw Kayin , Rakhine , Myeik , Dawei , and Mon by using regular expression ( RE ) patterns . 

Building an annotated corpus in the molecular - biology domain
Corpus annotation is now a key topic for all areas of natural language processing ( NLP ) and information extraction ( IE ) which employ supervised learning . With the explosion of results in molecular - biology there is an increased need for IE to extract knowledge to support database building and to search intelligently for information in online journal collections . To support this we are building a corpus of annotated abstracts taken from National Library of Medicine ' s MEDLINE database . In this paper we report on this new corpus , its ontological basis , and our experience in designing the annotation scheme . Experimental results are shown for inter - annotator agreement and comments are made on methodological considerations . 

An algorithm for resolution of Anaphora in English text
In this paper , we provide an algorithm for resolution of anaphors in English text . Anaphora is an important phenomenon in linguistics which enhances the beauty of a language by eliminating frequent use of words and substituting them for their equivalent pronoun / noun phrase . This linguistic tinkering often comes with an overhead of resolving the appropriate anaphors to their antecedents for use in almost every field of Natural Language Processing ( NLP ) like Machine Translation , Information Retrieval , Knowledge Extraction , and Text Summarization amongst others . We present in this paper , the study of the Anaphora Resolution system and the results of the proposed algorithm . 

Attention Based R & CNN Medical Question Answering System in Chinese
This paper proposes a Chinese medical question answering system based on CNN s with self - attention embedded model . The key idea of the proposed framework is first understanding whole sentences via LSTM , obtaining the feature map by the convolution layer of CNN , then further - comprehending of question and answer by self - attention and finally pass through to polling layer of CNN to enhance accuracy . In addition , due to the word segmentation issue in the Chinese especially in medical field , character embedding is applied to enhance the accuracy . The experimental results show that the proposed framework improves the accuracy around 80% . 

An Automated Learner for Extracting New Ontology Relations
Recently , the NLP community has shown a renewed interest in automatic recognition of semantic relations between pairs of words in text which called lexical semantics . This approach to semantics is concerned with psychological facts associated with the meaning of words . Lexical semantics is an important task with many potential applications including but not limited to , Information Retrieval , Information Extraction , Text Summarization , and Language Modeling . As this task " automatic recognition of semantic relations between pairs of words in text " can be used in many NLP applications , its implementation are demanding and may include many potential methodologies . And as it includes semantic processing , the results produced still need enhancements and the outcome was always limited in terms of domain or coverage . In this research we developed a buffered system that handle the whole process of extracting causation relations in general domain ontologies . The main achievement of this work is the heavy analysis of statistical and semantic information of causation relation context to generate the learner . The system also builds relation resources that made it possible to learn from itself , were each time it runs the resources incremented with new relations information recording all the statistics of such relation , making its performance enhanced each time it runs . Also we present a novel approach of learning based on the best lexical patterns extracted , besides two new algorithms the CIA and PS that provide the final set of rules for mining causation to enrich ontologies . 

CRFs - Based Chinese Named Entity Recognition with Improved Tag Set
Chinese named entity recognition is one of the most important tasks in NLP . The paper mainly describes our work on NER tasks . The paper built up a system under the framework of conditional random fields ( CRFs ) model . With an improved tag set the system gets an F - value of 93 . 49 using SIGHAN2007 MSRA corpus . 

LTAG dependency parsing with bidirectional incremental construction
In this paper , we first introduce a new architecture for parsing , bidirectional incremental parsing . We propose a novel algorithm for incremental construction , which can be applied to many structure learning problems in NLP . We apply this algorithm to LTAG dependency parsing , and achieve significant improvement on accuracy over the previous best result on the same data set . 

Integrating StockTwits with sentiment analysis for better prediction of stock price movement
Sentiment Analysis is new way of machine learning to extract opinion orientation ( positive , negative , neutral ) from a text segment written for any product , organization , person or any other entity . Sentiment Analysis can be used to predict the mood of people that have impact on stock prices , therefore it can help in prediction of actual stock movement . In order to exploit the benefits of sentiment analysis in stock market industry we have performed sentiment analysis on tweets related to Apple products , which are extracted from StockTwits ( a social networking site ) from 2010 to 2017 . Along with tweets , we have also used market index data which is extracted from Yahoo Finance for the same period . The sentiment score of a tweet is calculated by sentiment analysis of tweets through SVM . As a result each tweet is categorized as bullish or bearish . Then sentiment score and market data is used to build a SVM model to predict next day ' s stock movement . Results show that there is positive relation between people opinion and market data and proposed work has an accuracy of 76 . 65 % in stock prediction . 

Learning from Disagreement : A Survey
Many tasks in Natural Language Processing ( NLP ) and Computer Vision ( CV ) offer evidence that humans disagree , from objective tasks such as part - of - speech tagging to more subjective tasks such as classifying an image or deciding whether a proposition follows from certain premises . While most learning in artificial intelligence ( AI ) still relies on the assumption that a single ( gold ) interpretation exists for each item , a growing body of research aims to develop learning methods that do not rely on this assumption . In this survey , we review the evidence for disagreements on NLP and CV tasks , focusing on tasks for which substantial datasets containing this information have been created . We discuss the most popular approaches to training models from datasets containing multiple judgments potentially in disagreement . We systematically compare these different approaches by training them with each of the available datasets , considering several ways to evaluate the resulting models . Finally , we discuss the results in depth , focusing on four key research questions , and assess how the type of evaluation and the characteristics of a dataset determine the answers to these questions . Our results suggest , first of all , that even if we abandon the assumption of a gold standard , it is still essential to reach a consensus on how to evaluate models . This is because the relative performance of the various training methods is critically affected by the chosen form of evaluation . Secondly , we observed a strong dataset effect . With substantial datasets , providing many judgments by high - quality coders for each item , training directly with soft labels achieved better results than training from aggregated or even gold labels . This result holds for both hard and soft evaluation . But when the above conditions do not hold , leveraging both gold and soft labels generally achieved the best results in the hard evaluation . All datasets and models employed in this paper are freely available as supplementary materials . 

Semantic - based tag recommendation in scientific bookmarking systems
Recently , tagging has become a common way for users to organize and share digital content , and tag recommendation ( TR ) has become a very important research topic . Most of the recommendation approaches which are based on text embedding have utilized bag - of - words technique . On the other hand , proposed deep learning methods for capturing semantic meanings in the text , have been proved to be effective in various natural language processing ( NLP ) applications . In this paper , we present a content - based TR method that adopts deep recurrent neural networks to encode titles and abstracts of scientific articles into semantic vectors for enhancing the recommendation task , specifically bidirectional gated recurrent units ( bi - GRUs ) with attention mechanism . The experimental evaluation is performed on a dataset from CiteULike . The overall findings show that the proposed model is effective in representing scientific articles for tag recommendation . 

RECAST : Interactive Auditing of Automatic Toxicity Detection Models
As toxic language becomes nearly pervasive online , there has been increasing interest in leveraging the advancements in natural language processing ( NLP ) to automatically detect and remove toxic comments . Despite fairness concerns and limited interpretability , there is currently little work for auditing these systems in particular for end users . We present our ongoing work , Recast , an interactive tool for auditing toxicity detection models by visualizing explanations for predictions and providing alternative wordings for detected toxic speech . Recast displays the attention of toxicity detection models on user input , and provides an intuitive system for rewording impactful language within a comment with less toxic alternative words close in embedding space . Finally we propose a larger user study of Recast , with promising preliminary results , to validate it ’ s effectiveness and useability with end users . 

Text Summarization of Hindi Documents Using Rule Based Approach
Automatic summarization plays an important role in document processing system and information retrieval system . Generation of summary of a text document is a very important part of NLP . There are a number of scenarios where automatic construction of such summaries is useful . Text summarization is that process which convert a larger text into its shorter form maintaining its information . Summary of a longer text saves the reading time as it contain lesser number of lines but all important information of the original text document . In this paper we present a novel approach for text summarization of Hindi text document based on some linguistic rules . Dead wood words and phrases are also removed from the original document to generate the lesser number of words from the original text . Proposed system is tested on various Hindi inputs and accuracy of the system in form of number of lines extracted from original text containing important information of the original text document . 

GenERRate : generating errors for use in grammatical error detection
This paper explores the issue of automatically generated ungrammatical data and its use in error detection , with a focus on the task of classifying a sentence as grammatical or ungrammatical . We present an error generation tool called GenERRate and show how GenERRate can be used to improve the performance of a classifier on learner data . We describe initial attempts to replicate Cambridge Learner Corpus errors using GenERRate . 

Representing and accessing multi - level annotations in MMAX2
MMAX21 is a versatile , XML - based annotation tool which has already been used in a variety of annotation projects . It is also the tool of choice in the ongoing project DIANA - Summ , which deals with anaphora resolution and its application to spoken dialog summarization . The project uses the ICSI Meeting Corpus ( Janin et al . , 2003) , a corpus of multi - party dialogs which contains a considerable amount of simultaneous speech . It features a semi - automatically generated segmentation in which the corpus developers tried to track the flow of the dialog by inserting segment starts approximately whenever a person started talking . As a result , the corpus has some interesting structural properties , most notably overlap , that are challenging for an XML - based representation format . The following brief overview of MMAX2 focuses on this aspect , using examples from the ICSI Meeting Corpus . 

Generating grammar exercises
Grammar exercises for language learning fall into two distinct classes : those that are based on " real life sentences " extracted from existing documents or from the web ; and those that seek to facilitate language acquisition by presenting the learner with exercises whose syntax is as simple as possible and whose vocabulary is restricted to that contained in the textbook being used . In this paper , we introduce a framework ( called GramEx ) which permits generating the second type of grammar exercises . Using generation techniques , we show that a grammar can be used to semi - automatically generate grammar exercises which target a specific learning goal ; are made of short , simple sentences ; and whose vocabulary is restricted to that used in a given textbook . 

IQAS : Inference question answering system for handling temporal inference
One of the most crucial problems in any Natural Language Processing ( NLP ) task is the representation of time . This includes applications such as Information Retrieval techniques ( IR ) , Information Extraction ( IE ) and Question / answering systems ( QA ) . This paper deals with temporal information involving several forms of inference in Arabic language . 

An Analysis of Interaction and Engagement in YouTube Live Streaming Chat
Live streaming is a popular social media platform . Most live streaming services allow viewers to interact with each other and the broadcaster via text chat . Thus , exploring user behavior and chat communication within a live streaming context is increasingly important . In this paper , we explore YouTube general live streaming chat . We extracted a corpus in the political domain from Trump ’ s 2020 presidential campaign in the United States . Then , using advanced Natural Learning Processing ( NLP ) algorithms , we examined users ’ behavior in live chats . We focused on three elements of YouTube user behaviour : chat content , commentators ’ behaviour , and user engagement evoking factors . We observed that the chat messages are very emotional . They include a lot of emojis , some of which are domain - depended . Almost all messages express sentiment and the positive sentiment outweighs the negative . Abusive language is very common in the messages . However , while heavy users express more sentiment , the tend to use less abusive language . Although it is difficult to know exactly what was said in the video that caused the engagement of the chat participants , using a topic model , we were able to show that sarcasm evokes involvement . 

TV News Story Segmentation Using Deep Neural Network
TV news programs usually contain multiple stories on different topics , and it is essential to locate the story boundaries for the purposes of video content indexing , search , and curation . With the exponential growth of video content , story segmentation will enable the consumers to view their favorite content effortlessly , and the service providers to provide their customers with personalized services . Given the dynamic range of topics , smooth story transitions in news , and varying duration of individual story , automated news story segmentation is a challenging task . This paper focuses on using linguistic information extracted from closed caption as the initial attempt to tackle this challenge , and our future work will integrate both audio and visual . A convolutional neural network framework with attention mechanism is proposed . The model is trained and tested on TDT2 data set , and it achieves an outstanding F - measure of 0 . 789 on the validation set and a reasonable F - measure of 0 . 707 on the testing set . 

Toward A Real - Time Social Recommendation System
Recent research has investigated approaches and models to produce optimal results in social recommendation systems ( SRSs ) particularly in text - based form . The aim is to analyze the user generated - content ( UGC ) to suggest appropriate recommendations to interested users . However , users are often not satisfied with the initial recommendations because some models do not elicit their preferences at the beginning of the interaction nor do they understand their actual needs . In this paper , we propose a real - time SRSs called ChatWithRec that aims to improve the accuracy of recommendations by analyzing the user ' s contextual conversation dynamically , detect the topic , and then match it with a suitable advertisement . We used the Latent Dirichlet Allocation topic model ( LDA ) to analyze the user ' s conversation and perceive topics . We evaluated our system by applying several metrics like coherence , and F - score to evaluate the performance of ChatWithRec recommendation system . The results are encouraging , indicating that the system is fast , satisfies users by getting exactly what they seek in their conversation flow . 

Book review : L ' ESPACE EN FRANCAIS by Claude Vandeloise ( Editions du Seuil ) 
This simple yet engaging book examines the meaning of the words used to describe space and spatial relations . The words examined in this work in linguistics and cognitive science include the prepositions above / below , inside / outside , in front / behind , before / after , to the right / to the left , and others . This book will be of greatest interest to those working on the knowledge representation and semantics of spatial relations as part of a natural language processing ( NLP ) project . Applications might include parts of the NLP interface to a scene analysis system or NLP interface to an expert system advice on , for example , airplane repair where the airplane parts are in various spatial relations to one another and to the person using the system . Thus workers in NLP interfacing of various types may be interested . A good French / English dictionary may be needed to read Vandeloise although the language used is not difficult . Almost all of the issues and points examined by Vandeloise are directly applicable to English language semantics as well . 

Comparison of \ ell _{ 1 } \ ell _{ 1 } - Norm SVR and Sparse Coding Algorithms for Linear Regression
Support vector regression ( SVR ) is a popular function estimation technique based on Vapnik ' s concept of support vector machine . Among many variants , the l 1 - norm SVR is known to be good at selecting useful features when the features are redundant . Sparse coding ( SC ) is a technique widely used in many areas and a number of efficient algorithms are available . Both l 1 - norm SVR and SC can be used for linear regression . In this brief , the close connection between the l 1 - norm SVR and SC is revealed and some typical algorithms are compared for linear regression . The results show that the SC algorithms outperform the Newton linear programming algorithm , an efficient l 1 - norm SVR algorithm , in efficiency . The algorithms are then used to design the radial basis function ( RBF ) neural networks . Experiments on some benchmark data sets demonstrate the high efficiency of the SC algorithms . In particular , one of the SC algorithms , the orthogonal matching pursuit is two orders of magnitude faster than a well - known RBF network designing algorithm , the orthogonal least squares algorithm . 

Parsumist : A Persian text summarizer
The rapid growth of online information services causes the problem of information explosion . Automatic text summarization techniques are essential for dealing with this problem . The process of compacting a source document to reduce complexity and length , retaining the most important information is called text summarization . This paper introduces PARSUMIST ; a text summarization system for Persian documents . It can generate generic or topic / query - driven extract summaries for single or multiple Persian documents , using a combination of statistical , semantic and heuristic improved methods . In this paper we will first review the related works in this field and especially in Persian text summarization . Then we will present the architecture of PARSUMIST , its components and its features . The last section will evaluate the system and compare it to other existing ones . 

An NLP based text - to - speech synthesizer for Moroccan Arabic
The main objective of text - to - speech synthesis is to produce natural sounding speech from any text sequence regardless of its complexity and ambiguity . For this purpose , many approaches combine different models to text - to - Speech methods in order to enhance results . In this respect , the purpose of this paper is to present a text - to - speech synthesizer for Moroccan Arabic based on NLP rule - based and probabilistic models . 

Bandit Multiclass Linear Classification for the Group Linear Separable Case
We consider the online multiclass linear classification under the bandit feedback setting . Beygelzimer , Pal , Szorenyi , Thiruvenkatachari , Wei , and Zhang [ ICML ' 19 ] considered two notions of linear separability , weak and strong linear separability . When examples are strongly linearly separable with margin γ , they presented an algorithm based on MULTICLASS PERCEPTRON with mistake bound O ( K /γ 2 ) , where K is the number of classes . They employed rational kernel to deal with examples under the weakly linearly separable condition , and obtained the mistake bound of min ( K · 2 Õ ( K log2 ( 1 /γ ) ) , K · 2 Õ ( √1 /γ log K ) ) . In this paper , we refine the notion of weak linear separability to support the notion of class grouping , called group weak linear separable condition . This situation may arise from the fact that class structures contain inherent grouping . We show that under this condition , we can also use the rational kernel and obtain the mistake bound of K · 2 Õ ( √1 /γ log L ) ) , where L ≤ K represents the number of groups . 

Crawling Open - Source Data for Indicators of Human Trafficking
Information available from open - source mediums such as the web and social media are increasingly being used to aid the response to emergent crimes and reinforce existing Law Enforcement Agency intelligence capability . In this paper we discuss the rationale for , and development of , a framework for crawling open - source data for information regarding , and indicators of , Human Trafficking . The three - Level model presented aims to add value to the modelling , and increase the understanding of Human Trafficking in order to facilitate the detection and analysis of indicators to ultimately increase the value and range of information available to decision makers . 

Translating multi word terms into Korean from Chinese documents
This paper suggests a methodology which is aimed to translate the Chinese terms into Korean . Our basic idea is to use a method which combines domain - tuned dictionary and target language corpus . The translation system used in our work operated at three steps : ( 1 ) tokenization of the term candidate ; ( 2 ) translation of term candidates using domain - tuned bilingual lexicon ; ( 3 ) ranking the translation candidate to recommend the best translation according to the frequency of the target word cooccurrence in the corpus . And we have demonstrated the effectiveness of our approach by showing a high degree of accuracy evaluation . Our experiments indicate a significant improvement when measured against un - tuned Chinese - Korean MT system . 

COMPLEX : a computational lexicon for natural language systems
Although every natural language system needs a computational lexicon , each system puts different amounts and types of information into its lexicon according to its individual needs . However , some of the information needed across systems is shared or " identical " information . This paper presents our experience in planning and building COMPLEX , a computational lexicon designed to be a repository of shared lexical information for use by Natural Language Processing ( NLP ) systems . We have drawn primarily on explicit and implicit information from machine - readable dictionaries ( MRD ' s ) to create a broad coverage lexicon . 

Assessing the challenge of fine - grained named entity recognition and classification
Named Entity Recognition and Classification ( NERC ) is a well - studied NLP task typically focused on coarse - grained named entity ( NE ) classes . NERC for more fine - grained semantic NE classes has not been systematically studied . This paper quantifies the difficulty of fine - grained NERC ( FG - NERC ) when performed at large scale on the people domain . We apply unsupervised acquisition methods to construct a gold standard dataset for FG - NERC . This dataset is used to benchmark methods for classifying NEs at various levels of fine - grainedness using classical NERC techniques and global contextual information inspired from Word Sense Disambiguation approaches . Our results indicate high difficulty of the task and provide a ' strong ' baseline for future research . 

Adjacent reordering phrase - based translation models
We study a number of statistical phrase - based translation models that are constrained by adjacent reordering . Under this constraint , we derive polynomial decoding algorithms respectively for basic and Koehn ' s phrase - based models , with an implication that many statistical phrase - based models may also be decodable in polynomial time . Using NIST as a metric , we show that the presented decoding algorithms can achieve a relative improvement of about 2 . 7 % over Pharaoh under the same experimental conditions . 

Capturing a taxonomy of failures during automatic interpretation of questions posed in natural language
An important problem in artificial intelligence is capturing , from natural language , formal representationsallthat can be used by a reasoner to compute an answer . Many researchers have studied this problem by developing algorithms addressing specific phenomena in natural language interpretation , but few have studied ( or cataloged ) the types of failures associated with this problem . Knowledgeallof these failures can help researchers by providing a roadallmap of open research problems and help practitioners by providing a checklist of issues to address in order to build systems that can achieve good performance on this problem . allIn this paper , we present a study -- conducted in the context of the Halo Project -- cataloging the types of failures that occur when capturing knowledge from naturallanguage . We identified the categories of failures by examining a corpus of questions posed byallnaive usersallto a knowledge based question answering system and empirically demonstrated the generality of ourallcategorizations . We also describe available technologies that can address some of the failures we have identified . 

Multi - label Categorization of French Death Certificates using NLP and Machine Learning
The medical information represents an invaluable source of knowledge concerning the medical history of the patient , but the manner of their presentation make it badly exploited . The idea of this paper is based on the analysis of the death reports written in natural language , which are rich of information , and can be exploited in the calculation of mortality statistics , giving preventive solutions , as well as , help medical professional in their research . This paper proposes our approach to the task of Multi - label Categorization of French death certificates according to ICD - 10 ( International Classification of Diseases ) codes . This approach is based on Machine learning techniques , which is evaluated over C é piDC corpus . The experiment showed that our approach gives interesting results , with an average F1 - measue of 79 . 02% . 

Fusion of acoustic , linguistic and psycholinguistic features for Speaker Personality Traits recognition
Behavioral analytics is an emerging research area that aims at automatic understanding of human behavior . For the advancement of this research area , we are interested in the problem of learning the personality traits from spoken data . In this study , we investigated the contribution of different types of speech features to the automatic recognition of Speaker Personality Trait ( SPT ) across diverse speech corpora ( broadcast news and spoken conversation ) . We have extracted acoustic , linguistic , and psycholinguistic features and modeled their combination as input to the classification task . For the classification , we used Sequential Minimal Optimization for Support Vector Machine ( SMO ) together with Relief feature selection . The present study shows different levels of performance for automatically selected feature sets , and overall improved performance with their combination across diverse corpora . 

Summarising company announcements
This paper describes work that attempts to use language technology as a solution to the problem of information overload . The specific domain of application is the database of company announcements accessible via the Web site of the Australian Stock Exchange to meet regulatory requirements , over 100 , 000 documents a year are made available via this site , with only limited search facilities . We use a variety of techniques from language technology to make it easier to explore and manage the information in this data set . In this paper , we focus on our use of information extraction , which identifies and extracts important elements of information from a document , and text compaction , which applies linguistically - motived substitutions to reduce potential summary sentences to more compact forms . Together , these techniques provide a way of producing summaries of a significant proportion of the document base . 

Dialogue strategies for improving the usability of telephone human - machine communication
Interactions with spoken language systems may present breakdowns that are due to errors in the acoustic decoding of user utterances . Some of these errors have important consequences in reducing the naturalness of human - machine dialogues . In this paper we identify some typologies of recognition errors that cannot be recovered during the syntactico - semantic analysis , but that may be effectively approached at the dialogue level . We will describe how non - understanding and the effects of misrecognition are dealt with by Dialogos , a realtime spoken dialogue system that allows users to access a database of railway information by telephone . We will discuss the importance of supporting confirmation turns , and clarification and correction sub - dialogues . We will show the positive effects of robust dialogue management and dialogue state dependent language modeling , by taking into account both the recognition and understanding performance , and the success rate of dialogue transactions . 

Smarter Commerce : NLP SpokenWeb Based B2B Messaging
In a traditional B2B world , the enterprises invest substantial amount of money on hosting and maintaining B2B gateways for business interactions with suppliers and customers . There are multitude of B2B frameworks which are catering the needs of these enterprises . However , there is lack of solutions in emerging markets where adaption to technology is a challenge and investment on enterprise level B2B gateway products to perform business document exchanges is almost impossible . In this paper , we extend our earlier research work and implement a speech based B2B messaging solution which would enable MSME ( Micro , small and medium enterprises ) to interact with existing enterprise gateway infrastructure without any investment . Also , they can perform business document exchanges over phone in their regional language thus giving them the benefit to leverage the advantages of standard B2B processes . This service also offers key B2B protocol capabilities and features such as packaging , security and partner on - boarding . The proposed solution would enable the MSME to perform B2B business in an easier way and will help them go a long way in reducing costs . 

Extending Dempster - Shafer theory to overcome counter intuitive results
A popular formalism to model someone ' s degrees of belief is the Theory of Evidence , or Dempster - Shafer Theory . This theory provides a method for combining evidence from different sources without prior knowledge of their distributions . It is also possible to assign probability values to sets of possibilities rather than to single events only , and it is unnecessary to divide all the probability values among the events , once the remaining probability should be assigned to the environment and not to the remaining events , thus modeling more naturally certain classes of problems . However , it has some pitfalls caused by the non natural embodiment of the uncertainty in the results . In this paper we present a method of automatic embodiment of the uncertainty that overcomes the afore mentioned pitfalls , allowing the combination of evidence with higher degrees of conflict , and avoiding the excessive tendency toward the common possibility of otherwise disjoint hypotheses . This is accomplished by means of a new rule of combination of bodies of evidence that embodies in the numeric results the unknown belief and conflict among the evidence , naturally modeling the epistemic reasoning . 

Extracting clinical information from free - text of pathology and operation notes via Chinese natural language processing
Many of surgical records containing the clinical information are in electronic forms , but a lot of them are still in free - text format in China . In this paper , we have an attempt to extract information with the Nature Language Processing ( NLP ) approach . The procedure of NLP is made up of three steps . First , given 36 free - text of operation notes , a physician manually annotates the information which he is interested in . Second , we extract the features of the annotated information . Third , several logistic regression models are built . Totally , 14 clinical data are extracted . The NLP tool was tested 364 operation notes . The accuracy of extraction is between 67 . 3 %– 96 . 7% . Our results indicate that the performance of the features we used to build the machine learning is good in extracting useful information from free - text Chinese operation notes for liver cancer . In the future , these features would explored on more broader clinical settings . 

Research of Semantic Annotation Technology Based on Domain Ontology
This paper makes a brief research and analysis of current semantic annotation technology from different levels ; and proposed a Chinese semantic annotation method which based on home appliance domain ontology according to the shortcomings of current semantic annotation prototypes . The method combined article level and lexical level annotation using SVM classification and NLP technology , which can improve annotating integrity and provides theoretical basis for intelligent retrieval systems . 

VNLawBERT : A Vietnamese Legal Answer Selection Approach Using BERT Language Model
Recently , with the development of NLP ( Natural Language Processing ) methods and Deep Learning , there are several solutions to the problems in question answering systems that achieve superior results . However , there are not many solutions to question - answering systems in the Vietnamese legal domain . In this research , we propose an answer selection approach by fine - tuning the BERT language model on our Vietnamese legal question - answer pair corpus and achieve an 87 % F1 - Score . We further pre - train the original BERT model on a Vietnamese legal domain - specific corpus and achieve a higher F1 - Score than the original BERT at 90 . 6 % on the same task , which could reveal the potential of a new pre - trained language model in the legal area . 

How Should One Explore the Digital Library of the Future ? 
Motivated by the Foreword of Licklider ' s " Libraries of the Future " ( dedicated to Vannevar Bush ) , this keynote focuses on users , exploration , and future directions of the digital library ( DL ) field , which moves toward procognitive systems . Many different digital library " users , " each a member of a Society , engage in a diversity of Scenarios , often involving some aspect of exploration , usually of the DL content Streams . Services - - e . g . , searching , browsing , recommending , and visualizing - - help those users leverage knowledge Structures and Spatial representations . Following on the final sentence of Licklider ' s book , we " call for a formal base plus an overlay of experience , " leading to a new way to build better DLs . Licklider said we seek " the facts , concepts , principles , and ideas that lie behind the visible and tangible aspects of documents , " to help us acquire and use knowledge . Put simply : " The console of the procognitive system will have two special buttons , a silver one labeled ' Where am I " and a gold one labeled ' What should I do next ? ' " How can we build and use this ? For more than 55 years , researchers have applied artificial intelligence ( AI ) , natural language processing ( NLP ) , representations ( data , information , knowledge ) , question - answering , databases , human - computer interaction , and other techniques described by Licklider , to these challenges . We have a vast range of hardware and software services available , but without a more formal approach , will not enable adaptive self - organization and tailored exploration . The 5S framework can help us build , apply , and improve digital libraries to facilitate exploration , through a formal approach that will simplify such efforts , making them extensible through both human and computing agents . For example , to more easily build DLs , we propose collaboratively building knowledge graphs - - involving both User eXperience designers , subject matter experts , and developers - - that specify connections to services and workflows , enabling DL operation atop a workflow engine . User exploration , additional help by UX designers , recommendations of adaptations of existing workflows , and AI - based optimizations and solutions to new problems , will all expand the knowledge graph to ensure new and more helpful assistance . When this is accomplished , we must teach and learn about this next generation of digital libraries , further developing suitable curriculum and educational modules , that rest upon a solid theoretical foundation , helping spread understanding of key concepts and best practices . 

Where should I comment my code ? A dataset and model for predicting locations that need comments
Programmers should write code comments , but not on every line of code . We have created a machine learning model that suggests locations where a programmer should write a code comment . We trained it on existing commented code to learn locations that are chosen by developers . Once trained , the model can predict locations in new code . Our models achieved precision of 74 % and recall of 13 % in identifying comment - worthy locations . This first success opens the door to future work , both in the new where - to - comment problem and in guiding comment generation . Our code and data is available at http : / / groups . inf . ed . ac . uk / cup / comment - locator / . 

A Convolutional Neural Network with Word - level Attention for Text Classification
Text classification is a classic task in the NLP area which aims to predict the categories for given texts . Many neural network models are applied to this task with the development of the neural network technology . One of the typical neural network structures on the task is the convolutional neural network ( CNN ) . However , the existing traditional CNN model used for classification is not sensitive to the key words in the texts , which causes it cannot capture the important information from the texts . Therefore , in this paper , we propose a new attention - based convolutional neural network ( ACNN ) which is trained to pay more attention to important words to assist in classifying texts . We evaluate our model and the classic CNN model on several public datasets , and the result of our experiment shows that the new proposed ACNN model outperforms the basic CNN model . 

Temporal inferences in medical texts
The objectives of this paper are twofold , whereby the computer program is meant to be a particular implementation of a general natural language [ NL ] processing system [ NLPS ] which could be used for different domains . The first objective is to provide a theory for processing temporal information contained in a well - structured , technical text . The second objective is to argue for a knowledge - based approach to NLP in which the parsing procedure is driven by extra linguistic knowledge . The resulting computer program incorporates enough domain - specific and general knowledge so that the parsing procedure can be driven by the knowledge base of the program , while at the same time employing a descriptively adequate theory of syntactic processing , i . e . , X - bar syntax . My parsing algorithm not only supports the prevalent theories of knowledge - based parsing put forth in AI , but also uses a sound linguistic theory for the necessary syntactic information processing . 

An Effective Approach for Speech Enhancement by Multi - band MMSE Spectral Subtraction
This paper presents an effective single - channel speech enhancement algorithm that combines the merits of multi - band analysis and minimum - mean - squared - error ( MMSE ) spectral subtraction . Conventional spectral subtraction algorithm advocates subtraction of the noise spectrum estimate over the entire speech spectrum . Taking that most real - world noise are colored with a nonuniform spectrum into account , we introduce a scale factor which reflects the difference of noise influence in different bands to reduce the color noise . Evaluation results show that our approach can achieve a more significant noise reduction for both white and color noise as compared to existing spectral subtraction speech enhancement algorithm . 

Unsupervised cross - lingual lexical substitution
Cross - Lingual Lexical Substitution ( CLLS ) is the task that aims at providing for a target word in context , several alternative substitute words in another language . The proposed sets of translations may come from external resources or be extracted from textual data . In this paper , we apply for the first time an unsupervised cross - lingual WSD method to this task . The method exploits the results of a cross - lingual word sense induction method that identifies the senses of words by clustering their translations according to their semantic similarity . We evaluate the impact of using clustering information for CLLS by applying the WSD method to the SemEval - 2010 CLLS data set . Our system performs better on the ' out - of - ten ' measure than the systems that participated in the SemEval task , and is ranked medium on the other measures . We analyze the results of this evaluation and discuss avenues for a better overall integration of unsupervised sense clustering in this setting . 

Clinical Phrase Mining with Language Models
A vast amount of vital clinical data is available within unstructured texts such as discharge summaries and procedure notes in Electronic Medical Records ( EMRs ) . Automatically transforming such unstructured data into structured units is crucial for effective data analysis in the field of clinical informatics . Recognizing phrases that reveal important medical information in a concise and thorough manner is a fundamental step in this process . Existing systems that are built for opendomain texts are designed to detect mostly non - medical phrases , while tools designed specifically for extracting concepts from clinical texts are not scalable to large corpora and often leave out essential context surrounding those detected clinical concepts . We address these issues by proposing a framework , CliniPhrase , which adapts domain - specific deep neural network based language models ( such as ClinicalBERT ) to effectively and efficiently extract high - quality phrases from clinical documents with a limited amount of training data . Experimental results on the MIMIC - III dataset show that our method can outperform the current state - of - the - art techniques by up to 18 % in terms of F 1 measure while being very efficient ( up to 48 times faster ) . 

Automatic scoring of children ' s read - aloud text passages and word lists
Assessment of reading proficiency is typically done by asking subjects to read a text passage silently and then answer questions related to the text . An alternate approach , measuring reading - aloud proficiency , has been shown to correlate well with the aforementioned common method and is used as a paradigm in this paper . We describe a system that is able to automatically score two types of children ' s read speech samples ( text passages and word lists ) , using automatic speech recognition and the target criterion " correctly read words per minute " . Its performance is dependent on the data type ( passages vs . word lists ) as well as on the relative difficulty of passages or words for individual readers . Pearson correlations with human assigned scores are around 0 . 86 for passages and around 0 . 80 for word lists . 

Comparative Assessment of Indoor Positioning Technologies , Techniques , and Algorithms
Indoor positioning systems ( IPS ) are used to locate the position of objects in indoor environments . Due to its many real - world applications , IPS has garnered interest from both academia and industry . Each IPS is made up of three major components : sensor technology , position - finding technique , and operating algorithm . The goal of this paper is to examine and independently compare different types of components with the aim to understand and make useful suggestions for improvement . This paper also presents the past and current trends in IPS and predict the future trends in approaches to the design and implementation of IPS . 

Approximate scalable bounded space sketch for large data NLP
We exploit sketch techniques , especially the Count - Min sketch , a memory , and time efficient framework which approximates the frequency of a word pair in the corpus without explicitly storing the word pair itself . These methods use hashing to deal with massive amounts of streaming text . We apply Count - Min sketch to approximate word pair counts and exhibit their effectiveness on three important NLP tasks . Our experiments demonstrate that on all of the three tasks , we get performance comparable to Exact word pair counts setting and state - of - the - art system . Our method scales to 49 GB of unzipped web data using bounded space of 2 billion counters ( 8 GB memory) . 

Evaluation of Morphological Embeddings for the Russian Language
A number of morphology - based word embedding models were introduced in recent years . However , their evaluation was mostly limited to English , which is known to be a morphologically simple language . In this paper , we explore whether and to what extent incorporating morphology into word embeddings improves performance on downstream NLP tasks , in the case of morphologically rich Russian language . NLP tasks of our choice are POS tagging , Chunking , and NER - - for Russian language , all can be mostly solved using only morphology without understanding the semantics of words . Our experiments show that morphology - based embeddings trained with Skipgram objective do not outperform existing embedding model - - FastText . Moreover , a more complex , but morphology unaware model , BERT , allows to achieve significantly greater performance on the tasks that presumably require understanding of a word ' s morphology . 

Syntax errors just aren ' t natural : improving error reporting with language models
A frustrating aspect of software development is that compiler error messages often fail to locate the actual cause of a syntax error . An errant semicolon or brace can result in many errors reported throughout the file . We seek to find the actual source of these syntax errors by relying on the consistency of software : valid source code is usually repetitive and unsurprising . We exploit this consistency by constructing a simple N - gram language model of lexed source code tokens . We implemented an automatic Java syntax - error locator using the corpus of the project itself and evaluated its performance on mutated source code from several projects . Our tool , trained on the past versions of a project , can effectively augment the syntax error locations produced by the native compiler . Thus we provide a methodology and tool that exploits the naturalness of software source code to detect syntax errors alongside the parser . 

Automatic construction of Web directory using hyperlink and anchor text
This paper proposes a technique for automatically constructing Web directories from several sites . To construct the hierarchical structure of the directories , the technique finds Web pages with a super - sub relation , which are connected by hyperlinks , and replaces the relation with a super - sub hierarchical relation between directories . The technique constructs hierarchical directories by iterating the integration of directories . As a result of an experiment using five Web sites , it was possible to construct hierarchical directories containing Web pages from several sites . 

A first attempt to develop a lexical resource for Dogri language : Dogri WordNet using Hindi WordNet
In this paper an attempt is made to study and analyze the prototype model for developing a WordNet for Dogri language , and describes its specific characteristics and properties to develop WordNet . In terms of morphological and syntactic structure the characteristics of the Dogri language are discussed in this paper . As Dogri is written in the same script as Hindi ( Devanagri ) and there are minor variations in Hindi - Dogri words structure so we can follow the approaches used by Hindi Word Net to great extent . After the completion of Dogri Wordnet more extensive study will be undertaken for the development of advanced NLP tools like Spell checker , Parser , Stemmer , IR system for Dogri language . 

Automatic Infogram Generation for Online Journalism
Infographics is a tool for data visualization . It makes data easy to understand and interpret . An infographic is defined as a visual representation for data like a chart or a diagram . Infographics can help in many fields including education . This is due to the fact that information can be easily memorized and understood if was given in a visual form . Infographics are found everywhere and used in many different fields . Facebook timeline is considered as an infographic . On another hand , online journalism is increasingly gaining popularity . It is also considered as a source of big data that is rapidly expanding . Online newspapers and magazines provide a large population with daily important information . The aim of the work is to use data visualization , infographics and Natural Language Processing ( NLP ) techniques in online journalism . The aim of the work is to automatically visualize the information in an article in the form of infographics . 

Demo : Automatically Retrainable Self Improving Model for the Automated Classification of Software Incidents into Multiple Classes
Developers across most of the organizations face the issue of manually dealing with the classification of the software bug reports . Software bug reports often contain text and other useful information that are common for a particular type of bug . This information can be extracted using the techniques of Natural Language Processing and combined with the manual classification done by the developers until now to create a properly labelled data set for training a supervised learning model for automatically classifying the bug reports into their respective categories . Previous studies have only focused on binary classification of software incident reports as bug and non - bug . Our novel approach achieves an accuracy of 76 . 94 % for a 10 - factor classification problem on the bug repository created by Microsoft Dynamics 365 team . In addition , we propose a novel method for automatically retraining the model and updating it with developer feedback in case of misclassification that will significantly reduce the maintenance cost and effort . 

Transfer Pretrained Sentence Encoder to Sentiment Classification
Deep learning has been demonstrated to be very effective in many difficult tasks of computer vision ( CV ) and natural language processing ( NLP ) . But this usually depends on a large number of available training samples to learn , and powerful computing resources ( like GPU ) . In the tasks of computer vision , multiple deep layers of the model are initialized with weights pre - trained on the large - scale datasets like ImageNet , which improves the training speed of the network but does not reduce the performance of the network . Inspired by this , in this paper , in order to demonstrate that transfer is also effective in natural language processing ( NLP ) , we transfer the encoder parts of two models trained under different tasks of NLP to sentiment classification . In the result , we can find that even if we use a simple classifier , we still get good accuracy . And we propose a persuasive explanation for the phenomenon that why the similarity of the representation vectors obtained by different encoders under the same sentence is not high . 

Decomposable modeling in natural language processing
In this paper , we describe a framework for developing probabilistic classifiers in natural language processing . Our focus is on formulating models that capture the most important interdependencies among features , to avoid overfitting the data while also characterizing the data well . The class of probability models and the associated inference techniques described here were developed in mathematical statistics , and are widely used in artificial intelligence and applied statistics . Our goal is to make this model selection framework accessible to researchers in NLP , and provide pointers to available software and important references . In addition , we describe how the quality of the three determinants of classifier performance ( the features , the form of the model , and the parameter estimates ) can be separately evaluated . We also demonstrate the classification performance of these models in a large - scale experiment involving the disambiguation of 34 words taken from the HECTOR word sense corpus ( Hanks 1996 ) . In 10 - fold cross - validations , the model search procedure performs significantly better than naive Bayes on 6 of the words without being significantly worse on any of them . 

Investigating the Use of Code Analysis and NLP to Promote a Consistent Usage of Identifiers
Meaningless identifiers as well as inconsistent use of identifiers in the source code might hinder code readability and result in increased software maintenance efforts . Over the past years , effort has been devoted to promoting a consistent usage of identifiers across different parts of a system through approaches exploiting static code analysis and Natural Language Processing ( NLP ) . These techniques have been evaluated in small - scale studies , but it is unclear how they compare to each other and how they complement each other . Furthermore , a full - fledged larger empirical evaluation is still missing . , , We aim at bridging this gap . We asked developers of five projects to assess the meaningfulness of the recommendations generated by three techniques , two already existing in the literature ( one exploiting static analysis , one using NLP ) and a novel one we propose . With a total of 922 rename refactorings evaluated , this is , to the best of our knowledge , the largest empirical study conducted to assess and compare rename refactoring tools promoting a consistent use of identifiers . Our study sheds light on the current state - of - the - art in rename refactoring recommenders , and indicates directions for future work . 

Automatic clustering of part - of - speech for vocabulary divided PLSA language model
PLSA is one of the most powerful language models for adaptation to a target speech . The vocabulary divided PLSA language model ( VD - PLSA ) shows higher performance than the conventional PLSA model because it can be adapted to the target topic and the target speaking style individually . However , all of the vocabulary must be manually divided into three categories ( topic , speaking style , and general category ) . In this paper , an automatic method for clustering parts - of - speech ( POS ) is proposed for VD - PLSA . Several corpora with different styles are prepared , and the distance between corpora in terms of POS is calculated . The " general tendency score " and " style tendency score " for each POS are calculated based on the distance between corpora . All of the POS are divided into three categories using two scores and appropriate thresholds . Experimental results showed the proposed method formed appropriate clusters , and VD - PLSA with acquired categories gave the highest performance of all other models . We applied the VD - PLSA into large vocabulary continuous speech recognition system . VD - PLSA improved the recognition accuracy for documents with lower out - of - vocabulary ratio , while other documents were not improved or slightly descended the accuracy . 

Natural Language Processing on Marketplace Product Review Sentiment Analysis
Marketplace is a platform that bridges buyer and seller to transact . In January 2020 , Central Bank reported that total transactions in 14 Indonesian marketplaces reached 23 . 27 trillion rupiahs . At the end of the transaction process , the buyer has the opportunity to give a review of the product that has been purchased . This research aims to sentiment analysis on marketplace product reviews . This research applies Natural Language Processing as a pre - processing of text in the sentiment analysis of marketplace product reviews using a machine learning approach with the Naive Bayes and K - NN algorithms . The test scenario obtained an average Naive Bayes accuracy of 52 . 4 % on the Unigram dataset and an average K - NN accuracy of 79 . 4 % on the Bigram dataset . The utilization of NLP , particularly the word normalizer , can increase the accuracy of 10 % for Naive Bayes and 4 % for K - NN . Sentiment analysis on marketplace product reviews is useful as an opportunity for product improvement for both seller and competitor , in the other side it is also used as a reference for other users before buying a product . 

Efficient Large Scale NLP Feature Engineering with Apache Spark
Feature engineering is a computationally time - consuming process in the end - to - end machine learning pipeline . Large amounts of text data are being generated on many heterogeneous sources and platforms on the internet . The compute resources needed to extract valuable features from these big datasets are increasing significantly . In this research , we evaluate the runtime of the RDD and the Spark - SQL APIs of the Apache Spark framework to extract text features from the corpus of english Wikipedia . As a result , we demonstrate the significant runtime performance of the SparkSQL compared to RDD API . 

Automatic Utterance Segmentation Tool for Speech Corpus
We collect the speech data for investigating an intra - speakers ' speech variability over a short and long time . In general , to reduce the load of speakers , the speech data are collected as one file from collecting start to collecting end . Hence , there are some noises , non - speech sections and mistaken sections in this file . Consequently , we must segment this file into individual utterances and select the useful utterances . This process requires a lot of time and efforts . In this paper , we propose an automatic utterance segmentation tool for dividing the collected speech data . The proposed tool is composed of four processes , which are a voice activity detection , speech recognition , a DP matching , and a correct of speech section . For evaluating the proposed tool , we conduct the evaluation experiments using a female speaker ' s speech data in our corpus . Experimental results show that the proposed method can reduce a filing time by 90 % compared to a manual filing . In This paper , first , we introduced the large speech corpus . This speech corpus contains is the speech data collected by specific speaker over long and short time periods . And , we explained the automatic utterance segmentation tool which we made in the case of corpus build . And inspected the validity . As a result , it was demonstrated that the automatic utterance segmentation tool was high - performance . Furthermore , it was demonstrated that speech corpus build became simple by using the automatic utterance segmentation tool . 

Development of GUI for Text - to - Speech Recognition using Natural Language Processing
Natural language processing is a widely used technique by which systems can understand the instructions for manipulating text or speech . In the present paper , a Text - to - speech synthesizer is developed that converts text into spoken word , by analysing and processing it using Natural Language Processing ( NLP ) and then using Digital Signal Processing ( DSP ) technology to convert this processed text into synthesized speech representation of the text . Here we developed a useful text - to - speech synthesizer in the form of a simple application that converts inputted text into synthesized speech and reads out to the user which can then be saved as an mp3 file . 

Evaluation of NLP systems
No abstract available . 

On the Usage of a Classical Arabic Corpus as a Language Resource : Related Research and Key Challenges
This article presents a literature review of computer - science - related research applied on hadith , a kind of Arabic narration which appeared in the 7th century . We study and compare existent works in several fields of Natural Language Processing ( NLP) , Information Retrieval ( IR) , and Knowledge Extraction ( KE) . Thus , we illicit their main drawbacks and identify some perspectives , which may be considered by the research community . We also study the characteristics of these types of documents , by enumerating the advantages / limits of using hadith as a language resource . Moreover , our study shows that previous studies used different collections of hadiths , thus making it hard to compare their results objectively . Besides , many preprocessing steps are recurrent through these applications , thus wasting a lot of time . Consequently , the key issues for building generic language resources from hadiths are discussed , taking into account the relevance of related literature and the wide community of researchers that are interested in these narrations . The ultimate goal is to structure hadith books for multiple usages , thus building common collections which may be exploited in future applications . 

An object - oriented model for the design of cross - domain dialogue systems
Our approach to speech - based dialogue modelling aims to exploit , in the context of an object - oriented architecture , dialogue processing abilities that are common to many application domains . The coded objects that comprise the system contribute both recognition rules and processing rules ( heuristics) . A Domain Spotter supports the ability to move between domains and between individual skillsets . A Dialogue Model records individual concepts as they occur ; notes the extent to which concepts have been confirmed ; populates request templates ; and fulfils a remembering and reminding role as the system attempts to gather coherent information from an imperfect speech recognition component . Our work will aim to confirm the extent to which the potential strengths of an object - oriented paradigm ( system extensibility , component reuse , etc . ) can be realised in a natural language dialogue system , and the extent to which a functionally rich suite of collaborating and inheriting objects can support purposeful human - computer conversations that are adaptable in structure , and wide ranging in subject matter and skillsets . 

Towards ontology - based natural language processing
Conceptualising a domain has long been recognised as a prerequisite for understanding that domain and processing information about it . Ontologies are explicit specifications of conceptualisations which are now recognised as important components of information systems and information processing . In this paper , we describe a project in which ontologies are part of the reasoning process used for information management and for the presentation of information . Both accessing and presenting information are mediated via natural language and the ontologies are coupled with the lexicon used in the natural language component . 

BoydCut : Bidirectional LSTM - CNN Model for Thai Sentence Segmenter
Sentence is imperative in order to build the top level of NLP ( Natural Language Processing ) applications such as Information retrieval , News Summarization , Knowledge graph . In Thai language , neither each word token is not separated by using just only space like English language , nor the sentence is verified its boundary by using full stop symbols . This paper proposes BoydCut , an NLP framework for identifying sentence boundaries based on Bidirectional LSTM - CNN Model . We develop this framework by utilizing the combination of character , word , and part of speech features . With Bidirectional LSTM , it can learn sequent of word - level in sentences and learn extracted features from character level . With the benefit of the combination Bidirectional LSTM - CNN Model , we do not need feature engineering for feature extraction that can be saved a lot of cost and time in order to build a sentence segmentation model . We also simply design the experiments in a different bucket of features extracted from a deep sequential model . The result empirically shows well perform in internal and external data , as well as help a lot in order to build several useful on the top level of NLP applications . 

Sentiment Analysis for Review Rating Prediction in a Travel Journal
This paper presents sentiment analysis to predict numerical rating of text reviews in a web - based travel journal application . The application allows users to record and provide text reviews on tourist spots visited . Text reviews undergo parts - of - speech ( POS ) tagging , rule - based phrase chunking and dependency parsing to extract opinion phrases in noun - adjective and noun - verb pairs from the original text . Each pair is further classified to one of the four categories : accommodation , food , entertainment and tourist attraction using the noun against a curated bag - of - words ( BOW ) to ensure that only relevant statements are included in the scoring . Word Sense Disambiguation is performed to correctly identify the word sense that matches the meaning of the sentence using WordNet . SentiWordNet , a lexical resource for sentiment analysis , was used to determine polarity score representing the emotional intensity of the review . The system predicted star rating was compared with the actual author rating in Google Maps and with human annotator ratings who are asked to label the text reviews . The predicted rating scored low mean absolute error ( MAE ) between the system and human rating which means that the rating predicted is closer to human interpretation of the text reviews . Overall rating prediction accuracy is 82% . 

The Next Gen Security Operation Center
Due to the evolving Cyber threat landscape , Cyber criminals have found new and ingenious ways of breaching defenses in networks . Due to the sheer destruction these threat actors can cause to an organization , most modern - day organizations have focused their attention towards protecting their critical infrastructure and sensitive information through multiple methods . The main defense against both internal and external threats to an organization has been the implementation of the Security Operations Center ( SOC ) which is responsible for monitoring , analyzing and mitigating incoming threats . At the heart of the Security Operations Center , lies the Security Information and Event Management system ( SIEM ) which is utilized by SOC analysts as the centralized point where all security notifications from various security technologies including firewalls , IPS / IDS and Anti - Virus logs are collected and visualized . The effective operation of SOC in an organization is dependent on how well the SIEM filters log events and generates actual alerts . Here lies the major problem faced by SOC analysts in detecting threats . If proper alert correlation is not accomplished , analysts would have to deal with too much alert noise due to a high false positive count . This would ultimately cause analysts to miss critical security incidents , thus causing severe implications to the organization ' s security . The performance of a SIEM can be enhanced through adding various functionalities such as Threat Hunting , Threat Intelligence and malware identification and prevention in order to reduce false positive alarms , threat framework and machine learning which would increase the accuracy and efficiency of the overall Security Operations process of an organization . Even though many products which provide these additional functionalities exist in the current market , they can be too expensive for smaller scale organizations to handle . Our aim is to make security operations deliverable to any organization regardless of the size and scale without any financial implications and enhance its functionalities with the aid of Advanced Machine Learning Techniques . 

Curation Technologies for Cultural Heritage Archives : Analysing and transforming a heterogeneous data set into an interactive curation workbench
We present a platform that enables the semantic analysis , enrichment , visualisation and presentation of a document collection in a way that enables human users to intuitively interact and explore the collection , in short , a curation platform or workbench . The data set used is the result of a research project , carried out by scholars from South Korea , in which official German government documents on the German re - unification were collected , intellectually curated , analysed , interpreted and published in multiple volumes . The documents we worked with are mostly in German , a small subset , mostly summaries , is in Korean . This paper describes the original research project that generated the data set and focuses upon a description of the platform and Natural Language Processing ( NLP ) pipeline adapted and extended for this project ( e . g . , OCR was added ) . Our key objective is to develop an interactive curation workbench that enables users to interact with the data set in several different ways that go beyond the current version of the published document collection as a set of PDF documents that are available online . The paper concludes with suggestions regarding the improvement of the platform and future work . 

Exploiting external knowledge sources to improve kernel - based Word Sense Disambiguation
This paper proposes a novel approach to improve the kernel - based word sense disambiguation ( WSD ) . We first explain why linear kernels are more suitable to WSD and many other natural language processing problems than translation - invariant kernels . Based on the linear kernel , two external knowledge sources are integrated . One comprises a set of linguistic rules to find the crucial features . For the other , a distributional similarity thesaurus is used to alleviate data sparseness by generalizing crucial features when they do not match the word - form exactly . The experiments show that we have outperformed the state - of - the - art system on the benchmark data from English lexical sample task of SemEval - 2007 and the improvement is statistically significant . 

Rule - based acquisition and maintenance of lexical and semantic knowledge
The lexicons for Knowledge - Based Machine Translation systems require knowledge intensive morphological , syntactic and semantic information . This information is often used in different ways and usually formatted for a specific NLP system . This tends to make both the acquisition and maintenance of lexical databases cumbersome , inefficient and error - prone . In order to solve these problems , we have developed a program called cooL which automates the acquisition and maintenance processes and allows us to standardize and centralize the databases . This system is currently being used in the ESTRATO machine translation project at the Center for Machine Translation . 

Generic NLP technologies : language , knowledge and information extraction
We have witnessed significant progress in NLP applications such as information extraction ( IE ) , summarization , machine translation , cross - lingual information retrieval ( CLIR ) , etc . The progress will be accelerated by advances in speech technology , which not only enables us to interact with systems via speech but also to store and retrieve texts input via speech . 

360 Degree Profiling and Social Linkage Analysis of Persons of Interest
Scourge of Terror Incidents is increasing because of advent of newer methodologies , weapons and techniques . National Security Agencies control these insurgent terror incidents by maintaining records of Persons of Interests for their tracking . As the person ' s data can come from multiple sources and in various formats , therefore analysing it with traditional methods becomes major challenge . Also viewing linkages between different entities becomes cumbersome . This paper presents a Machine learning and Computer Vision driven solution , which will help Security Agencies to combat terrorism . This solution will not only build 360 degree Person ' s profile but also show his linkages with various Incidents of Interest and Entities of Interest . The solution proposed is efficient in terms of performance and speed because it leverages the power of distributed computing and multithreading . So , we are building a Big Data & AI based distributed high performance system which will characterize the 4 V ' s of Big Data : Volume , Velocity , Variety and Veracity . 

Twitter As a Multilingual Source of Comparable Corpora
This article describes a new method to build comparable corpora from Twitter . Our strategy relies on the fact that Twitter is one of the most popular online social microblog allowing large audiences to express their thoughts and reactions about specific events or breaking news in various languages . Given two languages and a particular topic , We propose the exploitation of tweets in the two selected languages whose content is focused on the selected topic from the microblog Twitter in order to construct a comparable corpus . 

Developed Credit Card Fraud Detection Alert Systems via Notification of LINE Application
As nowadays , prevention of fraud is another important issue , researcher have initiated the idea of applying suspicious frauds in credit cards to line application . The objectives of this research are : 1 ) for developing the suspected credit card fraud via API LINE Notify . 2 ) Measure the accuracy of the developed system in the notification to prevent suspicious fraud credit card . The measurement method is comprised of five steps which are : 1 ) Analysis of work systems is a study and analysis of problems to determine needs . 2 ) System design is the process of designing research tools . 3 ) Developing a system is the process of developing research tools . 4 ) A test of the tools is executed 5 ) Summary of results , discussion results , and suggestions . The measurement results of efficiency , accuracy , and completeness of the data were in a very good level , equal to 86 . 67% . The results of the measurement of efficiency to the conditions set are very good , equal to 80 . 00 % . The results of the measurement on time very good , equal to 86 . 67% . In conclusion , the developed system accomplishes all research goals . 

A Study on Text Classification for Applications in Special Education
Text classification , or text categorization , is the process that categorizes text into organized groups . Text classifiers can parse text and categorize it based on its content . The text analysis is done using Natural Language Processing ( NLP) . In recent years we have much more understanding to learning disabilities and how to diagnose and categorize their impact . In this paper we studied learning difficulties as a classification problem and use the NLP tool to solve it . In addition , the results of classifiers such as Naive Bayes , SVM and CNN are compared . The precision results show that the Na ï ve Bayes classifier had the lowest accuracy of 71 . 72 % , compared to the other two , where the CNN classifier has a 82 . 79 % accuracy and the SVM has the best accuracy in percentage of 93 . 03 % . 

Nonlocal context modeling and adaptive prediction for lossless image coding
Properly designed context models can increase the compression gain . In this paper , we propose a new lossless image coding scheme with two proposed algorithms : nonlocal context modeling and adaptive prediction ( NCMAP ) . Since structural self - similarity often exists in natural images , we use the probability to measure the similarity between the powers of prediction errors for the pixels to be coded . Furthermore , the spatial distance and the intensity range are also considered for context generation . Moreover , a prediction scheme that adaptively combines the weighted edge - directed prediction ( WEDP ) and the nonlocal predictor ( NLP ) is also proposed . With the proposed context generating and prediction strategies , better compression performances can be achieved . Simulations show that the proposed scheme outperforms existing methods for lossless image compression . 

Sentiment Analysis of Tamil - English Code - Switched Text on Social Media Using Sub - Word Level LSTM
Social media are the ultimate platforms to express the opinion and to facilitate the creation and sharing of information , ideas , career interests and other forms of expression via virtual communities and networks . Analysing the sentiment features in these ideas in the public posts of social media users will lead to building more accurate behavioural patterns . Importance of these behavioural patterns with respect to the marketing and business perspective has been focused here . When considering the traditional Facebook marketing platform , efficiency and effectiveness of the marketing are very low since the advertisers do not happen to have a proper understanding of the customers that they should address . Thus , to overcome this issue , a system is proposed to identify the behavioural patterns of Facebook users by analysing their social media contents such as posts , comments , interactions , and also reviews and critics on products to enhance the effectiveness of the Facebook marketing . This system mainly focuses on Facebook users in Sri Lanka . Natural language processing is used to process text - based posts ( uploaded and shared ) and comments of users in order to build a behavioural profile for the users . This system process text data which is composed by using both English and Tamil languages , in code - switching language pattern . 

Learning Knowledge Embeddings by Combining Limit - based Scoring Loss
In knowledge graph embedding models , the margin - based ranking loss as the common loss function is usually used to encourage discrimination between golden triplets and incorrect triplets , which has proved effective in many translation - based models for knowledge graph embedding . However , we find that the loss function cannot ensure the fact that the scoring of correct triplets must be low enough to fulfill the translation . In this paper , we present a limit - based scoring loss to provide lower scoring of a golden triplet , and then to extend two basic translation models TransE and TransH , separately to TransE - RS and TransH - RS by combining limit - based scoring loss with margin - based ranking loss . Both the presented models have low complexities of parameters benefiting for application on large scale graphs . In experiments , we evaluate our models on two typical tasks including triplet classification and link prediction , and also analyze the scoring distributions of positive and negative triplets by different models . Experimental results show that the introduced limit - based scoring loss is effective to improve the capacities of knowledge graph embedding . 

Automatic construction of biomedical abbreviations dictionary from text
The size and growth rate of biomedical abbreviation are increasing very fast , automatic construction of biomedical abbreviations dictionary from text helps to understand biomedical literature , and to update existing databases , ontologies , and dictionaries . This paper proposes a new method for automatic construction of biomedical abbreviations dictionary from text by combining string matching algorithm and searching algorithm . The string matching algorithm extracts abbreviations and their longforms . The searching algorithm corrects the false longforms produced by the string matching algorithm . The searching algorithm is based on the idea that readers often lookup relative articles to judge the longform of an abbreviation is correct or not . Our experiments show that the algorithm has high precision ( 97 . 5 %) and recall ( 82 . 2 %) . And because tagged corpus is not necessary , the method has high efficiency . 

Twitter part - of - speech tagging using pre - classification Hidden Markov model
Hidden Markov models ( HMM ) have been widely used in natural language processing ( NLP ) , especially in syntactic level applications , which appears naturally as short - range - dependent sequence recognition problems . But the structure of HMM limits the usage of global knowledge including the sentiment analysis of the text , which has become an increasingly popular research topic in NLP now . In this paper , we propose a novel treatment of HMM model to use the result of sentimental subjectivity analysis in syntactic level task , i . e . part - of - speech ( POS ) tagging . The subjectivity information is introduced as a pre - classification procedure into the interval - type HMM . The subjectivity degree of the testing sentence is used as a combination factor to choose an appropriate value from the interval . Experiments results on public tagging data sets shows that the proposed approach enhanced the performance of POS tagging . 

Chinese word segmentation without using lexicon and hand - crafted training data
Chinese word segmentation is the first step in any Chinese NLP system . This paper presents a new algorithm for segmenting Chinese texts without making use of any lexicon and hand - crafted linguistic resource . The statistical data required by the algorithm , that is , mutual information and the difference of t - score between characters , is derived automatically from raw Chinese corpora . The preliminary experiment shows that the segmentation accuracy of our algorithm is acceptable . We hope the gaining of this approach will be beneficial to improving the performance ( especially in ability to cope with unknown words and ability to adapt to various domains ) of the existing segmenters , though the algorithm itself can also be utilized as a stand - alone segmenter in some NLP applications . 

Multi - Document Summarization as Applied in Information Retrieval
In this paper we presented the use of multi - document summarization as postprocessing step in information retrieval ( IR ) . We examined the differences between requirements for general multi - document summarization and requirements when it is applied for IR , and highlighted the requirements for clustering and context information extraction , which is much helpful to the users for browsing and searching relative results . To generate this type of summary , we first cluster the retrieved documents by their topics using a repeated bisection algorithm , and extract the centroid words for each cluster . The final summary is generated on the base of the query words and the cluster centroids , containing query - centered information as well as context information . 

Estimating the number of remaining links in traceability recovery ( journal - first abstract ) 
Although very important in software engineering , establishing traceability links between software artifacts is extremely tedious , error - prone , and it requires significant effort . Even when approaches for automated traceability recovery exist , these provide the requirements analyst with a , usually very long , ranked list of candidate links that needs to be manually inspected . In this paper we introduce an approach called Estimation of the Number of Remaining Links ( ENRL ) which aims at estimating , via Machine Learning ( ML ) classifiers , the number of remaining positive links in a ranked list of candidate traceability links produced by a Natural Language Processing techniques - based recovery approach . We have evaluated the accuracy of the ENRL approach by considering several ML classifiers and NLP techniques on three datasets from industry and academia , and concerning traceability links among different kinds of software artifacts including requirements , use cases , design documents , source code , and test cases . Results from our study indicate that : ( i ) specific estimation models are able to provide accurate estimates of the number of remaining positive links ; ( ii ) the estimation accuracy depends on the choice of the NLP technique , and ( iii ) univariate estimation models outperform multivariate ones . 

Comparison of Machine Learning Algorithm ' s Performance Based on Decision making in Autonomous Car
This paper presents the performance comparison of popular supervised learning algorithms : SVM , MLP , CNN , DT , and RF . These algorithms are used for road images recognition . All images are collected by our model car . They are labeled with four different classes : left , right , forward and stop . We use 90 % of them for training and use 10 % for testing each model . The result shows that , CNN has the best accuracy about 83 . 45 % . 

Identifying entity aspects in microblog posts
Online reputation management is about monitoring and handling the public image of entities ( such as companies ) on the Web . An important task in this area is identifying " aspects " of the entity of interest ( such as products , services , competitors , key people , etc . ) given a stream of microblog posts referring to the entity . In this paper we compare different IR techniques and opinion target identification methods for automatically identifying aspects and find that ( i ) simple statistical methods such as TF . IDF are a strong baseline for the task , significantly outperforming opinion - oriented methods , and ( ii ) only considering terms tagged as nouns improves the results for all the methods analyzed . 

The fairness of ranking procedure in pair - wise preference learning
In pair - wise preference learning , a crucial point is how to decode the predictions of the pair - wise preference to a final preference order - a ranking procedure . Simple voting , iterated choice , and Slater - optimal ranking are usual techniques , but their ranking results are usually very different from each other . Hitherto , experimentation is the main method of estimating the ranking approaches , and the formal estimation is still an open question . The main contribution of this paper is the definition of a framework to import the fairness theory of preference aggregation to estimate the ranking procedure , where every pair - wise preference learner is seen as an agent and the ranking procedure is seen as a special case of multiple agents ' preferences aggregation . In addition , by transformed into a special aggregation case of RANK voting rule , if there are at least three labels , then simple voting and iterated choice are proved to be not fair for their dependence to irrelevant alternatives . 

Semi natural language algorithm to programming language interpreter
The conversion of an algorithm to code is still at an early stage . Effective conversion of algorithms mentioned in natural English language to code will enable programmers to focus on logic building and free them of syntactical worries , further it will also aid the visually impaired programmers . Although beneficial , implementation of such a converter encounters numerous challenges like limitations imposed due to semantics of the English language , case frames , etc . In this paper we have introduced an interpreter that is capable of converting algorithms in English to C code whose flexibility of interpretation has been enhanced by using synonyms and by the introduction of a personalised training model whose concept has been outlined below . We have defined the conceptual model along with a user scenario which demonstrates the functioning of our model . 

On Satisfying the Android OS Community : User Feedback Still Central to Developers ' Portfolios
End - users play an integral role in identifying requirements , validating software features ' usefulness , locating defects , and in software product evolution in general . Their role in these activities is especially prominent in online application distribution platforms ( OADPs ) , where software is developed for many potential users , and for which the traditional processes of requirements gathering and negotiation with a single group of end - users do not apply . With such vast access to end - users , however , comes the challenge of how to prioritize competing requirements in order to satisfy previously unknown user groups , especially with early releases of a product . One highly successful product that has managed to overcome this challenge is the Android Operating System ( OS ) . While the requirements of early versions of the Android OS likely benefited from market research , new features in subsequent releases appear to have benefitted extensively from user reviews . Thus , lessons learned about how Android developers have managed to satisfy the user community over time could usefully inform other software products . We have used data mining and natural language processing ( NLP ) techniques to investigate the issues that were logged by the Android community , and how Google ' s remedial efforts correlated with users ' requests . We found very strong alignment between end - users ' top feature requests and Android developers ' responses , particularly for the more recent Android releases . Our findings suggest that effort spent responding to end - users ' loudest calls may be integral to software systems ' survival , and a product ' s overall success . 

Synchronization Control for Microgrid Seamless Reconnection
In the three - phase distribution network , the microgrid is widely used for increasing reliability and stability of the power grid . The microgrid is a small power grid that can connect to the utility grid via circuit breaker or switch at the point of common coupling ( PCC ) . On standalone mode , the microgrid will supply power to the local loads by itself , and change to provide or consume power from the utility grid on grid - connected mode . However , during transfer from standalone to grid - connected mode , if the microgrid is reconnected without synchronization , the out - of - phase reclosing might be occur . It is a major cause that can affect to the power system due to large inrush current or overvoltage during transition mode . Thus , in order to dispose of this problem , the frequency , phase , and amplitude voltage between two grids need to synchronize before reconnection . In this paper , a synchronization control has been proposed for microgrid seamless reconnection . The additional Distributed Generator ( DG ) is installed at PCC and assigned to be Dispatch Unit ( DU ) . DU is controlled by Droop Control for generating the electric power to adjust the frequency , phase , and amplitude voltage of the microgrid at PCC during the synchronization process in order to reconnect to the utility grid . The proposed method is performed by eliminating the difference of frequency , phase , and amplitude voltage during transition modes . The simulation has been performed in Matlab / Simulink . 

Improving dependency analysis by syntactic parser combination
The goal of this article is to present our work about a combination of several syntactic parsers to produce a more robust parser . We have built a platform which allows us to compare syntactic parsers for a given language by splitting their results in elementary pieces , normalizing them , and comparing them with reference results . The same platform is used to combine several parsers to produce a dependency parser that has larger coverage and is more robust than its component parsers . In the future , it should be possible to " compile " the knowledge extracted from several analyzers into an autonomous dependency parser . 

A survey on the role of negation in sentiment analysis
This paper presents a survey on the role of negation in sentiment analysis . Negation is a very common linguistic construction that affects polarity and , therefore , needs to be taken into consideration in sentiment analysis . We will present various computational approaches modeling negation in sentiment analysis . We will , in particular , focus on aspects , such as level of representation used for sentiment analysis , negation word detection and scope of negation . We will also discuss limits and challenges of negation modeling on that task . 

Named Entity Recognition Through Bidirectional LSTM In Natural Language Texts Obtained Through Audio Interfaces
This article describes artificial neural network use case for named entity recognition for text obtained through audio interfaces . The efficiency of the training method based on modified data set was shown . 

English - Japanese example - based machine translation using abstract linguistic representations
This presentation describes an example - based English - Japanese machine translation system in which an abstract linguistic representation layer is used to extract and store bilingual translation knowledge , transfer patterns between languages , and generate output strings . Abstraction permits structural neutralizations that facilitate learning of translation examples across languages with radically different surface structure characteristics , and allows MT development to proceed within a largely language - independent NLP architecture . Comparative evaluation indicates that after training in a domain the English - Japanese system is statistically indistinguishable from a non - customized commercially available MT system in the same domain . 

Improving Hidden Markov Model for very low resource languages : An analysis for Assamese parts of speech tagging
Recent advances in research in the field of natural language processing have facilitated many new applications . Natural Language Processing basically involves automating the synthesis and generation of natural languages . However , most of the developments have happened only for a few dominant languages spoken widely like English , Chinese , etc . Some of the developments are also observed for dominant Indian languages like " Hindi " , " Bengali " Tamil , etc . However , research into most of the other languages spoken in the world is at a very primitive stage . This paper aims to highlight issues related to one such language for which many resources are not available in a computable format . We report our initial work on Assamese with limited resources . Assamese is the official language of Assam , India and it is also the most spoken Indian language in North East India . We report results of a few experiments carried with a very low amount of training , as resources available for Assamese are not adequate . Part of speech tagging is fundamental to any NLP application . Though some amount of research has been conducted for automatic parts of speech tagging , they have used large amount of data that are not available in public domain . We have modified the viterbi algorithm used for Hiden Markov Model and applied it for automatic parts of speech tagging for Assamese . We have also applied certain language characteristics that can be used for improving accuracy . The key is to use a training dataset as small as possible so that the dependency on training data is limited . We conclude the paper with a brief discussion on the scope of Assamese POS tagging in the future . 

An Evaluation Set for Tibetan Sentences Similarity Computing
Sentence is not only the basic unit of natural language , but also the research object of Nature Language Process . Sentence similarity computing is the basic of text similarity computing , so sentence similarity evaluation set is an essential data set for similarity technology research . Only by establishing an appropriate evaluation set can we objectively evaluate the advantages and disadvantages of similarity computing methods . In order to objectively evaluate the performance of Tibetan sentence similarity , this paper designs the construction scheme of Tibetan sentence similarity evaluation set , and builds the evaluation set TSS _ 320 for evaluating Tibetan sentence similarity , based on the analysis of the construction method of English and Chinese sentence similarity evaluation set , also combined with the characteristics of Tibetan sentences . The validity of the evaluation set is verified by statistical methods . 

WHAT : an XSLT - based infrastructure for the integration of natural language processing components
The idea of the Whiteboard project is to integrate deep and shallow natural language processing components in order to benefit from their synergy . The project came up with the first fully integrated hybrid system consisting of a fast HPSG parser that utilizes tokenization , PoS , morphology , lexical , named entity , phrase chunk and ( for German ) topological sentence field analyses from shallow components . This integration increases robustness , directs the search space and hence reduces processing time of the deep parser . In this paper , we focus on one of the central integration facilities , the XSLT - based Whiteboard Annotation Transformer ( WHAT ) , report on the benefits of XSLT - based NLP component integration , and present examples of XSL transformation of shallow and deep annotations used in the integrated architecture . The infrastructure is open , portable and well suited for , but not restricted to the development of hybrid NLP architectures as well as NLP applications . 

Verb - particle constructions and lexical resources
In this paper we investigate the phenomenon of verb - particle constructions , discussing their characteristics and their availability for use with NLP systems . We concentrate in particular on the coverage provided by some electronic resources . Given the constantly growing number of verb - particle combinations , possible ways of extending the coverage of the available resources are investigated , taking into account regular patterns found in some productive combinations of verbs and particles . We discuss , in particular , the use of Levin ' s ( 1993 ) classes of verbs as a means to obtain productive verb - particle constructions , and discuss the issues involved in adopting such an approach . 

Cascading XSL filters for content selection in multilingual document generation
Content selection is a key factor of any successful document generation system . This paper shows how a content selection algorithm has been implemented using an efficient combination of XML / XSL technology and the framework of RST for discourse modeling . The system generates multilingual documents adapted to user profiles in a learning environment for the web . This CourseViewGenerator applies simplified RST schemes to the elaboration of a master document in XML from which content segments are chosen to suit the user ' s needs . The personalisation of the document is achieved through the application of a sequence of filtering levels of text selection based on the user aspects given as input . These cascading filters are implemented in XSL . 

Illuminating trouble tickets with sublanguage theory
A study was conducted to explore the potential of Natural Language Processing ( NLP ) - based knowledge discovery approaches for the task of representing and exploiting the vital information contained in field service ( trouble ) tickets for a large utility provider . Analysis of a subset of tickets , guided by sublanguage theory , identified linguistic patterns , which were translated into rule - based algorithms for automatic identification of tickets ' discourse structure . The subsequent data mining experiments showed promising results , suggesting that sublanguage is an effective framework for the task of discovering the historical and predictive value of trouble ticket data . 

Ontology Based Domain Knowledge Construction
This paper presents a representation framework for domain knowledge based on the analysis of Chinese festival by ontology analysis approach ; it also discusses the organization method for describing domain data based on ontology . At the same time , the paper summarizes the ontology study domain . It is conceptualized using method of domain knowledge by the framework , and that is to say , to construct the concept , the taxonomy of concept and the concept properties for the domain knowledge is based on ontology . 

Incremental knowledge acquisition for extracting temporal relations
We present KAFTIE - an incremental knowledge acquisition framework which utilizes expert knowledge to build high quality knowledge base annotators . Using KAFTIE , a knowledge base was built based on a small data set that outperforms machine learning algorithms trained on a much bigger data set for the task of recognizing temporal relations . In particular , this can be incorporated to bootstrap the process of labeling data for domains where annotated data is not available . 

A survey of arabic named entity recognition and classification
As more and more Arabic textual information becomes available through the Web in homes and businesses , via Internet and Intranet services , there is an urgent need for technologies and tools to process the relevant information . Named Entity Recognition NER is an Information Extraction task that has become an integral part of many other Natural Language Processing NLP tasks , such as Machine Translation and Information Retrieval . Arabic NER has begun to receive attention in recent years . The characteristics and peculiarities of Arabic , a member of the Semitic languages family , make dealing with NER a challenge . The performance of an Arabic NER component affects the overall performance of the NLP system in a positive manner . This article attempts to describe and detail the recent increase in interest and progress made in Arabic NER research . The importance of the NER task is demonstrated , the main characteristics of the Arabic language are highlighted , and the aspects of standardization in annotating named entities are illustrated . Moreover , the different Arabic linguistic resources are presented and the approaches used in Arabic NER field are explained . The features of common tools used in Arabic NER are described , and standard evaluation metrics are illustrated . In addition , a review of the state of the art of Arabic NER research is discussed . Finally , we present our conclusions . Throughout the presentation , illustrative examples are used for clarification . 

Electrically controlled dynamics of energy transfer in pure nematic liquid crystals
The photorefractive effect in homeotropically aligned layer of pure nematic liquid crystal 5CB is investigated by the dynamic holographic technique . It is found the voltage controlled energy transfer between two interacting waves . The efficiency of the energy transfer , its direction and time characteristics are investigated . A surface charge - dependent photorefractive effect as well as a bulk effect determined by the molecular interaction are discussed as two complementary mechanisms of the dynamical grating recording . 

Efficient author community generation on Nlp based relevance feature detection
Many researchers provided different feature discovery techniques , but that shows high time complexity according to the LDA process . It does not provided any user preference for document search history , to avoid this problem a NLP techniques is used . NLP provide maximum pattern for search input so it is generate maximum pattern output . Ranking of the each document is according to the new patterns that generated from NLP . Community generation of documents will be done based on the cluster information ; It will help document users to find other documents in the domain . Cluster the whole documents using the technique and applying the advanced ranking . So that the advanced ranking of the relevant feature produce the author community generation process . so that authors can communicate each other . 

Incorporating PV inverter control schemes for planning active distribution networks
Summary form only given . The distribution network planning under active network management ( ANM ) schemes is becoming of interest due to substantial benefits in facilitating the increasing integration of renewable energy sources . This paper presents various potential ANM schemes based on the photovoltaic inverter control ( PVIC ) considering enhanced utilization of the inverter reactive power capability . Depending on the active power generation of PV arrays , inverter size and desired reactive power settings , several PVIC schemes are proposed . The PVIC schemes are incorporated in the optimal power flow ( OPF ) and formulated as a nonlinear programming ( NLP ) problem . In this study , the PVIC schemes are applied to maximize the total wind - distributed generation ( DG ) penetration on a typical U . K . distribution system . Various case studies are presented and compared to evaluate the performance . The results show that the proposed schemes can significantly increase the wind penetration levels by 45 . 4 % and up to 92 . 3 % . 

Mining impact of protein modifications on protein - protein interactions from literature
Identification of protein - protein interactions ( PPIs ) and downstream functional events is important to biologists because these are key to building protein networks and biological pathways . Of high relevance are the PPIs involving proteins with some post - translational modification ( PTM ) , since PTMs constitute one way of regulating protein function . However , this type of PPIs is not yet well represented in public databases ( such as UniProtKB and IntAct ) , which would link the PPI information to the unmodified forms of proteins . Hence , literature is still the primary source for PPI information linked to modified forms of proteins . In this work , we propose to automate the process that mines and combines PPI information and PTM information from the biomedical literature via a novel , rule - based system . Further , the results are ranked based on the relations between PPI and PTM information . Currently , we limit the scope of our system to phosphorylation , a common PTM . 

A Multi - layer Bidirectional Transformer Encoder for Pre - trained Word Embedding : A Survey of BERT
Language modeling is the task of assigning a probability distribution over sequences of words that matches the distribution of a language . A language model is required to represent the text to a form understandable from the machine point of view . A language model is capable to predict the probability of a word occurring in the context - related text . Although it sounds formidable , in the existing research , most of the language models are based on unidirectional training . In this paper , we have investigated a bi - directional training model - BERT ( Bidirectional Encoder Representations from Transformers ) . BERT builds on top of the bidirectional idea as compared to other word embedding models ( like Elmo ) . It practices the comparatively new transformer encoder - based architecture to compute word embedding . In this paper , it has been described that how this model is to be producing or achieving state - of - the - art results on various NLP tasks . BERT has the capability to train the model in bi - directional over a large corpus . All the existing methods are based on unidirectional training ( either the left or the right ) . This bi - directionality of the language model helps to obtain better results in the context - related classification tasks in which the word ( s ) was used as input vectors . Additionally , BERT is outlined to do multi - task learning using context - related datasets . It can perform different NLP tasks simultaneously . This survey focuses on the detailed representation of the BERT - based technique for word embedding , its architecture , and the importance of this model for pre - training purposes using a large corpus . 

Scraping Unstructured Data to Explore the Relationship between Rainfall Anomalies and Vector - Borne Disease Outbreaks
According to the World Health Organization ( WHO ) , vector - borne diseases such as malaria and dengue account for 17 % of all infectious disease cases and lead to more than 700 , 000 deaths per year . Tracking and predicting the spread of vector - borne diseases is a vital task that could save hundreds of thousands of lives annually . Oftentimes , the first reports of vector - borne disease outbreaks occur through emails and online reporting systems long before they are officially documented . Tracking and predicting the emergence and spread of vector - borne disease outbreaks requires extracting data from these unstructured sources in combination with historical weather and climate data to understand the underlying background triggers and disease dynamics . In this work , we develop a data extraction pipeline for the online outbreak reporting website ProMED - mail that utilizes a web scraper , transformer neural network summarizer , and named entity recognizer to obtain a dataset of malaria , dengue , zika , and chikungunya outbreaks over the last 30 years . This scraped dataset was further analyzed in association with global rainfall anomalies derived from NASA ’ s Integrated Multi - satellitE Retrievals for GPM [ Global Precipitation Mission ] ( IMERG ) dataset . This preliminary analysis was to understand the effect of global rainfall patterns on the spread of vector - borne diseases . Analysis of the ProMED - mail and GPM data shows that vector - borne disease outbreaks are clustered towards the tropics and outbreaks are often amplified during the rainy seasons . Our scraped dataset can be a valuable tool in creating comprehensive georeferenced disease records for modeling and predicting future outbreaks . 

Identification of Unsuitable Content for Children in Video Gaming Forums
Online forums for videogames fans may represent a threat of exposure to topics and language non - suitable for their age . The following case study shows how the analysis of topics threads based on interactions of micro messages extracted from videogames forums allowed us to identify the challenges associated to the processing of this particular type of content . An original technique for segmentation of groups of interactions was evaluated with the objective of applying algorhythms for topics detection . Despite the high volume of micro messages that were highly disperse , overloaded with emojis and which contained slang associated with every community and videogame ; it was possible to detect inappropriate topics according to the age associated to the category of the videogame . 

Data extraction from Web forums based on similarity of page layout
Web forums contain a wealth of information resources . Forum data can be widely used in areas such as Internet community mining , information retrieval and public opinion analysis and so on . This paper solves the problems of what should be extracted and how to extract from the Web forums . Aimed at the limitation of current methods to extract data from Web forums , an automated method is proposed to extract metadata from Web forum pages . The method processes in two steps . We firstly recognizes the topic - block by making full use of the special layout of the Web forum pages , then extract metadata from the topic - block by making use of statistical regularity of the metadata , the whole process done without manual work . Experimental results show that this method performs well both in adjustability and accuracy . 

Multi - dimensional annotation and alignment in an English - German translation corpus
This paper presents the compilation of the CroCo Corpus , an English - German translation corpus . Corpus design , annotation and alignment are described in detail . In order to guarantee the searchability and exchangeability of the corpus , XML stand - off mark - up is used as representation format for the multi - layer annotation . On this basis it is shown how the corpus can be queried using XQuery . Furthermore , the generalisation of results in terms of linguistic and translational research questions is briefly discussed . 

Adversarial Attacks on Deep Models for Financial Transaction Records
Machine learning models using transaction records as inputs are popular among financial institutions . The most efficient models use deep - learning architectures similar to those in the NLP community , posing a challenge due to their tremendous number of parameters and limited robustness . In particular , deep - learning models are vulnerable to adversarial attacks : a little change in the input harms the model ' s output . In this work , we examine adversarial attacks on transaction records data and defenses from these attacks . The transaction records data have a different structure than the canonical NLP or time - series data , as neighboring records are less connected than words in sentences , and each record consists of both discrete merchant code and continuous transaction amount . We consider a black - box attack scenario , where the attack doesn ' t know the true decision model and pay special attention to adding transaction tokens to the end of a sequence . These limitations provide a more realistic scenario , previously unexplored in the NLP world . The proposed adversarial attacks and the respective defenses demonstrate remarkable performance using relevant datasets from the financial industry . Our results show that a couple of generated transactions are sufficient to fool a deep - learning model . Further , we improve model robustness via adversarial training or separate adversarial examples detection . This work shows that embedding protection from adversarial attacks improves model robustness , allowing a wider adoption of deep models for transaction records in banking and finance . 

A novel Chinese - English on translation method using mix - language web pages
In this paper , we propose a novel Chinese - English organization name translation method with the assistance of mix - language web resources . Firstly , all the implicit out - of - vocabulary terms in the input Chinese organization name are recognized by a CRFs model . Then the input Chinese organization name is translated without considering these recognized out - of - vocabulary terms . Secondly , we construct some efficient queries to find the mix - language web pages that contain both the original input organization name and its correct translation . At last , a similarity matching and limited expansion based translation identification approach is proposed to identify the correct translation from the returned web pages . Experimental results show that our method is effective for Chinese organization name translation and can improve performance of Chinese organization name translation significantly . 

Defending against SQL Injection Attacks in Web Applications using Machine Learning and Natural Language Processing
Today , most organizations use web applications for the delivery of services over the Internet . The risks to web applications have increased as their use has risen . SQL Injection Attack is a commonly exploited vulnerability used for stealing credentials , destroying and compromising data , and bypassing authentication and authorization controls of a web application . Traditional methods of detecting SQL injection attacks include software and hardware - based Web Application Firewalls , programmatic defense techniques like input filtering , input validation , using parameterized queries etc . and static and dynamic analysis are not sufficient for detection and prevention of SQLIA in web applications . In this paper , we present an approach to detecting SQLIA using NLP and Machine Learning . Experimental results show that the approach can detect SQLIA with precision , recall and an f1 - score of 99 . 9 . 

A Bangla Semantic Parser using Context - Free - Grammar
This research work describes a computer system for understanding the parsing of Bangla sentences . It draws on recent developments in Natural Language Processing ( NLP ) research to look at the past , present , and future of NLP technology in a new light . The research work of Bangla Language Processing ( BLP ) was started in late1980s in Bangladesh and it already produced some substantive results . NLP is a theory - motivated range of computational techniques for the automatic analysis and representation of human language . Here the researchers present a semantic parser for analyzing Bangla language semantics applying Bangla Lexicon . Semantic parsing augments the stratum of comprehension of NLP than the syntactic parsing , that is primarily indulges in untangling the syntactic ambiguities of the words . In this work , the researchers have tried to develop a Context - Free - Grammar ( CFG ) for semantic parser . It is the grammar that consist rules with a single symbol on the left - hand side of the rewrite rules . The researchers have tried to make a rule based grammar using CFG . Tokenize of sentence is extensive deed of this research . According to every token , the researchers present their pronunciation and morphological attachment . Finally , the researchers have analyzed Bangla language semantics applying Bangla lexicon . 

Application of eCommerce for SMEs by using NLP principles
In this paper design issues of eCommerce are analyzed from the perspective of Neuro Linguistic Programming ( NLP ) to help Small and Medium - sized Enterprises ( SMEs ) in identifying and understanding better of their potential customers in the global space . 

Introduction to classification : likelihoods , margins , features , and kernels : tutorial for NAACL - HLT 2007
Statistical methods in NLP have exploited a variety of classification techniques as core building blocks for complex models and pipelines . In this tutorial , we will survey the basic techniques behind classification . We first consider the basic principles , including the principles of maximum likelihood and maximum margin . We then discuss several core classification technologies : naive Bayes , perceptrons , logistic regression , and support vector machines . The discussion will include the key optimization ideas behind their training and the empirical trade - offs between the various classifiers . Finally , we consider the extension to kernels and kernelized classification : what can kernels offer and what is their cost ? The presentation is targeted to NLP researchers new to these methods or those wanting to understand more about how these techniques are interconnected . 

A parameter estimation method for biological systems modelled by ODE / DDE models using spline approximation and differential evolution algorithm
The inverse problem of identifying unknown parameters of known structure dynamical biological systems , which are modelled by ordinary differential equations or delay differential equations , from experimental data is treated in this paper . A two stage approach is adopted : first , combine spline theory and Nonlinear Programming ( NLP ) , the parameter estimation problem is formulated as an optimization problem with only algebraic constraints ; then , a new differential evolution ( DE ) algorithm is proposed to find a feasible solution . The approach is designed to handle problem of realistic size with noisy observation data . Three cases are studied to evaluate the performance of the proposed algorithm : two are based on benchmark models with priori - determined structure and parameters ; the other one is a particular biological system with unknown model structure . In the last case , only a set of observation data available and in this case a nominal model is adopted for the identification . All the test systems were successfully identified by using a reasonable amount of experimental data within an acceptable computation time . Experimental evaluation reveals that the proposed method is capable of fast estimation on the unknown parameters with good precision . 

Computationally Efficient Learning of Quality Controlled Word Embeddings for Natural Language Processing
Deep learning ( DL ) has been used for many natural language processing ( NLP ) tasks due to its superior performance as compared to traditional machine learning approaches . In DL models for NLP , words are represented using word embeddings , which capture both semantic and syntactic information in text . However , 90 - 95 % of the DL trainable parameters are associated with the word embeddings , resulting in a large storage or memory footprint . Therefore , reducing the number of word embedding parameters is critical , especially with the increase of vocabulary size . In this work , we propose a novel approximate word embeddings approach for convolutional neural networks ( CNNs ) used for text classification tasks . The proposed approach significantly reduces the number of model trainable parameters without noticeably sacrificing in computing performance accuracy . Compared to other techniques , our proposed word embeddings technique does not require modifications to the DL model architecture . We evaluate the performance of the the proposed word embeddings on three classification tasks using two datasets , composed of Yelp and Amazon reviews . The results show that the proposed method can reduce the number of word embeddings parameters by 98 % and 99 % for the Yelp and Amazon datasets respectively , with no drop in computing accuracy . 

A lost speech reconstruction method using linguistic information
In recent years , IP telephone use has spread rapidly , thanks to the development of VoIP ( voice over IP ) technology . However , an unavoidable problem of the IP telephone is deterioration of speech due to packet loss , which often occurs on the wireless network . To overcome this problem , we propose a novel lost speech reconstruction method using speech recognition and synthesis . This proposed method uses linguistic information and can deal with the lack of syllable units , which conventional methods are unable to handle . We conducted subjective and objective evaluation experiments . These results showed the effectiveness of the proposed method . Although there is a processing delay in the proposed method , we believe that this method opens up new applications for speech recognition and speech synthesis technology . 

Assuring Nursing ' s Voice in the Electronic Health Record
Language is essential as a mode of communication to preserve and transmit knowledge and culture . The distinct language of a profession gives meaning to terms and expressions as well as knowledge of needs and appropriate responses in a given situations . Since Nightingale ' s direction to record observations , nurses have built their language to record purposes , experiences and meaning of their practice . Today , the nursing voice is missing in reporting software ; it is difficult to record the caring essence of nursing within electronic health records . There is a need for research and development of software to communicate nursing that takes place during nurse - client and nurse - community health encounters . A team of university professors and graduate students in nursing and computer science has joined to generate nursing practice research data for use in creating and testing nursing language software . Data reflect the complexity of caring in nursing grounded in a distinctive community nursing practice model . The software is intended to complement other electronic health records , assuring the voice of nursing in reporting health care . This paper emphasizes identification of the problem and literature review . The work of the research team , the research approaches and initial results are included . 

Ontological Knowledge Inferring Approach based on Term - Clustering and Intra - Cluster Permutations
Ontological representation of knowledge has the advantage of being easy to reason with , but ontology construction with knowledge facts , automatically acquiring them from open domain text is often challenging . This research introduces a novel approach to infer new ontological knowledge in a fully automated manner . Such ontological knowledge can be utilized in both constructing new ontologies and extending existing ontologies . Basic level triples that can be extracted from open domain text are used as the data source for this study . A simple mechanism has been introduced to convert the triple into an ontological knowledge fact and such ontological knowledge facts are further processed to infer new ontological knowledge . The main focus of this research is to infer new ontological knowledge using an advanced term - clustering mechanism followed by an intra - cluster permutation generation task . Generated permutations are potential to be selected as good ontological knowledge facts . Inferred ontological knowledge was tested with inter - rater agreement method with high reliability and variability . Results demonstrated that , out of 43 , 103 triples , this method inferred 127 , 874 ontological knowledge ( approximately 3 times ) of which 66 % were estimated to be effective . Finally , this research contributes a reliable approach which requires a single pass over the corpus of triples to infer a large number of ontological knowledge facts that can be used to construct / extend ontologies . 

Scalable processing of massive text data stores for NLP
The storage and processing of text are integral components of machine learning and natural language processing algorithms . As text - generating sources continue to enlarge the size of natural language data stores , the ability for individual systems to process overwhelming volumes of data becomes challenging . The emergence of systems capable of parallelizing text processing has enabled researchers to rapidly build , train , and deploy intricate NLP and machine learning models . In this tutorial , methods of parallel processing of massive text data stores for NLP and machine learning algorithms will be introduced . Tools for constructing models using a variety of approaches , ranging from typical frequency implementations to graph representations of text , will be reviewed . In addition , challenges for both industry and academia will be discussed . 

Identifying idealised vectors for emotion detection using CMA - ES
Detecting the emotional content of text is one of the most popular NLP tasks . In this paper , we propose a new methodology based on identifying " idealised " words that capture the essence of an emotion ; we define these words as having the minimal distance ( using some metric function ) between a word and the text that contains the relevant emotion ( e . g . a tweet , a sentence ) . We look for these words through searching the space of word embeddings using CMA - ES . Our method produces state of the art results , surpassing classic supervised learning methods . 

Domain walls and vortices in two - mode photonic systems
We investigate 1D and 2D domain - wall ( DW ) states in systems of two nonlinear - Schr ö dinger ( NLS ) equations , which are coupled by the linear mixing and by the XPM ( cross - phase - modulation ) . The system applies to the bimodal light propagation in nonlinear optics and two - component Bose - Einstein condensates . Approximate analytical solutions for the DWs are found near the point of the symmetry - breaking bifurcation of the CW ( continuous - wave ) states . An exact DW solution is obtained for ratio 3 ∶ 1 of the XPM and SPM coefficients . The DWs between flat asymmetric CW states , which are mirror images to each other , are stable , while all other species of the DWs , with zero crossing ( s ) in one or two components , are unstable . An effective potential of attraction between DWs is derived . An exact stable solution is also found for the DW trapped by an external single - peak potential . In the 2D geometry , stable two - component vortices are found , with topological charges s = 1 , 2 , 3 . Radial oscillations of annular DW - shaped pulsons , with s = 0 , 1 , 2 , are studied too . 

Discriminative models for semi - supervised natural language learning
An interesting question surrounding semi - supervised learning for NLP is : should we use discriminative models or generative models ? Despite the fact that generative models have been frequently employed in a semi - supervised setting since the early days of the statistical revolution in NLP , we advocate the use of discriminative models . The ability of discriminative models to handle complex , high - dimensional feature spaces and their strong theoretical guarantees have made them a very appealing alternative to their generative counterparts . Perhaps more importantly , discriminative models have been shown to offer competitive performance on a variety of sequential and structured learning tasks in NLP that are traditionally tackled via generative models , such as letter - to - phoneme conversion ( Jiampojamarn et al . , 2008 ) , semantic role labeling ( Toutanova et al . , 2005 ) , syntactic parsing ( Taskar et al . , 2004 ) , language modeling ( Roark et al . , 2004 ) , and machine translation ( Liang et al . , 2006 ) . While generative models allow the seamless integration of prior knowledge , discriminative models seem to outperform generative models in a " no prior " , agnostic learning setting . See Ng and Jordan ( 2002 ) and Toutanova ( 2006 ) for insightful comparisons of generative and discriminative models . 

Rethink e - Commerce Search
The quality of the search experience on an e - commerce site plays a critical role in customer conversion and the growth of the e - commerce business . In this talk , I will discuss the current status and challenges of product search . In particular , I will highlight the significant amount of effort it takes to create a high - quality product search engine using classical information retrieval methods . Then , I will discuss how recent advances in NLP and deep learning , especially the advent of large pre - trained language models , may change the status quo . While embedding - based retrieval has the potential to improve classical information retrieval methods , creating a machine learning - based , end - to - end system for general - purpose , web search is still extremely difficult . Nevertheless , I will argue that product search for e - commerce may prove to be an area where deep learning can create the first disruption to classical information retrieval systems . 

An Improved Convolutional Neural Network Algorithm for Multi - Label Classification
Recent years conventional neural network ( CNN ) has been applied to different natural language processing ( NLP ) tasks such as sentence classification , sentence modeling , etc . Some researchers use CNN to do multi - label classification but their work mainly focus on image rather than text . In this paper , we propose an improved CNN via hierarchical dirichlet process ( HDP ) model to deal with the multi - label classification problem in NLP . We first apply an HDP model to discard some words which are less important semantically . Then we use word embedding methods to transform words to vectors . Finally , we train CNN based on word vectors . Experimental results demonstrate that our method is superior to most traditional multi - label classification methods and TextCNN in terms of performance . 

Question - Answering Systems Development Based on Big Data Analysis
This article describes the process of Question - Answering Systems Development Based on Big Data Analysis . The methods and tools of the developed product , namely Naive Bayer Classifier , Linear Support Vector Machin , Logistic Regression Natural Language Processing , Information Retrieval and Python , are also proved , and why we chose them is substantiated . In the paper , experiments were performed . And a model was built , and in the next section , its work was shown on a control example . A comparative analysis of the construction and speed of all models was conducted . 

Automatic Detection of Arabic Non - Anaphoric Pronouns for Improving Anaphora Resolution
Anaphora resolution is one of the most difficult tasks in NLP . The ability to identify non - referential pronouns before attempting an anaphora resolution task would be significant , since the system would not have to attempt resolving such pronouns and hence end up with fewer errors . In addition , the number of non - referential pronouns has been found to be non - trivial in many domains . The task of detecting non - referential pronouns could also be incorporated into a part - of - speech tagger or a parser , or treated as an initial step in semantic interpretation . In this article , I describe a machine learning method for identifying non - referential pronouns in an annotated subsegment of the Penn Arabic Treebank using three different feature settings . I achieve an accuracy of 97 . 22 % with 52 different features extracted from a small window size of - 5 /+ 5 tokens surrounding each potentially non - referential pronoun . 

Analysis of anisotropic optical waveguides using a three - dimensional finite difference method
The model describing excitation and distribution of electromagnetic waves in an anisotropic optical waveguide in the three - dimensional case is presented in this paper . The finite difference method ( FDM ) has been employed for the Maxwell equation discretization for a random anisotropic medium . The resulting system of linear algebraic equations for the electric - field components in an inhomogeneous anisotropic medium is solved by the method of biconjugate gradient . 

Sentiment mining : An approach for Bengali and Tamil tweets
This paper presents a proposed work for extracting the sentiments from tweets in Indian Language . We proposed a system that deal with the goal to extract the sentiments from Bengali & Tamil tweets . Our aim is to classify a given Bengali or Tamil tweets into three sentiment classes namely positive , negative or neutral . In recent time , Twitter gain much attention to NLP researchers as it is most widely used platform that allows the user to share there opinion in form of tweets . The proposed methodology used unigram and bi - gram models along with different supervised machine learning techniques . We also consider the use of features generated from lexical resources such as Wordnets and Emoticons Tagger . 

Determining Physical Location of Wireless Access Point using Smart Devices
Indoor Position System ( IPS ) is a method for locating objects in an enclosed space . This paper applies the concept of IPS to locate the physical location of wireless access point . The proposed system can be used for locating the suspected access point in the building . The system uses at least three smartphones to form the points in two - dimensional geometry . These smartphones will gather Wi - Fi ` s information and send them to the server . Then , trilateration is applied to calculate the distance and the location of the targeted access point . The physical location will then send back to each smartphone . The smartphone can use this position to navigate the user to the targeted access point . The result shows that , in the enclosed space , the error of the proposed method is around 3 . 5 meters . 

An NLP - based Tool for Software Artifacts Analysis
Software developers rely on various repositories and communication channels to exchange relevant information about their ongoing tasks and the status of overall project progress . In this context , semi - structured and unstructured software artifacts have been leveraged by researchers to build recommender systems aimed at supporting developers in different tasks , such as transforming user feedback in maintenance and evolution tasks , suggesting experts , or generating software documentation . More specifically , Natural Language ( NL ) parsing techniques have been successfully leveraged to automatically identify ( or extract ) the relevant information embedded in unstructured software artifacts . However , such techniques require the manual identification of patterns to be used for classification purposes . To reduce such a manual effort , we propose an NL parsing - based tool for software artifacts analysis named NEON that can automate the mining of such rules , minimizing the manual effort of developers and researchers . Through a small study involving human subjects with NL processing and parsing expertise , we assess the performance of NEON in identifying rules useful to classify app reviews for software maintenance purposes . Our results show that more than one - third of the rules inferred by NEON are relevant for the proposed task . Demo webpage : https : / / github . com / adisorbo / NEON _ toolView less

Automated software vulnerability assessment with concept drift
Software Engineering researchers are increasingly using Natural Language Processing ( NLP ) techniques to automate Software Vulnerabilities ( SVs ) assessment using the descriptions in public repositories . However , the existing NLP - based approaches suffer from concept drift . This problem is caused by a lack of proper treatment of new ( out - of - vocabulary ) terms for the evaluation of unseen SVs over time . To perform automated SVs assessment with concept drift using SVs ' descriptions , we propose a systematic approach that combines both character and word features . The proposed approach is used to predict seven Vulnerability Characteristics ( VCs ) . The optimal model of each VC is selected using our customized time - based cross - validation method from a list of eight NLP representations and six well - known Machine Learning models . We have used the proposed approach to conduct large - scale experiments on more than 100 , 000 SVs in the National Vulnerability Database ( NVD ) . The results show that our approach can effectively tackle the concept drift issue of the SVs ' descriptions reported from 2000 to 2018 in NVD even without retraining the model . In addition , our approach performs competitively compared to the existing word - only method . We also investigate how to build compact concept - drift - aware models with much fewer features and give some recommendations on the choice of classifiers and NLP representations for SVs assessment . 

A Comparative Study of Sentiment Analysis Using NLP and Different Machine Learning Techniques on US Airline Twitter Data
Today ' s business ecosystem has become very competitive . Customer satisfaction has become a major focus for business growth . Business organizations are spending a lot of money and human resources on various strategies to understand and fulfill their customer ' s needs . But , because of defective manual analysis on multifarious needs of customers , many organizations are failing to achieve customer satisfaction . As a result , they are losing customer ' s loyalty and spending extra money on marketing . We can solve the problems by implementing Sentiment Analysis . It is a combined technique of Natural Language Processing ( NLP ) and Machine Learning ( ML ) . Sentiment Analysis is broadly used to extract insights from wider public opinion behind certain topics , products , and services . We can do it from any online available data . In this paper , we have introduced two NLP techniques ( Bag - of - Words and TF - IDF ) and various ML classification algorithms ( Support Vector Machine , Logistic Regression , Multinomial Naive Bayes , Random Forest ) to find an effective approach for Sentiment Analysis on a large , imbalanced , and multi - classed dataset . Our best approaches provide 77 % accuracy using Support Vector Machine and Logistic Regression with Bag - of - Words technique . 

Utilizing cumulative logit models and human computation on automated speech assessment
We report two new approaches for building scoring models used by automated speech scoring systems . First , we introduce the Cumulative Logit Model ( CLM) , which has been widely used in modeling categorical outcomes in statistics . On a large set of responses to an English proficiency test , we systematically compare the CLM with two other scoring models that have been widely used , i . e . , linear regression and decision trees . Our experiments suggest that the CLM has advantages in its scoring performance and its robustness to limited - sized training data . Second , we propose a novel way to utilize human rating processes in automated speech scoring . Applying accurate human ratings on a small set of responses can improve the whole scoring system ' s performance while meeting cost and score - reporting time requirements . We find that the scoring difficulty of each speech response , which could be modeled by the degree to which it challenged human raters , could provide a way to select an optimal set of responses for the application of human scoring . In a simulation , we show that focusing on challenging responses can achieve a larger scoring performance improvement than simply applying human scoring on the same number of randomly selected responses . 

The development of the Index Thomisticus Treebank valency lexicon
We present a valency lexicon for Latin verbs extracted from the Index Thomisticus Treebank , a syntactically annotated corpus of Medieval Latin texts by Thomas Aquinas . In our corpus - based approach , the lexicon reflects the empirical evidence of the source data . Verbal arguments are induced directly from annotated data . The lexicon contains 432 Latin verbs with 270 valency frames . The lexicon is useful for NLP applications and is able to support annotation . 

Joint routing and topology formation in multihop UWB networks
This paper addresses the throughput optimization problem in multihop ultra - wideband ( UWB ) networks by jointly considering network topology formation and routing . Given a spatial distribution of UWB devices and traffic requirement , we want to form piconets and select paths to maximize the network throughput . Although there have been several works considering the problem of selecting paths to achieve the optimal throughput in multihop wireless networks , to the best of our knowledge , none of them takes the topology formation into the consideration . In this paper , we use Boolean matrices to model role assignment in UWB networks and formulate the throughput optimization problem as a nonlinear programming ( NLP ) problem . Since the throughput optimization problem is NP - hard , we give an upper bound of the optimal throughput by relaxing some constraints and using pseudo - Boolean optimization to linearize the NLP . We prove that the solution of the upper bound is at most three times of the optimal throughput . Based on the topology formed by solving the upper bound , we formulate a lower bound of the optimal throughput as a linear programming problem and use column generation to solve the lower bound . Numerical results show that the lower bound is very close to the upper bound . Simulation results demonstrate the effectiveness of the scheme . 

Semantic Multidimensional Scaling for Open - Domain Sentiment Analysis
The ability to understand natural language text is far from being emulated in machines . One of the main hurdles to overcome is that computers lack both the common and common - sense knowledge that humans normally acquire during the formative years of their lives . To really understand natural language , a machine should be able to comprehend this type of knowledge , rather than merely relying on the valence of keywords and word co - occurrence frequencies . In this article , the largest existing taxonomy of common knowledge is blended with a natural - language - based semantic network of common - sense knowledge . Multidimensional scaling is applied on the resulting knowledge base for open - domain opinion mining and sentiment analysis . 

Initial steps in optimal planning of a distribution system
This paper reports the initial steps of research on planning of rural networks for MV and LV . In this paper , two different cases are studied . In the first case , 100 loads are distributed uniformly on a 100 km transmission line in a distribution network and in the second case , the load structure become closer to the rural situation . In case 2 , 21 loads are located in a distribution system so that their distance is increasing , distance between load 1 and 2 is 3 km , between 2 and 3 is 6 km , etc) . These two models to some extent represent the distribution system in urban and rural areas , respectively . The objective function for the design of the optimal system consists of three main parts : cost of transformers , and MV and LV conductors . The bus voltage is expressed as a constraint and should be maintained within a standard level , rising or falling by no more than 5% . 

A novel interpolated N - gram language model based on class hierarchy
In this paper , we propose a novel interpolated language model that combines the interpolation and the backing - off along hierarchical classes based on class hierarchy . And the corresponding approach to the estimation of interpolation coefficients is also presented . We use the Minimum Discriminative Information ( MDI ) method to cluster the vocabulary into a word - clustering tree hierarchically . The tree is used to balance the generalization ability of classes ' and word specificity when estimating the likelihood of a n - gram event . Experiments are performed on Reuter ' s corpus using a vocabulary of 27 , 000 words . Results show a reduction on the test perplexity over the standard Modified KN n - gram approach by 12% . 

New Initiative : The Naturalness of Software
This paper describes a new research consortium , studying the Naturalness of Software . This initiative is supported by a pair of grants by the US National Science Foundation , totaling $ 2 , 600 , 000 : the first , exploratory ( " EAGER ") grant of $ 600 , 000 helped kickstart an inter - disciplinary effort , and demonstrate feasibility ; a follow - on full grant of $ 2 , 000 , 000 was recently awarded . The initiative is led by the author , who is at UC Davis , and includes investigators from Iowa State University and Carnegie - Mellon University ( Language Technologies Institute) . 

AI - enabled Project Initiation : An approach based on RFP Response Document
Project planning starts with the project initiation phase in which high - level project objectives , commitments , requirements , risks etc . are identified . Typically , the Project Manager involves multiple stakeholders such as HR , Admin , Infrastructure team for the initiation phase to understand and outline the requirements for each group . Current industry practice largely relies on the project manager ’ s experience to carry out the project initiation activities keeping in view the customer context and commitments made . Many times , important information is missed during the transfer of information from sales to delivery resulting in not meeting customer expectations and delivery slippages . Here we propose an AI - enabled approach to automatically extract and classify project initiation relevant information from Request For Proposal ( RFP ) response document using a combination of NLP and ML - based techniques . The approach is validated with real life RFP response documents for five customers . Overall ,   76 % accuracy was observed for question classification and   41 % information was found to be relevant for project initiation from the RFP response documents . In this paper , we share details of our approach , its implementation , early results , and lessons learnt . 

Says who ?: automatic text - based content analysis of television news
We perform an automatic analysis of television news programs , based on the closed captions that accompany them . Specifically , we collect all the news broadcasted in over 140 television channels in the US during a period of six months . We start by segmenting , processing , and annotating the closed captions automatically . Next , we focus on the analysis of their linguistic style and on mentions of people using NLP methods . We present a series of key insights about news providers , people in the news , and we discuss the biases that can be uncovered by automatic means . These insights are contrasted by looking at the data from multiple points of view , including qualitative assessment . 

Automatic Lecture Video Content Summarizationwith Attention - based Recurrent Neural Network
This paper propose an automatic summarization method for a lecture video transcript that uses attention based Recurrent Neural Network ( RNN ) to capture the content of the lecture . Our fully data - driven model also utilize segmentation to split the input video transcript in order to increase topic coherency in each segment . We also use linguistic - based feature to help the model identify important word and key topic in the segment which improves the quality of the summary . Our model shows a significant improvement in the term of ROUGE score compared to the baseline models . 

Memory - based text correction for preposition and determiner errors
We describe the Valkuil . net team entry for the HOO 2012 Shared Task . Our systems consists of four memory - based classifiers that generate correction suggestions for middle positions in small text windows of two words to the left and to the right . Trained on the Google 1TB 5 - gram corpus , the first two classifiers determine the presence of a determiner or a preposition between all words in a text in which the actual determiners and prepositions are masked . The second pair of classifiers determines which is the most likely correction given a masked determiner or preposition . The hyperparameters that govern the classifiers are optimized on the shared task training data . We point out a number of obvious improvements to boost the medium - level scores attained by the system . 

A new on - line digital conceptual model oriented corporate memory constructing : taking unstructured text as a case
The integration of knowledge can be considered as a guideline for managing problems that occur in the task of knowledge management , and more particularly , in the collaborative decision - making . Integration is necessary because it allows communication between different sources . Most of the proposed approaches provide limited support for all activities of the engineering process . We propose a new on - line digital conceptual model to treat the integration of corporate knowledge . Our approach exploits natural language processing , indexation and machine learning techniques to increase productivity of the knowledge engineering task during the integration task . Good experimental studies demonstrate the multidisciplinary applications of our approach . 

An Ensemble Learning for Detecting Situational Awareness Tweets during Environmental Hazards
The shift to social media platforms like Twitter during environmental hazards and emergencies has expanded recently . Yet , the classification of situational awareness tweet based on people post is a complicated process due to the high dimensionality of features . In this empirical study , A framework using machine learning and Natural Language Processing techniques was developed for two - stage binary classification of Twitter data . The First stage consists of four models : Random Forest , Support Vector Machine , Naive Bayes and Decision Trees . Whereas , the second stage includes an ensemble learning approach . Text features - TFIDF ( term frequency , inverse document frequency) , psychometric , and linguistic - were analyzed as predictors of binary classification to categorize each tweet as situational relevant or irrelevant automatically . A manually built and labeled dataset of 4 , 000 tweets were analyzed for situational awareness of environmental health hazards in Barbados from water , mosquito - borne diseases , and sewage during the period 2014 - 2018 . Based on the experiment , our model was able to achieve over 85 % accuracy on classifying tweets that contribute to situational awareness . Furthermore , the results indicate that applying ensemble learning in the second stage showed superior results compared to the combined features - based classification models . 

An automated method for constructing ontology
This paper proposed an automated method for ontology construction . The techniques used include machine learning and NLP ( Natural Language Processing) . These techniques are used to construct more accurate ontologies that represented as XML format . Firstly , Na ï ve Bayes classification algorithms is used to text categorization which determined what kind of label a document is assigned . Secondly , SVM algorithm is used to calculate document similarity to cluster similar documents that we imported . Thirdly , Luhn ' s summarization algorithm is employed in order to shorten the text and kept important essential terms for making the domain ontology . Finally , XML format is used to represent a domain ontology , which has the function of describing term ' s meaning and their lexical relationships . Additional The method developed by Python programming language and Natural Language Toolkit . 

The Development Trend and optimization Proposal of Voice Assistant
With the rapid development of artificial intelligence , this study indicates several existing issues in the voice assistants on the market , including Apple Siri , Google Assistant and Amazon Alexa , analyzes the possible improving orientations based on the operating principals , and predicts the future growing trends . This study makes use of previous researches related to voice assistant , analyzes correspondingly where the algorithms need to be improved , and puts forward some methods to overcome possible poor , inconsistent , and potentially dangerous responses of voice assistant to the users . It is proven that such issues are caused by algorithmic shortcomings . What is more , detailed analysis and related solutions to these problems will be discussed in this study . 

A statistical method for Uyghur tokenization
Tokenization is very important for Uyghur language processing . Tokenization of Uyghur , an agglutinative language , is quite different from other languages such as Chinese and English . In this paper we propose a two - steps statistical tokenization method for Uyghur . Two related factors , the feature template scheme and the manually tokenized corpora , are also discussed . The preliminary experiment results demonstrate that the proposed method is effective : the F - measure of tokenization reaches 88 . 9 % in the open test . 

Knowledge will propel machine understanding of content : extrapolating from current examples
Machine Learning has been a big success story during the AI resurgence . One particular stand out success relates to learning from a massive amount of data . In spite of early assertions of the unreasonable effectiveness of data , there is increasing recognition for utilizing knowledge whenever it is available or can be created purposefully . In this paper , we discuss the indispensable role of knowledge for deeper understanding of content where ( i ) large amounts of training data are unavailable , ( ii ) the objects to be recognized are complex , ( e . g . , implicit entities and highly subjective content ) , and ( iii ) applications need to use complementary or related data in multiple modalities / media . What brings us to the cusp of rapid progress is our ability to ( a ) create relevant and reliable knowledge and ( b ) carefully exploit knowledge to enhance ML / NLP techniques . Using diverse examples , we seek to foretell unprecedented progress in our ability for deeper understanding and exploitation of multimodal data and continued incorporation of knowledge in learning techniques . 

Multi - class confidence weighted algorithms
The recently introduced online confidence - weighted ( CW ) learning algorithm for binary classification performs well on many binary NLP tasks . However , for multi - class problems CW learning updates and inference cannot be computed analytically or solved as convex optimization problems as they are in the binary case . We derive learning algorithms for the multi - class CW setting and provide extensive evaluation using nine NLP datasets , including three derived from the recently released New York Times corpus . Our best algorithm out - performs state - of - the - art online and batch methods on eight of the nine tasks . We also show that the confidence information maintained during learning yields useful probabilistic information at test time . 

Language Scaling : Applications , Challenges and Approaches
Language scaling aims to deploy Natural Language Processing ( NLP ) applications economically across many countries / regions with different languages . Language scaling has been heavily invested by industry since many parties want to deploy their applications / services to global markets . At the same time , scaling out NLP applications to various languages , essentially a data science problem , remains a grand challenge due to the huge differences in the morphology , syntaxes , and pragmatics among different languages . We present a comprehensive survey and tutorial on language scaling . We start with a clear problem description for language scaling and an intuitive discussion on the overall challenges . Then , we outline two major categories of approaches to language scaling , namely , model transfer and data transfer . We present a taxonomy to summarize various methods in literature . A large part of the tutorial is organized to address various types of NLP applications . Finally , we discuss several important challenges in this area and future directions . 

Evaluating the Accuracy of Cloud NLP Services Using Ground - Truth Experiments
Cloud services for natural language processing ( NLP ) increasingly establish as viable alternatives to self - maintained and self - trained NLP pipelines . In particular , they feature low access barriers and management overhead , a pay - as - you - go pricing model , and elastic scalability allowing to process large amounts of natural language data ad hoc . Any deliberation about employing cloud NLP services in practice does , however , face the challenge that so far , little is known about the accuracy provided by such services as well as about how to conduct respective quality assessments . In this paper , we therefore present a method for evaluating the accuracy provided by cloud NLP services and apply it to cloud services for three prominent NLP tasks offered by Amazon , Google , Microsoft , and IBM . Our results show significantly different accuracies as well as different dependencies on the specifics of input data among the covered providers . Our insights therefore allow for a more evidence - based quality - driven choice of the provider to be used for NLP in practice . Furthermore , the general approach employed may also serve as a blueprint for additional future evaluations of cloud NLP services for other tasks or offered by other providers . 

Addressing the challenges of requirements ambiguity : A review of empirical literature
Ambiguity in natural language requirements has long been recognized as an inevitable challenge in requirements engineering ( RE ) . Various initiatives have been taken by RE researchers to address the challenges of ambiguity . In this paper the results of a mapping study are presented that focus on the application of Natural Language Processing ( NLP ) techniques for addressing ambiguity in requirements . Systematic review of the literature resulted in 174 studies on the subject published during 1995 to 2015 , and out of these only 28 are empirically evaluated studies that were selected . From of the resulting set of papers , 81 % have focused on detecting ambiguity ; whereas 4 % and 5 % are focusing on reducing and removing ambiguity respectively . Addressing syntactic , semantic , and lexical ambiguities has attracted more attention than other types . In spite of all the research efforts , there is a lack of empirical evaluation of NLP tools and techniques for addressing ambiguity in requirements . The results have pointed out some gaps in empirical results and have raised questions the designing of an analytical framework for research in this field . 

Detection and correction of real - word spelling errors in Persian language
Several statistical methods have already been proposed to detect and correct the real - word errors of a context . However , to the best of our knowledge , none of them has been applied on Persian language yet . In this paper , a statistical method based on mutual information of Persian words to deal with context sensitive spelling errors is presented . Different experiments show the accuracy of correction method on a test data which only contains one real - word error in each sentence to be about 80 . 5 % and 87 % with respect to precision and recall metrics . 

Attention - based Recurrent Neural Model for Named Entity Recognition in Chinese Social Media
NER ( Named Entity Recognition ) is one of the major tasks in NLP ( natural language processing ) . Although there are already some methods to solve NER problem , it is rare in Chinese social media , and less of them can make good use of external information . The POS ( part - of - speech ) of a word can convey a lot of information in NER , and the attention mechanism can help us successfully focus on these contents . For the above reasons , we propose an attention - based bidirectional LSTM ( Long Short - Term Memory ) model to address NER problem in Chinese social media . The attention mechanism utilizes the POS of every character of a sentence in the learning process . Experiments on the dataset of Chinese social media demonstrated the performance of the proposed neural model . 

A combination method of CRF with syntactic rules to identify opinion _ holder
This paper presents another aspect of sentiment analysis : identifying opinion _ holder in the opinionated sentences . To extract opinion _ holder , we firstly explore Conditional Random Field ( CRF ) based on six features including contextual , opinionated _ trigger words , POS tags , named entity , dependency and proposed sentence structure feature , and dependency is adjusted to be better helpful for containing contextual dependency information . Then we propose two novel syntactic rules with opinionated _ trigger words to directly identify opinion _ holder from the parse trees . The results show that the precision from CRF is much higher than that of syntactic rules , while the recall is lower than . So we combine CRF with syntactic rules used as additional three features including HolderNode , ChunkPosition and Paths for the CRF to train our model . The combination results of the system illustrate the higher recall and higher F - measure under the almost same high precision . 

Improving Voice Activity Detection by using Denoising - Based Techniques with Convolutional LSTM
The performance of voice activity detection ( VAD ) is drastically degraded when observed speech signals are from unseen noisy environments . In this paper , we propose denoisingbased VAD to cope with the unseen noises . The proposed VAD system mainly consists of two stages for denoising and speech / non - speech classification . In the first stage , either logmagnitude spectral estimator ( LSA ) or convolutional long shortterm memory neural network autoencoder ( CLAE ) is applied to eliminate the noises . The convolutional bidirectional long - shortterm memory deep neural network ( CBLDNN ) is employed for the speech / non - speech classification . The results showed that the proposed VAD was better than the baseline . Furthermore , our CLAE tends to outperform the LSA in denoising algorithms when the signal - to - noise ratio is 5dB . 

Automatic Football Match Event Detection from the Scoreboard using a Single - Shot MultiBox Detector
During a football match , the information is manually collected by humans . However , the correctness of the football match data is difficult to check because of the game ' s speed , and thus , human errors can occur . This paper presents an automatic football match event detection from the scoreboard using a deep learning algorithm . The proposed method can reduce human error and performs the detection faster . In this study , the detection was trained with 30 , 000 data of Goals , Substitutions and Cards scoreboard from 68 matches of English Premier League 2017 - 2018 broadcast videos . The detection was tested with 80 sub - testing videos . These videos were prepared from 20 full matches broadcast videos , which consisted of 12 full matches from the year 2017 - 2018 and 8 full matches from the year 2018 - 2019 . The proposed method contains three main steps : data gathering and augmentation , object detection for scoreboard visualization forms , and the event classification . The scoreboard detection is performed with a Single - Shot MultiBox Detector . The event classification employs the majority vote and time frame technique . The experimental results show an accuracy rate of 1 . 00 with the expected event scoreboards , comprised of Goal , Substitution , and Card events . 

DeepParse : A Trainable Postal Address Parser
Postal applications are among the first beneficiaries of the advancements in document image processing techniques due to their economic significance . To automate the process of postal services , it is necessary to integrate contributions from a wide range of image processing domains , from image acquisition and preprocessing to interpretation through symbol , character and word recognition . Lately , machine learning approaches are deployed for postal address processing . Parsing problem has been explored using different techniques , like regular expressions , Conditional Random Fields ( CRFs ) , Hidden Markov Models ( HMMs ) , Decision Trees and Support Vector Machines ( SVMs ) . These traditional techniques are designed on the assumption that the data is free from OCR errors which decreases the adaptability of the architecture in the real - world scenarios . Furthermore , their performance is affected in the presence of non - standardized addresses resulting in intermixing of similar classes . In this paper , we present the first trainable neural network based robust architecture DeepParse for postal address parsing that tackles these issues and can be applied to any Named Entity Recognition ( NER ) problem . The architecture takes the input at different granularity levels : characters , trigram characters and words to extract and learn the features and classify the addresses . The model was trained on a synthetically generated dataset and tested on the real - world addresses . DeepParse has also been tested on the NER dataset i . e . CoNLL2003 and gave the result of 90 . 44 % which is on par with the state - of - art technique . 

Hierarchical Recurrent Attention Networks for Context - Aware Education Chatbots
We propose a hierarchical network architecture for context - aware dialogue systems , that chooses which parts of the past conversation to focus on through a two - layer attention mechanism . The model can encode the parts of the historical dialog that are relevant to the current question to reason about the required response . We first assess the performance of our model on the Dialog bAbI task that involves a restaurant reservation system , where the goal is to book a table at a restaurant . We then train our model on a new hand - crafted dialogue data set , consisting of 7500 dialogues , to inform prospective students about the Data Science master program at University of Lyon . 

Session details : Text mining and NLP applications
No abstract available . 

TaskGenies : Automatically Providing Action Plans Helps People Complete Tasks
People complete tasks more quickly when they have concrete plans . However , they often fail to create such action plans . ( How ) can systems provide these concrete steps automatically ? This article demonstrates that these benefits can also be realized when these plans are created by others or reused from similar tasks . Four experiments test these approaches , finding that people indeed complete more tasks when they receive externally - created action plans . To automatically provide plans , we introduce the Genies workflow that combines benefits of crowd wisdom , collaborative refinement , and automation . We demonstrate and evaluate this approach through the TaskGenies system , and introduce an NLP similarity algorithm for reusing plans . We demonstrate that it is possible for people to create action plans for others , and we show that it can be cost effective . 

Extracting a semantic lexicon of French adjectives from a large lexicographic dictionary
We present a rule - based method to automatically create a large - coverage semantic lexicon of French adjectives by extracting paradigmatic relations from lexicographic definitions . Formalized adjectival resources are , indeed , scarce for French and they mostly focus on morphological and syntactic information . Our objective is , therefore , to contribute enriching the available set of resources by taking advantage of reliable lexicographic data and formalizing it with the well - established lexical functions formalism . The resulting semantic lexicon of French adjectives can be used in NLP tasks such as word sense disambiguation or machine translation . After presenting related work , we describe the extraction method and the formalization procedure of the data . Our method is then quantitatively and qualitatively evaluated . We discuss the results of the evaluation and conclude on some perspectives . 

Techniques for text planning with XSLT
We describe an approach to text planning that uses the XSLT template - processing engine to create logical forms for an external surface realizer . Using a realizer that can process logical forms with embedded alternatives provides a substitute for backtracking in the text - planning process . This allows the text planner to combine the strengths of the AI - planning and template - based traditions in natural language generation . 

A Natural Language Processing ( NLP ) Framework for Embedded Systems to Automatically Extract Verification Aspects from Textual Design Requirements
Embedded systems requirements are significantly different with respect to general purpose systems due to the safety - critical nature and the presence of temporal aspects . Particularly , the design requirements of embedded systems , comprise several temporal conditions , are first identified . Subsequently , a test engineer / system engineer analyzes the design requirements manually to identify the verification characteristics and develops the verification assertions / constraints accordingly . However , the manual analysis of design requirements for verification is time consuming task . Furthermore , high level of domain expertise is required to develop the correct and complete verification assertions from the design requirements . This article presents a novel Natural Language Processing ( NLP ) framework for embedded systems to analyze and automatically extract verification aspects from the textual design requirements . This leads to considerably simplify and accelerate the development of verification assertions . As a part of research , a complete AR2AA ( Automated Requirements 2 Assertions Analyzer ) tool is developed in C # by utilizing the SharpNLP and regular expression libraries . The usefulness of proposed framework is demonstrated through Car Collision and Avoidance System ( CCAS ) case study . The initial results prove that the proposed framework is highly effective for the analysis and development of verification assertions from the textual design requirements . 

A Code - Diverse Kannada - English Dataset For NLP Based Sentiment Analysis Applications
Due to expanded praxis of social media , there is an elevated interest in the Natural Language Processing ( NLP ) of textual substance . Code swapping is a ubiquitous paradox in multilingual nation and the social communication shows mixing of a low resourced language with a highly resourced language mostly written in non - native script in the same text . It is essential to refine the code swapped text to support distinctive NLP tasks such as Machine Translation , Automated Conversational Systems and Sentiment Analysis ( SA ) . The preeminent objective of SA is to identify and analyze the attitude , opinion , emotion or the sentiment in the dataset . Though there are multiple systems skilled on mono - dialectal dataset , all of them break down when it comes for code - diverse data because of the heightened intricacy of blending at various standards of text . Nonetheless , there exist a smaller number of assets for modelling such definitive code - mixed data and the Machine Learning or the Deep Learning algorithms enforcing supervised learning approach yield the better results compared to the unsupervised learning . Such datasets are available for Hindi - English , Tamil - English , Malayalam - English , Bengali - English , German - English , Spanish - English , Japanese - English , Arabic - English etc . Though our research is concentrated towards NLP for emotion and sentiment detection of Kannada , a vibrant south Indian language , to start with , we build the first ever platinum standard corpus for NLP applications of code - diverse text in Kannada - English , as there is no such resource in our native language . The performance analysis of our dataset through Krippendorff ’ s Alpha value of 0 . 89 indicates that it is a benchmark in development of Automatic Sentiment Analysis system for Kannada . 

Lexical Markup Framework ( LMF ) for NLP multilingual resources
Optimizing the production , maintenance and extension of lexical resources is one the crucial aspects impacting Natural Language Processing ( NLP ) . A second aspect involves optimizing the process leading to their integration in applications . With this respect , we believe that the production of a consensual specification on multilingual lexicons can be a useful aid for the various NLP actors . Within ISO , one purpose of LMF ( ISO - 24613 ) is to define a standard for lexicons that covers multilingual data . 

Entailment analysis for improving Chinese textual entailment system
Textual Entailment ( TE ) is a critical issue in natural language processing ( NLP ) ; many NLP applications can be benefited from the recognition of textual entailment ( RTE ) . In this paper we report our observation on how to improve the Chinese textual entailment system and the experiment results on the NTCIR - 10 RITE - 2 dataset . To complement the traditional machine learning approach , which treat every input pair equally with the same features and the same process , our system classify different entailment cases and treat them separately . The experiment results show great improvement . 

NLP - QA Framework Based on LSTM - RNN
The paper mainly majors in question answering system ( QA ) . The original natural response theory originated from Alan Turing ' s Turing Machine Theory in 1950 . Nowadays , the best way to implement a QA system is the Deep Learning . This project is based on the Seq2Seq model theory , I design and implement an automatic question answering system model based on LSTM - RNN algorithm . The paper completely describes the framework and design ideas of the entire system . it realizes the following aspects : 1 ) A Seq2Seq model based on LSTM - RNN . 2 ) We design the QA framework adapts to different chat scene . Results show that the average of perplexity is 2 . 92 . And model ' s loss is 1 . 07 . To some extent , it reduces the resistance of deep learning in developing . The design and implementation of the framework simplifies the development process and facilitates the user ' s own data set . 

Phishing Email Detection Using Robust NLP Techniques
Even with many successful phishing email detectors , phishing emails still cost businesses and individuals millions of dollars per year . Most of these models seem to ignore features like word count , stopword count , and punctuations ; they use features like n - grams and part of speech tagging . Previous phishing email research ignores or removes the stopwords , and features relating to punctuation only count as a minor part of the detector . Even with a strong unconventional focus on features like word counts , stopwords , punctuation , and uniqueness factors , an ensemble learning model based on a linear kernel SVM gave a true positive rate of 83 % and a true negative rate of 96 % . Moreover , these features are robustly detected even in noisy email data . It is much easier to detect our features than correct part - of - speech tags or named entities in emails . 

A supervised learning approach to automatic synonym identification based on distributional features
Distributional similarity has been widely used to capture the semantic relatedness of words in many NLP tasks . However , various parameters such as similarity measures must be hand - tuned to make it work effectively . Instead , we propose a novel approach to synonym identification based on supervised learning and distributional features , which correspond to the commonality of individual context types shared by word pairs . Considering the integration with pattern - based features , we have built and compared five synonym classifiers . The evaluation experiment has shown a dramatic performance increase of over 120 % on the F - 1 measure basis , compared to the conventional similarity - based classification . On the other hand , the pattern - based features have appeared almost redundant . 

Improving Quality of Use Case Documents through Learning and User Interaction
Use cases are widely used to capture user requirements based on interactions between different roles in the system . They are mostly documented in natural language and sometimes aided with graphical illustrations in the form of use case diagrams . Use cases serve as an important means to communicate among stakeholders , requirement engineers and system engineers as they are easy to understand and are produced early in the software development process . Having high quality use cases are beneficial in many ways , e . g . , in avoiding inconsistency / incompleteness in requirements , in guiding system design , in generating test cases . In this work , we propose an approach to improve the quality of use cases using techniques including natural language processing and machine learning . The central idea is to discover potential problems in use cases through active learning and human interaction and provide feedbacks in natural language . We conduct user studies with a real - world use case document . The results show that our method is helpful in improving use cases with a reasonable amount of user interaction . 

Sweeping through the topic space : bad luck ? Roll again ! 
Topic Models ( TM ) such as Latent Dirichlet Allocation ( LDA ) are increasingly used in Natural Language Processing applications . At this , the model parameters and the influence of randomized sampling and inference are rarely examined --- usually , the recommendations from the original papers are adopted . In this paper , we examine the parameter space of LDA topic models with respect to the application of Text Segmentation ( TS ) , specifically targeting error rates and their variance across different runs . We find that the recommended settings result in error rates far from optimal for our application . We show substantial variance in the results for different runs of model estimation and inference , and give recommendations for increasing the robustness and stability of topic models . Running the inference step several times and selecting the last topic ID assigned per token , shows considerable improvements . Similar improvements are achieved with the mode method : We store all assigned topic IDs during each inference iteration step and select the most frequent topic ID assigned to each word . These recommendations do not only apply to TS , but are generic enough to transfer to other applications . 

An ontology - based question answering method with the use of textual entailment
This paper presents a new method for ontology - based Question Answering ( QA ) with the use of textual entailment . In this method , a set of question patterns , called hypothesis questions , was automatically produced from a domain ontology , along with their corresponding SPARQL query templates for answer retrieval . Then the QA task was reduced to the problem of looking for the hypothesis question that was entailed by a user question and taking its corresponding query template to produce a complete query for retrieving the answers from underlying knowledge bases . An entailment engine was used to discover the entailed hypothesis questions with the help of question classification . An evaluation was carried out to assess the accuracy of the QA method , and the results revealed that most of the user questions ( 65% ) can be correctly answered with a semantic entailment engine enhanced by the domain ontology . 

Identifying and Analyzing Disaster - Related Tweet , Through Hashtag Monitoring Using Data Mining and NLP Techniques
Social networks provide more information on trends , ideas , and emotions during any such traumatic event as natural disasters . This paper proposes a model that monitors specific hashtag - related tweets , identifies the actual disaster tweet , and extracts meaningful data on disaster tweets , such as location and keywords . Government and organizations can use this data to support financial help , emergency evacuation , and volunteer . Monitoring Twitter and collecting data from many disaster periods , and analyze those data . This analysis data finds out where the Twitter users send the most catastrophic tweets and the most devastating event . An API developed from this data . This API served the disaster related location and corresponding disaster rate . A web app was developed based on the API . It detects the user ’ s location and displays the disaster statistics messages such as the disaster level related to the user ’ s area . Then users can receive this message via email or SMS by providing their phone numbers and emails . This work ’ s main objective is to identify the accurate disaster - related tweet by monitoring Twitter during the disaster and extracting meaningful data . 

Modeling Class Diagram using NLP in Object - Oriented Designing
Requirement ' s analysis and design is a multifaceted and time - consuming process . The success of software projects critically relies on careful & timely analysis and modeling of system requirements . Mostly , the requirements gathered from the stakeholders are written in some language ( probably English ) . In this regard , significant manual efforts are required for the formation of good class model which unfortunately results in time delays in the software industry . The problems associated with the requirement analysis and class modeling can be overcome by the appropriate employment of machine learning . In this paper , we propose a system , requirement engineering analysis & design ( READ ) to generate unified modeling language ( UML ) class diagram using natural language processing ( NLP ) and domain ontology techniques . We have implemented the READ system in Python and it successfully generates the UML class diagram i . e . , class name , attributes methods , and relationships from the textual requirements written in English . To assess the performance of the proposed system , we have evaluated it on publicly available standards and the experimental results show that it outperforms the existing techniques for object - oriented based software designing . 

Chinese sentence similarity based on word context and semantic
Sentence similarity computing plays an important role in the nature language processing . Many different methods are proposed to calculate sentence similarity including word , semantic , syntax and so on . In this paper , we proposed a sentence similarity method for travel question answering system by combining the word context information and semantic similarity together . We searched a series of context structures for keywords in a sentence . Experiment has been carried out to show the effectiveness of our method . 

Multimodal Sentiment Analysis Using Deep Learning
Since about a decade ago , deep learning has emerged as a powerful machine learning technique and produced state - of - the - art results in many application domains , ranging from computer vision and speech recognition to NLP . Applying deep learning to sentiment analysis has also become very popular recently . In this paper , we propose a comparative study for multimodal sentiment analysis using deep neural networks involving visual recognition and natural language processing . Initially we make different models for the model using text and another for image and see the results on various models and compare them . 

Trajectory Optimization of Two Cooperative Aircrafts Based on RHC
Trajectory optimization problem of two cooperative aircrafts in cooperative tactical engagement maneuvering is studied . The optimization problem of air combat for two aircrafts is established based on the analysis of vertical tactical engagement and three different performance measures and terminal constraints are proposed . The receding horizon control ( RHC ) model and the numerical solution based on simpson - direct - collocation is put forward . In order to improve the online performance , a BP neural network based approximation of the performance measures is proposed . Finally , a simulation shows that this method is feasible for trajectory optimization in cooperative air combat . 

Scenic area data analysis based on NLP and ridge regression
With the rapid development of Internet technology , many textual evaluation data of tourist destinations have accumulated on the Internet . Using NLP to conduct text mining on the data can effectively improve tourists ' satisfaction and has a long - term and positive effect on the scientific supervision of tourism enterprises and the optimal allocation of resources . This paper uses Python to pre - process the comment data , including de - duplication , removal of English text , conversion of traditional Chinese to simplified , text correction , and compression to remove words . The reviews are divided into five categories : service , location , facility , hygiene , and cost - performance . The Paddlehub library is used to calculate the emotional scores of all reviews in the five aspects of each scenic spot and hotel and subsequently calculate the percentage of positive , neutral , and negative reviews . Afterward , use Ridge Regression and k - fold cross - validation to establish a comprehensive evaluation model , which can obtain the total score of each scenic spot and hotel in five aspects , with MSE , RMSE , MAE to verify . Furthermore , a method of extracting characteristic words in scenic spots and hotels is proposed : firstly , use the LDA subject vocabulary mining ; next , select the TOP50 words through operations such as extracting keywords , selecting out nouns , filtering out irrelevant words , and synonymous merge ; lastly , two parts of words are integrated to get the characteristic words . Finally , according to the total score , the scenic spots and hotels are divided into three levels : high , medium , and low levels , while three groups of scenic spots and hotels of the same type are selected respectively ( each group has three scenic spots or hotels of different level) . Through the characteristic words and five aspects of the total score , we can compare and analyze the selected three groups of scenic spots and hotels to make a suggestion . 

Analyzing source code identifiers for code reuse using NLP techniques and WordNet
Massive amount of source codes are available free and open . Reusing those open source codes in projects can reduce the project duration and cost . Even though several Code Search Engines ( CSE ) are available , finding the most relevant code can be challenging . In this paper we propose a framework that can be used to overcome the above said challenge . The proposed solution starts with a Software Architecture ( Class Diagram ) in XML format and extracts information from the XML file , and then , it fetches relevant projects using three types of crawlers from GitHub , SourceForge , and GoogleCode . Then it finds the most relevant projects among the vast amount of downloaded projects . This research considers only Java projects . All java files in every project will be represented in Abstract Syntax Tree ( AST ) to extract identifiers ( class names , method names , and attributes name ) and comments . Action words ( verbs ) are extracted from comments using Part of Speech technique ( POS ) . Those identifiers and XML file information need to be analyzed for matching . If identifiers are matched , marks will be given to those identifiers , likewise marks will be added together and then if the total mark is greater than 50% , the . java file will be considered as a relevant code . Otherwise , WordNet will be used to get synonym of those identifiers and repeat the matching process using those synonyms . For connected word identifiers , camel case splitter and N - gram technique are used to separate those words . The Stanford Spellchecker is used to identify abbreviated words . The results indicate successful identification of relevant source codes . 

Does Self - Efficacy Correlate with Positive Emotion and Academic Performance in Collaborative Learning ? 
This full research paper studies the correlation of self - efficacy in computer science as well as learning and social skills with students ' academic performance and their emotions in collaborative learning environments . Self - efficacy is an essential part of social cognitive theory and provides the foundation for analyzing human thoughts , motivations , and actions . Studies show that students ' successful performance and accomplishment are directly affected by the level of self - efficacy . Therefore , analyzing self - efficacy in engineering education is important since it can impact the learning process in academic settings as well as provide a metric to track for improvement . Social cognitive theories also emphasize that students ' interaction with each other affects their learning process and how they perform in educational settings . In previous work [ 5] , we analyzed students ' conversations in low - stake teams in an introductory programming course ( CS1 ) and observed a strong positive correlation between students ' positive emotions while interacting with each other with their performance in the course . In this study , we focus on the correlation of self - efficacy with learner ' s emotion and performance . We measure students ' self - efficacy with a standard instrument called “ Student Attitudes Toward STEM ( S - STEM ) Survey” . For this purpose , we asked the participants to self - report on a 5 - point Likert - scaled survey including 20 questions . These 20 questions are grouped into 2 main categories of computer science and learning / social skills . Students ' emotions were extracted from their speeches in teams by applying natural language processing ( NLP ) methods . The result of data analysis shows a statistically significant correlation between overall self - efficacy and performance in the course and positive emotions during the teamwork . We further investigate which category of self - efficacy questions most correlate with students ' performance . The result shows self - efficacy in interpersonal skills and learning ability most impact students ' performance . 

An NLP - based ontology population for a risk management generic structure
In this paper we propose an NLP - based Ontology Population approach for a Generic Structure instantiation from natural language texts , in the domain of Risk Management . The approach is semi - automatic and based on combined NLP techniques using domain expert intervention for control and validation . It relies on the predicative power of verbs in the instantiation process . It is not domain dependent since it heavily relies on linguistic knowledge . We demonstrate the effectiveness of our method on the ontology of the PRIMA project ( supported by the European community ) and we populate this generic domain ontology via an available corpus . A first validation of the approach is done through an experiment with Chemical Fact Sheets from Environmental Protection Agency . 

Modelling field dependencies on structured documents with fuzzy logic
A new scenario has raised into the IR field with the increase in the use of mark - up languages . This new scenario has been defined as structured IR and is focused on documents with structure . The classic IR models have been extended in order to be applied to this document type . Generally these adaptations have been carried out by weighting the fields that form the document structure , and making the assumption of statistics independence between fields . This assumption force to an estimation of the different weights applied to each field . In this paper a new ranking function for structured IR is proposed . This new function is based on Fuzzy Logic , and its main aim is to model through heuristics and expert knowledge the relations between fields within a document . 

WWW 2008 workshop : NLPIX2008 summary
The amount of information available on the Web has increased rapidly , reaching levels that few would ever have imagined possible . We live in what could be called the " information - explosion era , " and this situation poses new problems for computer scientists . Users demand useful and reliable information from the Web in the shortest time possible , but the obstacles to fulfilling this demand are many including language barriers and the so - called " long tail . " Even worse , users may provide only vague specifications of the information that they actually want , so that a more concrete specification must somehow be inferred by Web access tools . Natural language processing ( NLP ) is one of the key technologies for solving the above Web usability problems . Almost all the Web page provide with the essential information in the form of natural language texts , and the amount of these text information is huge . In order to offer solutions to these problems we must perform searching and extracting information from the Web texts using NLP technologies . The aim of this workshop : NLP Challenges in the Information Explosion Era ( NLPIX 2008 ) is to bring researchers and practitioners together in order to discuss our most pressing needs with respect to accessing information on the Web , and to discuss new ideas in NLP technologies that might offer viable solutions for those issues . 

Parallel Implementation of Feasible Direction Algorithm for Large - Scale Sensor Network Location Problems
In order to improve the deficiency generated from uneven distribution of anchors in the distributed semidefinite programming ( SDP ) method , improved distributed method is proposed for solving Euclidean metric localization problems that arise from large - scale wireless sensor networks ( WSN ) . By introducing the change of factorization , nonlinear programming ( NLP ) model is presented on each subarea , and feasible direction algorithm is introduced for solving NLP problems , which can be executed in parallel . Numerical results on large - scale sensor network problems with more than 10000 nodes demonstrate that , the proposed method performs better than the distributed SDP method . 

Real Time Iterations for Mixed - Integer Model Predictive Control
The subject matter of the present paper is Model Predictive Control ( MPC ) . MPC is a well known approach to optimal control that tackles a long time - horizon control problem by sequentially optimizing small portions of it . MPC is generally regarded as a powerful tool for efficiently finding approximate solutions to complex optimal control problems . However , in the context of mixed - integer non - linear optimal control , MPC suffers from the high computational cost of mixed - integer non - linear programming . As a consequence , in real - time applications , the suitability of mixed - integer nonlinear MPC is limited . In this paper we present the Mixed - Integer Real Time Optimal Control ( MIRT - OC ) algorithm : a novel MPC technique that reduces the cost of each MPC iteration by reusing the information generated during past iterations . MIRT - OC extends the basic ideas behind LP / NLP - Branch & Bound to MPC . In LP / NLP - B & B , a mixed - integer convex optimization problem is tackled as a linear one where new linear constraints are added on the run in order to enforce the original nonlinear constraints on the solution . MIRT - OC in addition to using a LP / NLP - B & B procedure at each MPC iteration , introduces two forward shifting techniques . The first technique transforms the linearizations generated during one MPC iteration into linearizations for the sub - problem to solve in the subsequent MPC iteration . The second one extrapolates from the B & B tree built during the solution of one MPC sub - problem into a partially explored B & B tree for the next MPC sub - problem . Consequently , the non - linear MPC problem is tackled as a single mixed - integer linearly - constrained optimal control problem in which new linearizations are added on the run and , at each given moment , only a portion of the problem is optimized . The collected empirical data shows how the proposed algorithm is capable of providing sizeable computational savings , representing a first step towards a true real - time mixed - integer convex MPC scheme . 

Fine Grained Classification of Personal Data Entities with Language Models
Fine grained entity classification is the task of assigning context - specific , fine grained labels to entities extracted in an NLP Pipeline . Before the advent of language models , several artificial neural network models were proposed for this task . We revisit these models and compare them with BERT - based models for the specific task of classifying Personal Data Entities ( PDE) . We observe that using side information from rule - based annotators improves neural model performance on this task and can complement language models . 

A uniform method of grammar extraction and its applications
Grammars are core elements of many NLP applications . In this paper , we present a system that automatically extracts lexicalized grammars from annotated corpora . The data produced by this system have been used in several tasks , such as training NLP tools ( such as Supertaggers ) and estimating the coverage of hand - crafted grammars . We report experimental results on two of those tasks and compare our approaches with related work . 

Understanding Citizens ’ Emotional Pulse in a Smart City Using Artificial Intelligence
Over the past decade , smart city applications have gained significant attention in industrial informatics . However , little attention has been given to perceiving the emotions and perceptions of citizens who have a direct impact on smart city initiatives . In this article , we propose the use of publicly available abundant social media conversations that contain contextual information encompassing citizens ' emotions and perceptions , which could be considered to provide the means to feel the “ emotional pulse ” of a city . We propose an automated AI - based observation framework to detect the emergence of public emotions and negativity in conversations . We evaluated the applicability of the framework using 29 928 social media conversations toward the much - debated topic of self - driving vehicles which will become increasingly relevant to smart cities . The patterns and transitions of citizens ' collective emotions were modeled using the Natural Language Processing and Markov models while the negativity ( toxicity ) in conversations was evaluated using a deep learning based classifier . The framework could be adopted by industry leaders and government officials for smart observation of citizen opinions to improve security , communication , and policymaking . 

A new method for updating word senses in Hindi WordNet
Hindi WordNet , a rich computational lexicon is widely being used for many Hindi Natural Language Processing ( NLP ) applications . However it does not presently provide exhaustive list of senses for every word , which degrades the performance of such NLP applications . In this paper , we propose a graph based model and its associated techniques to automatically acquire words ' senses . In the literature no such method is available which is capable of automatically identify the senses of the Hindi words . We use a Hindi part of speech tagged corpus for building the graph model . The linkage between noun - noun concepts is extracted on the basis of syntactic and semantic relationships . All of the senses of a word including the sense which is not present in Hindi WordNet are extracted . Our method also finds the categories of similar words . Using this model applications of NLP can be achieved at a higher level . 

Automatic measurement of syntactic development in child language
To facilitate the use of syntactic information in the study of child language acquisition , a coding scheme for Grammatical Relations ( GRs ) in transcripts of parent - child dialogs has been proposed by Sagae , MacWhinney and Lavie ( 2004 ) . We discuss the use of current NLP techniques to produce the GRs in this annotation scheme . By using a statistical parser ( Charniak , 2000 ) and memory - based learning tools for classification ( Daelemans et al . , 2004 ) , we obtain high precision and recall of several GRs . We demonstrate the usefulness of this approach by performing automatic measurements of syntactic development with the Index of Productive Syntax ( Scarborough , 1990 ) at similar levels to what child language researchers compute manually . 

Co - Training for Demographic Classification Using Deep Learning from Label Proportions
Deep learning algorithms have recently produced state - of - the - art accuracy in many classification tasks , but this success is typically dependent on access to many annotated training examples . For domains without such data , an attractive alternative is to train models with light , or distant supervision . In this paper , we introduce a deep neural network for the Learning from Label Proportion ( LLP ) setting , in which the training data consist of bags of unlabeled instances with associated label distributions for each bag . We introduce a new regularization layer , Batch Averager , that can be appended to the last layer of any deep neural network to convert it from supervised learning to LLP . This layer can be implemented readily with existing deep learning packages . To further support domains in which the data consist of two conditionally independent feature views ( e . g . image and text ) , we propose a co - training algorithm that iteratively generates pseudo bags and refits the deep LLP model to improve classification accuracy . We demonstrate our models on demographic attribute classification ( gender and race / ethnicity ) , which has many applications in social media analysis , public health , and marketing . We conduct experiments to predict demographics of Twitter users based on their tweets and profile image , without requiring any user - level annotations for training . We find that the deep LLP approach outperforms baselines for both text and image features separately . Additionally , we find that co - training algorithm improves image and text classification by 4 % and 8 % absolute F1 , respectively . Finally , an ensemble of text and image classifiers further improves the absolute F1 measure by 4 % on average . 

An Empirical Study on Assessing the Quality of Use Case Metrics
Use cases are generally meant to describe the functional requirements of a software system . However , the use of some Natural Language ( NL ) text may inherently introduce language and interpretation related issues . Several tools and techniques have been proposed and available to assess the quality of use case specification , however , often performed manually . The precise and automated way of analyzing the quality of use cases in different aspects is a need due to volatile functionalities and rapid change in requirements . In this paper , we report the results of two separate experimental studies conducted , a replication of one another , to evaluate the significance and relevance of the use case quality assessment metrics . Our results revealed redundancies among the parameters associated with the quality measures and suggested modifications on the formulation of use case metrics which in turn make them complete , correct and consistent . Subsequently , we develop a tool support to automatically analyze the quality of use case specification on the basis of experimentally validated metrics . 

Multiple Choice Question Answer System using Ensemble Deep Neural Network
Question answer ( QA ) system is closely related to NLP and IR tasks . An automated QA system should understand the semantics of question and derive answers relevant to it . In case of MCQ system this tasks becomes difficult as the model needs to understand the semantics and select an answer from a given choice . In this paper we propose a ensemble approach to predict answers to Multiple choice question using LSTM model , hybrid LSTM - Convolution NN model and Multilayer Perception ( MLP ) model . Firstly , by using LSTM and hybrid LSTM - CNN models are trained parallel . Multilayer Perception is used to predict option to training dataset separately . The 8thGr - NDMC datasets are selected for model evaluation and comparison . The 8 th GR - NDMC is used for experimentation purpose . The observed results demonstrate that the proposed approach performs better than some other single forecasting models . 

A Deep Learning Framework for Coreference Resolution Based on Convolutional Neural Network
Recently many researches have shown that word embeddings are able to represent information from word related contexts or its nearest neighborhood words , and thus are applied in many NLP tasks successfully . In this paper , we propose convolutional neural network model to extent word embeddings to mention / antecedent representation . These representations are obtained through convoluting neighboring word embeddings and other contextual information for coreference resolution . We evaluate our system on the English portion of the CoNLL 2012 Shared Task dataset and show that the proposed system achieves a competitive performance compared with the state - of - the - art approaches . We also show that our proposed model especially improves the coreference resolution of long spans significantly . 

Medical Formulation Recognition ( MFR ) using Deep Feature Learning and One Class SVM
Specials medications are personalized formulations manufactured on demand for patients with unique prescription requirements and constitute an essential component of patient treatment . Specials are becoming increasingly in demand due to the need for personalized and precision medicine . The timely provision of optimal personalized medicine , however , is challenging , subject to strict regulatory processes , and is expert intensive . In this paper , we propose a new medical formulation engine ( MFE ) that performs semantic search across multiple disparate formulations archives to enable data driven formulation intelligence . We develop a new platform for medical formulations recognition ( MFR ) that curates a new dataset comprising formulations and non - formulations ( clinical ) text and uses a novel pipeline encompassing deep feature extraction and one - class support vector machine learning . The proposed MFR framework demonstrates promising performance and can be used as a benchmark for future research in formulations recognition . 

Detection of Fake Users in Twitter Using Network Representation and NLP
Social Media Platforms like Facebook , Twitter , Instagram , etc . have large user base all around the world that generates huge amounts of data every second . This includes a lot of posts by fake and spam users , typically used by many organizations around the globe to gain a competitive edge over others . In this work , we aim at detecting such user accounts on Twitter . We show how to distinguish between Genuine and Spam accounts in Twitter using a novel combination of feature engineering , network representation , and natural language processing techniques . 

Deep Natural Language Processing for Search and Recommender Systems
Search and recommender systems share many fundamental components including language understanding , retrieval and ranking , and language generation . Building powerful search and recommender systems requires processing natural language effectively and efficiently . Recent rapid growth of deep learning technologies has presented both opportunities and challenges in this area . This tutorial offers an overview of deep learning based natural language processing ( NLP ) for search and recommender systems from an industry perspective . It first introduces deep learning based NLP technologies , including language understanding and language generation . Then it details how those technologies can be applied to common tasks in search and recommender systems , including query and document understanding , retrieval and ranking , and language generation . Applications in LinkedIn production systems are presented . The tutorial concludes with discussion of future trend . 

Phase - compensator design using two - step mathematical programming
This paper presents a new technique for designing an all - pass phase compensator ( AP - PC ) using a two - step mathematical programming strategy . Such an AP - PC alters the phase characteristics of a nonlinear - phase digital system and achieves phase compensation . In this paper , we first briefly review the approximate phase - error expression that is a linear function of the AP - PC coefficients , and then use the linear programming ( LP ) strategy to determine the sub - optimal coefficients . After getting the sub - optimal solution , we use the resulting coefficients as the initial values for the further non - linear - programming ( NLP ) . Therefore , this design technique includes two steps ( LP plus NLP : LP - NLP ) . Combining the above two - step optimizations significantly improves the final design accuracy . An example is provided for checking the accuracy improvement by employing the LP - NLP design strategy . 

An approach towards making Honey Encryption easily available
Honey encryption ( HE ) was proposed by Juels and Ristenpart in 2014 as a solution against brute force attacks . As honey encryption provides fake messages when a wrong password is entered , it gives an extra layer of security to the user as all these honey messages which consist of sweet words in distribution transforming encoder ( DTE ) . As technology is progressing there are major concerns among users about security . In this paper we are providing ways for making honey encryption available for general and practical use . We have proposed an Application program interface ( API ) for this which would help in proper improvement in readily making honey encryption for the programmers so as to decrease the hassle of programming from scratch . We have proposed some methods for message space reduction in honey encryption and methods for its application on natural language message . Through this paper our approach for making honey encryption readily available so as to make work easy on it so that it could speed up the progress of honey encryption in research . 

Transfer Learning for Bilingual Content Classification
LinkedIn Groups provide a platform on which professionals with similar background , target and specialities can share content , take part in discussions and establish opinions on industry topics . As in most online social communities , spam content in LinkedIn Groups poses great challenges to the user experience and could eventually lead to substantial loss of active users . Building an intelligent and scalable spam detection system is highly desirable but faces difficulties such as lack of labeled training data , particularly for languages other than English . In this paper , we take the spam ( Spanish ) job posting detection as the target problem and build a generic machine learning pipeline for multi - lingual spam detection . The main components are feature generation and knowledge migration via transfer learning . Specifically , in the feature generation phase , a relatively large labeled data set is generated via machine translation . Together with a large set of unlabeled human written Spanish data , unigram features are generated based on the frequency . In the second phase , machine translated data are properly reweighted to capture the discrepancy from human written ones and classifiers can be built on top of them . To make effective use of a small portion of labeled data available in human written Spanish , an adaptive transfer learning algorithm is proposed to further improve the performance . We evaluate the proposed method on LinkedIn ' s production data and the promising results verify the efficacy of our proposed algorithm . The pipeline is ready for production . 

Domain Specific Sentence Level Mood Extraction from Malayalam Text
Natural Language Processing ( NLP ) is a field which studies the interactions between computers and natural languages . NLP is used to enable computers to attain the capability of manipulating natural languages with a level of expertise equivalent to humans . There exists a wide range of applications for NLP , of which sentiment analysis ( SA ) plays a major role . In sentimental analysis , the emotional polarity of a given text is analysed , and classified as positive , negative or neutral . A more difficult task is to refine the classification into different moods such as happy , sad , angry etc . Analysing a natural language for mood extraction is not at all an easy task for a computer . Even after achieving capabilities of massive amount of computation within a matter of seconds , understanding the sentiments embodied in phrases and sentences of textual information remains one of the toughest tasks for a computer till date . This paper focuses on tagging the appropriate mood in Malayalam text . Tagging is used to specify whether a sentence indicates a sad , happy or angry mood of the person involved or if the sentence contains just facts , devoid of emotions . This research work is heavily dependent on the language since the structures vary from language to language . Mood extraction and tagging has been successfully implemented for English and other European languages . For the south Indian language Malayalam , no significant work has yet been done on mood extraction . We will be focusing on domain - specific sentence - level mood extraction from Malayalam text . The extraction process involves parts - of - speech tagging of the input sentence , extracting the patterns from the input sentence which will contribute to the mood of the sentence , such as the adjective , adverb etc . , and finally scoring the sentence on an emotive scale by calculating the semantic orientation of the sentence using the extracted patterns . Sentiment classification is becoming a promising topic with the rise of social media such as blogs , social networking sites , where people express their views on various topics . Mood extraction enables computers to automate the activities performed by human for making decisions based on the moods of opinions expressed in Malayalam text . 

A computer - assisted dictionary - making system for Chinese English learner ' s dictionary
The paper reports our computer - assisted dictionary - making system for English - Chinese learner ' s dictionary . The system aims to enhance language quality and format consistency in dictionary compilation , which is realized by a number of linguistic analyzing modules and editorial assistant tools . The system embeds a ) a concordancer of equivalent words in English - Chinese bilingual corpora based on probability coefficients , b ) a collocation extraction tool over grammatical relations by shallow syntactic parsing , and c ) a colligation finder based on part - of - speech tagged corpora . All these lead to better language quality of learner ' s dictionaries . In addition , a desktop publishing tool and database human - machine interface are implemented to ensure format consistency in the entries produced by different lexicographers , thereby contributing to the quality of the dictionaries . The system streamlines and optimizes the processes of compiling , revising , editing , proofreading and type - setting . It distinguishes itself from other similar systems in that it pays more attentions to learners with much information derived from bilingual and learner ' s corpora by statistical techniques and shallow parsing . 

Pre - annotating Clinical Notes and Clinical Trial Announcements for Gold Standard Corpus Development : Evaluating the Impact on Annotation Speed and Potential Bias
In this study our aim was to present a series of experiments to evaluate the impact of pre - annotation : ( 1 ) on the speed of manual annotation of clinical notes and clinical trial announcements ; and ( 2 ) test for potential bias if pre - annotation is utilized . The gold standard was 900 clinical trial announcements from clinicaltrials . gov website and 1655 clinical notes annotated for diagnoses , signs , symptoms , UMLS CUI and SNOMED CT codes . Two dictionary - based methods were used to pre - annotate the text . Annotation time savings ranged from 2 . 89 % to 29 . 1 % per entity . The pre - annotation did not reduce the IAA or annotator performance but reduced the time to annotate in every experiment . Dictionary - based pre - annotation is a feasible and practical method to reduce cost of annotation without introducing bias in the process . 

Deep Sentiment Classification and Topic Discovery on Novel Coronavirus or COVID - 19 Online Discussions : NLP Using LSTM Recurrent Neural Network Approach
Internet forums and public social media , such as online healthcare forums , provide a convenient channel for users ( people / patients ) concerned about health issues to discuss and share information with each other . In late December 2019 , an outbreak of a novel coronavirus ( infection from which results in the disease named COVID - 19 ) was reported , and , due to the rapid spread of the virus in other parts of the world , the World Health Organization declared a state of emergency . In this paper , we used automated extraction of COVID - 19 - related discussions from social media and a natural language process ( NLP ) method based on topic modeling to uncover various issues related to COVID - 19 from public opinions . Moreover , we also investigate how to use LSTM recurrent neural network for sentiment classification of COVID - 19 comments . Our findings shed light on the importance of using public opinions and suitable computational techniques to understand issues surrounding COVID - 19 and to guide related decision - making . In addition , experiments demonstrated that the research model achieved an accuracy of 81 . 15 % - a higher accuracy than that of several other well - known machine - learning algorithms for COVID - 19 - Sentiment Classification . 

The Building of a CBD - Based Domain Ontology in Chinese
This paper describes a method of building a medical ontology prototype in Chinese . During the procedure , we explored the following questions , which are crucial for the task of ontology engineering : ( 1 ) are there some more computer - understandable knowledge description models ? We proposed a structural and fine - grained knowledge description model called concept - based description model ( CBD ) to describe the rich knowledge in the ontology . That is , to use other concept or the combination of the related concepts to represent the targeted concept , which is supposed to be more computer - understandable ; ( 2 ) during large scale ontology engineering , how to use NLP technologies to reduce domain expertspsila work to the minimal ? In our work , we used some NLP technologies to try to reduce domain expertspsila work to the minimal as possible as it can . The experiments show the significance of our method . 

Integrating word embeddings and traditional NLP features to measure textual entailment and semantic relatedness of sentence pairs
Recent years the distributed representations of words ( i . e . , word embeddings ) have been shown to be able to significantly improve performance in many natural language processing tasks , such as pos - of - tag tagging , chunking , named entity recognition and sentiment polarity judgement , etc . However , previous tasks only involve a single sentence . In contrast , this paper evaluates the effectiveness of word embeddings in sentence pair classification or regression problems . Specifically , we propose novel simple yet effective features based on word embeddings and extract many traditional linguistic features . Then these features serve as input of a classification / regression algorithm in isolation and in combination . Evaluations are conducted on three sentence pair classification / regression tasks , i . e . , textual entailment , cross - lingual textual entailment and semantic relatedness estimation . Experiments on benchmark datasets provided by Semantic Evaluation 2013 and 2014 showed that using word embeddings is able to significantly improve the performance and our results outperform the best achieved results so far . 

Evaluating Posts on the Steemit Blockchain : Analysis on Topics Based on Textual Cues
Online Social Networking platforms ( OSNs ) are part of the people ' s everyday life answering the deep - rooted need for communication among humans . During recent years , a new generation of social media based on blockchain became very popular , bringing the power of the technology to the service of social networks . Steemit is one such and employs the blockchain to implement a rewarding mechanism , adding a new , economic , layer to the social media service . The reward mechanism grants virtual tokens to the users capable of engaging other users on the platform , which can be either vested in the platform for increased influence or exchanged for fiat currency . The introduction of an economic layer on a social networking platform can seriously influence how people socialize . In this work , we tackle the problem of understanding how this new business model conditions the way people create contents . We performed term frequency and topic modelling analyses over the written contents published on the platforms between 2017 and 2019 . This analysis lets us understand the most common topics of the contents that appear in the platform . While personal mundane information still appears , along with contents related to arts , food , travels , and sport , we also see emerging a very strong presence of contents about blockchain , cryptocurrency and , more specifically , on Steemit itself and its users . 

Identifying science concepts and student misconceptions in an interactive essay writing tutor
We present initial steps towards an interactive essay writing tutor that improves science knowledge by analyzing student essays for misconceptions and recommending science webpages that help correct those misconceptions . We describe the five components in this system : identifying core science concepts , determining appropriate pedagogical sequences for the science concepts , identifying student misconceptions in essays , aligning student misconceptions to science concepts , and recommending webpages to address misconceptions . We provide initial models and evaluations of the models for each component . 

Automatic short answer marking
Our aim is to investigate computational linguistics ( CL ) techniques in marking short free text responses automatically . Successful automatic marking of free text answers would seem to presuppose an advanced level of performance in automated natural language understanding . However , recent advances in CL techniques have opened up the possibility of being able to automate the marking of free text responses typed into a computer without having to create systems that fully understand the answers . This paper describes some of the techniques we have tried so far vis -à- vis this problem with results , discussion and description of the main issues encountered . 1

Analysis of Various Sentiment Analysis Techniques of NLP
Sentiment Analysis and opinion mining complement each other . This approach is concerned with classifying the opinions of public regrading a product using NLP . This approach considers sentiments and viewpoint of individuals about the occurrence of an episode . Assessment mining is generally helpful in different domains such as business brand surveys , web - based media study , and movie analyses , and so forth . The sentiment analysis is an important method in formation of frameworks based on recommendations . The people give the content audits like web - based reviews , feedback or remarks on the web - based media and web - based business sites . This textual content is an essential wellspring of client ' s conclusions . In sentiment analysis , the main aim is to classify the users ' sentiments into positive , negative and unbiased . This type of analysis indicates the ubiquity or importance the product in the marketplace . Every human being has their different opinion , feelings , thoughts , and emotion for an event this can be known with the help of sentiment analysis . The extraction of features and classification are the steps in sentiment analysis . This work is centered around implementing a fresh approach to sentiment analysis . The sentiment analysis techniques have various phases which include pre - processing , feature extraction and classification . The various machine learning algorithms for sentiment analysis are reviewed in terms of certain parameters . 

Text Summarization and Categorization for Scientific and Health - Related Data
The increasing amount of unstructured health - related data has created a need for intelligent processing , summarizing , and categorizing these data to extract knowledge from them . My research goal in this dissertation is to develop Natural Language Processing ( NLP ) and Information Retrieval ( IR ) methods for better processing and understanding health - related textual information to promote health care and wellbeing of individuals . 

An NLP - based cognitive system for disease status identification in electronic health records
This paper presents a natural language processing ( NLP ) based cognitive decision support system that automatically identifies the status of a disease from the clinical notes of a patient record . The system relies on IBM Watson Patient Record NLP analytics and supervised or semi - supervised learning techniques . It uses unstructured text in clinical notes , data from the structured part of a patient record , and disease control targets from the clinical guidelines . We evaluated the system using de - identified patient records of 414 hypertensive patients from a multi - specialty hospital system in the U . S . The experimental results show that , using supervised learning methods , our system can achieve an average 0 . 86 F1 - score in identifying disease status passages and average accuracy of 0 . 77 in classifying the status as controlled or not . To the best of our knowledge , this is the first system to automatically identify disease control status from clinical notes . 

Statistical models for topic segmentation
Most documents are about more than one subject , but many NLP and IR techniques implicitly assume documents have just one topic . We describe new clues that mark shifts to new topics , novel algorithms for identifying topic boundaries and the uses of such boundaries once identified . We report topic segmentation performance on several corpora as well as improvement on an IR task that benefits from good segmentation . 

A Rule - Based Annotation System to Extract Tajweed Rules from Quran
Quran Recitation relies on identifying and applying different Tajweed rules [ÞæÇÚÏ ÇáÊÌæíÏ] such as Muddud [ãÏæÏ] and Tanween [Êäæíä] in the Quran text . This research is aimed at providing a tool that automatically finds and annotates letters that embody Tajweed rules in Quran text . This field remains an open research area due to the lack of open source NLP tools that support the Arabic language . Applying Natural Language Processing ( NLP ) techniques on Quran text to extract Tajweed letters is considered an important Information Extraction ( IE ) step . This research explores the field of applying IE techniques on Quran text . Rule based IE techniques are well known to achieve optimal results . This research explores NLP techniques on Quranic text using GATE , an open source flexible NLP environment . GATE is employed for this research to build the application that processes un - annotated Quranic text corpus . The developed application is evaluated using the well known IE evaluation metrics precision and recall . By comparing the system ' s automatically annotated text with a gold standard ( i . e . Quran text ) . The system proved to be efficient by achieving 100 % precision and recall of the implemented Tajweed rules . 

Synthesizing user interfaces using functional reactive web abstractions
In this paper , we describe a novel approach to synthesizing web user interfaces ( UI ) from declarative specifications given in a domain - specific language ( DSL ) that can later be read and written by natural language processing ( NLP ) techniques via human language interaction . We make use of F #' s metaprogramming capabilities and our reactive forms library developed for WebSharper . Our ultimate goal is to enable end users to describe entire web application UIs and carry on a conversation with our system , describing incremental changes to apply on an ongoing specification . 

NLP - guided Video Thin - slicing for Automated Scoring of Non - Cognitive , Behavioral Performance Tasks
We propose a novel and practical approach to model non - cognitive , behavioral performance tasks , by using natural language processing ( NLP ) to guide the selection of “ thin - slices ” that are representative of holistic video performances and demonstrate its promise . 

Keyphrase Extraction as Topic Identification Using Term Frequency and Synonymous Term Grouping
Keyphrase are usually used as a representative of in the document . This paper presents a method to improve keyphrase extraction by using synonymous term grouping . Topic identification is recognised by term frequency for keyphrase extraction . We utilize a language model including linguistic patterns and language knowledge such as morphology syntax . The language model is a probability of word sequence . The focus unit is a pattern of noun adjective combination The proposed method consist of five processes i . e . preprocessing , candidate selection , semantic - based topic clustering , topic ranking , and keyphrase selection . This experimental result has precision value 54 . 44 from dataset of IEEE and 39 . 99 from dataset of SamEval . 

Clickbait Detection in Telugu : Overcoming NLP Challenges in Resource - Poor Languages using Benchmarked Techniques
Clickbait headlines have become a nudge in social media and news websites . The methods to identify clickbaits are largely being developed for English . There is a need for the same in other languages as well with the increase in the usage of social media platforms in different languages . In this work , we present an annotated clickbait dataset of 112 , 657 headlines that can be used for building an automated clickbait detection system for Telugu , a resource - poor language . Our contribution in this paper includes ( i ) generation of the latest pre - trained language models , including RoBERTa , ALBERT , and ELECTRA trained on a large Telugu corpora of 8 , 015 , 588 sentences that we had collected , ( ii ) data analysis and benchmarking the performance of different approaches ranging from hand - crafted features to state - of - the - art models . We show that the pre - trained language models trained on Telugu outperform the existing pre - trained models viz . BERT - Mulingual - Case [ 1 ] , XLM - MLM [ 2 ] , and XLM - R [ 3 ] on clickbait task . On a large Telugu clickbait dataset of 112 , 657 samples , the Light Gradient Boosted Machines ( LGBM ) model achieves an F1 - score of 0 . 94 for clickbait headlines . For Non - Clickbait headlines , F1 - score of 0 . 93 is obtained which is similar to that of Clickbait class . We open - source our dataset , pre - trained models , and code ‘ We show that the pre - trained language models trained on Telugu outperform the existing pre - trained models viz . BERT - Mulingual - Case [ 1 ] , XLM - MLM [ 2 ] , and XLM - R [ 3 ] on clickbait task . On a large Telugu clickbait dataset of 112 , 657 samples , the Light Gradient Boosted Machines ( LGBM ) model achieves an F1 - score of 0 . 94 for clickbait headlines . For Non - Clickbait headlines , F1 - score of 0 . 93 is obtained which is similar to that of Clickbait class . We open - source our dataset , pre - trained models , and code 1 1 https : / / github . com / subbareddy248 / Clickbait - ResourcesView less

Part - of - Speech Tagging by Latent Analogy
Part - of - speech tagging is often a critical first step in various speech and language processing tasks . High - accuracy taggers ( e . g . , based on conditional random fields ) rely on well chosen feature functions to ensure that important characteristics of the empirical training distribution are reflected in the trained model . This makes them vulnerable to any discrepancy between training and tagging corpora , and , in particular , accuracy is adversely affected by the presence of out - of - vocabulary words . This paper explores an alternative tagging strategy based on the principle of latent analogy , which was originally introduced in the context of a speech synthesis application . In this approach , locally optimal tag subsequences emerge automatically from an appropriate representation of global sentence - level information . This solution eliminates the need for feature engineering , while exploiting a broader context more conducive to word sense disambiguation . Empirical evidence suggests that , in practice , tagging by latent analogy is essentially competitive with conventional Markovian techniques , while benefiting from substantially less onerous training costs . This opens up the possibility that integration with such techniques may lead to further improvements in tagging accuracy . 

Build a Morphosyntaxically Annotated Amazigh Corpus
Language resources are important for those working on computational methods to analyze and study languages . These resources are needed to help advancing the research in fields such as natural language processing , machine learning , information retrieval and text analysis in general . We describe the creation of morphosyntactically annotated corpus for Amazigh language that currently lacks them . We illustrate our approach for creating this corpus , that is more expensive but of high quality , using crowdsourcing and manual effort with appropriately skilled human participants . Qualitative and quantitative evaluations of the resources are also presented . 

Genetic algorithms and fuzzy approach to Gaussian mixture model for speaker recognition
Gaussian mixture models ( GMMs ) is currently the most popular approach to speaker recognition . In speaker recognition , the major problem is how to generate a set of the GMM for identification purposes based upon the training data . Due to the hill - climbing characteristic of the expectation maximization ( EM ) method , it is sensitive to the initial model parameters and easy to lead to a sub - optimal model in practice . To resolve this problem , this paper proposes a new hybrid training method based on the global searching capability of genetic algorithms ( GA ) and the effectiveness of fuzzy approach to obtain GMMs with optimized model parameters . Experimental results based on PKU - SRSC database showed that this method could obtain more optimized GMMs and better results than the hybrid GA based on the EM re - estimation and the traditional EM method . 

A Feedback Generation System to Enhance Learning at Primary School
Feedback is an essential aid for the learning process of students . Unfortunately , with an increase in workloads and student numbers , teachers find it time - consuming and difficult to produce valuable and meaningful feedback for students . To maintain an environment in which students can continue to receive beneficial feedback , a feedback generation system in primary schools in Mauritius has been developed . This system is based on English subject where students can perform exercises in the form of multiple - choice questions and fill - in - the - blank questions where immediate feedback is provided to them after it is requested . Natural Language Processing ( NLP ) techniques has been used to generate feedback to students . The system also provides auto - correction and auto - grading of exercises . The aim of this system is to save teacher ' s time in providing immediate content - based and meaningful feedback to enhance student learning . The desktop application was tested in some primary schools and positive results were obtained from both instructors and students . 

Requirements Complexity Definition and Classification using Natural Language Processing
This paper addresses the risks and impacts from use of subjective assessment of requirements complexity in planning , estimation , and test phases of software development . The risks are to delivery schedule and product quality and impacts are significant functionalities planned for later cycles , compressed times for testing and late detection of high severity defects . A solution to objectively determine complexity and ranking of requirements that mitigates these risks and impacts is discussed . Applying natural language processing , key words are identified in the given set of requirements , and their weights measured to determine the complexity class distribution and ranking . This classification and the ranked requirements data is used in estimation , planning , and defining development and test execution sequence . Objectively determined complexity classes improves accuracy in test estimation and planning . The complexity - based test sequencing mitigates risk to product quality and execution schedule by early discovery of high severity defects providing the project team a sufficient time to recover . This also lowers the project risk to subsequent phases as ranked complex requirements are tested early and simpler requirements are tested at the end of the cycle . Results and benefits obtained in applying this solution to several projects are presented contrasting with projects that did not use this . 

A cross - lingual induction technique for German adverbial participles
We provide a detailed comparison of strategies for implementing medium - to - low frequency phenomena such as German adverbial participles in a broad - coverage , rule - based parsing system . We show that allowing for general adverb conversion of participles in the German LFG grammar seriously affects its overall performance , due to increased spurious ambiguity . As a solution , we present a corpus - based cross - lingual induction technique that detects adverbially used participles in parallel text . In a grammar - based evaluation , we show that the automatically induced resource appropriately restricts the adverb conversion to a limited class of participles , and improves parsing quantitatively as well as qualitatively . 

SCoEmbeddings : encoding sentiment information into contextualized embeddings for sentiment analysis
Contextualized word representations such as ELMo embeddings , can capture rich semantic information and achieve impressive performance in a wide variety of NLP tasks . However , as problems found in Word2Vec and GloVe , we found that ELMo word embeddings also lack enough sentiment information , which may affect sentiment classification performance . Inspired by previous embedding refinement method with sentiment lexicon , we propose an approach that combines contextualized embeddings ( ELMo ) of the pre - trained model with sentiment information of lexicon to generate sentiment - contextualized embeddings , called SCoEmbeddings . Experimental results show that our SCoEmbeddings achieve higher accuracy than ELMo embeddings , Word2Vec embeddings , and refined Word2Vec embeddings on the SST - 5 dataset . Meanwhile , we also visualize embeddings and weights of SCoEmbeddings , demonstrating the effectiveness of our SCoEmbeddings . 

Automatic acquisition of English topic signatures based on a second language
We present a novel approach for automatically acquiring English topic signatures . Given a particular concept , or word sense , a topic signature is a set of words that tend to co - occur with it . Topic signatures can be useful in a number of Natural Language Processing ( NLP ) applications , such as Word Sense Disambiguation ( WSD ) and Text Summarisation . Our method takes advantage of the different way in which word senses are lexicalised in English and Chinese , and also exploits the large amount of Chinese text available in corpora and on the Web . We evaluated the topic signatures on a WSD task , where we trained a second - order vector cooccurrence algorithm on standard WSD datasets , with promising results . 

Mining data from simulation of beer production
Data mining is a methodology for the extraction of knowledge from data , especially , knowledge relating to a problem that we want to solve . Data mining from simulation outputs is performed in this paper , it focuses on techniques for extracting knowledge from simulation outputs for beer production and optimizing devices and labors with certain target . We first set up one simulation model for beer production process and construct optimization objective . Then we set up one data mining model based on witness miner . The mining results show that the model is able to fund important information affecting target , make manager diagnose the bottlenecks of the beer production process , and help manager to make decisions rapidly under uncertainty . 

ReSCo - CC : Unsupervised Identification of Key Disinformation Sentences
Disinformation is often presented in long textual articles , especially when it relates to domains such as health , often seen in relation to COVID - 19 . These articles are typically observed to have a number of trustworthy sentences among which core disinformation sentences are scattered . In this paper , we propose a novel unsupervised task of identifying sentences containing key disinformation within a document that is known to be untrustworthy . We design a three - phase statistical NLP solution for the task which starts with embedding sentences within a bespoke feature space designed for the task . Sentences represented using those features are then clustered , following which the key sentences are identified through proximity scoring . We also curate a new dataset with sentence level disinformation scorings to aid evaluation for this task ; the dataset is being made publicly available to facilitate further research . Based on a comprehensive empirical evaluation against techniques from related tasks such as claim detection and summarization , as well as against simplified variants of our proposed approach , we illustrate that our method is able to identify core disinformation effectively . 

Relation Extraction in Clinical Text using NLP Based Regular Expressions
This paper is about relation extraction system for medical field related data . The major aim of our work is to retrieve different medical related data and to find out the relation between extracted medical data . Medical data usually contains a lot of unstructured or semi - structured data , by implementing methods like labeling and path similarity analysis we are able to convert it into a structured or classified form . Other methods that we use in our work are web scrapping , regular expressions , and part - of - speech tagging . All these methods are implemented in python . 

Third workshop on exploiting semantic annotations in information retrieval ( ESAIR ): CIKM 2010 workshop
There is an increasing amount of structure on the Web as a result of modern Web languages , user tagging and annotation , and emerging robust NLP tools . These meaningful , semantic , annotations hold the promise to significantly enhance information access , by enhancing the depth of analysis of today ' s systems . Currently , we have only started exploring the possibilities and only begin to understand how these valuable semantic cues can be put to fruitful use . Unleashing the potential of semantic annotations requires us to think outside the box , by combining the insights of natural language processing ( NLP ) to go beyond bags of words , the insights of databases ( DB ) to use structure efficiently even when aggregating over millions of records , the insights of information retrieval ( IR ) in effective goal - directed search and evaluation , and the insights of knowledge management ( KM ) to get grips on the greater whole . The Workshop aims to bring together researchers from these different disciplines and work together on one of the greatest challenges in the years to come . The desired result of the workshop will be concrete insight into the potential of semantic annotations , and in concrete steps to take this research forward ; synchronize related research happening in NLP , DB , IR , and KM , in ways that combine the strengths of each discipline ; and have a lively , interactive workshop were everyone contributes and that inspires attendees to think " outside the box " . 

A phrase structure grammar of the Arabic language
A lot of work has been done in the field of natural language processing ( NLP ) for ARABIC . Few researchers have tried hard to apply on ARABIC the methods of computational linguistics , as for example , Definite Clause Grammar ( DCG ) [ 1 ] and Augmented Transition Networks ( ATN ) [ 2 ] . Because there is not a modern linguistic model for ARABIC grammar within the frame of computational linguistics , the results achieved are not comparable with that achieved on ENGLISH . In this paper , we represent the phrase structures covering ARABIC . They are completely different from ENGLISH . 

Antelogue : pronoun resolution for text and dialogue
Antelogue is a pronoun resolution prototype designed to be released as off - the - shelf software to be used autonomously or integrated with larger anaphora resolution or other NLP systems . It has modules to handle pronouns in both text and dialogue . In Antelogue , the problem of pronoun resolution is addressed as a two - step process : a ) acquiring information about properties of words and the entities they represent and b ) determining an algorithm that utilizes these features to make resolution decisions . A hybrid approach is implemented that combines known statistical and machine learning techniques for feature acquisition and a symbolic algorithm for resolution . 

Assisted curricula design based on generation of domain ontologies and the use of NLP techniques
The following work proposes an approach that allows educators to manage curricula of different academic disciplines , with the support of recommendations and suggestions of contents and educational materials . The recommendations will be process through by domain ontologies , constructed from digital texts and the use of natural language process techniques . These recommendations will also support students during the learning process on specific academic areas . 

Text summarization using enhanced MMR technique
Now a day when huge amount of documents and web contents are available , so reading of full content is somewhat difficult . Summarization is a way to give abstract form of large document so that the moral of the document can be communicated easily . Current research in automatic summarization is dominated by some effective , yet naive approaches : summarization through extraction , summarization through Abstraction and multi - document summarization . These techniques are used to building a summary of a document . Although there are a number of techniques implemented for the summarization of text for the single document or for the online web data or for any language . Here in this paper we are implemented an efficient technique for text summarization to reduce the computational cost and time and also the storage capacity . 

A Hybrid Trust Model for Cloud Service Provider Selection with NLP Support and Malicious User Feedback Filtering
With the rapid development of Cloud Computing and the increase in Cloud Service Providers ( Csp ) , there is an immense rise in the cloud services provided to the users . With a mass choice of CSPs , trust has become a major concern for both CSPs and the Cloud Service Users ( CSU) . It plays an important role in evaluating cloud services and helps in the robust selection of CSPs without compromising security , performance , and privacy . Research has been done in this field that talks about one - way trust between CSPs and CSUs but little to no work has been done which includes bi - directional trust in the same model . We propose a Trust model for cloud service selection that considers the bi - directional trust i . e . , trust based on user requirements and user feedback , making it more reliable in real - life scenarios . An NLP approach that extracts requirement values from both text and numerical input and a Malicious Feedback Filtering Mechanism are proposed . 

