relatedness curves for acquiring paraphrases
in this paper we investigate methods for computing similarity of two phrases based on their relatedness scores across all ranks k in a svd approximation of a phrase/term co-occurrence matrix . we confirm the major observations made in previous work and our preliminary experiments indicate that these methods can lead to reliable similarity scores which in turn can be used for the task of paraphrasing .

learning non-concatenative morphology
recent work in computational psycholinguistics shows that morpheme lexica can be acquired in an unsupervised manner from a corpus of words by selecting the lexicon that best balances productivity and reuse ( e.g . goldwater et al . ( 2009 ) and others ) . in this paper , we extend such work to the problem of acquiring non-concatenative morphology , proposing a simple model of morphology that can handle both concatenative and non-concatenative morphology and applying bayesian inference on two datasets of arabic and english verbs to acquire lexica . we show that our approach successfully extracts the non-contiguous triliteral root from arabic verb stems .

large-scale paraphrasing for natural language understanding
we examine the application of data-driven paraphrasing to natural language understanding . we leverage bilingual parallel corpora to extract a large collection of syntactic paraphrase pairs , and introduce an adaptation scheme that allows us to tackle a variety of text transformation tasks via paraphrasing . we evaluate our system on the sentence compression task . further , we use distributional similarity measures based on context vectors derived from large monolingual corpora to annotate our paraphrases with an orthogonal source of information . this yields significant improvements in our compression systems output quality , achieving state-of-the-art performance . finally , we propose a refinement of our paraphrases by classifying them into natural logic entailment relations . by extending the synchronous parsing paradigm towards these entailment relations , we will enable our system to perform recognition of textual entailment .

applying coreference to improve name recognition
we present a novel method of applying the results of coreference resolution to improve name recognition for chinese . we consider first some methods for gauging the confidence of individual tags assigned by a statistical name tagger . for names with low confidence , we show how these names can be filtered using coreference features to improve accuracy . in addition , we present rules which use coreference information to correct some name tagging errors . finally , we show how these gains can be magnified by clustering documents and using cross-document coreference in these clusters . these combined methods yield an absolute improvement of about 3.1 % in tagger f score .

subword and spatiotemporal models for identifying actionable information in haitian kreyol
crisis-affected populations are often able to maintain digital communications but in a sudden-onset crisis any aid organizations will have the least free resources to process such communications . information that aid agencies can actually act on , actionable information , will be sparse so there is great potential to ( semi ) automatically identify actionable communications . however , there are hurdles as the languages spoken will often be underresourced , have orthographic variation , and the precise definition of actionable will be response-specific and evolving . we present a novel system that addresses this , drawing on 40,000 emergency text messages sent in haiti following the january 12 , 2010 earthquake , predominantly in haitian kreyol . we show that keyword/ngram-based models using streaming maxent achieve up to f=0.21 accuracy . further , we find current state-ofthe-art subword models increase this substantially to f=0.33 accuracy , while modeling the spatial , temporal , topic and source contexts of the messages can increase this to a very accurate f=0.86 over direct text messages and f=0.90-0.97 over social media , making it a viable strategy for message prioritization .

a new e-learning paradigm through annotating operations
this paper proposes a new e-learning paradigm which enables the user to type in arbitrary sentences . the current nlp technologies , however , are not matured enough to perform full-automatic semantic or discourse analysis . thus , we take a different approach ; an instructor corrects the contents and its correction is embedded into the contents in an xml format called keml . the key/mouse operations of the user are also embedded as annotations . thus , the contents can be incomplete at the initial stage and become solid gradually as being utilized . we have implemented an e-learning system of group discussion for a foreign language class , which is demonstrated at the workshop .

exploiting rich syntactic information for relation extraction from biomedical articles
this paper proposes a ternary relation extraction method primarily based on rich syntactic information . we identify protein-organism-location relations in the text of biomedical articles . different kernel functions are used with an svm learner to integrate two sources of information from syntactic parse trees : ( i ) a large number of syntactic features that have been shown useful for semantic role labeling ( srl ) and applied here to the relation extraction task , and ( ii ) features from the entire parse tree using a tree kernel . our experiments show that the use of rich syntactic features significantly outperforms shallow word-based features . the best accuracy is obtained by combining srl features with tree kernels .

toward a totally unsupervised , language-independent method for the syllabification of written texts
unsupervised algorithms for the induction of linguistic knowledge should at best require as few basic assumptions as possible and at the same time in principle yield good results for any language . however , most of the time such algorithms are only tested on a few ( closely related ) languages . in this paper , an approach is presented that takes into account typological knowledge in order to induce syllabic divisions in a fully automatic manner based on reasonably-sized written texts . our approach is able to account for syllable structures of languages where other approaches would fail , thereby raising the question whether computational methods can really be claimed to be language-universal when they are not tested on the variety of structures that are found in the languages of the world .

paraphrase identification as probabilistic quasi-synchronous recognition
we present a novel approach to deciding whether two sentences hold a paraphrase relationship . we employ a generative model that generates a paraphrase of a given sentence , and we use probabilistic inference to reason about whether two sentences share the paraphrase relationship . the model cleanly incorporates both syntax and lexical semantics using quasi-synchronous dependency grammars ( smith and eisner , 2006 ) . furthermore , using a product of experts ( hinton , 2002 ) , we combine the model with a complementary logistic regression model based on state-of-the-art lexical overlap features . we evaluate our models on the task of distinguishing true paraphrase pairs from false ones on a standard corpus , giving competitive state-of-the-art performance .

its relation to temporal information
in this work we present an annotation framework to capture causality between events , inspired by timeml , and a language resource covering both temporal and causal relations . this data set is then used to build an automatic extraction system for causal signals and causal links between given event pairs . the evaluation and analysis of the systems performance provides an insight into explicit causality in text and the connection between temporal and causal relations .

learning-based named entity recognition for morphologically-rich ,
named entity recognition for morphologically rich , case-insensitive languages , including the majority of semitic languages , iranian languages , and indian languages , is inherently more difficult than its english counterpart . worse still , progress on machine learning approaches to named entity recognition for many of these languages is currently hampered by the scarcity of annotated data and the lack of an accurate part-of-speech tagger . while it is possible to rely on manually-constructed gazetteers to combat data scarcity , this gazetteer-centric approach has the potential weakness of creating irreproducible results , since these name lists are not publicly available in general . motivated in part by this concern , we present a learning-based named entity recognizer that does not rely on manually-constructed gazetteers , using bengali as our representative resource-scarce , morphologicallyrich language . our recognizer achieves a relative improvement of 7.5 % in fmeasure over a baseline recognizer . improvements arise from ( 1 ) using induced affixes , ( 2 ) extracting information from online lexical databases , and ( 3 ) jointly modeling part-of-speech tagging and named entity recognition .

a fast finite-state relaxation method for enforcing global constraints on sequence decoding
we describe finite-state constraint relaxation , a method for applying global constraints , expressed as automata , to sequence model decoding . we present algorithms for both hard constraints and binary soft constraints . on the conll-2004 semantic role labeling task , we report a speedup of at least 16x over a previous method that used integer linear programming .

medeval a swedish medical test collection
medeval is a swedish medical test collection where assessments have been made , not only for topical relevance , but also for target reader group : doctors or patients . the user of the test collection can choose if s/he wishes to search in the doctors or the patients scenarios where the topical relevance assessments have been adjusted with consideration to user group , or to search in a scenario which regards only topical relevance . medeval makes it possible to compare the effectiveness of search terms when it comes to retrieving documents aimed at the different user groups . medeval is also the first medical swedish test collection .

applying alternating structure optimization to word sense disambiguation rie kubota ando
this paper presents a new application of the recently proposed machine learning method alternating structure optimization ( aso ) , to word sense disambiguation ( wsd ) . given a set of wsd problems and their respective labeled examples , we seek to improve overall performance on that set by using all the labeled examples ( irrespective of target words ) for the entire set in learning a disambiguator for each individual problem . thus , in effect , on each individual problem ( e.g. , disambiguation of art ) we benefit from training examples for other problems ( e.g. , disambiguation of bar , canal , and so forth ) . we empirically study the effective use of aso for this purpose in the multitask and semi-supervised learning configurations . our performance results rival or exceed those of the previous best systems on several senseval lexical sample task data sets .

text chunking using regularized
many machine learning methods have recently been applied to natural language processing tasks . among them , the winnow algorithm has been argued to be particularly suitable for nlp problems , due to its robustness to irrelevant features . however in theory , winnow may not converge for nonseparable data . to remedy this problem , a modification called regularized winnow has been proposed . in this paper , we apply this new method to text chunking . we show that this method achieves state of the art performance with significantly less computation than previous approaches .

investigating the contribution of distributional semantic information for dialogue act classification
this paper presents a series of experiments in applying compositional distributional semantic models to dialogue act classification . in contrast to the widely used bag-ofwords approach , we build the meaning of an utterance from its parts by composing the distributional word vectors using vector addition and multiplication . we investigate the contribution of word sequence , dialogue act sequence , and distributional information to the performance , and compare with the current state of the art approaches . our experiment suggests that that distributional information is useful for dialogue act tagging but that simple models of compositionality fail to capture crucial information from word and utterance sequence ; more advanced approaches ( e.g . sequence- or grammar-driven , such as categorical , word vector composition ) are required .

measuring annotator agreement in a complex hierarchical dialogue act
we present a first analysis of interannotator agreement for the dit++ tagset of dialogue acts , a comprehensive , layered , multidimensional set of 86 tags . within a dimension or a layer , subsets of tags are often hierarchically organised . we argue that especially for such highly structured annotation schemes the well-known kappa statistic is not an adequate measure of inter-annotator agreement . instead , we propose a statistic that takes the structural properties of the tagset into account , and we discuss the application of this statistic in an annotation experiment . the experiment shows promising agreement scores for most dimensions in the tagset and provides useful insights into the usability of the annotation scheme , but also indicates that several additional factors influence annotator agreement . we finally suggest that the proposed approach for measuring agreement per dimension can be a good basis for measuring annotator agreement over the dimensions of a multidimensional annotation scheme .

automated collocation suggestion for japanese second language
this study addresses issues of japanese language learning concerning word combinations ( collocations ) . japanese learners may be able to construct grammatically correct sentences , however , these may sound unnatural . in this work , we analyze correct word combinations using different collocation measures and word similarity methods . while other methods use well-formed text , our approach makes use of a large japanese language learner corpus for generating collocation candidates , in order to build a system that is more sensitive to constructions that are difficult for learners . our results show that we get better results compared to other methods that use only wellformed text .

parsing speech repair without specialized grammar symbols
this paper describes a parsing model for speech with repairs that makes a clear separation between linguistically meaningful symbols in the grammar and operations specific to speech repair in the operation of the parser . this system builds a model of how unfinished constituents in speech repairs are likely to finish , and finishes them probabilistically with placeholder structure . these modified repair constituents and the restarted replacement constituent are then recognized together in the same way that two coordinated phrases of the same type are recognized .

jointly or separately : which is better for parsing heterogeneous dependencies
for languages such as english , several constituent-to-dependency conversion schemes are proposed to construct corpora for dependency parsing . it is hard to determine which scheme is better because they reflect different views of dependency analysis . we usually obtain dependency parsers of different schemes by training with the specific corpus separately . it neglects the correlations between these schemes , which can potentially benefit the parsers . in this paper , we study how these correlations influence final dependency parsing performances , by proposing a joint model which can make full use of the correlations between heterogeneous dependencies , and finally we can answer the following question : parsing heterogeneous dependencies jointly or separately , which is better we conduct experiments with two different schemes on the penn treebank and the chinese penn treebank respectively , arriving at the same conclusion that jointly parsing heterogeneous dependencies can give improved performances for both schemes over the individual models .

dude : a dialogue and understanding development environment , mapping business process models to information state update dialogue
we demonstrate a new development environment1 information state update dialogue systems which allows non-expert developers to produce complete spoken dialogue systems based only on a business process model ( bpm ) describing their application ( e.g . banking , cinema booking , shopping , restaurant information ) . the environment includes automatic generation of grammatical framework ( gf ) grammars for robust interpretation of spontaneous speech , and uses application databases to generate lexical entries and grammar rules . the gf grammar is compiled to an atk or nuance language model for speech recognition . the demonstration system allows users to create and modify spoken dialogue systems , starting with a definition of a business processmodel and ending with a working system . this paper describes the environment , its main components , and some of the research issues involved in its development .

a connectionist approach
this study presents a novel computational approach to the analysis of unaccusative/unergative distinction in turkish by employing feed-forward artificial neural networks with a backpropagation algorithm . the findings of the study reveal correspondences between semantic notions and syntactic manifestations of unaccusative/unergative distinction in this language , thus presenting a computational analysis of the distinction at the syntax/semantics interface . the approach is applicable to other languages , particularly the ones which lack an explicit diagnostic such as auxiliary selection but has a number of diagnostics instead .

improving readability of swedish electronic health records through lexical simplification : first results
this paper describes part of an ongoing effort to improve the readability of swedish electronic health records ( ehrs ) . an ehr contains systematic documentation of a single patients medical history across time , entered by healthcare professionals with the purpose of enabling safe and informed care . linguistically , medical records exemplify a highly specialised domain , which can be superficially characterised as having telegraphic sentences involving displaced or missing words , abundant abbreviations , spelling variations including misspellings , and terminology . we report results on lexical simplification of swedish ehrs , by which we mean detecting the unknown , out-ofdictionary words and trying to resolve them either as compounded known words , abbreviations or misspellings .

ballgame : a corpus for computational semantics
in this paper , we describe the baseball announcers language linked with general annotation of meaningful events ( ballgame ) project a text corpus for research in computional semantics . we collected pitch-by-pitch event data for a sample of baseball games and used this data to build an annotated corpus composed of transcripts of radio broadcasts of these games . our annotation links text from the broadcast to events in a formal representation of the semantics of the baseball game . we describe our corpus model , the annotation tool used to create the corpus , and conclude by discussing applications of this corpus in semantics research and natural language processing .

conditional random fields for word hyphenation
finding allowable places in words to insert hyphens is an important practical problem . the algorithm that is used most often nowadays has remained essentially unchanged for 25 years . this method is the tex hyphenation algorithm of knuth and liang . we present here a hyphenation method that is clearly more accurate . the new method is an application of conditional random fields . we create new training sets for english and dutch from the celex european lexical resource , and achieve error rates for english of less than 0.1 % for correctly allowed hyphens , and less than 0.01 % for dutch . experiments show that both the knuth/liang method and a leading current commercial alternative have error rates several times higher for both languages .

subtree mining for relation extraction from wikipedia
in this study , we address the problem of extracting relations between entities from wikipedias english articles . our proposed method first anchors the appearance of entities in wikipedias articles using neither named entity recognizer ( ner ) nor coreference resolution tool . it then classifies the relationships between entity pairs using svm with features extracted from the web structure and subtrees mined from the syntactic structure of text . we evaluate our method on manually annotated data from actual wikipedia articles .

aligning english strings with abstract meaning representation graphs
we align pairs of english sentences and corresponding abstract meaning representations ( amr ) , at the token level . such alignments will be useful for downstream extraction of semantic interpretation and generation rules . our method involves linearizing amr structures and performing symmetrized em training . we obtain 86.5 % and 83.1 % alignment f score on development and test sets .

accurate language identification of twitter messages
we present an evaluation of off-theshelf language identification systems as applied to microblog messages from twitter . a key challenge is the lack of an adequate corpus of messages annotated for language that reflects the linguistic diversity present on twitter . we overcome this through a mostly-automated approach to gathering language-labeled twitter messages for evaluating language identification . we present the method to construct this dataset , as well as empirical results over existing datasets and off-theshelf language identifiers . we also test techniques that have been proposed in the literature to boost language identification performance over twitter messages . we find that simple voting over three specific systems consistently outperforms any specific system , and achieves state-of-the-art accuracy on the task .

are very large context-free grammars tractable pierre boullier & benot sagot
in this paper , we present a method which , in practice , allows to use parsers for languages defined by very large context-free grammars ( over a million symbol occurrences ) . the idea is to split the parsing process in two passes . a first pass computes a sub-grammar which is a specialized part of the large grammar selected by the input text and various filtering strategies . the second pass is a traditional parser which works with the subgrammar and the input text . this approach is validated by practical experiments performed on a earley-like parser running on a test set with two large context-free grammars .

rule-based information extraction is dead ! long live rule-based information extraction systems !
the rise of big data analytics over unstructured text has led to renewed interest in information extraction ( ie ) . we surveyed the landscape of ie technologies and identified a major disconnect between industry and academia : while rule-based ie dominates the commercial world , it is widely regarded as dead-end technology by the academia . we believe the disconnect stems from the way in which the two communities measure the benefits and costs of ie , as well as academias perception that rulebased ie is devoid of research challenges . we make a case for the importance of rule-based ie to industry practitioners . we then lay out a research agenda in advancing the state-of-theart in rule-based ie systems which we believe has the potential to bridge the gap between academic research and industry practice .

detecting speech repairs incrementally using a noisy channel approach
unrehearsed spoken language often contains disfluencies . in order to correctly interpret a spoken utterance , any such disfluencies must be identified and removed or otherwise dealt with . operating on transcripts of speech which contain disfluencies , our particular focus here is the identification and correction of speech repairs using a noisy channel model . our aim is to develop a high-accuracy mechanism that can identify speech repairs in an incremental fashion , as the utterance is processed word-by-word . we also address the issue of the evaluation of such incremental systems . we propose a novel approach to evaluation , which evaluates performance in detecting and correcting disfluencies incrementally , rather than only assessing performance once the processing of an utterance is complete . this demonstrates some shortcomings in our basic incremental model , and so we then demonstrate a technique that improves performance on the detection of disfluencies as they happen .

a perspective-based approach for solving textual entailment recognition
the textual entailment recognition system that we discuss in this paper represents a perspective-based approach composed of two modules that analyze text-hypothesis pairs from a strictly lexical and syntactic perspectives , respectively . we attempt to prove that the textual entailment recognition task can be overcome by performing individual analysis that acknowledges us of the maximum amount of information that each single perspective can provide . we compare this approach with the system we presented in the previous edition of pascal recognising textual entailment challenge , obtaining an accuracy rate 17.98 % higher .

the szeged corpus : a pos tagged and syntactically annotated hungarian natural language corpus
the szeged corpus is a manually annotated natural language corpus currently comprising 1.2 million word entries , 145 thousand different word forms , and an additional 225 thousand punctuation marks . with this , it is the largest manually processed hungarian textual database that serves as a reference material for research in natural language processing as well as a learning database for machine learning algorithms and other software applications . language processing of the corpus texts so far included morpho-syntactic analysis , pos tagging and shallow syntactic parsing . semantic information was also added to a preselected section of the corpus to support automated information extraction . the present state of the szeged corpus is the result of three national projects and the cooperation of the university of szeged , department of informatics , morphologic ltd. budapest , and the research institute for linguistics at the hungarian academy of sciences . corpus texts have gone through different phases of natural language processing ( nlp ) and analysis . extensive and accurate manual annotation of the texts , incorporating over 124 person-months of manual work , is a great value of the corpus .

tree-cut and a lexicon based on systematic polysemy
this paper describes a lexicon organized around systematic polysemy : a set of word senses that are related in systematic and predictable ways . the lexicon is derived by a fully automatic extraction method which utilizes a clustering technique called tree-cut . we compare our lexicon to wordnet cousins , and the inter-annotator disagreement observed between wordnet semcor and dso corpora .

towards a multidimensional semantics of discourse markers in spoken dialogue
the literature contains a wealth of theoretical and empirical analyses of discourse marker functions in human communication . some of these studies address the phenomenon that discourse markers are often multifunctional in a given context , but do not study this in systematic and formal ways . in this paper we show that the use of multiple dimensions in distinguishing and annotating semantic units supports a more accurate analysis of the meaning of discourse markers . we present an empirically-based analysis of the semantic functions of discourse markers in dialogue . we demonstrate that the multiple functions , which a discourse marker may have , are automatically recognizable from utterance surface-features using machine-learning techniques .

lattice parsing to integrate speech recognition and rule-based machine
in this paper , we present a novel approach to integrate speech recognition and rulebased machine translation by lattice parsing . the presented approach is hybrid in two senses . first , it combines structural and statistical methods for language modeling task . second , it employs a chart parser which utilizes manually created syntax rules in addition to scores obtained after statistical processing during speech recognition . the employed chart parser is a unification-based active chart parser . it can parse word graphs by using a mixed strategy instead of being bottom-up or top-down only . the results are reported based on word error rate on the nist hub-1 word-lattices . the presented approach is implemented and compared with other syntactic language modeling techniques .

on statistical parsing of french with supervised and semi-supervised strategies
this paper reports results on grammatical induction for french . we investigate how to best train a parser on the french treebank ( abeill et al , 2003 ) , viewing the task as a trade-off between generalizability and interpretability . we compare , for french , a supervised lexicalized parsing algorithm with a semi-supervised unlexicalized algorithm ( petrov et al , 2006 ) along the lines of ( crabb and candito , 2008 ) . we report the best results known to us on french statistical parsing , that we obtained with the semi-supervised learning algorithm . the reported experiments can give insights for the task of grammatical learning for a morphologically-rich language , with a relatively limited amount of training data , annotated with a rather flat structure .

towards semi-supervised classification of discourse relations using
two of the main corpora available for training discourse relation classifiers are the rst discourse treebank ( rst-dt ) and the penn discourse treebank ( pdtb ) , which are both based on the wall street journal corpus . most recent work using discourse relation classifiers have employed fully-supervised methods on these corpora . however , certain discourse relations have little labeled data , causing low classification performance for their associated classes . in this paper , we attempt to tackle this problem by employing a semi-supervised method for discourse relation classification . the proposed method is based on the analysis of feature cooccurrences in unlabeled data . this information is then used as a basis to extend the feature vectors during training . the proposed method is evaluated on both rst-dt and pdtb , where it significantly outperformed baseline classifiers . we believe that the proposed method is a first step towards improving classification performance , particularly for discourse relations lacking annotated data .

narrative schema as world knowledge for coreference resolution
in this paper we describe the system with which we participated in the conll-2011 shared task on modelling coreference . our system is based on a cluster-ranking model proposed by rahman and ng ( 2009 ) , with novel semantic features based on recent research on narrative event schema ( chambers and jurafsky , 2009 ) . we demonstrate some improvements over the baseline when using schema information , although the effect varied between the metrics used . we also explore the impact of various features on our systems performance .

a localized prediction model for statistical machine translation
in this paper , we present a novel training method for a localized phrase-based prediction model for statistical machine translation ( smt ) . the model predicts blocks with orientation to handle local phrase re-ordering . we use a maximum likelihood criterion to train a log-linear block bigram model which uses realvalued features ( e.g . a language model score ) as well as binary features based on the block identities themselves , e.g . block bigram features . our training algorithm can easily handle millions of features . the best system obtains a % improvement over the baseline on a standard arabic-english translation task .

automatic compilation of travel information from automatically identified travel blogs
in this paper , we propose a method for compiling travel information automatically . for the compilation , we focus on travel blogs , which are defined as travel journals written by bloggers in diary form . we consider that travel blogs are a useful information source for obtaining travel information , because many bloggers ' travel experiences are written in this form . therefore , we identified travel blogs in a blog database and extracted travel information from them . we have confirmed the effectiveness of our method by experiment . for the identification of travel blogs , we obtained scores of 38.1 % for recall and 86.7 % for precision . in the extraction of travel information from travel blogs , we obtained 74.0 % for precision at the top 100 extracted local products , thereby confirming that travel blogs are a useful source of travel information .

event extraction in a plot advice agent
in this paper we present how the automatic extraction of events from text can be used to both classify narrative texts according to plot quality and produce advice in an interactive learning environment intended to help students with story writing . we focus on the story rewriting task , in which an exemplar story is read to the students and the students rewrite the story in their own words . the system automatically extracts events from the raw text , formalized as a sequence of temporally ordered predicate-arguments . these events are given to a machine-learner that produces a coarse-grained rating of the story . the results of the machine-learner and the extracted events are then used to generate fine-grained advice for the students .

legal docket-entry classification : where machine learning stumbles
we investigate the problem of binary text classification in the domain of legal docket entries . this work presents an illustrative instance of a domain-specific problem where the stateof-the-art machine learning ( ml ) classifiers such as svms are inadequate . our investigation into the reasons for the failure of these classifiers revealed two types of prominent errors which we call conjunctive and disjunctive errors . we developed simple heuristics to address one of these error types and improve the performance of the svms . based on the intuition gained from our experiments , we also developed a simple propositional logic based classifier using hand-labeled features , that addresses both types of errors simultaneously . we show that this new , but simple , approach outperforms all existing state-of-the-art ml models , with statistically significant gains . we hope this work serves as a motivating example of the need to build more expressive classifiers beyond the standard model classes , and to address text classification problems in such nontraditional domains .

semi-automatic construction of cross-period thesaurus
cross-period ( diachronic ) thesaurus construction aims to enable potential users to search for modern terms and obtain semantically related terms from earlier periods in history . this is a complex task not previously addressed computationally . in this paper we introduce a semi-automatic iterative query expansion ( qe ) scheme for supporting cross-period thesaurus construction . we demonstrate the empirical benefit of our scheme for a jewish crossperiod thesaurus and evaluate its impact on recall and on the effectiveness of lexicographer manual effort .

bitext-based resolution of german subject-object ambiguities
we present a method for disambiguating syntactic subjects from syntactic objects ( a frequent ambiguity ) in german sentences taken from an english-german bitext . we exploit the fact that subject and object are usually easily determined in english . we show that a simple method disambiguates some subjectobject ambiguities in german , while making few errors . we view this procedure as the first step in automatically acquiring ( mostly ) correct labeled data . we also evaluate using it to improve a state of the art statistical parser .

from gemini to diagen : improving development of speech dialogues for embedded systems natural language systems division speech services hamburg germany ulm germany
in this paper diagen is presented , a tool that provides support in generating code for embedded dialogue applications . by aid of it , the dialogue development process is speeded up considerably . at the same time it is guaranteed that only well-formed and well-defined constructs are used . having had its roots in the eu-funded project gemini , fundamental changes were necessary to adopt it to the requirements of the application environment . additionally within this paper the basics of embedded speech dialogue systems are covered .

efforts on machine learning over human-mediated translation edit rate
in this paper we describe experiments on predicting hter , as part of our submission in the shared task on quality estimation , in the frame of the 9th workshop on statistical machine translation . in our experiment we check whether it is possible to achieve better hter prediction by training four individual regression models for each one of the edit types ( deletions , insertions , substitutions , shifts ) , however no improvements were yielded . we also had no improvements when investigating the possibility of adding more data from other non-minimally post-edited and freely translated datasets . best hter prediction was achieved by adding deduplicated wmt13 data and additional features such as ( a ) rule-based language corrections ( language tool ) ( b ) pcfg parsing statistics and count of tree labels ( c ) position statistics of parsing labels ( d ) position statistics of tri-grams with low probability .

a cascaded machine learning approach to interpreting temporal expressions
a new architecture for identifying and interpreting temporal expressions is introduced , in which the large set of complex hand-crafted rules standard in systems for this task is replaced by a series of machine learned classifiers and a much smaller set of context-independent semantic composition rules . experiments with the tern 2004 data set demonstrate that overall system performance is comparable to the state-of-the-art , and that normalization performance is particularly good .

efficient optimization for bilingual sentence alignment based on linear regression educational testing service
this paper presents a study on optimizing sentence pair alignment scores of a bilingual sentence alignment module . five candidate scores based on perplexity and sentence length are introduced and tested . then a linear regression model based on those candidates is proposed and trained to predict sentence pairs alignment quality scores solicited from human subjects . experiments are carried out on data automatically collected from internet . the correlation between the scores generated by the linear regression model and the scores from human subjects is in the range of the inter-subject agreement score correlations . pearson 's correlation ranges from 0.53 up to 0.72 in our experiments .

morphological disambiguation of hebrew : a case study in classifier combination
morphological analysis and disambiguation are crucial stages in a variety of natural language processing applications , especially when languages with complex morphology are concerned . we present a system which disambiguates the output of a morphological analyzer for hebrew . it consists of several simple classifiers and a module which combines them under linguistically motivated constraints . we investigate a number of techniques for combining the predictions of the classifiers . our best result , 91.44 % accuracy , reflects a 25 % reduction in error rate compared with the previous state of the art .

rtv : tree kernels for thematic role classification
we present a simple , two-steps supervised strategy for the identification and classification of thematic roles in natural language texts . we employ no external source of information but automatic parse trees of the input sentences . we use a few attribute-value features and tree kernel functions applied to specialized structured features . the resulting system has an f1 of 75.44 on the semeval2007 closed task on semantic role labeling .

revisions that improve cohesion in multi-document summaries : a preliminary study
extractive summaries produced from multiple source documents suffer from an array of problems with respect to text cohesion . in this preliminary study , we seek to understand what problems occur in such summaries and how often . we present an analysis of a small corpus of manually revised summaries and discuss the feasibility of making such repairs automatically . additionally , we present a taxonomy of the problems that occur in the corpus , as well as the operators which , when applied to the summaries , can address these concerns . this study represents a first step toward identifying and automating revision operators that could work with current summarization systems in order to repair cohesion problems in multidocument summaries .

early deletion of fillers in processing conversational speech
this paper evaluates the benefit of deleting fillers ( e.g . you know , like ) early in parsing conversational speech . readability studies have shown that disfluencies ( fillers and speech repairs ) may be deleted from transcripts without compromising meaning ( jones et al , 2003 ) , and deleting repairs prior to parsing has been shown to improve its accuracy ( charniak and johnson , 2001 ) . we explore whether this strategy of early deletion is also beneficial with regard to fillers . reported experiments measure the effect of early deletion under in-domain and out-of-domain parser training conditions using a state-of-the-art parser ( charniak , 2000 ) . while early deletion is found to yield only modest benefit for in-domain parsing , significant improvement is achieved for out-of-domain adaptation . this suggests a potentially broader role for disfluency modeling in adapting text-based tools for processing conversational speech .

dynamically integrating cross-domain translation memory into phrase-based machine translation during decoding
our previous work focuses on combining translation memory ( tm ) and statistical machine translation ( smt ) when the tm database and the smt training set are the same . however , the tm database will deviate from the smt training set in the real task when time goes by . in this work , we concentrate on the task when the tm database and the smt training set are different and even from different domains . firstly , we dynamically merge the matched tm phrase-pairs into the smt phrase table to meet the real application . secondly , we propose an improved integrated model to distinguish the original and the newly-added phrase-pairs . thirdly , a simple but effective tm adaptation method is adopted to favor the consistent translations in cross-domain test . our experiments have shown that merging the tm phrasepairs achieves significant improvements . furthermore , the proposed approaches are significantly better than the tm , the smt and previous integration works for both in-domain and cross-domain tests .

english to indian languages machine transliteration system at
this paper reports about our work in the news 2010 shared task on transliteration generation held as part of acl 2010. one standard run and two non-standard runs were submitted for english to hindi and bengali transliteration while one standard and one nonstandard run were submitted for kannada and tamil . the transliteration systems are based on orthographic rules and phoneme based technology . the system has been trained on the news 2010 shared task on transliteration generation datasets . for the standard run , the system demonstrated mean f-score values of 0.818 for bengali , 0.714 for hindi , 0.663 for kannada and 0.563 for tamil . the reported mean f-score values of non-standard runs are 0.845 and 0.875 for bengali non-standard run1 and 2 , 0.752 and 0.739 for hindi nonstandard run-1 and 2 , 0.662 for kannada nonstandard run-1 and 0.760 for tamil nonstandard run-1 . non-standard run-2 for bengali has achieved the highest score among all the submitted runs . hindi non-standard run-1 and run-2 runs are ranked as the 5th and 6th among all submitted runs .

prominence and phrasing in spoken discourse processing
we review psycholinguistic research on the use of intonation in dialogue , focusing on our own recent work . in experiments using complex real-world tasks and nave speakers and listeners , we show that speakers reliably specific prosodic cues to signal their intensions , and that listeners use these cues to recognize syntactic and pragmatic aspects of discourse meaning .

sub-sentence division for tree-based machine translation
tree-based statistical machine translation models have made significant progress in recent years , especially when replacing 1-best trees with packed forests . however , as the parsing accuracy usually goes down dramatically with the increase of sentence length , translating long sentences often takes long time and only produces degenerate translations . we propose a new method named subsentence division that reduces the decoding time and improves the translation quality for tree-based translation . our approach divides long sentences into several sub-sentences by exploiting tree structures . large-scale experiments on the nist 2008 chinese-toenglish test set show that our approach achieves an absolute improvement of 1.1 bleu points over the baseline system in 50 % less time .

semi-supervised algorithm for human-computer dialogue mining
this paper describes the analysis of weak local coherence utterances during humancomputer conversation through the application of an emergent data mining technique , data crystallization . results reveal that by adding utterances with weak local relevance the performance of a baseline conversational partner , in terms of user satisfaction , showed betterment .

a fast fertility hidden markov model for word alignment using mcmc
a word in one language can be translated to zero , one , or several words in other languages . using word fertility features has been shown to be useful in building word alignment models for statistical machine translation . we built a fertility hidden markov model by adding fertility to the hidden markov model . this model not only achieves lower alignment error rate than the hidden markov model , but also runs faster . it is similar in some ways to ibm model 4 , but is much easier to understand . we use gibbs sampling for parameter estimation , which is more principled than the neighborhood method used in ibm model 4 .

an agent-based approach to chinese named entity recognition
chinese ne ( named entity ) recognition is a difficult problem because of the uncertainty in word segmentation and flexibility in language structure . this paper proposes the use of a rationality model in a multi-agent framework to tackle this problem . we employ a greedy strategy and use the ne rationality model to evaluate and detect all possible nes in the text . we then treat the process of selecting the best possible nes as a multi-agent negotiation problem . the resulting system is robust and is able to handle different types of ne effectively . our test on the met-2 test corpus indicates that our system is able to achieve high f1 values of above 92 % on all ne types .

comparison , selection and use of sentence alignment algorithms for new anil kumar singh
several algorithms are available for sentence alignment , but there is a lack of systematic evaluation and comparison of these algorithms under different conditions . in most cases , the factors which can significantly affect the performance of a sentence alignment algorithm have not been considered while evaluating . we have used a method for evaluation that can give a better estimate about a sentence alignment algorithms performance , so that the best one can be selected . we have compared four approaches using this method . these have mostly been tried on european language pairs . we have evaluated manually-checked and validated english-hindi aligned parallel corpora under different conditions . we also suggest some guidelines on actual alignment .

maximum likelihood estimation of feature-based distributions
motivated by recent work in phonotactic learning ( hayes and wilson 2008 , albright 2009 ) , this paper shows how to define feature-based probability distributions whose parameters can be provably efficiently estimated . the main idea is that these distributions are defined as a product of simpler distributions ( cf . ghahramani and jordan 1997 ) . one advantage of this framework is it draws attention to what is minimally necessary to describe and learn phonological feature interactions in phonotactic patterns . the bottom-up approach adopted here is contrasted with the top-down approach in hayes and wilson ( 2008 ) , and it is argued that the bottom-up approach is more analytically transparent .

program in computational linguistics
we present the design of a professional masters program in computational linguistics . this program can be completed in one-year of full-time study , or two-three years of part-time study . originally designed for cs professionals looking for additional training , the program has evolved in flexibility to accommodate students from more diverse backgrounds and with more diverse goals .

understanding seed selection in bootstrapping
bootstrapping has recently become the focus of much attention in natural language processing to reduce labeling cost . in bootstrapping , unlabeled instances can be harvested from the initial labeled seed set . the selected seed set affects accuracy , but how to select a good seed set is not yet clear . thus , an iterative seeding framework is proposed for bootstrapping to reduce its labeling cost . our framework iteratively selects the unlabeled instance that has the best goodness of seed and labels the unlabeled instance in the seed set . our framework deepens understanding of this seeding process in bootstrapping by deriving the dual problem . we propose a method called expected model rotation ( emr ) that works well on not well-separated data which frequently occur as realistic data . experimental results show that emr can select seed sets that provide significantly higher mean reciprocal rank on realistic data than existing naive selection methods or random seed sets .

accelerated training of maximum margin markov models for sequence xiaofeng yu wai lam
we present the first known empirical results on sequence labeling based on maximum margin markov networks ( m3n ) , which incorporate both kernel methods to efficiently deal with high-dimensional feature spaces , and probabilistic graphical models to capture correlations in structured data . we provide an efficient algorithm , the stochastic gradient descent ( sgd ) , to speedup the training procedure of m3n . using official dataset for noun phrase ( np ) chunking as a case study , the resulting optimizer converges to the same quality of solution over an order of magnitude faster than the structured sequential minimal optimization ( structured smo ) . our model compares favorably with current state-of-the-art sequence labeling approaches . more importantly , our model can be easily applied to other sequence labeling tasks .

classifying easy-to-read texts without parsing
document classification using automated linguistic analysis and machine learning ( ml ) has been shown to be a viable road forward for readability assessment . the best models can be trained to decide if a text is easy to read or not with very high accuracy , e.g . a model using 117 parameters from shallow , lexical , morphological and syntactic analyses achieves 98,9 % accuracy . in this paper we compare models created by parameter optimization over subsets of that total model to find out to which extent different high-performing models tend to consist of the same parameters and if it is possible to find models that only use features not requiring parsing . we used a genetic algorithm to systematically optimize parameter sets of fixed sizes using accuracy of a support vector machine classifier as fitness function . our results show that it is possible to find models almost as good as the currently best models while omitting parsing based features .

unsupervised mining of lexical variants from noisy text marina del rey , ca
the amount of data produced in usergenerated content continues to grow at a staggering rate . however , the text found in these media can deviate wildly from the standard rules of orthography , syntax and even semantics and present significant problems to downstream applications which make use of this noisy data . in this paper we present a novel unsupervised method for extracting domainspecific lexical variants given a large volume of text . we demonstrate the utility of this method by applying it to normalize text messages found in the online social media service , twitter , into their most likely standard english versions . our method yields a 20 % reduction in word error rate over an existing state-of-theart approach .

predicative adjectives : an unsupervised criterion to extract subjective spoken language systems and language technology spoken language systems
we examine predicative adjectives as an unsupervised criterion to extract subjective adjectives . we do not only compare this criterion with a weakly supervised extraction method but also with gradable adjectives , i.e . another highly subjective subset of adjectives that can be extracted in an unsupervised fashion . in order to prove the robustness of this extraction method , we will evaluate the extraction with the help of two different state-of-the-art sentiment lexicons ( as a gold standard ) .

paraphrase generation as monolingual translation : data and evaluation
in this paper we investigate the automatic generation and evaluation of sentential paraphrases . we describe a method for generating sentential paraphrases by using a large aligned monolingual corpus of news headlines acquired automatically from google news and a standard phrase-based machine translation ( pbmt ) framework . the output of this system is compared to a word substitution baseline . human judges prefer the pbmt paraphrasing system over the word substitution system . we demonstrate that bleu correlates well with human judgements provided that the generated paraphrased sentence is sufficiently different from the source sentence .

using paraphrasing verbs and prepositions su nam kim diarmuid o seaghdha
we present a brief overview of the main challenges in understanding the semantics of noun compounds and consider some known methods . we introduce a new task to be part of semeval-2010 : the interpretation of noun compounds using paraphrasing verbs and prepositions . the task is meant to provide a standard testbed for future research on noun compound semantics . it should also promote paraphrase-based approaches to the problem , which can benefit many nlp applications .

combination of arabic preprocessing schemes for statistical machine translation
statistical machine translation is quite robust when it comes to the choice of input representation . it only requires consistency between training and testing . as a result , there is a wide range of possible preprocessing choices for data used in statistical machine translation . this is even more so for morphologically rich languages such as arabic . in this paper , we study the effect of different word-level preprocessing schemes for arabic on the quality of phrase-based statistical machine translation . we also present and evaluate different methods for combining preprocessing schemes resulting in improved translation quality .

performance prediction for exponential language models
we investigate the task of performance prediction for language models belonging to the exponential family . first , we attempt to empirically discover a formula for predicting test set cross-entropy for n-gram language models . we build models over varying domains , data set sizes , and n-gram orders , and perform linear regression to see whether we can model test set performance as a simple function of training set performance and various model statistics . remarkably , we find a simple relationship that predicts test set performance with a correlation of 0.9997. we analyze why this relationship holds and show that it holds for other exponential language models as well , including class-based models and minimum discrimination information models . finally , we discuss how this relationship can be applied to improve language model performance .

discovering morphological paradigms from plain text using a dirichlet process mixture model sdl language weaver
we present an inference algorithm that organizes observed words ( tokens ) into structured inflectional paradigms ( types ) . it also naturally predicts the spelling of unobserved forms that are missing from these paradigms , and discovers inflectional principles ( grammar ) that generalize to wholly unobserved words . our bayesian generative model of the data explicitly represents tokens , types , inflections , paradigms , and locally conditioned string edits . it assumes that inflected word tokens are generated from an infinite mixture of inflectional paradigms ( string tuples ) . each paradigm is sampled all at once from a graphical model , whose potential functions are weighted finitestate transducers with language-specific parameters to be learned . these assumptions naturally lead to an elegant empirical bayes inference procedure that exploits monte carlo em , belief propagation , and dynamic programming . given 50100 seed paradigms , adding a 10million-word corpus reduces prediction error for morphological inflections by up to 10 % .

analysis and refinement of temporal relation aggregation
to obtain a complete temporal picture of a relation it is necessary to aggregate fragments of temporal information across relation instances in text . this process is non-trivial even for humans because temporal information can be imprecise and inconsistent , and systems face the additional challenge that each of their classifications is potentially false . even a small amount of incorrect proposed temporal information about a relation can severely affect the resulting aggregate temporal knowledge . we motivate and evaluate three methods to modify temporal relation information prior to aggregation to address this challenge .

type-checking in formally non-typed systems
type checking defines and constrains system output and intermediate representations . we report on the advantages of introducing multiple levels of type checking in deep parsing systems , even with untyped formalisms .

machine translation detection from monolingual web-text
we propose a method for automatically detecting low-quality web-text translated by statistical machine translation ( smt ) systems . we focus on the phrase salad phenomenon that is observed in existing smt results and propose a set of computationally inexpensive features to effectively detect such machine-translated sentences from a large-scale web-mined text . unlike previous approaches that require bilingual data , our method uses only monolingual text as input ; therefore it is applicable for refining data produced by a variety of web-mining activities . evaluation results show that the proposed method achieves an accuracy of 95.8 % for sentences and 80.6 % for text in noisy web pages .

domain adaptation with active learning for word sense disambiguation yee seng chan and hwee tou ng
when a word sense disambiguation ( wsd ) system is trained on one domain but applied to a different domain , a drop in accuracy is frequently observed . this highlights the importance of domain adaptation for word sense disambiguation . in this paper , we first show that an active learning approach can be successfully used to perform domain adaptation of wsd systems . then , by using the predominant sense predicted by expectation-maximization ( em ) and adopting a count-merging technique , we improve the effectiveness of the original adaptation process achieved by the basic active learning approach .

identifying types of claims in online customer reviews
in this paper we present a novel approach to categorizing comments in online reviews as either a qualified claim or a bald claim . we argue that this distinction is important based on a study of customer behavior in making purchasing decisions using online reviews . we present results of a supervised algorithm for learning this distinction . the two types of claims are expressed differently in language and we show that syntactic features capture this difference , yielding improvement over a bag-of-words baseline .

multilingual sentence generation
this paper presents an overview of a robust , broad-coverage , and application-independent natural language generation system . it demonstrates how the different language generation components function within a multilingual machine translation ( mt ) system , using the languages that we are currently working on ( english , spanish , japanese , and chinese ) . section 1 provides a system description . section 2 focuses on the generation components and their core set of rules . section 3 describes an additional layer of generation rules included to address applicationspecific issues . section 4 provides a brief description of the evaluation method and results for the mt system of which our generation components are a part .

resolving lexical ambiguity in tensor regression models of meaning and computer science mile end road
this paper provides a method for improving tensor-based compositional distributional models of meaning by the addition of an explicit disambiguation step prior to composition . in contrast with previous research where this hypothesis has been successfully tested against relatively simple compositional models , in our work we use a robust model trained with linear regression . the results we get in two experiments show the superiority of the prior disambiguation method and suggest that the effectiveness of this approach is modelindependent .

modelling the internal variability of mwes
the issue of flexibility of multiword expressions ( mwes ) is crucial towards their identification and extraction in running text , as well as their better understanding from a linguistic perspective . if we project a large mwe lexicon onto a corpus , projecting fixed forms suffers from low recall , while an unconstrained flexible search for lemmas yields a loss in precision . in this talk , i will describe a method aimed at maximising precision in the identification of mwes in flexible mode , building on the idea that internal variability can be modelled via so-called variation patterns . i will discuss the advantages and limitations of using variation patterns , compare their performance to that of association measures , and explore their usability in mwe extraction , too . about the speaker malvina nissim is a tenured researcher in computational linguistics at the university of bologna . her research focuses on the computational handling of several lexical semantics and discourse phenomena , such as the choice of referring expressions , semantic relations within compounds and in argument structure , multiword expressions , and , more recently , on the annotation and automatic detection of modality . she is also a co-founder and promoter of the senso comune project , devoted to the creation of a common knowledge base for italian via crowdsourcing . she graduated in linguistics from the university of pisa , and obtained her phd in linguistics from the university of pavia . before joining the university of bologna she was a post-doc at the university of edinburgh and at the institute for cognitive science and technology in rome . 51

marking time in developmental biology : annotating developmental events and their links with molecular events
current research in developmental biology aims to link developmental genetic pathways with the processes going on at cellular and tissue level . normal processes will only take place under specific sequential conditions at the level of the pathways . disrupting or altering pathways may mean disrupted or altered development . this paper is part of a larger work exploring methods of detecting and extracting information on developmental events from free text and on their relations in space and time .

towards unsupervised recognition of dialogue acts
when engaged in dialogues , people perform communicative actions to pursue specific communicative goals . speech acts recognition attracted computational linguistics since long time and could impact considerably a huge variety of application domains . we study the task of automatic labeling dialogues with the proper dialogue acts , relying on empirical methods and simply exploiting lexical semantics of the utterances . in particular , we present some experiments in supervised and unsupervised framework on both an english and an italian corpus of dialogue transcriptions . the evaluation displays encouraging results in both languages , especially in the unsupervised version of the methodology .

using reinforcement learning to create communication channel management strategies for diverse users
spoken dialogue systems typically do not manage the communication channel , instead using fixed values for such features as the amplitude and speaking rate . yet , the quality of a dialogue can be compromised if the user has difficulty understanding the system . in this proof-of-concept research , we explore using reinforcement learning ( rl ) to create policies that manage the communication channel to meet the needs of diverse users . towards this end , we first formalize a preliminary communication channel model , in which users provide explicit feedback regarding issues with the communication channel , and the system implicitly alters its amplitude to accommodate the users optimal volume . second , we explore whether rl is an appropriate tool for creating communication channel management strategies , comparing two different hand-crafted policies to policies trained using both a dialogue-length and a novel annoyance cost . the learned policies performed better than hand-crafted policies , with those trained using the annoyance cost learning an equitable tradeoff between users with differing needs and also learning to balance finding a users optimal amplitude against dialoguelength . these results suggest that rl can be used to create effective communication channel management policies for diverse users . index terms : communication channel , spoken dialogue systems , reinforcement learning , amplitude , diverse users

a pac-bayesian approach to minimum perplexity language modeling
despite the overwhelming use of statistical language models in speech recognition , machine translation , and several other domains , few high probability guarantees exist on their generalization error . in this paper , we bound the test set perplexity of two popular language models the n-gram model and class-based n-grams using pac-bayesian theorems for unsupervised learning . we extend the bound to sequence clustering , wherein classes represent longer context such as phrases . the new bound is dominated by the maximum number of sequences represented by each cluster , which is polynomial in the vocabulary size . we show that we can still encourage small sample generalization by sparsifying the cluster assignment probabilities . we incorporate our bound into an efficient hmm-based sequence clustering algorithm and validate the theory with empirical results on the resource management corpus .

joint wmt submission of the quaero project
this paper describes the joint quaero submission to the wmt 2011 machine translation evaluation . four groups ( rwth aachen university , karlsruhe institute of technology , limsi-cnrs , and systran ) of the quaero project submitted a joint translation for the wmt germanenglish task . each group translated the data sets with their own systems . then rwth system combination combines these translations to a better one . in this paper , we describe the single systems of each group . before we present the results of the system combination , we give a short description of the rwth aachen system combination approach .

deconstructing human literature reviews a framework for multi-document summar ization division of information studies
this study is conducted in the area of multidocument summarization , and develops a literature review framework based on a deconstruction of human-written literature review sections in information science research papers . the first part of the study presents the results of a multi-level discourse analysis to investigate their discourse and content characteristics . these findings were incorporated into a framework for literature reviews , focusing on their macro-level document structure and the sentence-level templates , as well as the information summarization strategies . the second part of this study discusses insights from this analysis , and how the framework can be adapted to automatic summaries resembling human written literature reviews . summaries generated from a partial implementation are evaluated against human written summaries and assessors comments are discussed to formulate recommendations for future work .

topic segmentation with a structured topic model national ict australia
we present a new hierarchical bayesian model for unsupervised topic segmentation . this new model integrates a point-wise boundary sampling algorithm used in bayesian segmentation into a structured topic model that can capture a simple hierarchical topic structure latent in documents . we develop an mcmc inference algorithm to split/merge segment ( s ) . experimental results show that our model outperforms previous unsupervised segmentation methods using only lexical information on chois datasets and two meeting transcripts and has performance comparable to those previous methods on two written datasets .

efficient computation of entropy gradient for semi-supervised conditional random fields
entropy regularization is a straightforward and successful method of semi-supervised learning that augments the traditional conditional likelihood objective function with an additional term that aims to minimize the predicted label entropy on unlabeled data . it has previously been demonstrated to provide positive results in linear-chain crfs , but the published method for calculating the entropy gradient requires significantly more computation than supervised crf training . this paper presents a new derivation and dynamic program for calculating the entropy gradient that is significantly more efficienthaving the same asymptotic time complexity as supervised crf training . we also present efficient generalizations of this method for calculating the label entropy of all sub-sequences , which is useful for active learning , among other applications .

handling noisy queries in cross language faq retrieval
recent times have seen a tremendous growth in mobile based data services that allow people to use short message service ( sms ) to access these data services . in a multilingual society it is essential that data services that were developed for a specific language be made accessible through other local languages also . in this paper , we present a service that allows a user to query a frequentlyasked-questions ( faq ) database built in a local language ( hindi ) using noisy sms english queries . the inherent noise in the sms queries , along with the language mismatch makes this a challenging problem . we handle these two problems by formulating the query similarity over faq questions as a combinatorial search problem where the search space consists of combinations of dictionary variations of the noisy query and its top-n translations . we demonstrate the effectiveness of our approach on a real-life dataset .

unsupervised segmentation of words using prior distributions of morph
we present a language-independent and unsupervised algorithm for the segmentation of words into morphs . the algorithm is based on a new generative probabilistic model , which makes use of relevant prior information on the length and frequency distributions of morphs in a language . our algorithm is shown to outperform two competing algorithms , when evaluated on data from a language with agglutinative morphology ( finnish ) , and to perform well also on english data .

evaluating word sense induction and discrimination systems
the goal of this task is to allow for comparison across sense-induction and discrimination systems , and also to compare these systems to other supervised and knowledgebased systems . in total there were 6 participating systems . we reused the semeval2007 english lexical sample subtask of task 17 , and set up both clustering-style unsupervised evaluation ( using ontonotes senses as gold-standard ) and a supervised evaluation ( using the part of the dataset for mapping ) . we provide a comparison to the results of the systems participating in the lexical sample subtask of task 17 .

tracking information flow between primary and secondary news sources
tracking information flow ( iflow ) is crucial to understanding the evolution of news stories . we present analysis and experiments for iflow between company announcements and newswire . error analysis shows that many fps are annotation errors and many fns are due to coarse-grained document-level modelling . experiments show that document meta-data features ( e.g. , category , length , timing ) improve f-scores relative to upper bound by 23 % .

an ontology-based view on prepositional senses
this paper describes ongoing work , aimed at producing a lexicon of prepositions , i.e . relations denoted by prepositions , to be used for information retrieval purposes . the work is ontology based , which for this project means that the ontological types of the arguments of the preposition are considered , rather than the word forms . thus , sense distinctions are made based on ontological constraints on the arguments .

through adverbial cues
mental modeling is crucial for natural humanrobot interactions ( hri ) . yet , effective mechanisms that enable reasoning about and communication of mental states are not available . we propose to utilize adverbial cues , routinely employed by humans , for this goal and present a novel algorithm that integrates adverbial modifiers with belief revision and expression , phrasing utterances based on gricean conversational maxims . the algorithm is demonstrated in a simple hri scenario .

sentence and expression level annotation of opinions in user-generated
in this paper , we introduce a corpus of consumer reviews from the rateitall and the eopinions websites annotated with opinion-related information . we present a two-level annotation scheme . in the first stage , the reviews are analyzed at the sentence level for ( i ) relevancy to a given topic , and ( ii ) expressing an evaluation about the topic . in the second stage , on-topic sentences containing evaluations about the topic are further investigated at the expression level for pinpointing the properties ( semantic orientation , intensity ) , and the functional components of the evaluations ( opinion terms , targets and holders ) . we discuss the annotation scheme , the inter-annotator agreement for different subtasks and our observations .

karlsruhe instiute of technolgy
this paper describes our phrase-based statistical machine translation ( smt ) system for the wmt10 translation task . we submitted translations for the german to english and english to german translation tasks . compared to state-of-the-art phrase-based systems we preformed additional preprocessing and used a discriminative word alignment approach . the word reordering was modeled using pos information and we extended the translation model with additional features .

the role of lexical resources in cjk natural language processing
the role of lexical resources is often understated in nlp research . the complexity of chinese , japanese and korean ( cjk ) poses special challenges to developers of nlp tools , especially in the area of word segmentation ( ws ) , information retrieval ( ir ) , named entity extraction ( ner ) , and machine translation ( mt ) . these difficulties are exacerbated by the lack of comprehensive lexical resources , especially for proper nouns , and the lack of a standardized orthography , especially in japanese . this paper summarizes some of the major linguistic issues in the development nlp applications that are dependent on lexical resources , and discusses the central role such resources should play in enhancing the accuracy of nlp tools .

maytag : a multi-staged approach to identifying complex events in textual data the mitre corporation
we present a novel application of nlp and text mining to the analysis of financial documents . in particular , we describe an implemented prototype , maytag , which combines information extraction and subject classification tools in an interactive exploratory framework . we present experimental results on their performance , as tailored to the financial domain , and some forward-looking extensions to the approach that enables users to specify classifications on the fly .

developing a biosurveillance application ontology for influenza - like - illness
increasing biosurveillance capacity is a public health priority in both the developed and the developing world . effective syndromic surveillance is especially important if we are to successfully identify and monitor disease outbreaks in their early stages . this paper describes the construction and preliminary evaluation of a syndromic surveillance orientated application ontology designed to facilitate the early identification of influenza-likeillness syndrome from emergency room clinical reports using natural language processing .

unsupervised parsing for generating surface-based relation extraction patterns
finding the right features and patterns for identifying relations in natural language is one of the most pressing research questions for relation extraction . in this paper , we compare patterns based on supervised and unsupervised syntactic parsing and present a simple method for extracting surface patterns from a parsed training set . results show that the use of surfacebased patterns not only increases extraction speed , but also improves the quality of the extracted relations . we find that , in this setting , unsupervised parsing , besides requiring less resources , compares favorably in terms of extraction quality .

in the negev , israel
this document overviews the strategy , effort and aftermath of the multiling 2013 multilingual summarization data collection . we describe how the data contributors of multiling collected and generated a multilingual multi-document summarization corpus on 10 different languages : arabic , chinese , czech , english , french , greek , hebrew , hindi , romanian and spanish . we discuss the rationale behind the main decisions of the collection , the methodology used to generate the multilingual corpus , as well as challenges and problems faced per language . this paper overviews the work on czech , hebrew and spanish languages .

user expertise modelling and adaptivity in a speech-based e-mail system
this paper describes the user expertise model in athosmail , a mobile , speech-based e-mail system . the model encodes the systems assumptions about the user expertise , and gives recommendations on how the system should respond depending on the assumed competence levels of the user . the recommendations are realized as three types of explicitness in the system responses . the system monitors the users competence with the help of parameters that describe e.g . the success of the users interaction with the system . the model consists of an online and an offline version , the former taking care of the expertise level changes during the same session , the latter modelling the overall user expertise as a function of time and repeated interactions .

two is bigger ( and better ) than one : the wikipedia bitaxonomy project dipartimento di informatica
we present wibi , an approach to the automatic creation of a bitaxonomy for wikipedia , that is , an integrated taxonomy of wikipage pages and categories . we leverage the information available in either one of the taxonomies to reinforce the creation of the other taxonomy . our experiments show higher quality and coverage than state-of-the-art resources like dbpedia , yago , menta , wikinet and wikitaxonomy . wibi is available at http : //wibitaxonomy.org .

concept discovery from text
broad-coverage lexical resources such as wordnet are extremely useful . however , they often include many rare senses while missing domain-specific senses . we present a clustering algorithm called cbc ( clustering by committee ) that automatically discovers concepts from text . it initially discovers a set of tight clusters called committees that are well scattered in the similarity space . the centroid of the members of a committee is used as the feature vector of the cluster . we proceed by assigning elements to their most similar cluster . evaluating cluster quality has always been a difficult task . we present a new evaluation methodology that is based on the editing distance between output clusters and classes extracted from wordnet ( the answer key ) . our experiments show that cbc outperforms several well-known clustering algorithms in cluster quality .

a deep architecture for semantic parsing
many successful approaches to semantic parsing build on top of the syntactic analysis of text , and make use of distributional representations or statistical models to match parses to ontology-specific queries . this paper presents a novel deep learning architecture which provides a semantic parsing system through the union of two neural models of language semantics . it allows for the generation of ontology-specific queries from natural language statements and questions without the need for parsing , which makes it especially suitable to grammatically malformed or syntactically atypical text , such as tweets , as well as permitting the development of semantic parsers for resourcepoor languages .

towards automatic question answering over social media by learning question equivalence patterns
many questions submitted to collaborative question answering ( cqa ) sites have been answered before . we propose an approach to automatically generating an answer to such questions based on automatically learning to identify equivalent questions . our main contribution is an unsupervised method for automatically learning question equivalence patterns from cqa archive data . these patterns can be used to match new questions to their equivalents that have been answered before , and thereby help suggest answers automatically . we experimented with our method approach over a large collection of more than 200,000 real questions drawn from the yahoo ! answers archive , automatically acquiring over 300 groups of question equivalence patterns . these patterns allow our method to obtain over 66 % precision on automatically suggesting answers to new questions , significantly outperforming conventional baseline approaches to question matching .

viterbi training for pcfgs : hardness results and competitiveness of uniform initialization
we consider the search for a maximum likelihood assignment of hidden derivations and grammar weights for a probabilistic context-free grammar , the problem approximately solved by viterbi training . we show that solving and even approximating viterbi training for pcfgs is np-hard . we motivate the use of uniformat-random initialization for viterbi em as an optimal initializer in absence of further information about the correct model parameters , providing an approximate bound on the log-likelihood .

a new dataset and method for automatically grading esol texts
we demonstrate how supervised discriminative machine learning techniques can be used to automate the assessment of english as a second or other language ( esol ) examination scripts . in particular , we use rank preference learning to explicitly model the grade relationships between scripts . a number of different features are extracted and ablation tests are used to investigate their contribution to overall performance . a comparison between regression and rank preference models further supports our method . experimental results on the first publically available dataset show that our system can achieve levels of performance close to the upper bound for the task , as defined by the agreement between human examiners on the same corpus . finally , using a set of outlier texts , we test the validity of our model and identify cases where the models scores diverge from that of a human examiner .

models for sentence compression : a comparison across domains ,
sentence compression is the task of producing a summary at the sentence level . this paper focuses on three aspects of this task which have not received detailed treatment in the literature : training requirements , scalability , and automatic evaluation . we provide a novel comparison between a supervised constituentbased and an weakly supervised wordbased compression algorithm and examine how these models port to different domains ( written vs. spoken text ) . to achieve this , a human-authored compression corpus has been created and our study highlights potential problems with the automatically gathered compression corpora currently used . finally , we assess whether automatic evaluation measures can be used to determine compression quality .

semi-supervised learning of morphological paradigms and lexicons
we present a semi-supervised approach to the problem of paradigm induction from inflection tables . our system extracts generalizations from inflection tables , representing the resulting paradigms in an abstract form . the process is intended to be language-independent , and to provide human-readable generalizations of paradigms . the tools we provide can be used by linguists for the rapid creation of lexical resources . we evaluate the system through an inflection table reconstruction task using wiktionary data for german , spanish , and finnish . with no additional corpus information available , the evaluation yields per word form accuracy scores on inflecting unseen base forms in different languages ranging from 87.81 % ( german nouns ) to 99.52 % ( spanish verbs ) ; with additional unlabeled text corpora available for training the scores range from 91.81 % ( german nouns ) to 99.58 % ( spanish verbs ) . we separately evaluate the system in a simulated task of swedish lexicon creation , and show that on the basis of a small number of inflection tables , the system can accurately collect from a list of noun forms a lexicon with inflection information ranging from 100.0 % correct ( collect 100 words ) , to 96.4 % correct ( collect 1000 words ) .

learning stochastic ot grammars : a bayesian approach using data augmentation and gibbs sampling
stochastic optimality theory ( boersma , 1997 ) is a widely-used model in linguistics that did not have a theoretically sound learning method previously . in this paper , a markov chain monte-carlo method is proposed for learning stochastic ot grammars . following a bayesian framework , the goal is finding the posterior distribution of the grammar given the relative frequencies of input-output pairs . the data augmentation algorithm allows one to simulate a joint posterior distribution by iterating two conditional sampling steps . this gibbs sampler constructs a markov chain that converges to the joint distribution , and the target posterior can be derived as its marginal distribution .

identifying generic noun phrases
this paper presents a supervised approach for identifying generic noun phrases in context . generic statements express rulelike knowledge about kinds or events . therefore , their identification is important for the automatic construction of knowledge bases . in particular , the distinction between generic and non-generic statements is crucial for the correct encoding of generic and instance-level information . generic expressions have been studied extensively in formal semantics . building on this work , we explore a corpus-based learning approach for identifying generic nps , using selections of linguistically motivated features . our results perform well above the baseline and existing prior work .

medical text summarization system based on named entity eiji aramaki yasuhide miura masatsugu tonoike
with the rapidly growing use of electronic health records , the possibility of large-scale clinical information extraction has drawn much attention . it is not , however , easy to extract information because these reports are written in natural language . to address this problem , this paper presents a system that converts a medical text into a table structure . this systems core technologies are ( 1 ) medical event recognition modules and ( 2 ) a negative event identification module that judges whether an event actually occurred or not . regarding the latter module , this paper also proposes an svm-based classifier using syntactic information . experimental results demonstrate empirically that syntactic information can contribute to the methods accuracy .

estimating and exploiting the entropy of sense distributions
word sense distributions are usually skewed . predicting the extent of the skew can help a word sense disambiguation ( wsd ) system determine whether to consider evidence from the local context or apply the simple yet effective heuristic of using the first ( most frequent ) sense . in this paper , we propose a method to estimate the entropy of a sense distribution to boost the precision of a first sense heuristic by restricting its application to words with lower entropy . we show on two standard datasets that automatic prediction of entropy can increase the performance of an automatic first sense heuristic .

regularized minimum error rate training
minimum error rate training ( mert ) remains one of the preferred methods for tuning linear parameters in machine translation systems , yet it faces significant issues . first , mert is an unregularized learner and is therefore prone to overfitting . second , it is commonly used on a noisy , non-convex loss function that becomes more difficult to optimize as the number of parameters increases . to address these issues , we study the addition of a regularization term to the mert objective function . since standard regularizers such as `2 are inapplicable to mert due to the scale invariance of its objective function , we turn to two regularizers`0 and a modification of `2 and present methods for efficiently integrating them during search . to improve search in large parameter spaces , we also present a new direction finding algorithm that uses the gradient of expected bleu to orient merts exact line searches . experiments with up to 3600 features show that these extensions of mert yield results comparable to pro , a learner often used with large feature sets .

extended phraseological information in a valence dictionary for nlp applications
the aim of this paper is to propose a far-reaching extension of the phraseological component of a valence dictionary for polish . the dictionary is the basis of two different parsers of polish ; its format has been designed so as to maximise the readability of the information it contains and its re-applicability . we believe that the extension proposed here follows this approach and , hence , may be an inspiration in the design of valence dictionaries for other languages .

analysis of link grammar on biomedical dependency corpus targeted at protein-protein interactions
in this paper , we present an evaluation of the link grammar parser on a corpus consisting of sentences describing protein-protein interactions . we introduce the notion of an interaction subgraph , which is the subgraph of a dependency graph expressing a protein-protein interaction . we measure the performance of the parser for recovery of dependencies , fully correct linkages and interaction subgraphs . we analyze the causes of parser failure and report specific causes of error , and identify potential modifications to the grammar to address the identified issues . we also report and discuss the effect of an extension to the dictionary of the parser .

an automated method to build a corpus of rhetorically-classified sentences in biomedical texts
the rhetorical classification of sentences in biomedical texts is an important task in the recognition of the components of a scientific argument . generating supervised machine learned models to do this recognition requires corpora annotated for

knowledge-based multilingual document analysis via di tor vergata
the growing availability of multilingual resources , like eurowordnet , has recently inspired the development of large scale linguistic technologies , e.g . multilingual ie and q & a , that were considered infeasible until a few years ago . in this paper a system for categorisation and automatic authoring of news streams in different languages is presented . in our system , a knowledge-based approach to information extraction is adopted as a support for hyperlinking . authoring across documents in different languages is triggered by named entities and event recognition . the matching of events in texts is carried out by discourse processing driven by a large scale world model . this kind of multilingual analysis relies on a lexical knowledge base of nouns ( i.e . the eurowordnet base concepts ) shared among english , spanish and italian lexicons . the impact of the design choices on the language independence and the possibilities it opens for automatic learning of the event hierarchy will be discussed .

phonotactic probability and the maori passive : a computational approach oiwi parker jones
two analyses of maori passives and gerunds have been debated in the literature . both assume that the thematic consonants in these forms are unpredictable . this paper reports on three computational experiments designed to test whether this assumption is sound . the results suggest that thematic consonants are predictable from the phonotactic probabilities of their active counterparts . this study has potential implications for allomorphy in other polynesian languages . it also exemplifies the benefits of using computational methods in linguistic analyses .

learning entity representation for entity disambiguation
we propose a novel entity disambiguation model , based on deep neural network ( dnn ) . instead of utilizing simple similarity measures and their disjoint combinations , our method directly optimizes document and entity representations for a given similarity measure . stacked denoising auto-encoders are first employed to learn an initial document representation in an unsupervised pre-training stage . a supervised fine-tuning stage follows to optimize the representation towards the similarity measure . experiment results show that our method achieves state-of-the-art performance on two public datasets without any manually designed features , even beating complex collective approaches .

english-chinese bi-directional oov translation based on web mining and supervised learning
in cross-language information retrieval ( clir ) , out-of-vocabulary ( oov ) detection and translation pair relevance evaluation still remain as key problems . in this paper , an english-chinese bi-directional oov translation model is presented , which utilizes web mining as the corpus source to collect translation pairs and combines supervised learning to evaluate their association degree . the experimental results show that the proposed model can successfully filter the most possible translation candidate with the lower computational cost , and improve the oov translation ranking effect , especially for popular new words .

simultaneous tokenization and part-of-speech tagging for arabic without a morphological analyzer linguistic data consortium
we describe an approach to simultaneous tokenization and part-of-speech tagging that is based on separating the closed and open-class items , and focusing on the likelihood of the possible stems of the openclass words . by encoding some basic linguistic information , the machine learning task is simplified , while achieving stateof-the-art tokenization results and competitive pos results , although with a reduced tag set and some evaluation difficulties .

robust sentiment detection on twitter from biased and noisy data
in this paper , we propose an approach to automatically detect sentiments on twitter messages ( tweets ) that explores some characteristics of how tweets are written and meta-information of the words that compose these messages . moreover , we leverage sources of noisy labels as our training data . these noisy labels were provided by a few sentiment detection websites over twitter data . in our experiments , we show that since our features are able to capture a more abstract representation of tweets , our solution is more effective than previous ones and also more robust regarding biased and noisy data , which is the kind of data provided by these sources .

domain adaptation for medical text translation using web resources
this paper describes adapting statistical machine translation ( smt ) systems to medical domain using in-domain and general-domain data as well as webcrawled in-domain resources . in order to complement the limited in-domain corpora , we apply domain focused webcrawling approaches to acquire indomain monolingual data and bilingual lexicon from the internet . the collected data is used for adapting the language model and translation model to boost the overall translation quality . besides , we propose an alternative filtering approach to clean the crawled data and to further optimize the domain-specific smt system . we attend the medical summary sentence unconstrained translation task of the ninth workshop on statistical machine translation ( wmt2014 ) . our systems achieve the second best bleu scores for czech-english , fourth for french-english , english-french language pairs and the third best results for reminding pairs .

topic modeling based classification of clinical reports
electronic health records ( ehrs ) contain important clinical information about patients . some of these data are in the form of free text and require preprocessing to be able to used in automated systems . efficient and effective use of this data could be vital to the speed and quality of health care . as a case study , we analyzed classification of ct imaging reports into binary categories . in addition to regular text classification , we utilized topic modeling of the entire dataset in various ways . topic modeling of the corpora provides interpretable themes that exist in these reports . representing reports according to their topic distributions is more compact than bag-of-words representation and can be processed faster than raw text in subsequent automated processes . a binary topic model was also built as an unsupervised classification approach with the assumption that each topic corresponds to a class . and , finally an aggregate topic classifier was built where reports are classified based on a single discriminative topic that is determined from the training dataset . our proposed topic based classifier system is shown to be competitive with existing text classification techniques and provides a more efficient and interpretable representation .

spoken language parsing using phrase-level grammars and trainable
in this paper , we describe a novel approach to spoken language analysis for translation , which uses a combination of grammar-based phrase-level parsing and automatic classification . the job of the analyzer is to produce a shallow semantic interlingua representation for spoken task-oriented utterances . the goal of our hybrid approach is to provide accurate real-time analyses while improving robustness and portability to new domains and languages .

bidirectional inter-dependencies of subjective expressions and targets and their value for a joint model
opinion mining is often regarded as a classification or segmentation task , involving the prediction of i ) subjective expressions , ii ) their target and iii ) their polarity . intuitively , these three variables are bidirectionally interdependent , but most work has either attempted to predict them in isolation or proposing pipeline-based approaches that can not model the bidirectional interaction between these variables . towards better understanding the interaction between these variables , we propose a model that allows for analyzing the relation of target and subjective phrases in both directions , thus providing an upper bound for the impact of a joint model in comparison to a pipeline model . we report results on two public datasets ( cameras and cars ) , showing that our model outperforms state-ofthe-art models , as well as on a new dataset consisting of twitter posts .

for recognising person entities in tweets
recognising entities in social media text is difficult . ner on newswire text is conventionally cast as a sequence labeling problem . this makes implicit assumptions regarding its textual structure . social media text is rich in disfluency and often has poor or noisy structure , and intuitively does not always satisfy these assumptions . we explore noise-tolerant methods for sequence labeling and apply discriminative post-editing to exceed state-of-the-art performance for person recognition in tweets , reaching an f1 of 84 % .

automatic dating of documents and temporal text classification
the frequency of occurrence of words in natural languages exhibits a periodic and a non-periodic component when analysed as a time series . this work presents an unsupervised method of extracting periodicity information from text , enabling time series creation and filtering to be used in the creation of sophisticated language models that can discern between repetitive trends and non-repetitive writing patterns . the algorithm performs in o ( n log n ) time for input of length n. the temporal language model is used to create rules based on temporal-word associations inferred from the time series . the rules are used to guess automatically at likely document creation dates , based on the assumption that natural languages have unique signatures of changing word distributions over time . experimental results on news items spanning a nine year period show that the proposed method and algorithms are accurate in discovering periodicity patterns and in dating documents automatically solely from their content .

the importance of sub-utterance prosody in predicting level of certainty
we present an experiment aimed at understanding how to optimally use acoustic and prosodic information to predict a speakers level of certainty . with a corpus of utterances where we can isolate a single word or phrase that is responsible for the speakers level of certainty we use different sets of sub-utterance prosodic features to train models for predicting an utterances perceived level of certainty . our results suggest that using prosodic features of the word or phrase responsible for the level of certainty and of its surrounding context improves the prediction accuracy without increasing the total number of features when compared to using only features taken from the utterance as a whole .

pointing to places in a deductive geospatial theory ecosystem science and
issues in the description of places are discussed in the context of a logical geospatial theory . this theory lies at the core of the system geologica , which deduces answers to geographical questions based on knowledge provided by multiple agents .

the nrc system for discriminating similar languages multilingual text processing
we describe the system built by the national research council canada for the discriminating between similar languages ( dsl ) shared task . our system uses various statistical classifiers and makes predictions based on a two-stage process : we first predict the language group , then discriminate between languages or variants within the group . language groups are predicted using a generative classifier with 99.99 % accuracy on the five target groups . within each group ( except english ) , we use a voting combination of discriminative classifiers trained on a variety of feature spaces , achieving an average accuracy of 95.71 % , with per-group accuracy between 90.95 % and 100 % depending on the group . this approach turns out to reach the best performance among all systems submitted to the open and closed tasks .

determining the specificity of terms using compositional and contextual information
this paper introduces new specificity determining methods for terms using compositional and contextual information . specificity of terms is the quantity of domain specific information that is contained in the terms . the methods are modeled as information theory like measures . as the methods dont use domain specific information , they can be applied to other domains without extra processes . experiments showed very promising result with the precision of 82.0 % when the methods were applied to the terms in mesh thesaurus .

lexical encoding of mwes
multiword expressions present a challenge for language technology , given their flexible nature . each type of multiword expression has its own characteristics , and providing a uniform lexical encoding for them is a difficult task to undertake . nonetheless , in this paper we present an architecture for the lexical encoding of these expressions in a database , that takes into account their flexibility . this encoding extends in a straightforward manner the one required for simplex ( single ) words , and maximises the information contained for them in the description of multiwords .

an unsupervised aspect-sentiment model for online reviews
with the increase in popularity of online review sites comes a corresponding need for tools capable of extracting the information most important to the user from the plain text data . due to the diversity in products and services being reviewed , supervised methods are often not practical . we present an unsupervised system for extracting aspects and determining sentiment in review text . the method is simple and flexible with regard to domain and language , and takes into account the influence of aspect on sentiment polarity , an issue largely ignored in previous literature . we demonstrate its effectiveness on both component tasks , where it achieves similar results to more complex semi-supervised methods that are restricted by their reliance on manual annotation and extensive knowledge sources .

atypical prosodic structure as an indicator of reading level and text difficulty
automatic assessment of reading ability builds on applying speech recognition tools to oral reading , measuring words correct per minute . this work looks at more fine-grained analysis that accounts for effects of prosodic context using a large corpus of read speech from a literacy study . experiments show that lower-level readers tend to produce relatively more lengthening on words that are not likely to be final in a prosodic phrase , i.e . in less appropriate locations . the results have implications for automatic assessment of text difficulty in that locations of atypical prosodic lengthening are indicative of difficult lexical items and syntactic constructions .

improving the efficiency of a wide-coverage ccg parser
the c & c ccg parser is a highly efficient linguistically motivated parser . the efficiency is achieved using a tightly-integrated supertagger , which assigns ccg lexical categories to words in a sentence . the integration allows the parser to request more categories if it can not find a spanning analysis . we present several enhancements to the cky chart parsing algorithm used by the parser . the first proposal is chart repair , which allows the chart to be efficiently updated by adding lexical categories individually , and we evaluate several strategies for adding these categories . the second proposal is to add constraints to the chart which require certain spans to be constituents . finally , we propose partial beam search to further reduce the search space . overall , the parsing speed is improved by over 35 % with negligible loss of accuracy or coverage .

keyword-based document clustering
document clustering is an aggregation of related documents to a cluster based on the similarity evaluation task between documents and the representatives of clusters . terms and their discriminating features of terms are the clue to the clustering and the discriminating features are based on the term and document frequencies . feature selection method on the basis of frequency statistics has a limitation to the enhancement of the clustering algorithm because it does not consider the contents of the cluster objects . in this paper , we adopt a content-based analytic approach to refine the similarity computation and propose a keyword-based clustering algorithm . experimental results show that content-based keyword weighting outperforms frequency-based weighting method .

relative clause extraction for syntactic simplification
this paper investigates non-destructive simplification , a type of syntactic text simplification which focuses on extracting embedded clauses from structurally complex sentences and rephrasing them without affecting their original meaning . this process reduces the average sentence length and complexity to make text simpler . although relevant for human readers with low reading skills or language disabilities , the process has direct applications in nlp . in this paper we analyse the extraction of relative clauses through a tagging approach . a dataset covering three genres was manually annotated and used to develop and compare several approaches for automatically detecting appositions and non-restrictive relative clauses . the best results are obtained by a ml model developed using crfsuite , followed by a rule based method .

smt helps bitext dependency parsing
we propose a method to improve the accuracy of parsing bilingual texts ( bitexts ) with the help of statistical machine translation ( smt ) systems . previous bitext parsing methods use human-annotated bilingual treebanks that are hard to obtain . instead , our approach uses an auto-generated bilingual treebank to produce bilingual constraints . however , because the auto-generated bilingual treebank contains errors , the bilingual constraints are noisy . to overcome this problem , we use large-scale unannotated data to verify the constraints and design a set of effective bilingual features for parsing models based on the verified results . the experimental results show that our new parsers significantly outperform state-of-theart baselines . moreover , our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline . especially notable is that our approach can be used in a purely monolingual setting with the help of smt .

annotating participant reference in english spoken conversation
in conversational language , references to people ( especially to the conversation participants , e.g. , i , you , and we ) are an essential part of many expressed meanings . in most conversational settings , however , many such expressions have numerous potential meanings , are frequently vague , and are highly dependent on social and situational context . this is a significant challenge to conversational language understanding systems one which has seen little attention in annotation studies . in this paper , we present a method for annotating verbal reference to people in conversational speech , with a focus on reference to conversation participants . our goal is to provide a resource that tackles the issues of vagueness , ambiguity , and contextual dependency in a nuanced yet reliable way , with the ultimate aim of supporting work on summarization and information extraction for conversation .

learning semantic hierarchies via word embeddings
semantic hierarchy construction aims to build structures of concepts linked by hypernymhyponym ( is-a ) relations . a major challenge for this task is the automatic discovery of such relations . this paper proposes a novel and effective method for the construction of semantic hierarchies based on word embeddings , which can be used to measure the semantic relationship between words . we identify whether a candidate word pair has hypernymhyponym relation by using the word-embedding-based semantic projections between words and their hypernyms . our result , an f-score of 73.74 % , outperforms the state-of-theart methods on a manually labeled test dataset . moreover , combining our method with a previous manually-built hierarchy extension method can further improve fscore to 80.29 % .

word to sentence level emotion tagging for bengali blogs
in this paper , emotion analysis on blog texts has been carried out for a less privileged language like bengali . ekmans six basic emotion types have been selected for reliable and semi automatic word level annotation . an automatic classifier has been applied for recognizing six basic emotion types for different words in a sentence . application of different scoring strategies to identify sentence level emotion tag based on the acquired word level emotion constituents have produced satisfactory performance .

a two-stage approach for generating unbiased estimates of text complexity educational testing service
many existing approaches for measuring text complexity tend to overestimate the complexity levels of informational texts while simultaneously underestimating the complexity levels of literary texts . we present a two-stage estimation technique that successfully addresses this problem . at stage 1 , each text is classified into one or another of three possible genres : informational , literary or mixed . next , at stage 2 , a complexity score is generated for each text by applying one or another of three possible prediction models : one optimized for application to informational texts , one optimized for application to literary texts , and one optimized for application to mixed texts . each model combines lexical , syntactic and discourse features , as appropriate , to best replicate human complexity judgments . we demonstrate that resulting text complexity predictions are both unbiased , and highly correlated with classifications provided by experienced educators .

grammatical inference and first language acquisition uni-mail , boulevard du pont-darve ,
one argument for parametric models of language has been learnability in the context of first language acquisition . the claim is made that logical arguments from learnability theory require non-trivial constraints on the class of languages . initial formalisations of the problem ( gold , 1967 ) are however inapplicable to this particular situation . in this paper we construct an appropriate formalisation of the problem using a modern vocabulary drawn from statistical learning theory and grammatical inference and looking in detail at the relevant empirical facts . we claim that a variant of the probably approximately correct ( pac ) learning framework ( valiant , 1984 ) with positive samples only , modified so it is not completely distribution free is the appropriate choice . some negative results derived from cryptographic problems ( kearns et al , 1994 ) appear to apply in this situation but the existence of algorithms with provably good performance ( ron et al , 1995 ) and subsequent work , shows how these negative results are not as strong as they initially appear , and that recent algorithms for learning regular languages partially satisfy our criteria . we then discuss the applicability of these results to parametric and nonparametric models .

learning structured models for phone recognition
we present a maximally streamlined approach to learning hmm-based acoustic models for automatic speech recognition . in our approach , an initial monophone hmm is iteratively refined using a split-merge em procedure which makes no assumptions about subphone structure or context-dependent structure , and which uses only a single gaussian per hmm state . despite the much simplified training process , our acoustic model achieves state-of-the-art results on phone classification ( where it outperforms almost all other methods ) and competitive performance on phone recognition ( where it outperforms standard cd triphone / subphone / gmm approaches ) . we also present an analysis of what is and is not learned by our system .

combining multiple information types in bayesian word segmentation
humans identify word boundaries in continuous speech by combining multiple cues ; existing state-of-the-art models , though , look at a single cue . we extend the generative model of goldwater et al ( 2006 ) to segment using syllable stress as well as phonemic form . our new model treats identification of word boundaries and prevalent stress patterns in the language as a joint inference task . we show that this model improves segmentation accuracy over purely segmental input representations , and recovers the dominant stress pattern of the data . additionally , our model retains high performance even without single-word utterances . we also demonstrate a discrepancy in the performance of our model and human infants on an artificial-language task in which stress cues and transition-probability information are pitted against one another . we argue that this discrepancy indicates a bound on rationality in the mechanisms of human segmentation .

training a naive bayes classifier via the em algorithm with a class crest , jst ( japan science and technology corporation )
combining a naive bayes classifier with the em algorithm is one of the promising approaches for making use of unlabeled data for disambiguation tasks when using local context features including word sense disambiguation and spelling correction . however , the use of unlabeled data via the basic em algorithm often causes disastrous performance degradation instead of improving classification performance , resulting in poor classification performance on average . in this study , we introduce a class distribution constraint into the iteration process of the em algorithm . this constraint keeps the class distribution of unlabeled data consistent with the class distribution estimated from labeled data , preventing the em algorithm from converging into an undesirable state . experimental results from using 26 confusion sets and a large amount of unlabeled data show that our proposed method for using unlabeled data considerably improves classification performance when the amount of labeled data is small .

speech summarization without lexical features for mandarin broadcast news
we present the first known empirical study on speech summarization without lexical features for mandarin broadcast news . we evaluate acoustic , lexical and structural features as predictors of summary sentences . we find that the summarizer yields good performance at the average fmeasure of 0.5646 even by using the combination of acoustic and structural features alone , which are independent of lexical features . in addition , we show that structural features are superior to lexical features and our summarizer performs surprisingly well at the average f-measure of 0.3914 by using only acoustic features . these findings enable us to summarize speech without placing a stringent demand on speech recognition accuracy .

non-monotonic sentence alignment via semisupervised learning
this paper studies the problem of nonmonotonic sentence alignment , motivated by the observation that coupled sentences in real bitexts do not necessarily occur monotonically , and proposes a semisupervised learning approach based on two assumptions : ( 1 ) sentences with high affinity in one language tend to have their counterparts with similar relatedness in the other ; and ( 2 ) initial alignment is readily available with existing alignment techniques . they are incorporated as two constraints into a semisupervised learning framework for optimization to produce a globally optimal solution . the evaluation with realworld legal data from a comprehensive legislation corpus shows that while existing alignment algorithms suffer severely from non-monotonicity , this approach can work effectively on both monotonic and non-monotonic data .

bootstrapping a multilingual part-of-speech tagger in one person-day
this paper presents a method for bootstrapping a fine-grained , broad-coverage part-of-speech ( pos ) tagger in a new language using only one personday of data acquisition effort . it requires only three resources , which are currently readily available in 60-100 world languages : ( 1 ) an online or hard-copy pocket-sized bilingual dictionary , ( 2 ) a basic library reference grammar , and ( 3 ) access to an existing monolingual text corpus in the language . the algorithm begins by inducing initial lexical pos distributions from english translations in a bilingual dictionary without pos tags . it handles irregular , regular and semi-regular morphology through a robust generative model using weighted levenshtein alignments . unsupervised induction of grammatical gender is performed via global modeling of contextwindow feature agreement . using a combination of these and other evidence sources , interactive training of context and lexical prior models are accomplished for fine-grained pos tag spaces . experiments show high accuracy , fine-grained tag resolution with minimal new human effort .

the design of a proofreading software service
web applications have the opportunity to check spelling , style , and grammar using a software service architecture . a software service authoring aid can offer contextual spell checking , detect real word errors , and avoid poor grammar checker suggestions through the use of large language models . here we present after the deadline , an open source authoring aid , used in production on wordpress.com , a blogging platform with over ten million writers . we discuss the benefits of the software service environment and how it affected our choice of algorithms . we summarize our design principles as speed over accuracy , simplicity over complexity , and do what works .

prior derivation models for formally syntax-based translation using linguistically syntactic parsing and tree kernels
this paper presents an improved formally syntax-based smt model , which is enriched by linguistically syntactic knowledge obtained from statistical constituent parsers . we propose a linguistically-motivated prior derivation model to score hypothesis derivations on top of the baseline model during the translation decoding . moreover , we devise a fast training algorithm to achieve such improved models based on tree kernel methods . experiments on an english-to-chinese task demonstrate that our proposed models outperformed the baseline formally syntaxbased models , while both of them achieved significant improvements over a state-of-theart phrase-based smt system .

a corpus-based lexical resource of german idioms project : collocations in the german language ,
in this paper , we present the design of a lexical resource focusing on german verb phrase idioms . the entry for a given idiom combines appropriate corpus examples with rich linguistic and lexicographic annotations , giving the user access to linguistic information coupled with the appropriate attested data and laying the groundwork for empirical , reproducible research .

trainable sentence planning for complex information presentation in spoken dialog systems
a challenging problem for spoken dialog systems is the design of utterance generation modules that are fast , flexible and general , yet produce high quality output in particular domains . a promising approach is trainable generation , which uses general-purpose linguistic knowledge automatically adapted to the application domain . this paper presents a trainable sentence planner for the match dialog system . we show that trainable sentence planning can produce output comparable to that of matchs template-based generator even for quite complex information presentations .

learning to shift the polarity of words for sentiment classification daisuke ikeda hiroya takamura lev-arie ratinov manabu okumura
we propose a machine learning based method of sentiment classification of sentences using word-level polarity . the polarities of words in a sentence are not always the same as that of the sentence , because there can be polarity-shifters such as negation expressions . the proposed method models the polarity-shifters . our model can be trained in two different ways : word-wise and sentence-wise learning . in sentence-wise learning , the model can be trained so that the prediction of sentence polarities should be accurate . the model can also be combined with features used in previous work such as bag-of-words and n-grams . we empirically show that our method almost always improves the performance of sentiment classification of sentences especially when we have only small amount of training data .

machine translation by interaction
a machine translation model has been proposed where an input is translated through both source-language and target-language paraphrasing processes . we have implemented our prototype model for the japanese-chinese language pair . this paper describes our core idea of translation , where a source language paraphraser and a language transfer cooperates in translation by exchanging information about the source input .

learning latent word representations for domain adaptation using supervised word clustering
domain adaptation has been popularly studied on exploiting labeled information from a source domain to learn a prediction model in a target domain . in this paper , we develop a novel representation learning approach to address domain adaptation for text classification with automatically induced discriminative latent features , which are generalizable across domains while informative to the prediction task . specifically , we propose a hierarchical multinomial naive bayes model with latent variables to conduct supervised word clustering on labeled documents from both source and target domains , and then use the produced cluster distribution of each word as its latent feature representation for domain adaptation . we train this latent graphical model using a simple expectation-maximization ( em ) algorithm . we empirically evaluate the proposed method with both cross-domain document categorization tasks on reuters-21578 dataset and cross-domain sentiment classification tasks on amazon product review dataset . the experimental results demonstrate that our proposed approach achieves superior performance compared with alternative methods .

hierarchical reinforcement learning for adaptive text generation
we present a novel approach to natural language generation ( nlg ) that applies hierarchical reinforcement learning to text generation in the wayfinding domain . our approach aims to optimise the integration of nlg tasks that are inherently different in nature , such as decisions of content selection , text structure , user modelling , referring expression generation ( reg ) , and surface realisation . it also aims to capture existing interdependencies between these areas . we apply hierarchical reinforcement learning to learn a generation policy that captures these interdependencies , and that can be transferred to other nlg tasks . our experimental resultsin a simulated environmentshow that the learnt wayfinding policy outperforms a baseline policy that takes reasonable actions but without optimization .

incremental parsing with monotonic adjoining operation
this paper describes an incremental parser based on an adjoining operation . by using the operation , we can avoid the problem of infinite local ambiguity in incremental parsing . this paper further proposes a restricted version of the adjoining operation , which preserves lexical dependencies of partial parse trees . our experimental results showed that the restriction enhances the accuracy of the incremental parsing .

a pos-based model for long-range reorderings in smt
in this paper we describe a new approach to model long-range word reorderings in statistical machine translation ( smt ) . until now , most smt approaches are only able to model local reorderings . but even the word order of related languages like german and english can be very different . in recent years approaches that reorder the source sentence in a preprocessing step to better match target sentences according to pos ( part-of-speech ) -based rules have been applied successfully . we enhance this approach to model long-range reorderings by introducing discontinuous rules . we tested this new approach on a germanenglish translation task and could significantly improve the translation quality , by up to 0.8 bleu points , compared to a system which already uses continuous posbased rules to model short-range reorderings .

a semi-automatic evaluation scheme : automated nuggetization for
in this paper we describe automatic information nuggetization and its application to text comparison . more specifically , we take a close look at how machine-generated nuggets can be used to create evaluation material . a semiautomatic annotation scheme is designed to produce gold-standard data with exceptionally high inter-human agreement .

relation detection between named entities : report of a shared task hugo goncalo oliveira cisuc , dei - fctuc
in this paper we describe the first evaluation contest ( track ) for portuguese whose goal was to detect and classify relations between named entities in running text , called rerelem . given a collection annotated with named entities belonging to ten different semantic categories , we marked all relationships between them within each document . we used the following fourfold relationship classification : identity , included-in , located-in , and other ( which was later on explicitly detailed into twenty different relations ) . we provide a quantitative description of this evaluation resource , as well as describe the evaluation architecture and summarize the results of the participating systems in the track .

looking for hyponyms in vector space
the task of detecting and generating hyponyms is at the core of semantic understanding of language , and has numerous practical applications . we investigate how neural network embeddings perform on this task , compared to dependency-based vector space models , and evaluate a range of similarity measures on hyponym generation . a new asymmetric similarity measure and a combination approach are described , both of which significantly improve precision . we release three new datasets of lexical vector representations trained on the bnc and our evaluation dataset for hyponym generation .

named entity recognition in estonian
the task of named entity recognition ( ner ) is to identify in text predefined units of information such as person names , organizations and locations . in this work , we address the problem of ner in estonian using supervised learning approach . we explore common issues related to building a ner system such as the usage of language-agnostic and languagespecific features , the representation of named entity tags , the required corpus size and the need for linguistic tools . for system training and evaluation purposes , we create a gold standard ner corpus . on this corpus , our crf-based system achieves an overall f1-score of 87 % .

exploiting structured ontology to organize scattered online opinions
we study the problem of integrating scattered online opinions . for this purpose , we propose to exploit structured ontology to obtain well-formed relevant aspects to a topic and use them to organize scattered opinions to generate a structured summary . particularly , we focus on two main challenges in implementing this idea , ( 1 ) how to select the most useful aspects from a large number of aspects in the ontology and ( 2 ) how to order the selected aspects to optimize the readability of the structured summary . we propose and explore several methods for solving these challenges . experimental results on two different data sets ( us presidents and digital cameras ) show that the proposed methods are effective for selecting aspects that can represent the major opinions and for generating coherent ordering of aspects .

chinese native language identification
we present the first application of native language identification ( nli ) to nonenglish data . motivated by theories of language transfer , nli is the task of identifying a writers native language ( l1 ) based on their writings in a second language ( the l2 ) . an nli system was applied to chinese learner texts using topicindependent syntactic models to assess their accuracy . we find that models using part-of-speech tags , context-free grammar production rules and function words are highly effective , achieving a maximum accuracy of 71 % . interestingly , we also find that when applied to equivalent english data , the model performance is almost identical . this finding suggests a systematic pattern of cross-linguistic transfer may exist , where the degree of transfer is independent of the l1 and l2 .

for coreference resolution
in this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources . these features represent knowledge mined from wordnet and wikipedia , as well as information about semantic role labels . we show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns .

towards assessing changes in degree of depression through facebook
depression is typically diagnosed as being present or absent . however , depression severity is believed to be continuously distributed rather than dichotomous . severity may vary for a given patient daily and seasonally as a function of many variables ranging from life events to environmental factors . repeated population-scale assessment of depression through questionnaires is expensive . in this paper we use survey responses and status updates from 28,749 facebook users to develop a regression model that predicts users degree of depression based on their facebook status updates . our user-level predictive accuracy is modest , significantly outperforming a baseline of average user sentiment . we use our model to estimate user changes in depression across seasons , and find , consistent with literature , users degree of depression most often increases from summer to winter . we then show the potential to study factors driving individuals level of depression by looking at its most highly correlated language features .

mining chinese-english parallel corpora from the web
parallel corpora are a crucial resource in research fields such as cross-lingual information retrieval and statistical machine translation , but only a few parallel corpora with high quality are publicly available nowadays . in this paper , we try to solve the problem by developing a system that can automatically mine high quality parallel corpora from the world wide web . the system contains a three-step process . the system uses a web spider to crawl certain hosts at first . then candidate parallel web page pairs are prepared from the downloaded page set . at last , each candidate pair is examined based on multiple standards . we develop novel strategies for the implementation of the system , which are then proved to be rather effective by the experiments towards a multilingual website .

speaking more like you : lexical , in spoken dialogue systems
when people engage in conversation , they adapt the way they speak to the speaking style of their conversational partner in a variety of ways . for example , they may adopt a certain way of describing something based upon the way their conversational partner describes it , or adapt their pitch range or speaking rate to a conversational partners . they may even align their turn-taking style or use of cue phrases to match their partners . these types of entrainment have been shown to correlate with various measures of task success and dialogue naturalness . while there is considerable evidence for lexical entrainment from laboratory experiments , much less is known about other types of acoustic-prosodic and discourse-level entrainment and little work has been done to examine entrainments in multiple modalities for the same dialogue . we will discuss work on entrainment in multiple dimensions in the columbia games corpus . our goal is to understand how the different varieties of entrainment correlate with one another and to determine which types of entrainment will be both useful and feasible for spoken dialogue systems .

unsupervised relation extraction from web documents
the idex system is a prototype of an interactive dynamic information extraction ( ie ) system . a user of the system expresses an information request in the form of a topic description , which is used for an initial search in order to retrieve a relevant set of documents . on basis of this set of documents , unsupervised relation extraction and clustering is done by the system . the results of these operations can then be interactively inspected by the user . in this paper we describe the relation extraction and clustering components of the idex system . preliminary evaluation results of these components are presented and an overview is given of possible enhancements to improve the relation extraction and clustering components .

object-extraction and question-parsing using ccg
accurate dependency recovery has recently been reported for a number of wide-coverage statistical parsers using combinatory categorial grammar ( ccg ) . however , overall figures give no indication of a parsers performance on specific constructions , nor how suitable a parser is for specific applications . in this paper we give a detailed evaluation of a ccg parser on object extraction dependencies found in wsj text . we also show how the parser can be used to parse questions for question answering . the accuracy of the original parser on questions is very poor , and we propose a novel technique for porting the parser to a new domain , by creating new labelled data at the lexical category level only . using a supertagger to assign categories to words , trained on the new data , leads to a dramatic increase in question parsing accuracy .

context-dependent regression testing for natural language processing
regression testing of natural language systems is problematic for two main reasons : component input and output is complex , and system behaviour is context-dependent . we have developed a generic approach which solves both of these issues . we describe our regression tool , contest , which supports context-dependent testing of dialogue system components , and discuss the regression test sets we developed , designed to effectively isolate components from changes and problems earlier in the pipeline . we believe that the same approach can be used in regression testing for other dialogue systems , as well as in testing any complex nlp system containing multiple components .

mildly non-projective dependency structures
syntactic parsing requires a fine balance between expressivity and complexity , so that naturally occurring structures can be accurately parsed without compromising efficiency . in dependency-based parsing , several constraints have been proposed that restrict the class of permissible structures , such as projectivity , planarity , multi-planarity , well-nestedness , gap degree , and edge degree . while projectivity is generally taken to be too restrictive for natural language syntax , it is not clear which of the other proposals strikes the best balance between expressivity and complexity . in this paper , we review and compare the different constraints theoretically , and provide an experimental evaluation using data from two treebanks , investigating how large a proportion of the structures found in the treebanks are permitted under different constraints . the results indicate that a combination of the well-nestedness constraint and a parametric constraint on discontinuity gives a very good fit with the linguistic data .

how to overtake google in mt quality - the baltic case
motivation of the language technology company tilde is to improve quality of machine translation for lesser resourced languages such as the languages of baltic countries . generic mt solutions like google translate perform poorly for these complex languages . to compensate the shortage of training data and to deal with rich morphology we are applying different approaches in combining statistical methods with linguistic rules . we will present the strategies applied and the results of various experiments . we will discuss application of the production systems that show significantly better translation quality comparing to the google translate . we will also outline how this work contributes to creation of the european infrastructure for automated translation . 96

leveraging domain-independent information in semantic parsing
semantic parsing is a domain-dependent process by nature , as its output is defined over a set of domain symbols . motivated by the observation that interpretation can be decomposed into domain-dependent and independent components , we suggest a novel interpretation model , which augments a domain dependent model with abstract information that can be shared by multiple domains . our experiments show that this type of information is useful and can reduce the annotation effort significantly when moving between domains .

can text analysis tell us something about technology progress abdulmohsen al -thubaity
a corpus -based diachronic analysis of patent documents , based mainly on the morphologically productive use of certain terms can help in tracking the evolution of key developments in a rapidly e volving specialist field . the patent texts were o btained from the us patent & trade marks offices on-line service and the terms were extracted automatically from the texts . the chosen specialist field was that of fast-switching devices and systems . the method presented draws from liter ature on biblio - and sciento -metrics , information extraction , corpus linguistics , and on aspects of english morphology . this interdisciplinary fram ework shows that the evolution of word -formation closely shadows the developments in a field of technology .

coordinate noun phrase disambiguation in a generative parsing model
in this paper we present methods for improving the disambiguation of noun phrase ( np ) coordination within the framework of a lexicalised history-based parsing model . as well as reducing noise in the data , we look at modelling two main sources of information for disambiguation : symmetry in conjunct structure , and the dependency between conjunct lexical heads . our changes to the baseline model result in an increase in np coordination dependency f-score from 69.9 % to 73.8 % , which represents a relative reduction in f-score error of 13 % .

softcardinality-core : improving text overlap with distributional measures for semantic textual similarity
soft cardinality has been shown to be a very strong text-overlapping baseline for the task of measuring semantic textual similarity ( sts ) , obtaining 3rd place in semeval-2012 . at *sem-2013 shared task , beside the plain textoverlapping approach , we tested within soft cardinality two distributional word-similarity functions derived from the ukwack corpus . unfortunately , we combined these measures with other features using regression , obtaining positions 18th , 22nd and 23rd among the 90 participants systems in the official ranking . already after the release of the gold standard annotations of the test data , we observed that using only the similarity measures without combining them with other features would have obtained positions 6th , 7th and 8th ; moreover , an arithmetic average of these similarity measures would have been 4th ( mean=0.5747 ) . this paper describes both the 3 systems as they were submitted and the similarity measures that would obtained those better results .

the descent of hierarchy , and selection in relational semantics
in many types of technical texts , meaning is embedded in noun compounds . a language understanding program needs to be able to interpret these in order to ascertain sentence meaning . we explore the possibility of using an existing lexical hierarchy for the purpose of placing words from a noun compound into categories , and then using this category membership to determine the relation that holds between the nouns . in this paper we present the results of an analysis of this method on twoword noun compounds from the biomedical domain , obtaining classification accuracy of approximately 90 % . since lexical hierarchies are not necessarily ideally suited for this task , we also pose the question : how far down the hierarchy must the algorithm descend before all the terms within the subhierarchy behave uniformly with respect to the semantic relation in question we find that the topmost levels of the hierarchy yield an accurate classification , thus providing an economic way of assigning relations to noun compounds .

the unified annotation of syntax and discourse in the copenhagen dependency treebanks matthias buch-kromann irn korzen
we propose a unified model of syntax and discourse in which text structure is viewed as a tree structure augmented with anaphoric relations and other secondary relations . we describe how the model accounts for discourse connectives and the syntax-discourse-semantics interface . our model is dependency-based , ie , words are the basic building blocks in our analyses . the analyses have been applied cross-linguistically in the copenhagen dependency treebanks , a set of parallel treebanks for danish , english , german , italian , and spanish which are currently being annotated with respect to discourse , anaphora , syntax , morphology , and translational equivalence .

lexical relationships from temporal patterns of web search queries enrique alfonseca massimiliano ciaramita keith hall
in this paper we investigate temporal patterns of web search queries . we carry out several evaluations to analyze the properties of temporal profiles of queries , revealing promising semantic and pragmatic relationships between words . we focus on two applications : query suggestion and query categorization . the former shows a potential for time-series similarity measures to identify specific semantic relatedness between words , which results in state-of-the-art performance in query suggestion while providing complementary information to more traditional distributional similarity measures . the query categorization evaluation suggests that the temporal profile alone is not a strong indicator of broad topical categories .

attention shifting for parsing speech
we present a technique that improves the efficiency of word-lattice parsing as used in speech recognition language modeling . our technique applies a probabilistic parser iteratively where on each iteration it focuses on a different subset of the wordlattice . the parsers attention is shifted towards word-lattice subsets for which there are few or no syntactic analyses posited . this attention-shifting technique provides a six-times increase in speed ( measured as the number of parser analyses evaluated ) while performing equivalently when used as the first-stage of a multi-stage parsing-based language model .

text simplification using synchronous dependency grammars : generalising automatically harvested rules
we present an approach to text simplification based on synchronous dependency grammars . our main contributions in this work are ( a ) a study of how automatically derived lexical simplification rules can be generalised to enable their application in new contexts without introducing errors , and ( b ) an evaluation of our hybrid system that combines a large set of automatically acquired rules with a small set of hand-crafted rules for common syntactic simplification . our evaluation shows significant improvements over the state of the art , with scores comparable to human simplifications .

predicting party affiliations from european parliament debates
this paper documents an ongoing effort to assess whether party group affiliation of participants in european parliament debates can be automatically predicted on the basis of the content of their speeches , using a support vector machine multi-class model . the work represents a joint effort between researchers within political science and language technology .

plaser : pronunciation learning via automatic speech recognition
plaser is a multimedia tool with instant feedback designed to teach english pronunciation for high-school students of hong kong whose mother tongue is cantonese chinese . the objective is to teach correct pronunciation and not to assess a students overall pronunciation quality . major challenges related to speech recognition technology include : allowance for non-native accent , reliable and corrective feedbacks , and visualization of errors . plaser employs hidden markov models to represent position-dependent english phonemes . they are discriminatively trained using the standard american english timit corpus together with a set of timit utterances collected from good local english speakers . there are two kinds of speaking exercises : minimal-pair exercises and word exercises . in the word exercises , plaser computes a confidence-based score for each phoneme of the given word , and paints each vowel or consonant segment in the word using a novel 3-color scheme to indicate their pronunciation accuracy . plaser was used by 900 students of grade 7 and 8 over a period of 23 months . about 80 % of the students said that they preferred using plaser over traditional english classes to learn pronunciation . a pronunciation test was also conducted before and after they used plaser .

distributed representations of geographically situated language
we introduce a model for incorporating contextual information ( such as geography ) in learning vector-space representations of situated language . in contrast to approaches to multimodal representation learning that have used properties of the object being described ( such as its color ) , our model includes information about the subject ( i.e. , the speaker ) , allowing us to learn the contours of a words meaning that are shaped by the context in which it is uttered . in a quantitative evaluation on the task of judging geographically informed semantic similarity between representations learned from 1.1 billion words of geo-located tweets , our joint model outperforms comparable independent models that learn meaning in isolation .

uniba-core : combining strategies for semantic textual similarity annalina caputo pierpaolo basile
this paper describes the uniba participation in the semantic textual similarity ( sts ) core task 2013. we exploited three different systems for computing the similarity between two texts . a system is used as baseline , which represents the best model emerged from our previous participation in sts 2012. such system is based on a distributional model of semantics capable of taking into account also syntactic structures that glue words together . in addition , we investigated the use of two different learning strategies exploiting both syntactic and semantic features . the former uses ensemble learning in order to combine the best machine learning techniques trained on 2012 training and test sets . the latter tries to overcome the limit of working with different datasets with varying characteristics by selecting only the more suitable dataset for the training purpose .

identifying perspectives at the document and sentence levels using
in this paper we investigate the problem of identifying the perspective from which a document was written . by perspective we mean a point of view , for example , from the perspective of democrats or republicans . can computers learn to identify the perspective of a document furthermore , can computers identify which sentences in a document strongly convey a particular perspective we develop statistical models to capture how perspectives are expressed at the document and sentence levels , and evaluate the proposed models on a collection of articles on the israeli-palestinian conflict . the results show that the statistical models can successfully learn how perspectives are reflected in word usage and identify the perspective of a document with very high accuracy .

zero subject detection for polish
this article reports on the first machine learning experiments on detection of null subjects in polish . it emphasizes the role of zero subject detection as the part of mention detection the initial step of endto-end coreference resolution . anaphora resolution is not studied in this article .

using an incremental robust parser to automatically generate semantic unl graphs
the unl project ( universal networking language ) proposes a standard for encoding the meaning of natural language utterances as semantic hypergraphs , intended to be used as pivot in multilingual information and communication systems . several deconverters permit to automatically translate unl utterances into natural languages . however , a rough enconvertion from natural language texts to unl expressions is usually done interactively with editors specially designed for the unl project or by hand ( which is very time-consuming and difficult to extrapolate to huge amounts of data ) . in this paper , we address the issue of using an existing incremental robust parser as main resource to enconverting french utterances into unl expressions .

the latin dependency treebank in a cultural heritage digital library the perseus project the perseus project
this paper describes the mutually beneficial relationship between a cultural heritage digital library and a historical treebank : an established digital library can provide the resources and structure necessary for efficiently building a treebank , while a treebank , as a language resource , is a valuable tool for audiences traditionally served by such libraries .

efficient support vector classifiers for named entity recognition
named entity ( ne ) recognition is a task in which proper nouns and numerical information are extracted from documents and are classified into categories such as person , organization , and date . it is a key technology of information extraction and open-domain question answering . first , we show that an ne recognizer based on support vector machines ( svms ) gives better scores than conventional systems . however , off-the-shelf svm classifiers are too inefficient for this task . therefore , we present a method that makes the system substantially faster . this approach can also be applied to other similar tasks such as chunking and part-of-speech tagging . we also present an svm-based feature selection method and an efficient training method .

feature subsumption for opinion analysis
lexical features are key to many approaches to sentiment analysis and opinion detection . a variety of representations have been used , including single words , multi-word ngrams , phrases , and lexicosyntactic patterns . in this paper , we use a subsumption hierarchy to formally define different types of lexical features and their relationship to one another , both in terms of representational coverage and performance . we use the subsumption hierarchy in two ways : ( 1 ) as an analytic tool to automatically identify complex features that outperform simpler features , and ( 2 ) to reduce a feature set by removing unnecessary features . we show that reducing the feature set improves performance on three opinion classification tasks , especially when combined with traditional feature selection .

automatic discovery of term similarities using pattern mining
term recognition and clustering are key topics in automatic knowledge acquisition and text mining . in this paper we present a novel approach to the automatic discovery of term similarities , which serves as a basis for both classification and clustering of domain-specific concepts represented by terms . the method is based on automatic extraction of significant patterns in which terms tend to appear . the approach is domain independent : it needs no manual description of domain-specific features and it is based on knowledge-poor processing of specific term features . however , automatically collected patterns are domain specific and identify significant contexts in which terms are used . beside features that represent contextual patterns , we use lexical and functional similarities between terms to define a combined similarity measure . the approach has been tested and evaluated in the domain of molecular biology , and preliminary results are presented .

between probabilistic context-free grammars and probabilistic finite automata
we consider the problem of computing the kullback-leibler distance , also called the relative entropy , between a probabilistic context-free grammar and a probabilistic finite automaton . we show that there is a closed-form ( analytical ) solution for one part of the kullback-leibler distance , viz . the cross-entropy . we discuss several applications of the result to the problem of distributional approximation of probabilistic context-free grammars by means of probabilistic finite automata .

to cache or not to cache experiments with adaptive models in statistical machine translation
we report results of our submissions to the wmt 2010 shared translation task in which we applied a system that includes adaptive language and translation models . adaptation is implemented using exponentially decaying caches storing previous translations as the history for new predictions . evidence from the cache is then mixed with the global background model . the main problem in this setup is error propagation and our submissions essentially failed to improve over the competitive baseline . there are slight improvements in lexical choice but the global performance decreases in terms of bleu scores .

integrating logical representations with probabilistic information using markov logic
first-order logic provides a powerful and flexible mechanism for representing natural language semantics . however , it is an open question of how best to integrate it with uncertain , probabilistic knowledge , for example regarding word meaning . this paper describes the first steps of an approach to recasting first-order semantics into the probabilistic models that are part of statistical relational ai . specifically , we show how discourse representation structures can be combined with distributional models for word meaning inside a markov logic network and used to successfully perform inferences that take advantage of logical concepts such as factivity as well as probabilistic information on word meaning in context .

zymake : a computational workflow system for machine learning and natural language processing
experiments in natural language processing and machine learning typically involve running a complicated network of programs to create , process , and evaluate data . researchers often write one or more unix shell scripts to glue together these various pieces , but such scripts are suboptimal for several reasons . without significant additional work , a script does not handle recovering from failures , it requires keeping track of complicated filenames , and it does not support running processes in parallel . in this paper , we present zymake as a solution to all these problems . zymake scripts look like shell scripts , but have semantics similar to makefiles . using zymake improves repeatability and scalability of running experiments , and provides a clean , simple interface for assembling components . a zymake script also serves as documentation for the complete workflow . we present a zymake script for a published set of nlp experiments , and demonstrate that it is superior to alternative solutions , including shell scripts and makefiles , while being far simpler to use than scientific grid computing systems .

kea : expression-level sentiment analysis from twitter data
this paper describes an expression-level sentiment detection system that participated in the subtask a of semeval-2013 task 2 : sentiment analysis in twitter . our system uses a supervised approach to learn the features from the training data to classify expressions in new tweets as positive , negative or neutral . the proposed approach helps to understand the relevant features that contribute most in this classification task .

learning to extract international relations from political context
we describe a new probabilistic model for extracting events between major political actors from news corpora . our unsupervised model brings together familiar components in natural language processing ( like parsers and topic models ) with contextual political information temporal and dyad dependenceto infer latent event classes . we quantitatively evaluate the models performance on political science benchmarks : recovering expert-assigned event class valences , and detecting real-world conflict . we also conduct a small case study based on our models inferences . a supplementary appendix , and replication software/data are available online , at : http : //brenocon.com/irevents

regularized least-squares classification for word sense
the paper describes rlsc-lin and rlsccomb systems which participated in the senseval-3 english lexical sample task . these systems are based on regularized least-squares classification ( rlsc ) learning method . we describe the reasons of choosing this method , how we applied it to word sense disambiguation , what results we obtained on senseval1 , senseval-2 and senseval-3 data and discuss some possible improvements .

first joint workshop on statistical parsing of morphologically rich languages
we summarize our approach taken in the spmrl 2014 shared task on parsing morphologically rich languages . our approach builds upon our contribution from last year , with a number of modifications and extensions . though this paper summarizes our contribution , a more detailed description and evaluation will be presented in the accompanying volume containing notes from the spmrl 2014 shared task .

large margin synchronous generation and its application to sentence compression
this paper presents a tree-to-tree transduction method for text rewriting . our model is based on synchronous tree substitution grammar , a formalism that allows local distortion of the tree topology and can thus naturally capture structural mismatches . we describe an algorithm for decoding in this framework and show how the model can be trained discriminatively within a large margin framework . experimental results on sentence compression bring significant improvements over a state-of-the-art model .

building and rening rhetorical-semantic relation models sasha blair-goldensohn and
we report results of experiments which build and refine models of rhetoricalsemantic relations such as cause and contrast . we adopt the approach of marcu and echihabi ( 2002 ) , using a small set of patterns to build relation models , and extend their work by refining the training and classification process using parameter optimization , topic segmentation and syntactic parsing . using human-annotated and automatically-extracted test sets , we find that each of these techniques results in improved relation classification accuracy .

deep syntax language models and statistical machine translation josef van genabith
hierarchical models increase the reordering capabilities of mt systems by introducing non-terminal symbols to phrases that map source language ( sl ) words/phrases to the correct position in the target language ( tl ) translation . building translations via discontiguous tl phrases increases the difficulty of language modeling , however , introducing the need for heuristic techniques such as cube pruning ( chiang , 2005 ) , for example . an additional possibility to aid language modeling in hierarchical systems is to use a language model that models fluency of words not using their local context in the string , as in traditional language models , but instead using the deeper context of a word . in this paper , we explore the potential of deep syntax language models providing an interesting comparison with the traditional string-based language model . we include an experimental evaluation that compares the two kinds of models independently of any mt system to investigate the possible potential of integrating a deep syntax language model into hierarchical smt systems .

ne tagging for urdu based on bootstrap pos learning
part of speech ( pos ) tagging and named entity ( ne ) tagging have become important components of effective text analysis . in this paper , we propose a bootstrapped model that involves four levels of text processing for urdu . we show that increasing the training data for pos learning by applying bootstrapping techniques improves ne tagging results . our model overcomes the limitation imposed by the availability of limited ground truth data required for training a learning model . both our pos tagging and ne tagging models are based on the conditional random field ( crf ) learning approach . to further enhance the performance , grammar rules and lexicon lookups are applied on the final output to correct any spurious tag assignments . we also propose a model for word boundary segmentation where a bigram hmm model is trained for character transitions among all positions in each word . the generated words are further processed using a probabilistic language model . all models use a hybrid approach that combines statistical models with hand crafted grammar rules .

using feature structures to improve verb translation in english-to-german statistical mt
scfg-based statistical mt models have proven effective for modelling syntactic aspects of translation , but still suffer problems of overgeneration . the production of german verbal complexes is particularly challenging since highly discontiguous constructions must be formed consistently , often from multiple independent rules . we extend a strong scfg-based string-to-tree model to incorporate a rich feature-structure based representation of german verbal complex types and compare verbal complex production against that of the reference translations , finding a high baseline rate of error . by developing model features that use source-side information to influence the production of verbal complexes we are able to substantially improve the type accuracy as compared to the reference .

towards exhaustive protein modification event extraction sampo pyysalo tomoko ohta makoto miwa junichi tsujii
protein modifications , in particular posttranslational modifications , have a central role in bringing about the full repertoire of protein functions , and the identification of specific protein modifications is important for understanding biological systems . this task presents a number of opportunities for the automatic support of manual curation efforts . however , the sheer number of different types of protein modifications is a daunting challenge for automatic extraction that has so far not been met in full , with most studies focusing on single modifications or a few prominent ones . in this work , aim to meet this challenge : we analyse protein modification types through ontologies , databases , and literature and introduce a corpus of 360 abstracts manually annotated in the bionlp shared task event representation for over 4500 mentions of proteins and 1000 statements of modification events of nearly 40 different types . we argue that together with existing resources , this corpus provides sufficient coverage of modification types to make effectively exhaustive extraction of protein modifications from text feasible .

a classification-based algorithm for consistency check of part-of-speech tagging for chinese corpora
ensuring consistency of part-of-speech ( pos ) tagging plays an important role in constructing high-quality chinese corpora . after analyzing the pos tagging of multi-category words in largescale corpora , we propose a novel consistency check method of pos tagging in this paper . our method builds a vector model of the context of multicategory words , and uses the k-nn algorithm to classify context vectors constructed from pos tagging sequences and judge their consistency . the experimental results indicate that the proposed method is feasible and effective .

forest reranking : discriminative parsing with non-local features
conventional n-best reranking techniques often suffer from the limited scope of the nbest list , which rules out many potentially good alternatives . we instead propose forest reranking , a method that reranks a packed forest of exponentially many parses . since exact inference is intractable with non-local features , we present an approximate algorithm inspired by forest rescoring that makes discriminative training practical over the whole treebank . our final result , an f-score of 91.7 , outperforms both 50-best and 100-best reranking baselines , and is better than any previously reported systems trained on the treebank .

assas-band , an affix-exception-list based urdu stemmer
both inflectional and derivational morphology lead to multiple surface forms of a word . stemming reduces these forms back to its stem or root , and is a very useful tool for many applications . there has not been any work reported on urdu stemming . the current work develops an urdu stemmer or assas-band and improves the performance using more precise affix based exception lists , instead of the conventional lexical lookup employed for developing stemmers in other languages . testing shows an accuracy of 91.2 % . further enhancements are also suggested .

a template-based abstractive meeting summarization : leveraging
in this paper , we present an automatic abstractive summarization system of meeting conversations . our system extends a novel multi-sentence fusion algorithm in order to generate abstract templates . it also leverages the relationship between summaries and their source meeting transcripts to select the best templates for generating abstractive summaries of meetings . our manual and automatic evaluation results demonstrate the success of our system in achieving higher scores both in readability and informativeness .

active semi-supervised learning for improving word alignment
word alignment models form an important part of building statistical machine translation systems . semi-supervised word alignment aims to improve the accuracy of automatic word alignment by incorporating full or partial alignments acquired from humans . such dedicated elicitation effort is often expensive and depends on availability of bilingual speakers for the language-pair . in this paper we study active learning query strategies to carefully identify highly uncertain or most informative alignment links that are proposed under an unsupervised word alignment model . manual correction of such informative links can then be applied to create a labeled dataset used by a semi-supervised word alignment model . our experiments show that using active learning leads to maximal reduction of alignment error rates with reduced human effort .

abstractive summarization of line graphs from popular media
in popular media generally have a discourse goal that contributes to achieving the communicative intent of a multimodal document . this paper presents our work on abstractive summarization of line graphs . our methodology involves hypothesizing the intended message of a line graph and using it as the core of a summary of the graphic . this core is then augmented with salient propositions that elaborate on the intended message .

exploiting web-derived selectional preference to improve statistical
in this paper , we present a novel approach which incorporates the web-derived selectional preferences to improve statistical dependency parsing . conventional selectional preference learning methods have usually focused on word-to-class relations , e.g. , a verb selects as its subject a given nominal class . this paper extends previous work to wordto-word selectional preferences by using webscale data . experiments show that web-scale data improves statistical dependency parsing , particularly for long dependency relationships . there is no data like more data , performance improves log-linearly with the number of parameters ( unique n-grams ) . more importantly , when operating on new domains , we show that using web-derived selectional preferences is essential for achieving robust performance .

semi-supervised speech act recognition in emails and forums
in this paper , we present a semi-supervised method for automatic speech act recognition in email and forums . the major challenge of this task is due to lack of labeled data in these two genres . our method leverages labeled data in the switchboarddamsl and the meeting recorder dialog act database and applies simple domain adaptation techniques over a large amount of unlabeled email and forum data to address this problem . our method uses automatically extracted features such as phrases and dependency trees , called subtree features , for semi-supervised learning . empirical results demonstrate that our model is effective in email and forum speech act recognition .

a distributional analysis of a lexicalized statistical parsing model
this paper presents some of the first data visualizations and analysis of distributions for a lexicalized statistical parsing model , in order to better understand their nature . in the course of this analysis , we have paid particular attention to parameters that include bilexical dependencies . the prevailing view has been that such statistics are very informative but suffer greatly from sparse data problems . by using a parser to constrain-parse its own output , and by hypothesizing and testing for distributional similarity with back-off distributions , we have evidence that finally explains that ( a ) bilexical statistics are actually getting used quite often but that ( b ) the distributions are so similar to those that do not include head words as to be nearly indistinguishable insofar as making parse decisions . finally , our analysis has provided for the first time an effective way to do parameter selection for a generative lexicalized statistical parsing model .

dealing with interpretation errors in tutorial dialogue naval air warfare training systems division
we describe an approach to dealing with interpretation errors in a tutorial dialogue system . allowing students to provide explanations and generate contentful talk can be helpful for learning , but the language that can be understood by a computer system is limited by the current technology . techniques for dealing with understanding problems have been developed primarily for spoken dialogue systems in informationseeking domains , and are not always appropriate for tutorial dialogue . we present a classification of interpretation errors and our approach for dealing with them within an implemented tutorial dialogue system .

graph connectivity measures for unsupervised parameter tuning of graph-based sense induction systems
word sense induction ( wsi ) is the task of identifying the different senses ( uses ) of a target word in a given text . this paper focuses on the unsupervised estimation of the free parameters of a graph-based wsi method , and explores the use of eight graph connectivity measures ( gcm ) that assess the degree of connectivity in a graph . given a target word and a set of parameters , gcm evaluate the connectivity of the produced clusters , which correspond to subgraphs of the initial ( unclustered ) graph . each parameter setting is assigned a score according to one of the gcm and the highest scoring setting is then selected . our evaluation on the nouns of semeval-2007 wsi task ( swsi ) shows that : ( 1 ) all gcm estimate a set of parameters which significantly outperform the worst performing parameter setting in both swsi evaluation schemes , ( 2 ) all gcm estimate a set of parameters which outperform the most frequent sense ( mfs ) baseline by a statistically significant amount in the supervised evaluation scheme , and ( 3 ) two of the measures estimate a set of parameters that performs closely to a set of parameters estimated in supervised manner .

solving relational similarity problems using the web as a corpus
we present a simple linguistically-motivated method for characterizing the semantic relations that hold between two nouns . the approach leverages the vast size of the web in order to build lexically-specific features . the main idea is to look for verbs , prepositions , and coordinating conjunctions that can help make explicit the hidden relations between the target nouns . using these features in instance-based classifiers , we demonstrate state-of-the-art results on various relational similarity problems , including mapping noun-modifier pairs to abstract relations like time , location and container , characterizing noun-noun compounds in terms of abstract linguistic predicates like cause , use , and from , classifying the relations between nominals in context , and solving sat verbal analogy problems . in essence , the approach puts together some existing ideas , showing that they apply generally to various semantic tasks , finding that verbs are especially useful features .

semanticrank : ranking keywords and sentences using semantic graphs
# ! # $ % & ' $ ! ! $ ! $ $ ! # & ( ! $ # ) & & * $ * $ $ + & semanticrank $ , ! ! # # & semantic graph $ ! , $ , # $ * !

learning script knowledge with web experiments
we describe a novel approach to unsupervised learning of the events that make up a script , along with constraints on their temporal ordering . we collect naturallanguage descriptions of script-specific event sequences from volunteers over the internet . then we compute a graph representation of the scripts temporal structure using a multiple sequence alignment algorithm . the evaluation of our system shows that we outperform two informed baselines .

acoustic transformations to improve the intelligibility of dysarthric speech
this paper describes modifications to acoustic speech signals produced by speakers with dysarthria in order to make those utterances more intelligible to typical listeners . these modifications include the correction of tempo , the adjustment of formant frequencies in sonorants , the removal of aberrant voicing , the deletion of phoneme insertion errors , and the replacement of erroneously dropped phonemes . through simple evaluations of intelligibility with nave listeners , we show that the correction of phoneme errors results in the greatest increase in intelligibility and is therefore a desirable mechanism for the eventual creation of augmentative application software for individuals with dysarthria .

shared task system description : measuring the compositionality of bigrams using statistical methodologies
the measurement of relative compositionality of bigrams is crucial to identify multi-word expressions ( mwes ) in natural language processing ( nlp ) tasks . the article presents the experiments carried out as part of the participation in the shared task distributional semantics and compositionality ( disco ) organized as part of the disco workshop in aclhlt 2011. the experiments deal with various collocation based statistical approaches to compute the relative compositionality of three types of bigram phrases ( adjective-noun , verbsubject and verb-object combinations ) . the experimental results in terms of both fine-grained and coarse-grained compositionality scores have been evaluated with the human annotated gold standard data . reasonable results have been obtained in terms of average point difference and coarse precision .

automatic extraction of briefing templates dipanjan das mohit kumar
an approach to solving the problem of automatic briefing generation from non-textual events can be segmenting the task into two major steps , namely , extraction of briefing templates and learning aggregators that collate information from events and automatically fill up the templates . in this paper , we describe two novel unsupervised approaches for extracting briefing templates from human written reports . since the problem is non-standard , we define our own criteria for evaluating the approaches and demonstrate that both approaches are effective in extracting domain relevant templates with promising accuracies .

a multi-pass sieve for coreference resolution
most coreference resolution models determine if two mentions are coreferent using a single function over a set of constraints or features . this approach can lead to incorrect decisions as lower precision features often overwhelm the smaller number of high precision ones . to overcome this problem , we propose a simple coreference architecture based on a sieve that applies tiers of deterministic coreference models one at a time from highest to lowest precision . each tier builds on the previous tiers entity cluster output . further , our model propagates global information by sharing attributes ( e.g. , gender and number ) across mentions in the same cluster . this cautious sieve guarantees that stronger features are given precedence over weaker ones and that each decision is made using all of the information available at the time . the framework is highly modular : new coreference modules can be plugged in without any change to the other modules . in spite of its simplicity , our approach outperforms many state-of-the-art supervised and unsupervised models on several standard corpora . this suggests that sievebased approaches could be applied to other nlp tasks .

using fmri activation to conceptual stimuli to evaluate methods for extracting conceptual representations from corpora colin kelly & anna korhonen
we present a series of methods for deriving conceptual representations from corpora and investigate the usefulness of the fmri data and machine learning methodology of mitchell et al ( 2008 ) as a basis for evaluating the different models . within this framework , the quality of a semantic model is quantified by its ability to predict the fmri activation associated with conceptual stimuli . mitchell et al used a manually-acquired set of verbs as the basis for their semantic model ; in this paper , we also consider automatically acquired feature-norm-like semantic representations . these models make different assumptions about the kinds of information available in corpora that is relevant to representing conceptual knowledge . our results indicate that automatically-acquired representations can make equally powerful predictions about the brain activity associated with the stimuli .

creating reverse bilingual dictionaries khang nhut lam
bilingual dictionaries are expensive resources and not many are available when one of the languages is resource-poor . in this paper , we propose algorithms for creation of new reverse bilingual dictionaries from existing bilingual dictionaries in which english is one of the two languages . our algorithms exploit the similarity between word-concept pairs using the english wordnet to produce reverse dictionary entries . since our algorithms rely on available bilingual dictionaries , they are applicable to any bilingual dictionary as long as one of the two languages has wordnet type lexical ontology .

analysis of semantic classes in medical text for question answering
to answer questions from clinical-evidence texts , we identify occurrences of the semantic classes disease , medication , patient outcome that are candidate elements of the answer , and the relations among them . additionally , we determine whether an outcome is positive or negative .

chart mining-based lexical acquisition with precision grammars
in this paper , we present an innovative chart mining technique for improving parse coverage based on partial parse outputs from precision grammars . the general approach of mining features from partial analyses is applicable to a range of lexical acquisition tasks , and is particularly suited to domain-specific lexical tuning and lexical acquisition using lowcoverage grammars . as an illustration of the functionality of our proposed technique , we develop a lexical acquisition model for english verb particle constructions which operates over unlexicalised features mined from a partial parsing chart . the proposed technique is shown to outperform a state-of-the-art parser over the target task , despite being based on relatively simplistic features .

for distributional similarity fg language technology
in this paper , we address the role of syntactic parsing for distributional similarity . on the one hand , we are exploring distributional similarities as an extrinsic test bed for unsupervised parsers . on the other hand , we explore whether single unsupervised parsers , or their combination , can contribute to better distributional similarities , or even replace supervised parsing as a preprocessing step for word similarity . we evaluate distributional thesauri against manually created taxonomies both for english and german for five unsupervised parsers . while for english , a supervised parser is the best single parser in this evaluation , we find an unsupervised parser to work best for german . for both languages , we show significant improvements in word similarity when combining features from supervised and unsupervised parsers . to our knowledge , this is the first work where unsupervised parsers are systematically evaluated extrinsically in a semantic task , and the first work to show that unsupervised parsing can complement and even replace supervised parsing , when used as a pre-processing feature .

potsdam : semantic dependency parsing by bidirectional graph-tree
we present the potsdam systems that participated in the semantic dependency parsing shared task of semeval 2014. they are based on linguistically motivated bidirectional transformations between graphs and trees and on utilization of syntactic dependency parsing . they were entered in both the closed track and the open track of the challenge , recording a peak average labeled f 1 score of 78.60 .

chinese syntactic parsing based on extended glr parsing
this paper presents an extended glr parsing algorithm with grammar pcfg* that is based on tomitas glr parsing algorithm and extends it further . we also define a new grammarpcfg* that is based on pcfg and assigns not only probability but also frequency associated with each rule . so our syntactic parsing system is implemented based on rule-based approach and statistics approach . furthermore our experiments are executed in two fields : chinese base noun phrase identification and full syntactic parsing . and the results of these two fields are compared from three ways . the experiments prove that the extended glr parsing algorithm with pcfg* is an efficient parsing method and a straightforward way to combine statistical property with rules . the experiment results of these two fields are presented in this paper .

multilingual and cross-lingual news topic tracking bruno pouliquen , ralf steinberger , camelia ignat , emilia ksper & irina temnikova
we are presenting a working system for automated news analysis that ingests an average total of 7600 news articles per day in five languages . for each language , the system detects the major news stories of the day using a group-average unsupervised agglomerative clustering process . it also tracks , for each cluster , related groups of articles published over the previous seven days , using a cosine of weighted terms . the system furthermore tracks related news across languages , in all language pairs involved . the cross-lingual news cluster similarity is based on a linear combination of three types of input : ( a ) cognates , ( b ) automatically detected references to geographical place names and ( c ) the results of a mapping process onto a multilingual classification system . a manual evaluation showed that the system produces good results .

predicting emotion in spoken dialogue from multiple knowledge sources
we examine the utility of multiple types of turn-level and contextual linguistic features for automatically predicting student emotions in human-human spoken tutoring dialogues . we first annotate student turns in our corpus for negative , neutral and positive emotions . we then automatically extract features representing acoustic-prosodic and other linguistic information from the speech signal and associated transcriptions . we compare the results of machine learning experiments using different feature sets to predict the annotated emotions . our best performing feature set contains both acoustic-prosodic and other types of linguistic features , extracted from both the current turn and a context of previous student turns , and yields a prediction accuracy of 84.75 % , which is a 44 % relative improvement in error reduction over a baseline . our results suggest that the intelligent tutoring spoken dialogue system we are developing can b guo e enhanced to automatically predict and adapt to student emotions .

jointly modeling aspects and opinions with a maxent-lda hybrid
discovering and summarizing opinions from online reviews is an important and challenging task . a commonly-adopted framework generates structured review summaries with aspects and opinions . recently topic models have been used to identify meaningful review aspects , but existing topic models do not identify aspect-specific opinion words . in this paper , we propose a maxent-lda hybrid model to jointly discover both aspects and aspect-specific opinion words . we show that with a relatively small amount of training data , our model can effectively identify aspect and opinion words simultaneously . we also demonstrate the domain adaptability of our model .

parser evaluation using derivation trees : a complement to evalb
this paper introduces a new technique for phrase-structure parser analysis , categorizing possible treebank structures by integrating regular expressions into derivation trees . we analyze the performance of the berkeley parser on ontonotes wsj and the english web treebank . this provides some insight into the evalb scores , and the problem of domain adaptation with the web data . we also analyze a test-ontrain dataset , showing a wide variance in how the parser is generalizing from different structures in the training material .

automatic feature engineering for answer selection and extraction
this paper proposes a framework for automatically engineering features for two important tasks of question answering : answer sentence selection and answer extraction . we represent question and answer sentence pairs with linguistic structures enriched by semantic information , where the latter is produced by automatic classifiers , e.g. , question classifier and named entity recognizer . tree kernels applied to such structures enable a simple way to generate highly discriminative structural features that combine syntactic and semantic information encoded in the input trees . we conduct experiments on a public benchmark from trec to compare with previous systems for answer sentence selection and answer extraction . the results show that our models greatly improve on the state of the art , e.g. , up to 22 % on f1 ( relative improvement ) for answer extraction , while using no additional resources and no manual feature engineering .

a latent variable model for geographic lexical variation
the rapid growth of geotagged social media raises new computational possibilities for investigating geographic linguistic variation . in this paper , we present a multi-level generative model that reasons jointly about latent topics and geographical regions . high-level topics such as sports or entertainment are rendered differently in each geographic region , revealing topic-specific regional distinctions . applied to a new dataset of geotagged microblogs , our model recovers coherent topics and their regional variants , while identifying geographic areas of linguistic consistency . the model also enables prediction of an authors geographic location from raw text , outperforming both text regression and supervised topic models .

unpmc : nave approach to extract keyphrases from scientific articles
we describe our method for extracting keyphrases from scientific articles which we participate in the shared task of semeval-2 evaluation exercise . even though general-purpose term extractors along with linguistically-motivated analysis allow us to extract elaborated morphosyntactic variation forms of terms , a nave statistic approach proposed in this paper is very simple and quite efficient for extracting keyphrases especially from wellstructured scientific articles . based on the characteristics of keyphrases with section information , we obtain 18.34 % for f-measure using top 15 candidates . we also show further improvement without any complications and we discuss this at the end of the paper .

user simulations for context-sensitive speech recognition in spoken
we use a machine learner trained on a combination of acoustic and contextual features to predict the accuracy of incoming n-best automatic speech recognition ( asr ) hypotheses to a spoken dialogue system ( sds ) . our novel approach is to use a simple statistical user simulation ( us ) for this task , which measures the likelihood that the user would say each hypothesis in the current context . such us models are now common in machine learning approaches to sds , are trained on real dialogue data , and are related to theories of alignment in psycholinguistics . we use a us to predict the users next dialogue move and thereby re-rank n-best hypotheses of a speech recognizer for a corpus of 2564 user utterances . the method achieved a significant relative reduction of word error rate ( wer ) of 5 % ( this is 44 % of the possible wer improvement on this data ) , and 62 % of the possible semantic improvement ( dialogue move accuracy ) , compared to the baseline policy of selecting the topmost asr hypothesis . the majority of the improvement is attributable to the user simulation feature , as shown by information gain analysis .

tightly packed tries : how to fit large models into memory , and make them load fast , too eric joanis samuel larkin
we present tightly packed tries ( tpts ) , a compact implementation of read-only , compressed trie structures with fast on-demand paging and short load times . we demonstrate the benefits of tpts for storing n-gram back-off language models and phrase tables for statistical machine translation . encoded as tpts , these databases require less space than flat text file representations of the same data compressed with the gzip utility . at the same time , they can be mapped into memory quickly and be searched directly in time linear in the length of the key , without the need to decompress the entire file . the overhead for local decompression during search is marginal .

adding redundant features for crfs-based sentence sentiment
in this paper , we present a novel method based on crfs in response to the two special characteristics of contextual dependency and label redundancy in sentence sentiment classification . we try to capture the contextual constraints on sentence sentiment using crfs . through introducing redundant labels into the original sentimental label set and organizing all labels into a hierarchy , our method can add redundant features into training for capturing the label redundancy . the experimental results prove that our method outperforms the traditional methods like nb , svm , maxent and standard chain crfs . in comparison with the cascaded model , our method can effectively alleviate the error propagation among different layers and obtain better performance in each layer .

learning simple wikipedia : a cogitation in ascertaining abecedarian language
text simplification is the process of changing vocabulary and grammatical structure to create a more accessible version of the text while maintaining the underlying information and content . automated tools for text simplification are a practical way to make large corpora of text accessible to a wider audience lacking high levels of fluency in the corpus language . in this work , we investigate the potential of simple wikipedia to assist automatic text simplification by building a statistical classification system that discriminates simple english from ordinary english . most text simplification systems are based on hand-written rules ( e.g. , pest ( carroll et al , 1999 ) and its module systar ( canning et al , 2000 ) ) , and therefore face limitations scaling and transferring across domains . the potential for using simple wikipedia for text simplification is significant ; it contains nearly 60,000 articles with revision histories and aligned articles to ordinary english wikipedia . using articles from simple wikipedia and ordinary wikipedia , we evaluated different classifiers and feature sets to identify the most discriminative features of simple english for use across domains . these findings help further understanding of what makes text simple and can be applied as a tool to help writers craft simple text .

exploring cross-language statistical machine translation for closely related south slavic languages
this work investigates the use of crosslanguage resources for statistical machine translation ( smt ) between english and two closely related south slavic languages , namely croatian and serbian . the goal is to explore the effects of translating from and into one language using an smt system trained on another . for translation into english , a loss due to cross-translation is about 13 % of bleu and for the other translation direction about 15 % . the performance decrease for both languages in both translation directions is mainly due to lexical divergences . several language adaptation methods are explored , and it is shown that very simple lexical transformations already can yield a small improvement , and that the most promising adaptation method is using a croatian-serbian smt system trained on a very small corpus .

language independent transliteration system using phrase based smt approach on substrings ibm cairo technology & development
everyday the newswire introduce events from all over the world , highlighting new names of persons , locations and organizations with different origins . these names appear as out of vocabulary ( oov ) words for machine translation , cross lingual information retrieval , and many other nlp applications . one way to deal with oov words is to transliterate the unknown words , that is , to render them in the orthography of the second language . we introduce a statistical approach for transliteration only using the bilingual resources released in the shared task and without any previous knowledge of the target languages . mapping the transliteration problem to the machine translation problem , we make use of the phrase based smt approach and apply it on substrings of names . in the english to russian task , we report acc ( accuracy in top-1 ) of 0.545 , mean f-score of 0.917 , and mrr ( mean reciprocal rank ) of 0.596. due to time constraints , we made a single experiment in the english to chinese task , reporting acc , mean f-score , and mrr of 0.411 , 0.737 , and 0.464 respectively . finally , it is worth mentioning that the system is language independent since the author is not aware of either languages used in the experiments .

machine translation with lattices and forests
traditional 1-best translation pipelines suffer a major drawback : the errors of 1best outputs , inevitably introduced by each module , will propagate and accumulate along the pipeline . in order to alleviate this problem , we use compact structures , lattice and forest , in each module instead of 1-best results . we integrate both lattice and forest into a single tree-to-string system , and explore the algorithms of lattice parsing , lattice-forest-based rule extraction and decoding . more importantly , our model takes into account all the probabilities of different steps , such as segmentation , parsing , and translation . the main advantage of our model is that we can make global decision to search for the best segmentation , parse-tree and translation in one step . medium-scale experiments show an improvement of +0.9 bleu points over a state-of-the-art forest-based baseline .

automatic prediction of cognate orthography using support vector machines
this paper describes an algorithm to automatically generate a list of cognates in a target language by means of support vector machines . while levenshtein distance was used to align the training file , no knowledge repository other than an initial list of cognates used for training purposes was input into the algorithm . evaluation was set up in a cognate production scenario which mimed a reallife situation where no word lists were available in the target language , delivering the ideal environment to test the feasibility of a more ambitious project that will involve language portability . an overall improvement of 50.58 % over the baseline showed promising horizons .

towards identifying the resolvability of threads in moocs
one important function of the discussion forums of massive open online courses ( moocs ) is for students to post problems they are unable to resolve and receive help from their peers and instructors . there are a large proportion of threads that are not resolved to the satisfaction of the students for various reasons . in this paper , we attack this problem by firstly constructing a conceptual model validated using a structural equation modeling technique , which enables us to understand the factors that influence whether a problem thread is satisfactorily resolved . we then demonstrate the robustness of these findings using a predictive model that illustrates how accurately those factors can be used to predict whether a thread is resolved or unresolved . experiments conducted on one mooc show that thread resolveability connects closely to our proposed five dimensions and that the predictive ensemble model gives better performance over several baselines .

matching inconsistently spelled names in automatic speech recognizer output for information retrieval
many proper names are spelled inconsistently in speech recognizer output , posing a problem for applications where locating mentions of named entities is critical . we model the distortion in the spelling of a name due to the speech recognizer as the effect of a noisy channel . the models follow the framework of the ibm translation models . the model is trained using a parallel text of closed caption and automatic speech recognition output . we also test a string edit distance based method . the effectiveness of these models is evaluated on a name query retrieval task . our methods result in a 60 % improvement in f1 . we also demonstrate why the problem has not been critical in trec and tdt tasks .

experiments with geographic knowledge for information extraction
here we present work on using spatial knowledge in conjunction with information extraction ( ie ) . considerable volume of location data was imported in a knowledge base ( kb ) with entities of general importance used for semantic annotation , indexing , and retrieval of text . the semantic web knowledge representation standards are used , namely rdf ( s ) . an extensive upper-level ontology with more than two hundred classes is designed . with respect to the locations , the goal was to include the most important categories considering public and tasks not specially related to geography or related areas . the locations data is derived from number of publicly available resources and combined to assure best performance for domainindependent named-entity recognition in text . an evaluation and comparison to high performance ie application is given .

a polynomial-time parsing algorithm for tt-mctag
this paper investigates the class of treetuple mctag with shared nodes , ttmctag for short , an extension of tree adjoining grammars that has been proposed for natural language processing , in particular for dealing with discontinuities and word order variation in languages such as german . it has been shown that the universal recognition problem for this formalism is np-hard , but so far it was not known whether the class of languages generated by tt-mctag is included in ptime . we provide a positive answer to this question , using a new characterization of ttmctag .

using three way data for word sense discrimination
in this paper , an extension of a dimensionality reduction algorithm called nonnegative matrix factorization is presented that combines both bag of words data and syntactic data , in order to find semantic dimensions according to which both words and syntactic relations can be classified . the use of three way data allows one to determine which dimension ( s ) are responsible for a certain sense of a word , and adapt the corresponding feature vector accordingly , subtracting one sense to discover another one . the intuition in this is that the syntactic features of the syntax-based approach can be disambiguated by the semantic dimensions found by the bag of words approach . the novel approach is embedded into clustering algorithms , to make it fully automatic . the approach is carried out for dutch , and evaluated against eurowordnet .

uva : language modeling techniques for web people search
in this paper we describe our participation in the semeval 2007 web people search task . our main aim in participating was to adapt language modeling tools for the task , and to experiment with various document representations . our main finding is that single pass clustering , using title , snippet and body to represent documents , is the most effective setting .

which noun phrases denote which concepts
resolving polysemy and synonymy is required for high-quality information extraction . we present conceptresolver , a component for the never-ending language learner ( nell ) ( carlson et al , 2010 ) that handles both phenomena by identifying the latent concepts that noun phrases refer to . conceptresolver performs both word sense induction and synonym resolution on relations extracted from text using an ontology and a small amount of labeled data . domain knowledge ( the ontology ) guides concept creation by defining a set of possible semantic types for concepts . word sense induction is performed by inferring a set of semantic types for each noun phrase . synonym detection exploits redundant information to train several domain-specific synonym classifiers in a semi-supervised fashion . when conceptresolver is run on nells knowledge base , 87 % of the word senses it creates correspond to real-world concepts , and 85 % of noun phrases that it suggests refer to the same concept are indeed synonyms .

a salience driven approach to robust input interpretation in multimodal conversational systems
to improve the robustness in multimodal input interpretation , this paper presents a new salience driven approach . this approach is based on the observation that , during multimodal conversation , information from deictic gestures ( e.g. , point or circle ) on a graphical display can signal a part of the physical world ( i.e. , representation of the domain and task ) of the application which is salient during the communication . this salient part of the physical world will prime what users tend to communicate in speech and in turn can be used to constrain hypotheses for spoken language understanding , thus improving overall input interpretation . our experimental results have indicated the potential of this approach in reducing word error rate and improving concept identification in multimodal conversation .

evaluating word order recursively over permutation-forests
automatically evaluating word order of mt system output at the sentence-level is challenging . at the sentence-level , ngram counts are rather sparse which makes it difficult to measure word order quality effectively using lexicalized units . recent approaches abstract away from lexicalization by assigning a score to the permutation representing how word positions in system output move around relative to a reference translation . metrics over permutations exist ( e.g. , kendal tau or spearman rho ) and have been shown to be useful in earlier work . however , none of the existing metrics over permutations groups word positions recursively into larger phrase-like blocks , which makes it difficult to account for long-distance reordering phenomena . in this paper we explore novel metrics computed over permutation forests ( pefs ) , packed charts of permutation trees ( pets ) , which are tree decompositions of a permutation into primitive ordering units . we empirically compare pefs metric against five known reordering metrics on wmt13 data for ten language pairs . the pefs metric shows better correlation with human ranking than the other metrics almost on all language pairs . none of the other metrics exhibits as stable behavior across language pairs .

using predicate-argument structures for information extraction
in this paper we present a novel , customizable ie paradigm that takes advantage of predicate-argument structures . we also introduce a new way of automatically identifying predicate argument structures , which is central to our ie paradigm . it is based on : ( 1 ) an extended set of features ; and ( 2 ) inductive decision tree learning . the experimental results prove our claim that accurate predicate-argument structures enable high quality ie results .

emma : a novel evaluation metric for morphological analysis
we present a novel evaluation metric for morphological analysis ( emma ) that is both linguistically appealing and empirically sound . emma uses a graphbased assignment algorithm , optimized via integer linear programming , to match morphemes of predicted word analyses to the analyses of a morphologically rich answer key . this is necessary especially for unsupervised morphology analysis systems which do not have access to linguistically motivated morpheme labels . across 3 languages , emma scores of 14 systems have a substantially greater positive correlation with mean average precision in an information retrieval ( ir ) task than do scores from the metric currently used by the morpho challenge ( mc ) competition series . we compute emma and mc metric scores for 93 separate system-language pairs from the 2007 , 2008 , and 2009 mc competitions , demonstrating that emma is not susceptible to two types of gaming that have plagued recent mc competitions : ambiguity hijacking and shared morpheme padding .

combining neural networks and statistics for chinese word sense disambiguation
the input of network is the key problem for chinese word sense disambiguation utilizing the neural network . this paper presents an input model of neural network that calculates the mutual information between contextual words and ambiguous word by using statistical method and taking the contextual words to certain number beside the ambiguous word according to ( -m , +n ) . the experiment adopts triple-layer bp neural network model and proves how the size of training set and the value of m and n affect the performance of neural network model . the experimental objects are six pseudowords owning three word-senses constructed according to certain principles . tested accuracy of our approach on a close-corpus reaches 90.31 % , , and 89.62 % on a open-corpus . the experiment proves that the neural network model has good performance on word sense disambiguation .

developing a typology of dialogue acts : some boundary problems
the paper gives an overview of a typology of dialogue acts used for annotating estonian spoken dialogues . several problems of the classification and determining of dialogue acts are considered . our further aim is to develop a dialogue system which can interact with the user in natural language following the norms and rules of human-human communication .

probabilistic frame induction jackie chi kit cheung
in natural-language discourse , related events tend to appear near each other to describe a larger scenario . such structures can be formalized by the notion of a frame ( a.k.a . template ) , which comprises a set of related events and prototypical participants and event transitions . identifying frames is a prerequisite for information extraction and natural language generation , and is usually done manually . methods for inducing frames have been proposed recently , but they typically use ad hoc procedures and are difficult to diagnose or extend . in this paper , we propose the first probabilistic approach to frame induction , which incorporates frames , events , and participants as latent topics and learns those frame and event transitions that best explain the text . the number of frame components is inferred by a novel application of a split-merge method from syntactic parsing . in end-to-end evaluations from text to induced frames and extracted facts , our method produces state-of-the-art results while substantially reducing engineering effort .

multi-scale personalization for voice search applications
voice search applications provide a very convenient and direct access to a broad variety of services and information . however , due to the vast amount of information available and the open nature of the spoken queries , these applications still suffer from recognition errors . this paper explores the utilization of personalization features for the post-processing of recognition results in the form of n-best lists . personalization is carried out from three different angles : short-term , long-term and web-based , and a large variety of features are proposed for use in a log-linear classification framework . experimental results on data obtained from a commercially deployed voice search system show that the combination of the proposed features leads to a substantial sentence error rate reduction . in addition , it is shown that personalization features which are very different in nature can successfully complement each other .

stability and accuracy in incremental speech recognition
conventional speech recognition approaches usually wait until the user has finished talking before returning a recognition hypothesis . this results in spoken dialogue systems that are unable to react while the user is still speaking . incremental speech recognition ( isr ) , where partial phrase results are returned during user speech , has been used to create more reactive systems . however , isr output is unstable and so prone to revision as more speech is decoded . this paper tackles the problem of stability in isr . we first present a method that increases the stability and accuracy of isr output , without adding delay . given that some revisions are unavoidable , we next present a pair of methods for predicting the stability and accuracy of isr results . taken together , we believe these approaches give isr more utility for real spoken dialogue systems .

zipfr : word frequency distributions in r
we introduce the zipfr package , a powerful and user-friendly open-source tool for lnre modeling of word frequency distributions in the r statistical environment . we give some background on lnre models , discuss related software and the motivation for the toolkit , describe the implementation , and conclude with a complete sample session showing a typical lnre analysis .

multi-document biography summarization
in this paper we describe a biography summarization system using sentence classification and ideas from information retrieval . although the individual techniques are not new , assembling and applying them to generate multi-document biographies is new . our system was evaluated in duc2004 . it is among the top performers in task 5short summaries focused by person questions .

an automatic evaluation method for localization oriented lexicalised ebmt system
to help developing a localization oriented ebmt system , an automatic machine translation evaluation method is implemented which adopts edit distance , cosine correlation and dice coefficient as criteria . experiment shows that the evaluation method distinguishes well between good translations and bad ones . to prove that the method is consistent with human evaluation , 6 mt systems are scored and compared . theoretical analysis is made to validate the experimental results . correlation coefficient and significance tests at 0.01 level are made to ensure the reliability of the results . linear regression equations are calculated to map the automatic scoring results to human scorings .

multidisciplinary instruction with the natural language toolkit
the natural language toolkit ( nltk ) is widely used for teaching natural language processing to students majoring in linguistics or computer science . this paper describes the design of nltk , and reports on how it has been used effectively in classes that involve different mixes of linguistics and computer science students . we focus on three key issues : getting started with a course , delivering interactive demonstrations in the classroom , and organizing assignments and projects . in each case , we report on practical experience and make recommendations on how to use nltk to maximum effect .

tagging at sentence level
emotion , the private state of a human entity , is becoming an important topic in natural language processing ( nlp ) with increasing use of search engines . the present task aims to manually annotate the sentences in a web based bengali blog corpus with the emotional components such as emotional expression ( word/phrase ) , intensity , associated holder and topic ( s ) . ekmans six emotion classes ( anger , disgust , fear , happy , sad and surprise ) along with three types of intensities ( high , general and low ) are considered for the sentence level annotation . presence of discourse markers , punctuation marks , negations , conjuncts , reduplication , rhetoric knowledge and especially emoticons play the contributory roles in the annotation process . different types of fixed and relaxed strategies have been employed to measure the agreement of the sentential emotions , intensities , emotional holders and topics respectively . experimental results for each emotion class at word level on a small set of the whole corpus have been found satisfactory .

efficient elicitation of annotations for human evaluation of machine
a main output of the annual workshop on statistical machine translation ( wmt ) is a ranking of the systems that participated in its shared translation tasks , produced by aggregating pairwise sentencelevel comparisons collected from human judges . over the past few years , there have been a number of tweaks to the aggregation formula in attempts to address issues arising from the inherent ambiguity and subjectivity of the task , as well as weaknesses in the proposed models and the manner of model selection . we continue this line of work by adapting the trueskill tm algorithm an online approach for modeling the relative skills of players in ongoing competitions , such as microsofts xbox live to the human evaluation of machine translation output . our experimental results show that trueskill outperforms other recently proposed models on accuracy , and also can significantly reduce the number of pairwise annotations that need to be collected by sampling non-uniformly from the space of system competitions .

optimizing word alignment combination for phrase table training
combining word alignments trained in two translation directions has mostly relied on heuristics that are not directly motivated by intended applications . we propose a novel method that performs combination as an optimization process . our algorithm explicitly maximizes the effectiveness function with greedy search for phrase table training or synchronized grammar extraction . experimental results show that the proposed method leads to significantly better translation quality than existing methods . analysis suggests that this simple approach is able to maintain accuracy while maximizing coverage .

combining constituent parsers
combining the 1-best output of multiple parsers via parse selection or parse hybridization improves f-score over the best individual parser ( henderson and brill , 1999 ; sagae and lavie , 2006 ) . we propose three ways to improve upon existing methods for parser combination . first , we propose a method of parse hybridization that recombines context-free productions instead of constituents , thereby preserving the structure of the output of the individual parsers to a greater extent . second , we propose an efficient lineartime algorithm for computing expected f-score using minimum bayes risk parse selection . third , we extend these parser combination methods from multiple 1-best outputs to multiple n-best outputs . we present results on wsj section 23 and also on the english side of a chinese-english parallel corpus .

mining clinical documents
early recognition of distinguishing patterns of a novel pandemic disease is important . we introduce a methodological approach based on popular data mining techniques to extract key features and temporal patterns of swine ( h1n1 ) flu that is discriminated from swine flu like symptoms .

are acts scores increasing with better translation quality
this paper gives a detailed description of the act ( accuracy of connective translation ) metric , a reference-based metric that assesses only connective translations . act relies on automatic word-level alignment ( using giza++ ) between a source sentence and respectively the reference and candidate translations , along with other heuristics for comparing translations of discourse connectives . using a dictionary of equivalents , the translations are scored automatically or , for more accuracy , semi-automatically . the accuracy of the act metric was assessed by human judges on sample data for english/french , english/arabic , english/italian and english/german translations ; the act scores are within 2-5 % of human scores . the actual version of act is available only for a limited language pairs . consequently , we are participating only for the english/french and english/german language pairs . our hypothesis is that act metric scores increase with better translation quality in terms of human evaluation .

a noisy channel model framework for grammatical correction
we report on the tor system that participated in the 2013 conll shared task on grammatical correction . the system was a provisional implementation of a beam search correction over a noisy channel model . although the results on the shared task test set were poor , the approach may still be promising , as there are many aspects of the current implementation that could be optimised . grammatical correction is inherently difficult both to perform and to evaluate . as such , possible improvements to the evaluation are also discussed .

contextual phenomena and thematic relations in database qa dialogues : results from a wizard-of-oz experiment
considering data obtained from a corpus of database qa dialogues , we address the nature of the discourse structure needed to resolve the several kinds of contextual phenomena found in our corpus . we look at the thematic relations holding between questions and the preceding context and discuss to which extent thematic relatedness plays a role in discourse structure .

social ( distributed ) language modeling , clustering and dialectometry
we present ongoing work in a scalable , distributed implementation of over 200 million individual language models , each capturing a single users dialect in a given language ( multilingual users have several models ) . these have a variety of practical applications , ranging from spam detection to speech recognition , and dialectometrical methods on the social graph . users should be able to view any content in their language ( even if it is spoken by a small population ) , and to browse our site with appropriately translated interface ( automatically generated , for locales with little crowd-sourced community effort ) .

a daml+oil-compliant chinese lexical ontology
this paper presents an ongoing task that will construct a daml+oil-compliant chinese lexical ontology . the ontology mainly comprises three components : a hierarchical taxonomy consisting of a set of concepts and a set of relations describing the relationships among the concepts , a set of lexical entries associated with the concepts and relations , and a set of axioms describing the constraints on the ontology . it currently contains 1,075 concepts , 65,961 lexical entries associated with the concepts , 299 relations among the concepts excluding the hypernym and hyponym relations , 27,004 relations between the lexical entries and the concepts , and 79,723 relations associating the lexical entries with the concepts .

meerkat mafia : multilingual and cross-level semantic textual similarity systems
we describe umbcs systems developed for the semeval 2014 tasks on multilingual semantic textual similarity ( task 10 ) and cross-level semantic similarity ( task 3 ) . our best submission in the multilingual task ranked second in both english and spanish subtasks using an unsupervised approach . our best systems for cross-level task ranked second in paragraph-sentence and first in both sentence-phrase and word-sense subtask . the system ranked first for the phraseword subtask but was not included in the official results due to a late submission .

towards syntax-aware compositional distributional semantic models fabio massimo zanzotto
compositional distributional semantics models ( cdsms ) are traditionally seen as an entire different world with respect to tree kernels ( tks ) . in this paper , we show that under a suitable regime these two approaches can be regarded as the same and , thus , structural information and distributional semantics can successfully cooperate in csdms for nlp tasks . leveraging on distributed trees , we present a novel class of cdsms that encode both structure and distributional meaning : the distributed smoothed trees ( dsts ) . by using dsts to compute the similarity among sentences , we implicitly define the distributed smoothed tree kernels ( dstks ) . experiment with our dsts show that dstks approximate the corresponding smoothed tree kernels ( stks ) . thus , dsts encode both structural and distributional semantics of text fragments as stks do . experiments on rte and sts show that distributional semantics encoded in dstks increase performance over structure-only kernels .

co-occurrence cluster features for lexical substitutions in context
this paper examines the influence of features based on clusters of co-occurrences for supervised word sense disambiguation and lexical substitution . cooccurrence cluster features are derived from clustering the local neighborhood of a target word in a co-occurrence graph based on a corpus in a completely unsupervised fashion . clusters can be assigned in context and are used as features in a supervised wsd system . experiments fitting a strong baseline system with these additional features are conducted on two datasets , showing improvements . cooccurrence features are a simple way to mimic topic signatures ( martnez et al , 2008 ) without needing to construct resources manually . further , a system is described that produces lexical substitutions in context with very high precision .

enriching entity translation discovery using selective temporality
this paper studies named entity translation and proposes selective temporality as a new feature , as using temporal features may be harmful for translating atemporal entities . our key contribution is building an automatic classifier to distinguish temporal and atemporal entities then align them in separate procedures to boost translation accuracy by 6.1 % .

corpus-based induction of syntactic structure : models of dependency and constituency
we present a generative model for the unsupervised learning of dependency structures . we also describe the multiplicative combination of this dependency model with a model of linear constituency . the product model outperforms both components on their respective evaluation metrics , giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing . we also demonstrate that the combined model works and is robust cross-linguistically , being able to exploit either attachment or distributional regularities that are salient in the data .

induction of greedy controllers for deterministic treebank
most statistical parsers have used the grammar induction approach , in which a stochastic grammar is induced from a treebank . an alternative approach is to induce a controller for a given parsing automaton . such controllers may be stochastic ; here , we focus on greedy controllers , which result in deterministic parsers . we use decision trees to learn the controllers . the resulting parsers are surprisingly accurate and robust , considering their speed and simplicity . they are almost as fast as current part-ofspeech taggers , and considerably more accurate than a basic unlexicalized pcfg parser . we also describe markov parsing models , a general framework for parser modeling and control , of which the parsers reported here are a special case .

language-independent parsing with empty elements
we present a simple , language-independent method for integrating recovery of empty elements into syntactic parsing . this method outperforms the best published method we are aware of on english and a recently published method on chinese .

bilingual lexicon generation using non-aligned signatures
bilingual lexicons are fundamental resources . modern automated lexicon generation methods usually require parallel corpora , which are not available for most language pairs . lexicons can be generated using non-parallel corpora or a pivot language , but such lexicons are noisy . we present an algorithm for generating a high quality lexicon from a noisy one , which only requires an independent corpus for each language . our algorithm introduces non-aligned signatures ( nas ) , a cross-lingual word context similarity score that avoids the over-constrained and inefficient nature of alignment-based methods . we use nas to eliminate incorrect translations from the generated lexicon . we evaluate our method by improving the quality of noisy spanish-hebrew lexicons generated from two pivot english lexicons . our algorithm substantially outperforms other lexicon generation methods .

using distributional similarity of multi-way translations to predict multiword expression compositionality
we predict the compositionality of multiword expressions using distributional similarity between each component word and the overall expression , based on translations into multiple languages . we evaluate the method over english noun compounds , english verb particle constructions and german noun compounds . we show that the estimation of compositionality is improved when using translations into multiple languages , as compared to simply using distributional similarity in the source language . we further find that string similarity complements distributional similarity .

classification of discourse coherence relations : an exploratory study using multiple knowledge sources , james pustejovsky , catherine havasi , and roser saur the mitre corporation
in this paper we consider the problem of identifying and classifying discourse coherence relations . we report initial results over the recently released discourse graphbank ( wolf and gibson , 2005 ) . our approach considers , and determines the contributions of , a variety of syntactic and lexico-semantic features . we achieve 81 % accuracy on the task of discourse relation type classification and 70 % accuracy on relation identification .

the link structure of language communities and its implication for rizza camus caminero
since its inception , the world wide web ( www ) has exponentially grown to shelter billions of monolingual and multilingual web pages that can be navigated through hyperlinks . its structural properties provide useful information in presenting the sociolinguistic properties of the web . in this study , about 26 million web pages under the south east asian country code toplevel-domains ( cctlds ) are analyzed , several language communities are identified , and the graph structure of these communities are analyzed . the distance between language communities are calculated by a distance metrics based on the number of outgoing links between web pages . intermediary languages are identified by graph analysis . by creating a language subgraph , the size and diameter of its stronglyconnected components are derived , as these values are useful parameters for languagespecific crawling . performing a link structure analysis of the web pages can be a useful tool for socio-linguistic and technical research purposes .

feature selection for a rich hpsg grammar using decision trees
this paper examines feature selection for log linear models over rich constraint-based grammar ( hpsg ) representations by building decision trees over features in corresponding probabilistic context free grammars ( pcfgs ) . we show that single decision trees do not make optimal use of the available information ; constructed ensembles of decision trees based on different feature subspaces show significant performance gains ( 14 % parse selection error reduction ) . we compare the performance of the learned pcfg grammars and log linear models over the same features .

modeling human sentence processing data with a statistical
it has previously been assumed in the psycholinguistic literature that finite-state models of language are crucially limited in their explanatory power by the locality of the probability distribution and the narrow scope of information used by the model . we show that a simple computational model ( a bigram part-of-speech tagger based on the design used by corley and crocker ( 2000 ) ) makes correct predictions on processing difficulty observed in a wide range of empirical sentence processing data . we use two modes of evaluation : one that relies on comparison with a control sentence , paralleling practice in human studies ; another that measures probability drop in the disambiguating region of the sentence . both are surprisingly good indicators of the processing difficulty of garden-path sentences . the sentences tested are drawn from published sources and systematically explore five different types of ambiguity : previous studies have been narrower in scope and smaller in scale . we do not deny the limitations of finite-state models , but argue that our results show that their usefulness has been underestimated .

using cycles and quasi-cycles to disambiguate dictionary glosses dipartimento di informatica
we present a novel graph-based algorithm for the automated disambiguation of glosses in lexical knowledge resources . a dictionary graph is built starting from senses ( vertices ) and explicit or implicit relations in the dictionary ( edges ) . the approach is based on the identification of edge sequences which constitute cycles in the dictionary graph ( possibly with one edge reversed ) and relate a source to a target word sense . experiments are performed on the disambiguation of ambiguous words in the glosses of wordnet and two machine-readable dictionaries .

syntax is from mars while semantics from venus ! insights from spectral analysis of distributional similarity networks
we study the global topology of the syntactic and semantic distributional similarity networks for english through the technique of spectral analysis . we observe that while the syntactic network has a hierarchical structure with strong communities and their mixtures , the semantic network has several tightly knit communities along with a large core without any such welldefined community structure .

factoid question answering with web , mobile and speech interfaces
in this paper we describe the web and mobile-phone interfaces to our multilanguage factoid question answering ( qa ) system together with a prototype speech interface to our english-language qa system . using a statistical , data-driven approach to factoid question answering has allowed us to develop qa systems in five languages in a matter of months . in the web-based system , which is accessible at http : //asked.jp , we have combined the qa system output with standard search-engine-like results by integrating it with an open-source web search engine . the prototype speech interface is based around a voicexml application running on the voxeo developer platform . recognition of the users question is performed on a separate speech recognition server dedicated to recognizing questions . an adapted version of the sphinx-4 recognizer is used for this purpose . once the question has been recognized correctly it is passed to the qa system and the resulting answers read back to the user by speech synthesis . our approach is modular and makes extensive use of opensource software . consequently , each component can be easily and independently improved and easily extended to other languages .

classification of south african languages using text and acoustic based methods : a case of six selected languages peleira nicholas zulu
language variations are generally known to have a severe impact on the performance of human language technology systems . in order to predict or improve system performance , a thorough investigation into these variations , similarities and dissimilarities , is required . distance measures have been used in several applications of speech processing to analyze different varying speech attributes . however , not much work has been done on language distance measures , and even less work has been done involving south african languages . this study explores two methods for measuring the linguistic distance of six south african languages . it concerns a text based method , ( the levenshtein distance ) , and an acoustic approach using extracted mean pitch values . the levenshtein distance uses parallel word transcriptions from all six languages with as little as 144 words , whereas the pitch method is text-independent and compares mean language pitch differences . cluster analysis resulting from the distance matrices from both methods correlates closely with human perceptual distances and existing literature about the six languages .

interpreting semantic relations in noun compounds via verb semantics su nam kim and timothy baldwin
we propose a novel method for automatically interpreting compound nouns based on a predefined set of semantic relations . first we map verb tokens in sentential contexts to a fixed set of seed verbs using wordnet : :similarity and mobys thesaurus . we then match the sentences with semantic relations based on the semantics of the seed verbs and grammatical roles of the head noun and modifier . based on the semantics of the matched sentences , we then build a classifier using timbl . the performance of our final system at interpreting ncs is 52.6 % .

three reasons to adopt tag-based surface realisation
surface realisation from flat semantic formulae is known to be exponential in the length of the input . in this paper , we argue that tag naturally supports the integration of three main ways of reducing complexity : polarity filtering , delayed adjunction and empty semantic items elimination . we support these claims by presenting some preliminary results of the tag-based surface realiser geni .

improving sparse word similarity models with asymmetric measures
we show that asymmetric models based on tversky ( 1977 ) improve correlations with human similarity judgments and nearest neighbor discovery for both frequent and middle-rank words . in accord with tverskys discovery that asymmetric similarity judgments arise when comparing sparse and rich representations , improvement on our two tasks can be traced to heavily weighting the feature bias toward the rarer word when comparing high- and midfrequency words .

extraction of tree adjoining grammars from a treebank for korean
we present the implementation of a system which extracts not only lexicalized grammars but also feature-based lexicalized grammars from korean sejong treebank . we report on some practical experiments where we extract tag grammars and tree schemata . above all , full-scale syntactic tags and well-formed morphological analysis in sejong treebank allow us to extract syntactic features . in addition , we modify treebank for extracting lexicalized grammars and convert lexicalized grammars into tree schemata to resolve limited lexical coverage problem of extracted lexicalized grammars .

automatic acquisition of english topic signatures based on a second language
we present a novel approach for automatically acquiring english topic signatures . given a particular concept , or word sense , a topic signature is a set of words that tend to co-occur with it . topic signatures can be useful in a number of natural language processing ( nlp ) applications , such as word sense disambiguation ( wsd ) and text summarisation . our method takes advantage of the different way in which word senses are lexicalised in english and chinese , and also exploits the large amount of chinese text available in corpora and on the web . we evaluated the topic signatures on a wsd task , where we trained a second-order vector cooccurrence algorithm on standard wsd datasets , with promising results .

lexical normalisation of short text messages : makn sens a # twitter
twitter provides access to large volumes of data in real time , but is notoriously noisy , hampering its utility for nlp . in this paper , we target out-of-vocabulary words in short text messages and propose a method for identifying and normalising ill-formed words . our method uses a classifier to detect ill-formed words , and generates correction candidates based on morphophonemic similarity . both word similarity and context are then exploited to select the most probable correction candidate for the word . the proposed method doesnt require any annotations , and achieves state-of-the-art performance over an sms corpus and a novel dataset based on twitter .

towards an annotated corpus of discourse relations in hindi
we describe our initial efforts towards developing a large-scale corpus of hindi texts annotated with discourse relations . adopting the lexically grounded approach of the penn discourse treebank ( pdtb ) , we present a preliminary analysis of discourse connectives in a small corpus . we describe how discourse connectives are represented in the sentence-level dependency annotation in hindi , and discuss how the discourse annotation can enrich this level for research and applications . the ultimate goal of our work is to build a hindi discourse relation bank along the lines of the pdtb . our work will also contribute to the cross-linguistic understanding of discourse connectives .

a finite-state model of human sentence processing
it has previously been assumed in the psycholinguistic literature that finite-state models of language are crucially limited in their explanatory power by the locality of the probability distribution and the narrow scope of information used by the model . we show that a simple computational model ( a bigram part-of-speech tagger based on the design used by corley and crocker ( 2000 ) ) makes correct predictions on processing difficulty observed in a wide range of empirical sentence processing data . we use two modes of evaluation : one that relies on comparison with a control sentence , paralleling practice in human studies ; another that measures probability drop in the disambiguating region of the sentence . both are surprisingly good indicators of the processing difficulty of garden-path sentences . the sentences tested are drawn from published sources and systematically explore five different types of ambiguity : previous studies have been narrower in scope and smaller in scale . we do not deny the limitations of finite-state models , but argue that our results show that their usefulness has been underestimated .

error mining in parsing results projet
we introduce an error mining technique for automatically detecting errors in resources that are used in parsing systems . we applied this technique on parsing results produced on several million words by two distinct parsing systems , which share the syntactic lexicon and the pre-parsing processing chain . we were thus able to identify missing and erroneous information in these resources .

educational testing service beata beigman klebanov educational testing service
we present an automated system that computes multi-cue associations and generates associated-word suggestions , using lexical co-occurrence data from a large corpus of english texts . the system performs expansion of cue words to their inflectional variants , retrieves candidate words from corpus data , finds maximal associations between candidates and cues , computes an aggregate score for each candidate , and outputs an n-best list of candidates . we present experiments using several measures of statistical association , two methods of score aggregation , ablation of resources and applying additional filters on retrieved candidates . the system achieves 18.6 % precision on the cogalex-4 shared task data . results with additional evaluation methods are presented . we also describe an annotation experiment which suggests that the shared task may underestimate the appropriateness of candidate words produced by the corpus-based system .

predicting cloze task quality for vocabulary training adam skory maxine eskenazi
computer generation of cloze tasks still falls short of full automation ; most current systems are used by teachers as authoring aids . improved methods to estimate cloze quality are needed for full automation . we investigated lexical reading difficulty as a novel automatic estimator of cloze quality , to which cooccurrence frequency of words was compared as an alternate estimator . rather than relying on expert evaluation of cloze quality , we submitted open cloze tasks to workers on amazon mechanical turk ( amt ) and discuss ways to measure of the results of these tasks . results show one statistically significant correlation between the above measures and estimators , which was lexical co-occurrence and cloze easiness . reading difficulty was not found to correlate significantly . we gave subsets of cloze sentences to an english teacher as a gold standard . sentences selected by co-occurrence and cloze easiness were ranked most highly , corroborating the evidence from amt .

first- and second-order expectation semirings with applications to minimum-risk training on translation forests
many statistical translation models can be regarded as weighted logical deduction . under this paradigm , we use weights from the expectation semiring ( eisner , 2002 ) , to compute first-order statistics ( e.g. , the expected hypothesis length or feature counts ) over packed forests of translations ( lattices or hypergraphs ) . we then introduce a novel second-order expectation semiring , which computes second-order statistics ( e.g. , the variance of the hypothesis length or the gradient of entropy ) . this second-order semiring is essential for many interesting training paradigms such as minimum risk , deterministic annealing , active learning , and semi-supervised learning , where gradient descent optimization requires computing the gradient of entropy or risk . we use these semirings in an open-source machine translation toolkit , joshua , enabling minimum-risk training for a benefit of up to 1.0 bleu point .

syntax-driven machine translation as a model of esl
in this work , we model the writing revision process of english as a second language ( esl ) students with syntaxdriven machine translation methods . we compare two approaches : tree-tostring transformations ( yamada and knight , 2001 ) and tree-to-tree transformations ( smith and eisner , 2006 ) . results suggest that while the tree-totree model provides a greater coverage , the tree-to-string approach offers a more plausible model of esl learners revision writing process .

modeling reference interviews as a basis for improving automatic qa
the automatic qa system described in this paper uses a reference interview model to allow the user to guide and contribute to the qa process . a set of system capabilities was designed and implemented that defines how the users contributions can help improve the system . these include tools , called the query template builder and the knowledge base builder , that tailor the document processing and qa system to a particular domain by allowing a subject matter expert to contribute to the query representation and to the domain knowledge . during the qa process , the system can interact with the user to improve query terminology by using spell checking , answer type verification , expansions and acronym clarifications . the system also has capabilities that depend upon , and expand the users history of interaction with the system , including a user profile , reference resolution , and question similarity modules

learning to say it well : reranking realizations by predicted synthesis quality
this paper presents a method for adapting a language generator to the strengths and weaknesses of a synthetic voice , thereby improving the naturalness of synthetic speech in a spoken language dialogue system . the method trains a discriminative reranker to select paraphrases that are predicted to sound natural when synthesized . the ranker is trained on realizer and synthesizer features in supervised fashion , using human judgements of synthetic voice quality on a sample of the paraphrases representative of the generators capability . results from a cross-validation study indicate that discriminative paraphrase reranking can achieve substantial improvements in naturalness on average , ameliorating the problem of highly variable synthesis quality typically encountered with todays unit selection synthesizers .

a cascaded approach for social media text normalization of turkish
text normalization is an indispensable stage for natural language processing of social media data with available nlp tools . we divide the normalization problem into 7 categories , namely ; letter case transformation , replacement rules & lexicon lookup , proper noun detection , deasciification , vowel restoration , accent normalization and spelling correction . we propose a cascaded approach where each ill formed word passes from these 7 modules and is investigated for possible transformations . this paper presents the first results for the normalization of turkish and tries to shed light on the different challenges in this area . we report a 40 percentage points improvement over a lexicon lookup baseline and nearly 50 percentage points over available spelling correctors .

parse selection with a german hpsg grammar
we report on some recent parse selection experiments carried out with gg , a large-scale hpsg grammar for german . using a manually disambiguated treebank derived from the verbmobil corpus , we achieve over 81 % exact match accuracy compared to a 21.4 % random baseline , corresponding to an error reduction rate of 3.8 .

an ordering of terms based on semantic
term selection methods typically employ a statistical measure to filter or weight terms . term expansion for ir may also depend on statistics , or use some other , non-metric method based on a lexical resource . at the same time , a wide range of semantic similarity measures have been developed to support natural language processing tasks such as word sense disambiguation . this paper combines the two approaches and proposes an algorithm that provides a semantic order of terms based on a semantic relatedness measure . this semantic order can be exploited by term weighting and term expansion methods .

webanno : a flexible , web-based and visually supported system for distributed annotations
we present webanno , a general purpose web-based annotation tool for a wide range of linguistic annotations . webanno offers annotation project management , freely configurable tagsets and the management of users in different roles . webanno uses modern web technology for visualizing and editing annotations in a web browser . it supports arbitrarily large documents , pluggable import/export filters , the curation of annotations across various users , and an interface to farming out annotations to a crowdsourcing platform . currently webanno allows part-ofspeech , named entity , dependency parsing and co-reference chain annotations . the architecture design allows adding additional modes of visualization and editing , when new kinds of annotations are to be supported .

learning sentiment-specific word embedding for twitter sentiment classification
we present a method that learns word embedding for twitter sentiment classification in this paper . most existing algorithms for learning continuous word representations typically only model the syntactic context of words but ignore the sentiment of text . this is problematic for sentiment analysis as they usually map words with similar syntactic context but opposite sentiment polarity , such as good and bad , to neighboring word vectors . we address this issue by learning sentimentspecific word embedding ( sswe ) , which encodes sentiment information in the continuous representation of words . specifically , we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text ( e.g . sentences or tweets ) in their loss functions . to obtain large scale training corpora , we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons . experiments on applying sswe to a benchmark twitter sentiment classification dataset in semeval 2013 show that ( 1 ) the sswe feature performs comparably with hand-crafted features in the top-performed system ; ( 2 ) the performance is further improved by concatenating sswe with existing feature set .

first joint workshop on statistical parsing of morphologically rich languages exploring options for fast domain adaptation of dependency parsers
the paper explores different domain-independent techniques to adapt a dependency parser trained on a general-language corpus to parse web texts ( online reviews , newsgroup posts , weblogs ) : co-training , word clusters , and a crowd-sourced dictionary . we examine the relative utility of these techniques as well as different ways to put them together to achieve maximum parsing accuracy . while we find that co-training and word clusters produce the most promising results , there is little additive improvement when combining the two techniques , which suggests that in the absence of large grammatical discrepancies between the training and test domains , they address largely the same problem , that of unknown vocabulary , with word clusters being a somewhat more effective solution for it . our highest results were achieved by a combination of word clusters and co-training , significantly improving on the baseline , by up to 1.67 % . evaluation of the best configurations on the sancl-2012 test data ( petrov and mcdonald , 2012 ) showed that they outperform all the shared task submissions that used a single parser to parse test data , averaging the results across all the test sets .

automatic prediction of parser accuracy
statistical parsers have become increasingly accurate , to the point where they are useful in many natural language applications . however , estimating parsing accuracy on a wide variety of domains and genres is still a challenge in the absence of gold-standard parse trees . in this paper , we propose a technique that automatically takes into account certain characteristics of the domains of interest , and accurately predicts parser performance on data from these new domains . as a result , we have a cheap ( no annotation involved ) and effective recipe for measuring the performance of a statistical parser on any given domain .

learning the information status of noun phrases in spoken dialogues
an entity in a dialogue may be old , new , or mediated/inferrable with respect to the hearers beliefs . knowing the information status of the entities participating in a dialogue can therefore facilitate its interpretation . we address the under-investigated problem of automatically determining the information status of discourse entities . specifically , we extend nissims ( 2006 ) machine learning approach to information-status determination with lexical and structured features , and exploit learned knowledge of the information status of each discourse entity for coreference resolution . experimental results on a set of switchboard dialogues reveal that ( 1 ) incorporating our proposed features into nissims feature set enables our system to achieve stateof-the-art performance on information-status classification , and ( 2 ) the resulting information can be used to improve the performance of learning-based coreference resolvers .

classification of feedback expressions in multimodal data
this paper addresses the issue of how linguistic feedback expressions , prosody and head gestures , i.e . head movements and face expressions , relate to one another in a collection of eight video-recorded danish map-task dialogues . the study shows that in these data , prosodic features and head gestures significantly improve automatic classification of dialogue act labels for linguistic expressions of feedback .

extra-linguistic constraints on stance recognition in ideological debates kazi saidul hasan and vincent ng
determining the stance expressed by an author from a post written for a twosided debate in an online debate forum is a relatively new problem . we seek to improve anand et als ( 2011 ) approach to debate stance classification by modeling two types of soft extra-linguistic constraints on the stance labels of debate posts , user-interaction constraints and ideology constraints . experimental results on four datasets demonstrate the effectiveness of these inter-post constraints in improving debate stance classification .

a joint sequence translation model with integrated reordering
we present a novel machine translation model which models translation by a linear sequence of operations . in contrast to the n-gram model , this sequence includes not only translation but also reordering operations . key ideas of our model are ( i ) a new reordering approach which better restricts the position to which a word or phrase can be moved , and is able to handle short and long distance reorderings in a unified way , and ( ii ) a joint sequence model for the translation and reordering probabilities which is more flexible than standard phrase-based mt . we observe statistically significant improvements in bleu over moses for german-to-english and spanish-to-english tasks , and comparable results for a french-to-english task .

helping our own : text massaging for computational linguistics as a new shared task lexical computing ltd
in this paper , we propose a new shared task called hoo : helping our own . the aim is to use tools and techniques developed in computational linguistics to help people writing about computational linguistics . we describe a text-to-text generation scenario that poses challenging research questions , and delivers practical outcomes that are useful in the first case to our own community and potentially much more widely . two specific factors make us optimistic that this task will generate useful outcomes : one is the availability of the acl anthology , a large corpus of the target text type ; the other is that cl researchers who are non-native speakers of english will be motivated to use prototype systems , providing informed and precise feedback in large quantity . we lay out our plans in detail and invite comment and critique with the aim of improving the nature of the planned exercise .

universal conceptual cognitive annotation ( ucca )
syntactic structures , by their nature , reflect first and foremost the formal constructions used for expressing meanings . this renders them sensitive to formal variation both within and across languages , and limits their value to semantic applications . we present ucca , a novel multi-layered framework for semantic representation that aims to accommodate the semantic distinctions expressed through linguistic utterances . we demonstrate uccas portability across domains and languages , and its relative insensitivity to meaning-preserving syntactic variation . we also show that ucca can be effectively and quickly learned by annotators with no linguistic background , and describe the compilation of a uccaannotated corpus .

a simple bayesian modelling approach to event extraction from twitter
with the proliferation of social media sites , social streams have proven to contain the most up-to-date information on current events . therefore , it is crucial to extract events from the social streams such as tweets . however , it is not straightforward to adapt the existing event extraction systems since texts in social media are fragmented and noisy . in this paper we propose a simple and yet effective bayesian model , called latent event model ( lem ) , to extract structured representation of events from social media . lem is fully unsupervised and does not require annotated data for training . we evaluate lem on a twitter corpus . experimental results show that the proposed model achieves 83 % in f-measure , and outperforms the state-of-the-art baseline by over 7 % .

a quantitative analysis of lexical differences between genders in
in this work , we provide an empirical analysis of differences in word use between genders in telephone conversations , which complements the considerable body of work in sociolinguistics concerned with gender linguistic differences . experiments are performed on a large speech corpus of roughly 12000 conversations . we employ machine learning techniques to automatically categorize the gender of each speaker given only the transcript of his/her speech , achieving 92 % accuracy . an analysis of the most characteristic words for each gender is also presented . experiments reveal that the gender of one conversation side influences lexical use of the other side . a surprising result is that we were able to classify male-only vs. female-only conversations with almost perfect accuracy .

latent dirichlet allocation with topic-in-set knowledge
latent dirichlet allocation is an unsupervised graphical model which can discover latent topics in unlabeled data . we propose a mechanism for adding partial supervision , called topic-in-set knowledge , to latent topic modeling . this type of supervision can be used to encourage the recovery of topics which are more relevant to user modeling goals than the topics which would be recovered otherwise . preliminary experiments on text datasets are presented to demonstrate the potential effectiveness of this method .

learning image embeddings using convolutional neural networks for improved multi-modal semantics
we construct multi-modal concept representations by concatenating a skip-gram linguistic representation vector with a visual concept representation vector computed using the feature extraction layers of a deep convolutional neural network ( cnn ) trained on a large labeled object recognition dataset . this transfer learning approach brings a clear performance gain over features based on the traditional bag-of-visual-word approach . experimental results are reported on the wordsim353 and men semantic relatedness evaluation tasks . we use visual features computed using either imagenet or esp game images .

statistical models for text normalization and machine translation
irish and scottish gaelic are closely-related languages that together with manx gaelic make up the goidelic branch of the celtic family . we present a statistical model for translation from scottish gaelic to irish that we hope will facilitate communication between the two language communities , especially in social media . an important aspect of this work is to overcome the orthographical differences between the languages , many of which were introduced in a major spelling reform of irish in the 1940s and 1950s . prior to that date , the orthographies of the two languages were quite similar , thanks in part to a shared literary tradition . as a consequence of this , machine translation from scottish gaelic to irish has a great deal in common with the problem of normalizing pre-standard irish texts , a problem with applications to lexicography and information retrieval . we show how a single statistical model can be used effectively in both contexts .

a corpus-based approach to topic in danish dialog
we report on an investigation of the pragmatic category of topic in danish dialog and its correlation to surface features of nps . using a corpus of 444 utterances , we trained a decision tree system on 16 features . the system achieved nearhuman performance with success rates of 8489 % and f1-scores of 0.630.72 in 10fold cross validation tests ( human performance : 89 % and 0.78 ) . the most important features turned out to be preverbal position , definiteness , pronominalisation , and non-subordination . we discovered that nps in epistemic matrix clauses ( e.g . i think . . . ) were seldom topics and we suspect that this holds for other interpersonal matrix clauses as well .

a generalized language model as the combination of skipped n-grams and modified kneser-ney smoothing paul georg wagner and till speicher
we introduce a novel approach for building language models based on a systematic , recursive exploration of skip n-gram models which are interpolated using modified kneser-ney smoothing . our approach generalizes language models as it contains the classical interpolation with lower order models as a special case . in this paper we motivate , formalize and present our approach . in an extensive empirical experiment over english text corpora we demonstrate that our generalized language models lead to a substantial reduction of perplexity between 3.1 % and 12.7 % in comparison to traditional language models using modified kneser-ney smoothing . furthermore , we investigate the behaviour over three other languages and a domain specific corpus where we observed consistent improvements . finally , we also show that the strength of our approach lies in its ability to cope in particular with sparse training data . using a very small training data set of only 736 kb text we yield improvements of even 25.7 % reduction of perplexity .

noise as a tool for spoken language identification
segmental snr ( signal to noise ratio ) is considered to be a reasonable measure of perceptual quality of speech . however it only reflects the distortion in time dependent contour of the signal due to noise . objective measures such as log area ratio ( lar ) , itakura-saitio distortion ( is ) , log-likelihood ratio ( llr ) and weighted spectral slope ( wss ) are better measures of perceptual speech quality as they represent deviation in the spectrum . noise affects the speech time contour and the corresponding frequency content . different languages have some peculiar characteristics due to variation in the phonetic content and their distribution . distortion introduced by noise and application of enhancement algorithm varies for different phonemes . in this paper a novel idea of using noise and speech enhancement as means of identifying a language is presented , using objective measures of speech quality . study is done on three spoken indian regional languages namely kashmiri , bangla and manipuri , when corrupted by white noise . it is found that the objective measures of noisy speech , when determined using corresponding clear and enhanced speech are different for different languages over a range of snr , giving clue to the type of the language in use .

meteor : an automatic metric for mt evaluation with high levels of correlation with human judgments
meteor is an automatic metric for machine translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality , significantly outperforming the more commonly used bleu metric . it is one of several automatic metrics used in this years shared task within the acl wmt-07 workshop . this paper recaps the technical details underlying the metric and describes recent improvements in the metric . the latest release includes improved metric parameters and extends the metric to support evaluation of mt output in spanish , french and german , in addition to english .

extending a broad-coverage parser for a general nlp toolkit
with the rapid growth of real world applications for nlp systems , there is a genuine demand for a general toolkit from which programmers with no linguistic knowledge can build specific nlp systems . such a toolkit should have a parser that is general enough to be used across domains , and yet accurate enough for each specific application . in this paper , we describe a parser that extends a broad-coverage parser , minipar ( lin , 2001 ) , with an adaptable shallow parser so as to achieve both generality and accuracy in handling domain specific nl problems . we test this parser on our corpus and the results show that the accuracy is significantly higher than a system that uses minipar alone .

robox : ccg with structured perceptron for supervised semantic parsing of robotic spatial commands
we use a combinatory categorial grammar ( ccg ) parser with a structured perceptron learner to address shared task 6 of semeval-2014 , supervised semantic parsing of robotic spatial commands . our system reaches an accuracy of 79 % ignoring spatial context and 87 % using the spatial planner , showing that ccg can successfully be applied to the task .

discovering entailment relations using textual entailment patterns fabio massimo zanzotto maria teresa pazienza , marco pennacchiotti
in this work we investigate methods to enable the detection of a specific type of textual entailment ( strict entailment ) , starting from the preliminary assumption that these relations are often clearly expressed in texts . our method is a statistical approach based on what we call textual entailment patterns , prototypical sentences hiding entailment relations among two activities . we experimented the proposed method using the entailment relations of wordnet as test case and the web as corpus where to estimate the probabilities ; obtained results will be shown .

experiments in telugu ner : a conditional random field approach praneeth m shishtla , karthik gali , prasad pingali and vasudeva varma
named entity recognition ( ner ) is the task of identifying and classifying tokens in a text document into predefined set of classes . in this paper we show our experiments with various feature combinations for telugu ner . we also observed that the prefix and suffix information helps a lot in finding the class of the token . we also show the effect of the training data on the performance of the system . the best performing model gave an f=1 measure of 44.91. the language independent features gave an f=1 measure of 44.89 which is close to f=1 measure obtained even by including the language dependent features .

unsupervised relation extraction from web documents
the idex system is a prototype of an interactive dynamic information extraction ( ie ) system . a user of the system expresses an information request in the form of a topic description , which is used for an initial search in order to retrieve a relevant set of documents . on basis of this set of documents , unsupervised relation extraction and clustering is done by the system . the results of these operations can then be interactively inspected by the user . in this paper we describe the relation extraction and clustering components of the idex system . preliminary evaluation results of these components are presented and an overview is given of possible enhancements to improve the relation extraction and clustering components .

an intelligent procedure assistant
we will demonstrate the latest version of an ongoing project to create an intelligent procedure assistant for use by astronauts on the international space station ( iss ) . the system functionality includes spoken dialogue control of navigation , coordinated display of the procedure text , display of related pictures , alarms , and recording and playback of voice notes . the demo also exemplifies several interesting component technologies . speech recognition and language understanding have been developed using the open source regulus 2 toolkit . this implements an approach to portable grammar-based language modelling in which all models are derived from a single linguistically motivated unification grammar . domain-specific cfg language models are produced by first specialising the grammar using an automatic corpus-based method , and then compiling the resulting specialised grammars into cfg form . translation between language centered and domain centered semantic representations is carried out by alterf , another open source toolkit , which combines rule-based and corpusbased processing in a transparent way .

named entity recognition using an hmm-based chunk tagger
this paper proposes a hidden markov model ( hmm ) and an hmm-based chunk tagger , from which a named entity ( ne ) recognition ( ner ) system is built to recognize and classify names , times and numerical quantities . through the hmm , our system is able to apply and integrate four types of internal and external evidences : 1 ) simple deterministic internal feature of the words , such as capitalization and digitalization ; 2 ) internal semantic feature of important triggers ; 3 ) internal gazetteer feature ; 4 ) external macro context feature . in this way , the ner problem can be resolved effectively . evaluation of our system on muc-6 and muc-7 english ne tasks achieves f-measures of 96.6 % and 94.1 % respectively . it shows that the performance is significantly better than reported by any other machine-learning system . moreover , the performance is even consistently better than those based on handcrafted rules .

automatically predicting peer-review helpfulness
identifying peer-review helpfulness is an important task for improving the quality of feedback that students receive from their peers . as a first step towards enhancing existing peerreview systems with new functionality based on helpfulness detection , we examine whether standard product review analysis techniques also apply to our new context of peer reviews . in addition , we investigate the utility of incorporating additional specialized features tailored to peer review . our preliminary results show that the structural features , review unigrams and meta-data combined are useful in modeling the helpfulness of both peer reviews and product reviews , while peer-review specific auxiliary features can further improve helpfulness prediction .

negative deceptive opinion spam
the rising influence of user-generated online reviews ( cone , 2011 ) has led to growing incentive for businesses to solicit and manufacture deceptive opinion spamfictitious reviews that have been deliberately written to sound authentic and deceive the reader . recently , ott et al ( 2011 ) have introduced an opinion spam dataset containing gold standard deceptive positive hotel reviews . however , the complementary problem of negative deceptive opinion spam , intended to slander competitive offerings , remains largely unstudied . following an approach similar to ott et al ( 2011 ) , in this work we create and study the first dataset of deceptive opinion spam with negative sentiment reviews . based on this dataset , we find that standard n-gram text categorization techniques can detect negative deceptive opinion spam with performance far surpassing that of human judges . finally , in conjunction with the aforementioned positive review dataset , we consider the possible interactions between sentiment and deception , and present initial results that encourage further exploration of this relationship .

hit : web based scoring method for english lexical substitution
this paper describes the hit system and its participation in semeval-2007 english lexical substitution task . two main steps are included in our method : candidate substitute extraction and candidate scoring . in the first step , candidate substitutes for each target word in a given sentence are extracted from wordnet . in the second step , the extracted candidates are scored and ranked using a web-based scoring method . the substitute ranked first is selected as the best substitute . for the multiword subtask , a simple wordnet-based approach is employed .

the direct-info scenario
we describe the way we adapted a text analysis tool for annotating with the linguistic description scheme of mpeg-7 text related to and extracted from multimedia content . practically applied in the direct-info ec r & d project we show how such linguistic annotation contributes to semantic annotation of multimodal analysis systems , demonstrating also the use of the xml schema of mpeg-7 for supporting cross-media semantic content annotation .

supersense tagging of unknown nouns using semantic similarity
the limited coverage of lexical-semantic resources is a significant problem for nlp systems which can be alleviated by automatically classifying the unknown words . supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into wordnet . ciaramita and johnson ( 2003 ) present a tagger which uses synonym set glosses as annotated training examples . we describe an unsupervised approach , based on vector-space similarity , which does not require annotated examples but significantly outperforms their tagger . we also demonstrate the use of an extremely large shallow-parsed corpus for calculating vector-space semantic similarity .

finding word substitutions using a distributional similarity baseline and immediate context overlap
this paper deals with the task of finding generally applicable substitutions for a given input term . we show that the output of a distributional similarity system baseline can be filtered to obtain terms that are not simply similar but frequently substitutable . our filter relies on the fact that when two terms are in a common entailment relation , it should be possible to substitute one for the other in their most frequent surface contexts . using the google 5-gram corpus to find such characteristic contexts , we show that for the given task , our filter improves the precision of a distributional similarity system from 41 % to 56 % on a test set comprising common transitive verbs .

whats in a message
in this paper we present the first step in a larger series of experiments for the induction of predicate/argument structures . the structures that we are inducing are very similar to the conceptual structures that are used in frame semantics ( such as framenet ) . those structures are called messages and they were previously used in the context of a multi-document summarization system of evolving events . the series of experiments that we are proposing are essentially composed from two stages . in the first stage we are trying to extract a representative vocabulary of words . this vocabulary is later used in the second stage , during which we apply to it various clustering approaches in order to identify the clusters of predicates and argumentsor frames and semantic roles , to use the jargon of frame semantics . this paper presents in detail and evaluates the first stage .

cross-lingual alignment and completion of wikipedia templates
for many languages , the size of wikipedia is an order of magnitude smaller than the english wikipedia . we present a method for cross-lingual alignment of template and infobox attributes in wikipedia . the alignment is used to add and complete templates and infoboxes in one language with information derived from wikipedia in another language . we show that alignment between english and dutch wikipedia is accurate and that the result can be used to expand the number of template attribute-value pairs in dutch wikipedia by 50 % . furthermore , the alignment provides valuable information for normalization of template and attribute names and can be used to detect potential inconsistencies .

combining shallow and linguistically motivated features in native language identification serhiy bykh sowmya vajjala julia krivanek detmar meurers
we explore a range of features and ensembles for the task of native language identification as part of the nli shared task ( tetreault et al , 2013 ) . starting with recurring word-based ngrams ( bykh and meurers , 2012 ) , we tested different linguistic abstractions such as partof-speech , dependencies , and syntactic trees as features for nli . we also experimented with features encoding morphological properties , the nature of the realizations of particular lemmas , and several measures of complexity developed for proficiency and readability classification ( vajjala and meurers , 2012 ) . employing an ensemble classifier incorporating all of our features we achieved an accuracy of 82.2 % ( rank 5 ) in the closed task and 83.5 % ( rank 1 ) in the open-2 task . in the open-1 task , the word-based recurring ngrams outperformed the ensemble , yielding 38.5 % ( rank 2 ) . overall , across all three tasks , our best accuracy of 83.5 % for the standard toefl11 test set came in second place .

wordnet-based text document clustering
text document clustering can greatly simplify browsing large collections of documents by reorganizing them into a smaller number of manageable clusters . algorithms to solve this task exist ; however , the algorithms are only as good as the data they work on . problems include ambiguity and synonymy , the former allowing for erroneous groupings and the latter causing similarities between documents to go unnoticed . in this research , nave , syntax-based disambiguation is attempted by assigning each word a part-of-speech tag and by enriching the bag-ofwords data representation often used for document clustering with synonyms and hypernyms from wordnet .

can you repeat that using word repetition to improve spoken term detection
we aim to improve spoken term detection performance by incorporating contextual information beyond traditional ngram language models . instead of taking a broad view of topic context in spoken documents , variability of word co-occurrence statistics across corpora leads us to focus instead the on phenomenon of word repetition within single documents . we show that given the detection of one instance of a term we are more likely to find additional instances of that term in the same document . we leverage this burstiness of keywords by taking the most confident keyword hypothesis in each document and interpolating with lower scoring hits . we then develop a principled approach to select interpolation weights using only the asr training data . using this re-weighting approach we demonstrate consistent improvement in the term detection performance across all five languages in the babel program .

data driven language transfer hypotheses
language transfer , the preferential second language behavior caused by similarities to the speakers native language , requires considerable expertise to be detected by humans alone . our goal in this work is to replace expert intervention by data-driven methods wherever possible . we define a computational methodology that produces a concise list of lexicalized syntactic patterns that are controlled for redundancy and ranked by relevancy to language transfer . we demonstrate the ability of our methodology to detect hundreds of such candidate patterns from currently available data sources , and validate the quality of the proposed patterns through classification experiments .

are these documents written from different perspectives a test of different perspectives based on statistical distribution divergence
in this paper we investigate how to automatically determine if two document collections are written from different perspectives . by perspectives we mean a point of view , for example , from the perspective of democrats or republicans . we propose a test of different perspectives based on distribution divergence between the statistical models of two collections . experimental results show that the test can successfully distinguish document collections of different perspectives from other types of collections .

space characters in chinese semi-structured texts
space characters can have an important role in disambiguating text . however , few , if any , chinese information extraction systems make full use of space characters . however , it seems that treatment of space characters is necessary , especially in cases of extracting information from semi-structured documents . this investigation aims to address the importance of space characters in chinese information extraction by parsing some semi-structured documents with two similar grammars - one with treatment for space characters , the other ignoring it . this paper also introduces two post processing filters to further improve treatment of space characters . results show that the grammar that takes account of spaces clearly out-performs the one that ignores them , and so concludes that space characters can play a useful role in information extraction .

exploring mwes for knowledge acquisition from corporate technical documents
high frequency can convert a word sequence into a multiword expression ( mwe ) , i.e. , a collocation . in this paper , we use collocations as well as syntactically-flexible , lexicalized phrases to analyze job specification documents ( a kind of corporate technical document ) for subsequent acquisition of automated knowledge elicitation . we propose the definition of structural and functional patterns of specific corporate documents by analyzing the contexts and sections in which the expression occurs . such patterns and its automated processing are the basis for identifying organizational domain knowledge and business information which is used later for the first instances of requirement elicitation processes in software engineering .

a unified representation for morphological , syntactic , semantic , and
this paper reports on the syn-ra ( syntax-based reference annotation ) project , an on-going project of annotating german newspaper texts with referential relations . the project has developed an inventory of anaphoric and coreference relations for german in the context of a unified , xml-based annotation scheme for combining morphological , syntactic , semantic , and anaphoric information . the paper discusses how this unified annotation scheme relates to other formats currently discussed in the literature , in particular the annotation graph model of bird and liberman ( 2001 ) and the pie-in-thesky scheme for semantic annotation .

optimizing algorithms for pronoun resolution
the paper aims at a deeper understanding of several well-known algorithms and proposes ways to optimize them . it describes and discusses factors and strategies of factor interaction used in the algorithms . the factors used in the algorithms and the algorithms themselves are evaluated on a german corpus annotated with syntactic and coreference information ( negra ) ( skut et al , 1997 ) . a common format for pronoun resolution algorithms with several open parameters is proposed , and the parameter settings optimal on the evaluation data are given .

dialogue segmentation with large numbers of volunteer internet
this paper shows the results of an experiment in dialogue segmentation . in this experiment , segmentation was done on a level of analysis similar to adjacency pairs . the method of annotation was somewhat novel : volunteers were invited to participate over the web , and their responses were aggregated using a simple voting method . though volunteers received a minimum of training , the aggregated responses of the group showed very high agreement with expert opinion . the group , as a unit , performed at the top of the list of annotators , and in many cases performed as well as or better than the best annotator .

an unsupervised model for text message normalization
cell phone text messaging users express themselves briefly and colloquially using a variety of creative forms . we analyze a sample of creative , non-standard text message word forms to determine frequent word formation processes in texting language . drawing on these observations , we construct an unsupervised noisy-channel model for text message normalization . on a test set of 303 text message forms that differ from their standard form , our model achieves 59 % accuracy , which is on par with the best supervised results reported on this dataset .

two step chinese named entity recognition based on conditional random fields models
this paper mainly describes a chinese named entity recognition ( ner ) system ner @ iscas , which integrates text , partof-speech and a small-vocabularycharacter-lists feature and heristic postprocess rules for msra ner open track under the framework of conditional random fields ( crfs ) model .

using semantic and syntactic graphs for call classication dilek hakkani-tur gokhan tur
in this paper , we introduce a new data representation format for language processing , the syntactic and semantic graphs ( ssgs ) , and show its use for call classification in spoken dialog systems . for each sentence or utterance , these graphs include lexical information ( words ) , syntactic information ( such as the part of speech tags of the words and the syntactic parse of the utterance ) , and semantic information ( such as the named entities and semantic role labels ) . in our experiments , we used written language as the training data while computing ssgs and tested on spoken language . in spite of this mismatch , we have shown that this is a very promising approach for classifying complex examples , and by using ssgs it is possible to reduce the call classification error rate by 4.74 % relative .

species disambiguation for biomedical term identification
an important task in information extraction ( ie ) from biomedical articles is term identification ( ti ) , which concerns linking entity mentions ( e.g. , terms denoting proteins ) in text to unambiguous identifiers in standard databases ( e.g. , refseq ) . previous work on ti has focused on species-specific documents . however , biomedical documents , especially full-length articles , often talk about entities across a number of species , in which case resolving species ambiguity becomes an indispensable part of ti . this paper describes our rule-based and machine-learning based approaches to species disambiguation and demonstrates that performance of ti can be improved by over 20 % if the correct species are known . we also show that using the species predicted by the automatic species taggers can improve ti by a large margin .

titpi : web people search task using semi-supervised clustering approach
most of the previous works that disambiguate personal names in web search results employ agglomerative clustering approaches . however , these approaches tend to generate clusters that contain a single element depending on a certain criterion of merging similar clusters . in contrast to such previous works , we have adopted a semisupervised clustering approach to integrate similar documents into a labeled document . moreover , our proposed approach is characterized by controlling the fluctuation of the centroid of a cluster in order to generate more accurate clusters .

the benefits of errors : learning an ot grammar with a structured candidate set
we compare three recent proposals adding a topology to ot : mccarthys persistent ot , smolenskys ics and bros sa-ot . to test their learnability , constraint rankings are learnt from sa-ots output . the errors in the output , being more than mere noise , follow from the topology . thus , the learner has to reconstructs her competence having access only to the teachers performance .

a cognitive model of semantic network learning
child semantic development includes learning the meaning of words as well as the semantic relations among words . a presumed outcome of semantic development is the formation of a semantic network that reflects this knowledge . we present an algorithm for simultaneously learning word meanings and gradually growing a semantic network , which adheres to the cognitive plausibility requirements of incrementality and limited computations . we demonstrate that the semantic connections among words in addition to their context is necessary in forming a semantic network that resembles an adults semantic knowledge .

multilingual term extraction from domain-specific corpora using morphological structure
morphologically complex terms composed from greek or latin elements are frequent in scientific and technical texts . word forming units are thus relevant cues for the identification of terms in domainspecific texts . this article describes a method for the automatic extraction of terms relying on the detection of classical prefixes and word-initial combining forms . word-forming units are identified using a regular expression . the system then extracts terms by selecting words which either begin or coalesce with these elements . next , terms are grouped in families which are displayed as a weighted list in html format .

deriunlp : a context based approach to automatic keyphrase unit for natural language processing unit for natural language processing
the deri unlp team participated in the semeval 2010 task # 5 with an unsupervised system that automatically extracts keyphrases from scientific articles . our approach does not only consider a general description of a term to select keyphrase candidates but also context information in the form of skill types . even though our system analyses only a limited set of candidates , it is still able to outperform baseline unsupervised and supervised approaches .

cluster-based language model for sentence retrieval in chinese
sentence retrieval plays a very important role in question answering system . in this paper , we present a novel cluster-based language model for sentence retrieval in chinese question answering which is motivated in part by sentence clustering and language model . sentence clustering is used to group sentences into clusters . language model is used to properly represent sentences , which is combined with sentences model , cluster/topic model and collection model . for sentence clustering , we propose two approaches that are onesentence-multi-topics and onesentence-one-topic respectively . from the experimental results on 807 chinese testing questions , we can conclude that the proposed cluster-based language model outperforms over the standard language model for sentence retrieval in chinese question answering .

automatic satire detection : are you having a laugh
we introduce the novel task of determining whether a newswire article is true or satirical . we experiment with svms , feature scaling , and a number of lexical and semantic feature types , and achieve promising results over the task .

extracting multiword expressions with a semantic tagger
automatic extraction of multiword expressions ( mwe ) presents a tough challenge for the nlp community and corpus linguistics . although various statistically driven or knowledge-based approaches have been proposed and tested , efficient mwe extraction still remains an unsolved issue . in this paper , we present our research work in which we tested approaching the mwe issue using a semantic field annotator . we use an english semantic tagger ( usas ) developed at lancaster university to identify multiword units which depict single semantic concepts . the meter corpus ( gaizauskas et al , 2001 ; clough et al , 2002 ) built in sheffield was used to evaluate our approach . in our evaluation , this approach extracted a total of 4,195 mwe candidates , of which , after manual checking , 3,792 were accepted as valid mwes , producing a precision of 90.39 % and an estimated recall of 39.38 % . of the accepted mwes , 68.22 % or 2,587 are low frequency terms , occurring only once or twice in the corpus . these results show that our approach provides a practical solution to mwe extraction .

syntax to semantics transformation : application to treebanking
mapping between syntax and semantics is one of the most promising research topics in corpus annotation . this paper deals with the implementation of an semi-automatic transformation from a syntactically-tagged corpus into a semantic-tagged one . the method has been experimentally applied to a 1600-sentence treebank ( the uam spanish treebank ) . results of evaluation are provided as well as prospective work in comparing syntax and semantics in written and spoken annotated corpora .

a unified approach in speech-to-speech translation : integrating features of speech recognition and machine translation taro watanabe and frank soong and wai kit lo
based upon a statistically trained speech translation system , in this study , we try to combine distinctive features derived from the two modules : speech recognition and statistical machine translation , in a loglinear model . the translation hypotheses are then rescored and translation performance is improved . the standard translation evaluation metrics , including bleu , nist , multiple reference word error rate and its position independent counterpart , were optimized to solve the weights of the features in the log-linear model . the experimental results have shown significant improvement over the baseline ibm model 4 in all automatic translation evaluation metrics . the largest was for bleu , by 7.9 % absolute .

on the applicability of readability models to web texts sowmya vajjala detmar meurers seminar fur sprachwissenschaft
an increasing range of features is being used for automatic readability classification . the impact of the features typically is evaluated using reference corpora containing graded reading material . but how do the readability models and the features they are based on perform on real-world web texts in this paper , we want to take a step towards understanding this aspect on the basis of a broad range of lexical and syntactic features and several web datasets we collected . applying our models to web search results , we find that the average reading level of the retrieved web documents is relatively high . at the same time , documents at a wide range of reading levels are identified and even among the top-10 search results one finds documents at the lower levels , supporting the potential usefulness of readability ranking for the web . finally , we report on generalization experiments showing that the features we used generalize well across different web sources .

lexical chains as document features defence r & d organisation ,
document clustering and classification is usually done by representing the documents using a bag of words scheme . this scheme ignores many of the linguistic and semantic features contained in text documents . we propose here an alternative representation for documents using lexical chains . we compare the performance of the new representation against the old one on a clustering task . we show that lexical chain based features give better results than the bag of words based features , while achieving almost 30 % reduction in the dimensionality of the feature vectors resulting in faster execution of the algorithms .

experiments with an annotation scheme for a knowledge-rich noun phrase
this paper presents observations on our experience with an annotation scheme that was used in the training of a state-of-the-art noun phrase semantic interpretation system . the system relies on cross-linguistic evidence from a set of five romance languages : spanish , italian , french , portuguese , and romanian . given a training set of english noun phrases in context along with their translations in the five romance languages , our algorithm automatically learns a classification function that is later on applied to unseen test instances for semantic interpretation . as training and test data we used two text collections of different genre : europarl and cluvi . the training data was annotated with contextual features based on two stateof-the-art classification tag sets .

a stacked , voted , stacked model for named entity recognition
this paper investigates stacking and voting methods for combining strong classifiers like boosting , svm , and tbl , on the named-entity recognition task . we demonstrate several effective approaches , culminating in a model that achieves error rate reductions on the development and test sets of 63.6 % and 55.0 % ( english ) and 47.0 % and 51.7 % ( german ) over the conll-2003 standard baseline respectively , and 19.7 % over a strong adaboost baseline model from conll-2002 .

bitam : bilingual topic admixture models for word alignment
we propose a novel bilingual topical admixture ( bitam ) formalism for word alignment in statistical machine translation . under this formalism , the parallel sentence-pairs within a document-pair are assumed to constitute a mixture of hidden topics ; each word-pair follows a topic-specific bilingual translation model . three bitam models are proposed to capture topic sharing at different levels of linguistic granularity ( i.e. , at the sentence or word levels ) . these models enable wordalignment process to leverage topical contents of document-pairs . efficient variational approximation algorithms are designed for inference and parameter estimation . with the inferred latent topics , bitam models facilitate coherent pairing of bilingual linguistic entities that share common topical aspects . our preliminary experiments show that the proposed models improve word alignment accuracy , and lead to better translation quality .

a probabilistic morphological analyzer for syriac peter mcclanahan , george busby , robbie haertel , kristian heal ,
we define a probabilistic morphological analyzer using a data-driven approach for syriac in order to facilitate the creation of an annotated corpus . syriac is an under-resourced semitic language for which there are no available language tools such as morphological analyzers . we introduce novel probabilistic models for segmentation , dictionary linkage , and morphological tagging and connect them in a pipeline to create a probabilistic morphological analyzer requiring only labeled data . we explore the performance of models with varying amounts of training data and find that with about 34,500 labeled tokens , we can outperform a reasonable baseline trained on over 99,000 tokens and achieve an accuracy of just over 80 % . when trained on all available training data , our joint model achieves 86.47 % accuracy , a 29.7 % reduction in error rate over the baseline .

flexible guidance generation using user model in spoken dialogue systems
we address appropriate user modeling in order to generate cooperative responses to each user in spoken dialogue systems . unlike previous studies that focus on users knowledge or typical kinds of users , the user model we propose is more comprehensive . specifically , we set up three dimensions of user models : skill level to the system , knowledge level on the target domain and the degree of hastiness . moreover , the models are automatically derived by decision tree learning using real dialogue data collected by the system . we obtained reasonable classification accuracy for all dimensions . dialogue strategies based on the user modeling are implemented in kyoto city bus information system that has been developed at our laboratory . experimental evaluation shows that the cooperative responses adaptive to individual users serve as good guidance for novice users without increasing the dialogue duration for skilled users .

finding hedges by chasing weasels : hedge detection using wikipedia tags and shallow linguistic features
we investigate the automatic detection of sentences containing linguistic hedges using corpus statistics and syntactic patterns . we take wikipedia as an already annotated corpus using its tagged weasel words which mark sentences and phrases as non-factual . we evaluate the quality of wikipedia as training data for hedge detection , as well as shallow linguistic features .

overcoming the curse of sentence length for neural machine translation using automatic segmentation bart van merri enboer kyunghyun cho cifar senior fellow
the authors of ( cho et al. , 2014a ) have shown that the recently introduced neural network translation systems suffer from a significant drop in translation quality when translating long sentences , unlike existing phrase-based translation systems . in this paper , we propose a way to address this issue by automatically segmenting an input sentence into phrases that can be easily translated by the neural network translation model . once each segment has been independently translated by the neural machine translation model , the translated clauses are concatenated to form a final translation . empirical results show a significant improvement in translation quality for long sentences .

learning to annotate scientific publications
annotating scientific publications with keywords and phrases is of great importance to searching , indexing , and cataloging such documents . unlike previous studies that focused on usercentric annotation , this paper presents our investigation of various annotation characteristics on service-centric annotation . using a large number of publicly available annotated scientific publications , we characterized and compared the two different types of annotation processes . furthermore , we developed an automatic approach of annotating scientific publications based on a machine learning algorithm and a set of novel features . when compared to other methods , our approach shows significantly improved performance . experimental data sets and evaluation results are publicly available at the supplementary website1 .

building the syntactic reference corpus of medieval french using notabene rdf annotation tool
in this paper , we introduce the notabene rdf annotation tool free software used to build the syntactic reference corpus of medieval french . it relies on a dependency-based model to manually annotate old french texts from the base de franais mdival and the nouveau corpus damsterdam . notabene uses owl ontologies to frame the terminology used in the annotation , which is displayed in a tree-like view of the annotation . this tree widget alows easy grouping and tagging of words and structures . to increase the quality of the annotation , two annotators work independently on the same texts at the same time and notabene can also generate automatic comparisons between both analyses . the rdf format can be used to export the data to several other formats : namely , tigerxml ( for querying the data and extracting structures ) and graphviz dot format ( for quoting syntactic description in research papers ) . first , we will present the syntactic reference corpus of medieval french project ( srcmf ) ( 1 ) . then , we will show how the notabene rdf annotation tool software is used within the project ( 2 ) . in our conclusion , we will stress further developments of the tool ( 3 ) .

strategies for sustainable mt for basque :
we present some language technology applications that have proven to be effective tools to promote the use of basque , a european less privileged language . we also present the strategy we have followed for almost twenty years to develop those applications as the top of an integrated environment of language resources , language foundations , language tools and other applications . when we have faced a difficult task such as machine translation to basque , our strategy has worked well . we have had good results in a short time just reusing previous works for basque , reusing other open-source tools , and developing just a few new modules in collaboration with other groups . in addition , new reusable tools and formats have been produced .

comparing multilingual comparable articles based on opinions
multilingual sentiment analysis attracts increased attention as the massive growth of multilingual web contents . this conducts to study opinions across different languages by comparing the underlying messages written by different people having different opinions . in this paper , we propose sentiment based comparability measures ( scm ) to compare opinions in multilingual comparable articles without translating source/target into the same language . this will allow media trackers ( journalists ) to automatically detect public opinion split across huge multilingual web contents . to develop scm , we need either to get or to build parallel sentiment corpora . because this kind of corpora are not available , we decided to build them . for that , we propose a new method to automatically label parallel corpora with sentiment classes . then we use the extracted parallel sentiment corpora to develop multilingual sentiment analysis system . experimental results show that , the proposed measure can capture differences in terms of opinions . the results also show that comparable articles variate in their objectivity and positivity .

novel word-sense identification , jey han lau and timothy baldwin
automatic lexical acquisition has been an active area of research in computational linguistics for over two decades , but the automatic identification of new word-senses has received attention only very recently . previous work on this topic has been limited by the availability of appropriate evaluation resources . in this paper we present the largest corpus-based dataset of diachronic sense differences to date , which we believe will encourage further work in this area . we then describe several extensions to a state-of-the-art topic modelling approach for identifying new word-senses . this adapted method shows superior performance on our dataset of two different corpus pairs to that of the original method for both : ( a ) types having taken on a novel sense over time ; and ( b ) the token instances of such novel senses .

the olac metadata set and controlled vocabularies linguistic data consortium
as language data and associated technologies proliferate and as the language resources community rapidly expands , it has become difficult to locate and reuse existing resources . are there any lexical resources for such-and-such a language what tool can work with transcripts in this particular format what is a good format to use for linguistic data of this type questions like these dominate many mailing lists , since web search engines are an unreliable way to find language resources . this paper describes a new digital infrastructure for language resource discovery , based on the open archives initiative , and called olac the open language archives community . the olac metadata set and the associated controlled vocabularies facilitate consistent description and focussed searching . we report progress on the metadata set and controlled vocabularies , describing current issues and soliciting input from the language resources community .

towards multimodal modeling of physicians diagnostic confidence and self-awareness using medical narratives cecilia ovesdotter alm
misdiagnosis is a problem in the medical field , often related to physicians cognitive errors . overconfidence is considered a major cause of such errors . intelligent diagnostic support systems could benefit from understanding how aware physicians are of their performance when they estimate their confidence in a diagnosis ( i.e . a physicians diagnostic self-awareness ) . shedding light on the cognitive processes related to such awareness could also help improve medical education . we use a multimodal dataset of medical narratives to computationally model diagnostic confidence and self-awareness based on physicians linguistic and eye movement behaviors . dermatologists viewed images of cutaneous conditions , providing a description , diagnosis , and certainty level for each image case , while their speech and eye movements were recorded . we define both a generalized and a personalized approach to binning confidence levels , used in classification experiments . we also introduce truly multimodal features , which focus on combining linguistic and eye movement data into multimodal attributes . results indicate that combinations of multiple modalities can outperform their constituent modalities in isolation for these problems .

a model of local coherence effects in human sentence processing as consequences of updates from bottom-up prior to posterior beliefs
human sentence processing involves integrating probabilistic knowledge from a variety of sources in order to incrementally determine the hierarchical structure for the serial input stream . while a large number of sentence processing effects have been explained in terms of comprehenders rational use of probabilistic information , effects of local coherences have not . we present here a new model of local coherences , viewing them as resulting from a belief-update process , and show that the relevant probabilities in our model are calculable from a probabilistic earley parser . finally , we demonstrate empirically that an implemented version of the model makes the correct predictions for the materials from the original experiment demonstrating local coherence effects .

automatically determining a proper length for multi-document summarization : a bayesian nonparametric approach
document summarization is an important task in the area of natural language processing , which aims to extract the most important information from a single document or a cluster of documents . in various summarization tasks , the summary length is manually defined . however , how to find the proper summary length is quite a problem ; and keeping all summaries restricted to the same length is not always a good choice . it is obviously improper to generate summaries with the same length for two clusters of documents which contain quite different quantity of information . in this paper , we propose a bayesian nonparametric model for multidocument summarization in order to automatically determine the proper lengths of summaries . assuming that an original document can be reconstructed from its summary , we describe the reconstruction by a bayesian framework which selects sentences to form a good summary . experimental results on duc2004 data sets and some expanded data demonstrate the good quality of our summaries and the rationality of the length determination .

overview of bionlp09 shared task on event extraction
the paper presents the design and implementation of the bionlp09 shared task , and reports the final results with analysis . the shared task consists of three sub-tasks , each of which addresses bio-molecular event extraction at a different level of specificity . the data was developed based on the genia event corpus . the shared task was run over 12 weeks , drawing initial interest from 42 teams . of these teams , 24 submitted final results . the evaluation results are encouraging , indicating that state-of-the-art performance is approaching a practically applicable level and revealing some remaining challenges .

question type classication s wabash ave
question terminology is a set of terms which appear in keywords idioms and xed expressions commonly observed in questions . this paper investigates ways to automatically extract question terminology from a corpus of questions and represent them for the purpose of classifying by question type . our key interest is to see whether or not semantic features can enhance the representation of strongly lexical nature of question sentences . we compare two feature sets one with lexical features only and another with a mixture of lexical and semantic features for evaluation . we measure the classication accuracy made by two machine learning algorithms c and pebls by using a procedure called domain crossvalidation which eectively mea sures the domain transferability of features .

graph-based event coreference resolution
in this paper , we address the problem of event coreference resolution as specified in the automatic content extraction ( ace ) program . in contrast to entity coreference resolution , event coreference resolution has not received great attention from researchers . in this paper , we first demonstrate the diverse scenarios of event coreference by an example . we then model event coreference resolution as a spectral graph clustering problem and evaluate the clustering algorithm on ground truth event mentions using ecm-f measure . we obtain the ecm-f scores of 0.8363 and 0.8312 respectively by using two methods for computing coreference matrices .

robust parsing : more with less
covering as many phenomena as possible is a traditional goal of parser development , but the broader a grammar is made , the blunter it may become , as rare constructions influence the behaviour on simple sentences that were already solved correctly . we observe the effects of intentionally removing support for specific constructions from a broad-coverage grammar of german . we show that accuracy of analysing sentences from the negra corpus can be improved not only for sentences that do not need the extra coverage , but even when including those that do .

topological dependency trees : a constraint-based account of linear precedence
we describe a new framework for dependency grammar , with a modular decomposition of immediate dependency and linear precedence . our approach distinguishes two orthogonal yet mutually constraining structures : a syntactic dependency tree and a topological dependency tree . the syntax tree is nonprojective and even non-ordered , while the topological tree is projective and partially ordered .

anders bjorkelund love hafdell pierre nugues
this paper describes our contribution to the semantic role labeling task ( srl-only ) of the conll-2009 shared task in the closed challenge ( hajic et al , 2009 ) . our system consists of a pipeline of independent , local classifiers that identify the predicate sense , the arguments of the predicates , and the argument labels . using these local models , we carried out a beam search to generate a pool of candidates . we then reranked the candidates using a joint learning approach that combines the local models and proposition features . to address the multilingual nature of the data , we implemented a feature selection procedure that systematically explored the feature space , yielding significant gains over a standard set of features . our system achieved the second best semantic score overall with an average labeled semantic f1 of 80.31. it obtained the best f1 score on the chinese and german data and the second best one on english .

revisiting pivot language approach for machine translation
this paper revisits the pivot language approach for machine translation . first , we investigate three different methods for pivot translation . then we employ a hybrid method combining rbmt and smt systems to fill up the data gap for pivot translation , where the sourcepivot and pivot-target corpora are independent . experimental results on spoken language translation show that this hybrid method significantly improves the translation quality , which outperforms the method using a source-target corpus of the same size . in addition , we propose a system combination approach to select better translations from those produced by various pivot translation methods . this method regards system combination as a translation evaluation problem and formalizes it with a regression learning model . experimental results indicate that our method achieves consistent and significant improvement over individual translation outputs .

ranking multidocument event descriptions for building thematic timelines
this paper tackles the problem of timeline generation from traditional news sources . our system builds thematic timelines for a general-domain topic defined by a user query . the system selects and ranks events relevant to the input query . each event is represented by a one-sentence description in the output timeline . we present an inter-cluster ranking algorithm that takes events from multiple clusters as input and that selects the most salient and relevant events . a cluster , in our work , contains all the events happening in a specific date . our algorithm utilizes the temporal information derived from a large collection of extensively temporal analyzed texts . such temporal information is combined with textual contents into an event scoring model in order to rank events based on their salience and query-relevance .

parsing biomedical literature
techniques evaluated on the genia corpus of medline abstracts [ 1,2 ] . we begin by observing that the penn treebank ( ptb ) is lexically impoverished when measured on various genres of scientific and technical writing , and that this significantly impacts parse accuracy . to resolve this without requiring in-domain treebank data , we show how existing domain-specific lexical resources may be leveraged to augment ptb-training : part-of-speech tags , dictionary collocations , and namedentities . using a state-of-the-art statistical parser [ 3 ] as our baseline , our lexically-adapted parser achieves a 14.2 % reduction in error . with oracleknowledge of named-entities , this error reduction improves to 21.2 % .

heideltime : high quality rule-based extraction and normalization of
in this paper , we describe heideltime , a system for the extraction and normalization of temporal expressions . heideltime is a rule-based system mainly using regular expression patterns for the extraction of temporal expressions and knowledge resources as well as linguistic clues for their normalization . in the tempeval-2 challenge , heideltime achieved the highest fscore ( 86 % ) for the extraction and the best results in assigning the correct value attribute , i.e. , in understanding the semantics of the temporal expressions .

textual emotion processing from event analysis
textual emotion recognition has gained a lot of attention recent years ; it is however less developed due to the complexity nature of emotion . in this paper , we start with the discussion of a number of fundamental yet unresolved issues concerning emotion , which includes its definition , representation and technology . we then propose an alternative solution for emotion recognition taking into account of emotion causes . two pilot experiments are done to justify our proposal . the first experiment explores the impact of emotion recognition . it shows that the context contains rich and crucial information that effectively help emotion recognition . the other experiment examines emotion cause events in the context . we find that most emotions are expressed with the presence of causes . the experiments prove that emotion cause serves as an important cue for emotion recognition . we suggest that the combination of both emotion study and event analysis would be a fruitful direction for deep emotion processing .

whats in a message interpreting geo-referenced data for the
in this paper we describe content determination issues involved in the atlas.txt project , which aims to automatically describe georeferenced information such as census data as text for the visually-impaired ( vi ) . texts communicating geo-referenced census information contain census data abstractions and their corresponding geographic references . because visually impaired users find interpreting geographic references hard , we hypothesized

product named entity recognition based on hierarchical hidden
a hierarchical hidden markov model ( hhmm ) based approach of product named entity recognition ( ner ) from chinese free text is presented in this paper . characteristics and challenges in product ner is also investigated and analyzed deliberately compared with general ner . within a unified statistical framework , the approach we proposed is able to make probabilistically reasonable decisions to a global optimization by leveraging diverse range of linguistic features and knowledge sources . experimental results show that our approach performs quite well in two different domains .

towards ontology-based natural language processing
conceptualising a domain has long been recognised as a prerequisite for understanding that domain and processing information about it . ontologies are explicit specifications of conceptualisations which are now recognised as important components of information systems and information processing . in this paper , we describe a project in which ontologies are part of the reasoning process used for information management and for the presentation of information . both accessing and presenting information are mediated via natural language and the ontologies are coupled with the lexicon used in the natural language component .

coordination disambiguation without any similarities
the use of similarities has been one of the main approaches to resolve the ambiguities of coordinate structures . in this paper , we present an alternative method for coordination disambiguation , which does not use similarities . our hypothesis is that coordinate structures are supported by surrounding dependency relations , and that such dependency relations rather yield similarity between conjuncts , which humans feel . based on this hypothesis , we built a japanese fully-lexicalized generative parser that includes coordination disambiguation . experimental results on web sentences indicated the effectiveness of our approach , and endorsed our hypothesis .

limited memory incremental coreference resolution
we propose an algorithm for coreference resolution based on analogy with shift-reduce parsing . by reconceptualising the task in this way , we unite ranking- and cluster-based approaches to coreference resolution , which have until now been largely orthogonal . additionally , our framework naturally lends itself to rich discourse modelling , which we use to define a series of psycholinguistically motivated features . we achieve conll scores of 63.33 and 62.91 on the conll-2012 dev and test splits of the ontonotes 5 corpus , beating the publicly available state of the art systems . these results are also competitive with the best reported research systems despite our system having low memory requirements and a simpler model .

an empirical study on class-based word sense disambiguation ruben izquierdo & armando suarez
as empirically demonstrated by the last senseval exercises , assigning the appropriate meaning to words in context has resisted all attempts to be successfully addressed . one possible reason could be the use of inappropriate set of meanings . in fact , wordnet has been used as a de-facto standard repository of meanings . however , to our knowledge , the meanings represented by wordnet have been only used for wsd at a very fine-grained sense level or at a very coarse-grained class level . we suspect that selecting the appropriate level of abstraction could be on between both levels . we use a very simple method for deriving a small set of appropriate meanings using basic structural properties of wordnet . we also empirically demonstrate that this automatically derived set of meanings groups senses into an adequate level of abstraction in order to perform class-based word sense disambiguation , allowing accuracy figures over 80 % .

sarcasm as contrast between a positive sentiment and negative situation
a common form of sarcasm on twitter consists of a positive sentiment contrasted with a negative situation . for example , many sarcastic tweets include a positive sentiment , such as love or enjoy , followed by an expression that describes an undesirable activity or state ( e.g. , taking exams or being ignored ) . we have developed a sarcasm recognizer to identify this type of sarcasm in tweets . we present a novel bootstrapping algorithm that automatically learns lists of positive sentiment phrases and negative situation phrases from sarcastic tweets . we show that identifying contrasting contexts using the phrases learned through bootstrapping yields improved recall for sarcasm recognition .

computing weakest readings cluster of excellence
we present an efficient algorithm for computing the weakest readings of semantically ambiguous sentences . a corpus-based evaluation with a large-scale grammar shows that our algorithm reduces over 80 % of sentences to one or two readings , in negligible runtime , and thus makes it possible to work with semantic representations derived by deep large-scale grammars .

selecting query term alterations for web search by exploiting query
query expansion by word alterations ( alternative forms of a word ) is often used in web search to replace word stemming . this allows users to specify particular word forms in a query . however , if many alterations are added , query traffic will be greatly increased . in this paper , we propose methods to select only a few useful word alterations for query expansion . the selection is made according to the appropriateness of the alteration to the query context ( using a bigram language model ) , or according to its expected impact on the retrieval effectiveness ( using a regression model ) . our experiments on two trec collections will show that both methods only select a few expansion terms , but the retrieval effectiveness can be improved significantly .

experiment with n-gram posteriors anil kumar singh
this paper describes the machine learning algorithm and the features used by limsi for the quality estimation shared task . our submission mainly aims at evaluating the usefulness for quality estimation of ngram posterior probabilities that quantify the probability for a given n-gram to be part of the system output .

improving pivot-based statistical machine translation using random walk
this paper proposes a novel approach that utilizes a machine learning method to improve pivot-based statistical machine translation ( smt ) . for language pairs with few bilingual data , a possible solution in pivot-based smt using another language as a `` bridge '' to generate source-target translation . however , one of the weaknesses is that some useful sourcetarget translations can not be generated if the corresponding source phrase and target phrase connect to different pivot phrases . to alleviate the problem , we utilize markov random walks to connect possible translation phrases between source and target language . experimental results on european parliament data , spoken language data and web data show that our method leads to significant improvements on all the tasks over the baseline system .

infoxtract location normalization : a hybrid approach to geographic references in information extraction
ambiguity is very high for location names . for example , there are 23 cities named buffalo in the u.s. based on our previous work , this paper presents a refined hybrid approach to geographic references using our information extraction engine infoxtract . the infoxtract location normalization module consists of local pattern matching and discourse co-occurrence analysis as well as default senses . multiple knowledge sources are used in a number of ways : ( i ) pattern matching driven by local context , ( ii ) maximum spanning tree search for discourse analysis , and ( iii ) applying default sense heuristics and extracting default senses from the web . the results are benchmarked with 96 % accuracy on our test collections that consist of both news articles and tourist guides . the performance contribution for each component of the module is also benchmarked and discussed .

an effective hybrid machine learning approach for coreference
we present a hybrid machine learning approach for coreference resolution . in our method , we use crfs as basic training model , use active learning method to generate combined features so as to make existed features used more effectively ; at last , we proposed a novel clustering algorithm which used both the linguistics knowledge and the statistical knowledge . we built a coreference resolution system based on the proposed method and evaluate its performance from three aspects : the contributions of active learning ; the effects of different clustering algorithms ; and the resolution performance of different kinds of nps . experimental results show that additional performance gain can be obtained by using active learning method ; clustering algorithm has a great effect on coreference resolutions performance and our clustering algorithm is very effective ; and the key of coreference resolution is to improve the performance of the normal nouns resolution , especially the pronouns resolution .

the role of implicit argumentation in nominal srl
nominals frequently surface without overtly expressed arguments . in order to measure the potential benefit of nominal srl for downstream processes , such nominals must be accounted for . in this paper , we show that a state-of-the-art nominal srl system with an overall argument f1 of 0.76 suffers a performance loss of more than 9 % when nominals with implicit arguments are included in the evaluation . we then develop a system that takes implicit argumentation into account , improving overall performance by nearly 5 % . our results indicate that the degree of implicit argumentation varies widely across nominals , making automated detection of implicit argumentation an important step for nominal srl .

pre-reordering for machine translation using transition-based walks on dependency parse trees antonio valerio miceli-barone dipartimento di informatica dipartimento di informatica
we propose a pre-reordering scheme to improve the quality of machine translation by permuting the words of a source sentence to a target-like order . this is accomplished as a transition-based system that walks on the dependency parse tree of the sentence and emits words in target-like order , driven by a classifier trained on a parallel corpus . our system is capable of generating arbitrary permutations up to flexible constraints determined by the choice of the classifier algorithm and input features .

probabilistic models for disambiguation of an hpsg-based chart generator
we describe probabilistic models for a chart generator based on hpsg . within the research field of parsing with lexicalized grammars such as hpsg , recent developments have achieved efficient estimation of probabilistic models and high-speed parsing guided by probabilistic models . the focus of this paper is to show that two essential techniques model estimation on packed parse forests and beam search during parsing are successfully exported to the task of natural language generation . additionally , we report empirical evaluation of the performance of several disambiguation models and how the performance changes according to the feature set used in the models and the size of training data .

of an automatically annotated corpus
the creation of a gold standard corpus ( gsc ) is a very laborious and costly process . silver standard corpus ( ssc ) annotation is a very recent direction of corpus development which relies on multiple systems instead of human annotators . in this paper , we investigate the practical usability of an ssc when a machine learning system is trained on it and tested on an unseen benchmark gsc . the main focus of this paper is how an ssc can be maximally exploited . in this process , we inspect several hypotheses which might have influenced the idea of ssc creation . empirical results suggest that some of the hypotheses ( e.g . a positive impact of a large ssc despite of having wrong and missing annotations ) are not fully correct . we show that it is possible to automatically improve the quality and the quantity of the ssc annotations . we also observe that considering only those sentences of ssc which contain annotations rather than the full ssc results in a performance boost .

acquisition of desires before beliefs : a computational investigation
the acquisition of belief verbs lags behind the acquisition of desire verbs in children . some psycholinguistic theories attribute this lag to conceptual differences between the two classes , while others suggest that syntactic differences are responsible . through computational experiments , we show that a probabilistic verb learning model exhibits the pattern of acquisition , even though there is no difference in the model in the difficulty of the semantic or syntactic properties of belief vs. desire verbs . our results point to the distributional properties of various verb classes as a potentially important , and heretofore unexplored , factor in the observed developmental lag of belief verbs .

discriminative classifiers for deterministic dependency parsing
deterministic parsing guided by treebankinduced classifiers has emerged as a simple and efficient alternative to more complex models for data-driven parsing . we present a systematic comparison of memory-based learning ( mbl ) and support vector machines ( svm ) for inducing classifiers for deterministic dependency parsing , using data from chinese , english and swedish , together with a variety of different feature models . the comparison shows that svm gives higher accuracy for richly articulated feature models across all languages , albeit with considerably longer training times . the results also confirm that classifier-based deterministic parsing can achieve parsing accuracy very close to the best results reported for more complex parsing models .

tulipa : towards a multi-formalism parsing environment for cnrs - loria
in this paper , we present an open-source parsing environment ( tubingen linguistic parsing architecture , tulipa ) which uses range concatenation grammar ( rcg ) as a pivot formalism , thus opening the way to the parsing of several mildly context-sensitive formalisms . this environment currently supports tree-based grammars ( namely tree-adjoining grammars ( tag ) and multi-component treeadjoining grammars with tree tuples ( tt-mctag ) ) and allows computation not only of syntactic structures , but also of the corresponding semantic representations . it is used for the development of a tree-based grammar for german .

better informed training of latent syntactic features
we study unsupervised methods for learning refinements of the nonterminals in a treebank . following matsuzaki et al ( 2005 ) and prescher ( 2005 ) , we may for example split np without supervision into np [ 0 ] and np [ 1 ] , which behave differently . we first propose to learn a pcfg that adds such features to nonterminals in such a way that they respect patterns of linguistic feature passing : each nodes nonterminal features are either identical to , or independent of , those of its parent . this linguistic constraint reduces runtime and the number of parameters to be learned . however , it did not yield improvements when training on the penn treebank . an orthogonal strategy was more successful : to improve the performance of the em learner by treebank preprocessing and by annealing methods that split nonterminals selectively . using these methods , we can maintain high parsing accuracy while dramatically reducing the model size .

from natural language specifications to program input parsers
we present a method for automatically generating input parsers from english specifications of input file formats . we use a bayesian generative model to capture relevant natural language phenomena and translate the english specification into a specification tree , which is then translated into a c++ input parser . we model the problem as a joint dependency parsing and semantic role labeling task . our method is based on two sources of information : ( 1 ) the correlation between the text and the specification tree and ( 2 ) noisy supervision as determined by the success of the generated c++ parser in reading input examples . our results show that our approach achieves 80.0 % f-score accuracy compared to an f-score of 66.7 % produced by a state-of-the-art semantic parser on a dataset of input format specifications from the acm international collegiate programming contest ( which were written in english for humans with no intention of providing support for automated processing ) .1

creating a bi-lingual entailment corpus through translations with
this paper reports on experiments in the creation of a bi-lingual textual entailment corpus , using non-experts workforce under strict cost and time limitations ( $ 100 , 10 days ) . to this aim workers have been hired for translation and validation tasks , through the crowdflower channel to amazon mechanical turk . as a result , an accurate and reliable corpus of 426 english/spanish entailment pairs has been produced in a more cost-effective way compared to other methods for the acquisition of translations based on crowdsourcing . focusing on two orthogonal dimensions ( i.e . reliability of annotations made by non experts , and overall corpus creation costs ) , we summarize the methodology we adopted , the achieved results , the main problems encountered , and the lessons learned .

building lexicon for sentiment analysis from massive collection of html
recognizing polarity requires a list of polar words and phrases . for the purpose of building such lexicon automatically , a lot of studies have investigated ( semi- ) unsupervised method of learning polarity of words and phrases . in this paper , we explore to use structural clues that can extract polar sentences from japanese html documents , and build lexicon from the extracted polar sentences . the key idea is to develop the structural clues so that it achieves extremely high precision at the cost of recall . in order to compensate for the low recall , we used massive collection of html documents . thus , we could prepare enough polar sentence corpus .

whats in their corpora of dialog , video , & lidar messages
this paper briefly sketches new work-inprogress ( i ) developing task-based scenarios where human-robot teams collaboratively explore real-world environments in which the robot is immersed but the humans are not , ( ii ) extracting and constructing multi-modal interval corpora from dialog , video , and lidar messages that were recorded in ros bagfiles during task sessions , and ( iii ) testing automated methods to identify , track , and align co-referent content both within and across modalities in these interval corpora . the pre-pilot study and its corpora provide a unique , empirical starting point for our longerterm research objective : characterizing the balance of explicitly shared and tacitly assumed information exchanged during effective teamwork .

semi-automatic entity set refinement
state of the art set expansion algorithms produce varying quality expansions for different entity types . even for the highest quality expansions , errors still occur and manual refinements are necessary for most practical uses . in this paper , we propose algorithms to aide this refinement process , greatly reducing the amount of manual labor required . the methods rely on the fact that most expansion errors are systematic , often stemming from the fact that some seed elements are ambiguous . using our methods , empirical evidence shows that average r-precision over random entity sets improves by 26 % to 51 % when given from 5 to 10 manually tagged errors . both proposed refinement models have linear time complexity in set size allowing for practical online use in set expansion systems .

k-means and graph-based approaches for chinese word sense
this paper details our experiments carried out at word sense induction task . for the foreign language ( especially english ) , there have been many studies of word sense induction ( wsi ) , and the approaches and the techniques are more and more mature . however , the study of chinese wsi is just getting started , and there has not been a better way to solve the problems encountered . wsi can be divided into two categories : supervised manner and unsupervised manner . but in the light of the high cost of supervised manner , we introduce novel solutions to automatic and unsupervised wsi . in this paper , we propose two different systems . the first one is called k-means-based chinese word sense induction in an unsupervised manner while the second one is graph-based chinese word sense induction . in the experiments , the first system has achieved a 0.7729 fscore on average while the second one has achieved a 0.6067 fscore .

crfs-based named entity recognition incorporated with heuristic entity list searching
chinese named entity recognition is one of the most important tasks in nlp . two kinds of challenges we confront are how to improve the performance in one corpus and keep its performance in another different corpus . we use a combination of statistical models , i.e . a language model to recognize person names and two crfs models to recognize location names and organization names respectively . we also incorporate an efficient heuristic named entity list searching process into the framework of statistical model in order to improve both the performance and the adaptability of the statistical ner system . we participate in the ner tests on open tracks of msra . the testing results show that our system can performs well .

automatic identification of bengali noun-noun compounds using random forest vivekananda gayen kamal sarkar
this paper presents a supervised machine learning approach that uses a machine learning algorithm called random forest for recognition of bengali noun-noun compounds as multiword expression ( mwe ) from bengali corpus . our proposed approach to mwe recognition has two steps : ( 1 ) extraction of candidate multi-word expressions using chunk information and various heuristic rules and ( 2 ) training the machine learning algorithm to recognize a candidate multi-word expression as multi-word expression or not . a variety of association measures , syntactic and linguistic clues are used as features for identifying mwes . the proposed system is tested on a bengali corpus for identifying noun-noun compound mwes from the corpus .

homophones and tonal patterns in english-chinese transliteration
the abundance of homophones in chinese significantly increases the number of similarly acceptable candidates in english-to-chinese transliteration ( e2c ) . the dialectal factor also leads to different transliteration practice . we compare e2c between mandarin chinese and cantonese , and report work in progress for dealing with homophones and tonal patterns despite potential skewed distributions of individual chinese characters in the training data .

conversational strategies for robustly managing dialog in public spaces
open environments present an attention management challenge for conversational systems . we describe a kiosk system ( based on ravenclawolympus ) that uses simple auditory and visual information to interpret human presence and manage the systems attention . the system robustly differentiates intended interactions from unintended ones at an accuracy of 93 % and provides similar task completion rates in both a quiet room and a public space .

cognate identification and alignment using practical orthographies
we use an iterative process of multi-gram alignment between associated words in different languages in an attempt to identify cognates . to maximise the amount of data , we use practical orthographies instead of consistently coded phonetic transcriptions . first results indicate that using practical orthographies can be useful , the more so when dealing with large amounts of data .

extracting regulatory gene expression networks from pubmed
we present an approach using syntactosemantic rules for the extraction of relational information from biomedical abstracts . the results show that by overcoming the hurdle of technical terminology , high precision results can be achieved . from abstracts related to bakers yeast , we manage to extract a regulatory network comprised of 441 pairwise relations from 58,664 abstracts with an accuracy of 8390 % . to achieve this , we made use of a resource of gene/protein names considerably larger than those used in most other biology related information extraction approaches . this list of names was included in the lexicon of our retrained part-of-speech tagger for use on molecular biology abstracts . for the domain in question an accuracy of 93.697.7 % was attained on pos-tags . the method is easily adapted to other organisms than yeast , allowing us to extract many more biologically relevant relations .

towards style transformation from written-style to audio-style
in this paper , we address the problem of optimizing the style of textual content to make it more suitable to being listened to by a user as opposed to being read . we study the differences between the written style and the audio style by consulting the linguistics and journalism literatures . guided by this study , we suggest a number of linguistic features to distinguish between the two styles . we show the correctness of our features and the impact of style transformation on the user experience through statistical analysis , a style classification task , and a user study .

unsupervised coreference resolution in a nonparametric bayesian model
we present an unsupervised , nonparametric bayesian approach to coreference resolution which models both global entity identity across a corpus as well as the sequential anaphoric structure within each document . while most existing coreference work is driven by pairwise decisions , our model is fully generative , producing each mention from a combination of global entity properties and local attentional state . despite being unsupervised , our system achieves a 70.3 muc f1 measure on the muc-6 test set , broadly in the range of some recent supervised results .

a dependency based statistical translation model dipartimento di informatica dipartimento di informatica antonio valerio miceli barone dipartimento di informatica
we present a translation model based on dependency trees . the model adopts a treeto-string approach and extends phrasebased translation ( pbt ) by using the dependency tree of the source sentence for selecting translation options and for reordering them . decoding is done by translating each node in the tree and combining its translations with those of its head in alternative orders with respect to its siblings . reordering of the siblings exploits a heuristic based on the syntactic information from the parse tree which is learned from the corpus . the decoder uses the same phrase tables produced by a pbt system for looking up translations of single words or of partial sub-trees . a mathematical model is presented and experimental results are discussed .

wosit : a word sense induction toolkit for search result clustering and diversification dipartimento di informatica
in this demonstration we present wosit , an api for word sense induction ( wsi ) algorithms . the toolkit provides implementations of existing graph-based wsi algorithms , but can also be extended with new algorithms . the main mission of wosit is to provide a framework for the extrinsic evaluation of wsi algorithms , also within end-user applications such as web search result clustering and diversification .

logarithmic opinion pools for conditional random fields division of informatics and software engineering division of informatics
recent work on conditional random fields ( crfs ) has demonstrated the need for regularisation to counter the tendency of these models to overfit . the standard approach to regularising crfs involves a prior distribution over the model parameters , typically requiring search over a hyperparameter space . in this paper we address the overfitting problem from a different perspective , by factoring the crf distribution into a weighted product of individual expert crf distributions . we call this model a logarithmic opinion pool ( lop ) of crfs ( lop-crfs ) . we apply the lop-crf to two sequencing tasks . our results show that unregularised expert crfs with an unregularised crf under a lop can outperform the unregularised crf , and attain a performance level close to the regularised crf . lop-crfs therefore provide a viable alternative to crf regularisation without the need for hyperparameter search .

benchmarking of statistical dependency parsers for french marie candito ! , joakim nivre ! , pascal denis ! and enrique henestroza anguiano !
we compare the performance of three statistical parsing architectures on the problem of deriving typed dependency structures for french . the architectures are based on pcfgs with latent variables , graph-based dependency parsing and transition-based dependency parsing , respectively . we also study the inuence of three types of lexical information : lemmas , morphological features , and word clusters . the results show that all three systems achieve competitive performance , with a best labeled attachment score over 88 % . all three parsers benet from the use of automatically derived lemmas , while morphological features seem to be less important . word clusters have a positive effect primarily on the latent variable parser .

using conditional random fields to extract contexts and answers of questions from online forums
online forum discussions often contain vast amounts of questions that are the focuses of discussions . extracting contexts and answers together with the questions will yield not only a coherent forum summary but also a valuable qa knowledge base . in this paper , we propose a general framework based on conditional random fields ( crfs ) to detect the contexts and answers of questions from forum threads . we improve the basic framework by skip-chain crfs and 2d crfs to better accommodate the features of forums for better performance . experimental results show that our techniques are very promising .

semeval-2010 task 5 : automatic keyphrase extraction from scientific articles
this paper describes task 5 of the workshop on semantic evaluation 2010 ( semeval-2010 ) . systems are to automatically assign keyphrases or keywords to given scientific articles . the participating systems were evaluated by matching their extracted keyphrases against manually assigned ones . we present the overall ranking of the submitted systems and discuss our findings to suggest future directions for this task .

clustering technique in multi-document personal name disambiguation ministry of education , china ministry of education , china ministry of education , china
focusing on multi-document personal name disambiguation , this paper develops an agglomerative clustering approach to resolving this problem . we start from an analysis of pointwise mutual information between feature and the ambiguous name , which brings about a novel weight computing method for feature in clustering . then a trade-off measure between within-cluster compactness and among-cluster separation is proposed for stopping clustering . after that , we apply a labeling method to find representative feature for each cluster . finally , experiments are conducted on word-based clustering in chinese dataset and the result shows a good effect .

phrase-based statistical language generation using
most previous work on trainable language generation has focused on two paradigms : ( a ) using a statistical model to rank a set of generated utterances , or ( b ) using statistics to inform the generation decision process . both approaches rely on the existence of a handcrafted generator , which limits their scalability to new domains . this paper presents bagel , a statistical language generator which uses dynamic bayesian networks to learn from semantically-aligned data produced by 42 untrained annotators . a human evaluation shows that bagel can generate natural and informative utterances from unseen inputs in the information presentation domain . additionally , generation performance on sparse datasets is improved significantly by using certainty-based active learning , yielding ratings close to the human gold standard with a fraction of the data .

and communications technology
this paper reports our recent work of tool development for language resource construction . to make a revision of asian wordnet which is automatically generated by using the existing english translation dictionary , we propose an online collaborative tool which can organize multiple translations . to support the work of syntactic dependency tree annotation , we develop an editing suite which integrates the utilities for word segmentation , pos tagging and dependency tree into a sequence of editing .

wikiwalk : random walks on wikipedia for semantic relatedness
computing semantic relatedness of natural language texts is a key component of tasks such as information retrieval and summarization , and often depends on knowledge of a broad range of real-world concepts and relationships . we address this knowledge integration issue by computing semantic relatedness using personalized pagerank ( random walks ) on a graph derived from wikipedia . this paper evaluates methods for building the graph , including link selection strategies , and two methods for representing input texts as distributions over the graph nodes : one based on a dictionary lookup , the other based on explicit semantic analysis . we evaluate our techniques on standard word relatedness and text similarity datasets , finding that they capture similarity information complementary to existing wikipedia-based relatedness measures , resulting in small improvements on a stateof-the-art measure .

ctemp : a chinese temporal parser for extracting and normalizing temporal information
information extraction , question answering and summarization . in this paper , we present a temporal parser for extracting and normalizing temporal expressions from chinese texts . an integrated temporal framework is proposed , which includes basic temporal concepts and the classification of temporal expressions . the identification of temporal expressions is fulfilled by powerful chart-parsing based on grammar rules and constraint rules . we evaluated the system on a substantial corpus and obtained promising results .

boosting n-gram coverage for unsegmented languages using multiple text segmentation approach
automatic word segmentation errors , for languages having a writing system without word boundaries , negatively affect the performance of language models . as a solution , the use of multiple , instead of unique , segmentation has recently been proposed . this approach boosts n-gram counts and generates new n-grams . however , it also produces bad n-grams that affect the language models ' performance . in this paper , we study more deeply the contribution of our multiple segmentation approach and experiment on an efficient solution to minimize the effect of adding bad n-grams .

a text-based search interface for multimedia dialectics
the growing popularity of multimedia documents requires language technologies to approach automatic language analysis and generation from yet another perspective : that of its use in multimodal communication . in this paper , we present a support tool for cosmoroe , a theoretical framework for modelling multimedia dialectics . the tool is a text-based search interface that facilitates the exploration of a corpus of audiovisual files , annotated with the cosmoroe relations .

translation and extension of concepts across languages
we present a method which , given a few words defining a concept in some language , retrieves , disambiguates and extends corresponding terms that define a similar concept in another specified language . this can be very useful for cross-lingual information retrieval and the preparation of multi-lingual lexical resources . we automatically obtain term translations from multilingual dictionaries and disambiguate them using web counts . we then retrieve web snippets with cooccurring translations , and discover additional concept terms from these snippets . our term discovery is based on coappearance of similar words in symmetric patterns . we evaluate our method on a set of language pairs involving 45 languages , including combinations of very dissimilar ones such as russian , chinese , and hebrew for various concepts . we assess the quality of the retrieved sets using both human judgments and automatically comparing the obtained categories to corresponding english wordnet synsets .

case , constructions , framenet , and the deep lexicon
three major contributions that charles fillmore made in linguistics play an important role in the enterprise of deep lexical semantics , which is the effort to link lexical meaning to underlying abstract core theories . i will discuss how case relates to lexical decompositions , how motivated constructions span the borderline between syntax and semantics , and how the frames of framenet provide an excellent first step in deep inference .

weighted parsing of trees
we show how parsing of trees can be formalized in terms of the intersection of two tree languages . the focus is on weighted regular tree grammars and weighted tree adjoining grammars . potential applications are discussed , such as parameter estimation across formalisms .

mining new word translations from comparable corpora
new words such as names , technical terms , etc appear frequently . as such , the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations . comparable corpora such as news documents of the same period from different news agencies are readily available . in this paper , we present a new approach to mining new word translations from comparable corpora , by using context information to complement transliteration information . we evaluated our approach on six months of chinese and english gigaword corpora , with encouraging results .

multimodal menu-based dialogue with speech cursor in dico ii+
this paper describes dico ii+ , an in-vehicle dialogue system demonstrating a novel combination of flexible multimodal menu-based dialogueand a speech cursor which enables menu navigation as well as browsing long list using haptic input and spoken output .

generating focused topic-specific sentiment lexicons
we present a method for automatically generating focused and accurate topicspecific subjectivity lexicons from a general purpose polarity lexicon that allow users to pin-point subjective on-topic information in a set of relevant documents . we motivate the need for such lexicons in the field of media analysis , describe a bootstrapping method for generating a topic-specific lexicon from a general purpose polarity lexicon , and evaluate the quality of the generated lexicons both manually and using a trec blog track test set for opinionated blog post retrieval . although the generated lexicons can be an order of magnitude more selective than the general purpose lexicon , they maintain , or even improve , the performance of an opinion retrieval system .

automatic extraction of data deposition sentences :
research in the biomedical domain can have a major impact through open sharing of data produced . in this study , we use machine learning for the automatic identification of data deposition sentences in research articles . articles containing deposition sentences are correctly identified with 73 % f-measure . these results show the potential impact of our method for literature curation .

a new syntactic metric for evaluation of machine translation
machine translation ( mt ) evaluation aims at measuring the quality of a candidate translation by comparing it with a reference translation . this comparison can be performed on multiple levels : lexical , syntactic or semantic . in this paper , we propose a new syntactic metric for mt evaluation based on the comparison of the dependency structures of the reference and the candidate translations . the dependency structures are obtained by means of a weighted constraints dependency grammar parser . based on experiments performed on english to german translations , we show that the new metric correlates well with human judgments at the system level .

nested named entity recognition
many named entities contain other named entities inside them . despite this fact , the field of named entity recognition has almost entirely ignored nested named entity recognition , but due to technological , rather than ideological reasons . in this paper , we present a new technique for recognizing nested named entities , by using a discriminative constituency parser . to train the model , we transform each sentence into a tree , with constituents for each named entity ( and no other syntactic structure ) . we present results on both newspaper and biomedical corpora which contain nested named entities . in three out of four sets of experiments , our model outperforms a standard semi-crf on the more traditional top-level entities . at the same time , we improve the overall f-score by up to 30 % over the flat model , which is unable to recover any nested entities .

statistical dialog management methodologies for real applications
in this paper we present a proposal for the development of dialog systems that , on the one hand , takes into account the benefits of using standards like voicexml , whilst on the other , includes a statistical dialog module to avoid the effort of manually defining the dialog strategy . this module is trained using a labeled dialog corpus , and selects the next system response considering a classification process that takes into account the dialog history . thus , system developers only need to define a set of voicexml files , each including a system prompt and the associated grammar to recognize the users responses to the prompt . we have applied this technique to develop a dialog system in voicexml that provides railway information in spanish .

the second release of the rasp system
we describe the new release of the rasp ( robust accurate statistical parsing ) system , designed for syntactic annotation of free text . the new version includes a revised and more semantically-motivated output representation , an enhanced grammar and part-of-speech tagger lexicon , and a more flexible and semi-supervised training method for the structural parse ranking model . we evaluate the released version on the wsj using a relational evaluation scheme , and describe how the new release allows users to enhance performance using ( in-domain ) lexical information .

recognising nested named entities in biomedical text
although recent named entity ( ne ) annotation efforts involve the markup of nested entities , there has been limited focus on recognising such nested structures . this paper introduces and compares three techniques for modelling and recognising nested entities by means of a conventional sequence tagger . the methods are tested and evaluated on two biomedical data sets that contain entity nesting . all methods yield an improvement over the baseline tagger that is only trained on flat annotation .

clustering clauses for high-level relation detection : an
recently , there has been a rise of interest in unsupervised detection of highlevel semantic relations involving complex units , such as phrases and whole sentences . typically such approaches are faced with two main obstacles : data sparseness and correctly generalizing from the examples . in this work , we describe the clustered clause representation , which utilizes information-based clustering and inter-sentence dependencies to create a simplified and generalized representation of the grammatical clause . we implement an algorithm which uses this representation to detect a predefined set of high-level relations , and demonstrate our models effectiveness in overcoming both the problems mentioned .

hindi-to-urdu machine translation through transliteration
we present a novel approach to integrate transliteration into hindi-to-urdu statistical machine translation . we propose two probabilistic models , based on conditional and joint probability formulations , that are novel solutions to the problem . our models consider both transliteration and translation when translating a particular hindi word given the context whereas in previous work transliteration is only used for translating oov ( out-of-vocabulary ) words . we use transliteration as a tool for disambiguation of hindi homonyms which can be both translated or transliterated or transliterated differently based on different contexts . we obtain final bleu scores of 19.35 ( conditional probability model ) and 19.00 ( joint probability model ) as compared to 14.30 for a baseline phrase-based system and 16.25 for a system which transliterates oov words in the baseline system . this indicates that transliteration is useful for more than only translating oov words for language pairs like hindi-urdu .

methods for amharic part-of-speech tagging
the paper describes a set of experiments involving the application of three state-ofthe-art part-of-speech taggers to ethiopian amharic , using three different tagsets . the taggers showed worse performance than previously reported results for english , in particular having problems with unknown words . the best results were obtained using a maximum entropy approach , while hmm-based and svmbased taggers got comparable results .

a common framework for syntactic annotation
it is widely recognized that the proliferation of annotation schemes runs counter to the need to re-use language resources , and that standards for linguistic annotation are becoming increasingly mandatory . to answer this need , we have developed a representation framework comprised of an abstract model for a variety of different annotation types ( e.g. , morpho-syntactic tagging , syntactic annotation , co-reference annotation , etc . ) , which can be instantiated in different ways depending on the annotators approach and goals . in this paper we provide an overview of our representation framework and demonstrate its applicability to syntactic annotation . we show how the framework can contribute to comparative evaluation and merging of parser output and diverse syntactic annotation schemes .

contexts , patterns , interrelations - new ways of presenting multi-word
this contribution presents the newest version of our wortverbindungsfelder ( fields of multi-word expressions ) , an experimental lexicographic resource that focusses on aspects of mwes that are rarely addressed in traditional descriptions : contexts , patterns and interrelations . the mwe fields use data from a very large corpus of written german ( over 6 billion word forms ) and are created in a strictly corpus-based way . in addition to traditional lexicographic descriptions , they include quantitative corpus data which is structured in new ways in order to show the usage specifics . this way of looking at mwes gives insight in the structure of language and is especially interesting for foreign language learners .

proposition knowledge graphs gabriel stanovsky omer levy ido dagan
open information extraction ( open ie ) is a promising approach for unrestricted information discovery ( id ) . while open ie is a highly scalable approach , allowing unsupervised relation extraction from open domains , it currently has some limitations . first , it lacks the expressiveness needed to properly represent and extract complex assertions that are abundant in text . second , it does not consolidate the extracted propositions , which causes simple queries above open ie assertions to return insufficient or redundant information . to address these limitations , we propose in this position paper a novel representation for id propositional knowledge graphs ( pkg ) . pkgs extend the open ie paradigm by representing semantic inter-proposition relations in a traversable graph . we outline an approach for constructing pkgs from single and multiple texts , and highlight a variety of high-level applications that may leverage pkgs as their underlying information discovery and representation framework .

a measure of syntactic flexibility for automatically identifying multiword expressions in corpora
natural languages contain many multi-word sequences that do not display the variety of syntactic processes we would expect given their phrase type , and consequently must be included in the lexicon as multiword units . this paper describes a method for identifying such items in corpora , focussing on english verb-noun combinations . in an evaluation using a set of dictionary-published mwes we show that our method achieves greater accuracy than existing mwe extraction methods based on lexical association .

modeling context in scenario template creation
we describe a graph-based approach to scenario template creation , which is the task of creating a representation of multiple related events , such as reports of different hurricane incidents . we argue that context is valuable to identify important , semantically similar text spans from which template slots could be generalized . to leverage context , we represent the input as a set of graphs where predicate-argument tuples are vertices and their contextual relations are edges . a context-sensitive clustering framework is then applied to obtain meaningful tuple clusters by examining their intrinsic and extrinsic similarities . the clustering framework uses expectation maximization to guide the clustering process . experiments show that : 1 ) our approach generates high quality clusters , and 2 ) information extracted from the clusters is adequate to build high coverage templates .

measuring term informativeness in context
measuring term informativeness is a fundamental nlp task . existing methods , mostly based on statistical information in corpora , do not actually measure informativeness of a term with regard to its semantic context . this paper proposes a new lightweight feature-free approach to encode term informativeness in context by leveraging web knowledge . given a term and its context , we model contextaware term informativeness based on semantic similarity between the context and the terms most featured context in a knowledge base , wikipedia . we apply our method to three applications : core term extraction from snippets ( text segment ) , scientific keywords extraction ( paper ) , and back-of-the-book index generation ( book ) . the performance is state-of-theart or close to it for each application , demonstrating its effectiveness and generality .

inducing latent semantic relations for structured distributional sujay kumar jauhar
structured distributional semantic models aim to improve upon simple vector space models of semantics by hypothesizing that the meaning of a word is captured more effectively through its relational rather than its raw distributional signature . in accordance , they extend the vector space paradigm by structuring elements with relational information that decompose distributional signatures over discrete relation dimensions . however , the number and nature of these relations remains an open research question , with most previous work in the literature employing syntactic dependencies as surrogates for truly semantic relations . in this paper we propose a novel structured distributional semantic model with latent relation dimensions , and instantiate it using latent relational analysis . evaluation of our model yields results that significantly outperform several other distributional approaches on two semantic tasks and performs competitively on a third relation classification task .

when is self-training effective for parsing
self-training has been shown capable of improving on state-of-the-art parser performance ( mcclosky et al , 2006 ) despite the conventional wisdom on the matter and several studies to the contrary ( charniak , 1997 ; steedman et al , 2003 ) . however , it has remained unclear when and why selftraining is helpful . in this paper , we test four hypotheses ( namely , presence of a phase transition , impact of search errors , value of non-generative reranker features , and effects of unknown words ) . from these experiments , we gain a better understanding of why self-training works for parsing . since improvements from selftraining are correlated with unknown bigrams and biheads but not unknown words , the benefit of self-training appears most influenced by seeing known words in new combinations .

unsupervised models for coreference resolution
we present a generative model for unsupervised coreference resolution that views coreference as an em clustering process . for comparison purposes , we revisit haghighi and kleins ( 2007 ) fully-generative bayesian model for unsupervised coreference resolution , discuss its potential weaknesses and consequently propose three modifications to their model . experimental results on the ace data sets show that our model outperforms their original model by a large margin and compares favorably to the modified model .

joint word alignment and bilingual named entity recognition using dual decomposition
translated bi-texts contain complementary language cues , and previous work on named entity recognition ( ner ) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages . however , most previous approaches to bilingual tagging assume word alignments are given as fixed input , which can cause cascading errors . we observe that ner label information can be used to correct alignment mistakes , and present a graphical model that performs bilingual ner tagging jointly with word alignment , by combining two monolingual tagging models with two unidirectional alignment models . we introduce additional cross-lingual edge factors that encourage agreements between tagging and alignment decisions . we design a dual decomposition inference algorithm to perform joint decoding over the combined alignment and ner output space . experiments on the ontonotes dataset demonstrate that our method yields significant improvements in both ner and word alignment over state-of-the-art monolingual baselines .

extracting important sentences with support vector machines
extracting sentences that contain important information from a document is a form of text summarization . the technique is the key to the automatic generation of summaries similar to those written by humans . to achieve such extraction , it is important to be able to integrate heterogeneous pieces of information . one approach , parameter tuning by machine learning , has been attracting a lot of attention . this paper proposes a method of sentence extraction based on support vector machines ( svms ) . to confirm the methods performance , we conduct experiments that compare our method to three existing methods . results on the text summarization challenge ( tsc ) corpus show that our method offers the highest accuracy . moreover , we clarify the different features effective for extracting different document genres .

how well can we learn interpretable entity types from text
many nlp applications rely on type systems to represent higher-level classes . domain-specific ones are more informative , but have to be manually tailored to each task and domain , making them inflexible and expensive . we investigate a largely unsupervised approach to learning interpretable , domain-specific entity types from unlabeled text . it assumes that any common noun in a domain can function as potential entity type , and uses those nouns as hidden variables in a hmm . to constrain training , it extracts co-occurrence dictionaries of entities and common nouns from the data . we evaluate the learned types by measuring their prediction accuracy for verb arguments in several domains . the results suggest that it is possible to learn domain-specific entity types from unlabeled data . we show significant improvements over an informed baseline , reducing the error rate by 56 % .

discriminative sentence compression with soft syntactic evidence
we present a model for sentence compression that uses a discriminative largemargin learning framework coupled with a novel feature set defined on compressed bigrams as well as deep syntactic representations provided by auxiliary dependency and phrase-structure parsers . the parsers are trained out-of-domain and contain a significant amount of noise . we argue that the discriminative nature of the learning algorithm allows the model to learn weights relative to any noise in the feature set to optimize compression accuracy directly . this differs from current state-of-the-art models ( knight and marcu , 2000 ) that treat noisy parse trees , for both compressed and uncompressed sentences , as gold standard when calculating model parameters .

coreference resolution using semantic relatedness information from automatically discovered patterns
semantic relatedness is a very important factor for the coreference resolution task . to obtain this semantic information , corpusbased approaches commonly leverage patterns that can express a specific semantic relation . the patterns , however , are designed manually and thus are not necessarily the most effective ones in terms of accuracy and breadth . to deal with this problem , in this paper we propose an approach that can automatically find the effective patterns for coreference resolution . we explore how to automatically discover and evaluate patterns , and how to exploit the patterns to obtain the semantic relatedness information . the evaluation on ace data set shows that the pattern based semantic information is helpful for coreference resolution .

domain adaptation of maximum entropy language models
we investigate a recently proposed bayesian adaptation method for building style-adapted maximum entropy language models for speech recognition , given a large corpus of written language data and a small corpus of speech transcripts . experiments show that the method consistently outperforms linear interpolation which is typically used in such cases .

rediscovering annotation projection for cross-lingual parser induction
previous research on annotation projection for parser induction across languages showed only limited success and often required substantial language-specific post-processing to fix inconsistencies and to lift the performance onto a useful level . model transfer was introduced as another quite successful alternative and much research has been devoted to this paradigm recently . in this paper , we revisit annotation projection and show that the previously reported results are mainly spoiled by the flaws of evaluation with incompatible annotation schemes . lexicalized parsers created on projected data are especially harmed by such discrepancies . however , recently developed cross-lingually harmonized annotation schemes remove this obstacle and restore the abilities of syntactic annotation projection . we demonstrate this by applying projection strategies to a number of european languages and a selection of human and machine-translated data . our results outperform the simple direct transfer approach by a large margin and also pave the road to cross-lingual parsing without gold pos labels .

modifying a natural language processing system for european languages to treat arabic in information processing and information retrieval applications
the goal of many natural language processing platforms is to be able to someday correctly treat all languages . each new language , especially one from a new language family , provokes some modification and design changes . here we present the changes that we had to introduce into our platform designed for european languages in order to handle a semitic language . treatment of arabic was successfully integrated into our cross language information retrieval system , which is visible online .

using translation consensus between decoders beijing , china tianjin , china
this paper presents collaborative decoding ( co-decoding ) , a new method to improve machine translation accuracy by leveraging translation consensus between multiple machine translation decoders . different from system combination and mbr decoding , which postprocess the n-best lists or word lattice of machine translation decoders , in our method multiple machine translation decoders collaborate by exchanging partial translation results . using an iterative decoding approach , n-gram agreement statistics between translations of multiple decoders are employed to re-rank both full and partial hypothesis explored in decoding . experimental results on data sets for nist chinese-to-english machine translation task show that the co-decoding method can bring significant improvements to all baseline decoders , and the outputs from co-decoding can be used to further improve the result of system combination .

compositional matrix-space models of language fzi forschungszentrum informatik
we propose cmsms , a novel type of generic compositional models for syntactic and semantic aspects of natural language , based on matrix multiplication . we argue for the structural and cognitive plausibility of this model and show that it is able to cover and combine various common compositional nlp approaches ranging from statistical word space models to symbolic grammar formalisms .

for probabilistic synchronous context-free grammars
we present a method for the computation of prefix probabilities for synchronous contextfree grammars . our framework is fairly general and relies on the combination of a simple , novel grammar transformation and standard techniques to bring grammars into normal forms .

predicting the semantic compositionality of prefix verbs
in many applications , replacing a complex word form by its stem can reduce sparsity , revealing connections in the data that would not otherwise be apparent . in this paper , we focus on prefix verbs : verbs formed by adding a prefix to an existing verb stem . a prefix verb is considered compositional if it can be decomposed into a semantically equivalent expression involving its stem . we develop a classifier to predict compositionality via a range of lexical and distributional features , including novel features derived from web-scale ngram data . results on a new annotated corpus show that prefix verb compositionality can be predicted with high accuracy . our system also performs well when trained and tested on conventional morphological segmentations of prefix verbs .

unitor-core typed : combining text similarity and semantic filters through sv regression
this paper presents the unitor system that participated in the *sem 2013 shared task on semantic textual similarity ( sts ) . the task is modeled as a support vector ( sv ) regression problem , where a similarity scoring function between text pairs is acquired from examples . the proposed approach has been implemented in a system that aims at providing high applicability and robustness , in order to reduce the risk of over-fitting over a specific datasets . moreover , the approach does not require any manually coded resource ( e.g . wordnet ) , but mainly exploits distributional analysis of unlabeled corpora . a good level of accuracy is achieved over the shared task : in the typed sts task the proposed system ranks in 1st and 2nd position .

correcting dependency annotation errors
building on work detecting errors in dependency annotation , we set out to correct local dependency errors . to do this , we outline the properties of annotation errors that make the task challenging and their existence problematic for learning . for the task , we define a feature-based model that explicitly accounts for non-relations between words , and then use ambiguities from one model to constrain a second , more relaxed model . in this way , we are successfully able to correct many errors , in a way which is potentially applicable to dependency parsing more generally .

acquiring knowledge from the web to be used as selectors for noun
this paper presents a method of acquiring knowledge from the web for noun sense disambiguation . words , called selectors , are acquired which take the place of an instance of a target word in its local context . the selectors serve for the system to essentially learn the areas or concepts of wordnet that the sense of a target word should be a part of . the correct sense is chosen based on a combination of the strength given from similarity and relatedness measures over wordnet and the probability of a selector occurring within the local context . our method is evaluated using the coarse-grained all-words task from semeval 2007. experiments reveal that pathbased similarity measures perform just as well as information content similarity measures within our system . overall , the results show our system is out-performed only by systems utilizing training data or substantially more annotated data .

dynamic path prediction and recommendation in a museum environment
this research is concerned with making recommendations to museum visitors based on their history within the physical environment , and textual information associated with each item in their history . we investigate a method of providing such recommendations to users through a combination of language modelling techniques , geospatial modelling of the physical space , and observation of sequences of locations visited by other users in the past . this study compares and analyses different methods of path prediction including an adapted naive bayes method , document similarity , visitor feedback and measures of lexical similarity .

determining case in arabic : learning complex linguistic behavior requires complex linguistic features
this paper discusses automatic determination of case in arabic . this task is a major source of errors in full diacritization of arabic . we use a gold-standard syntactic tree , and obtain an error rate of about 4.2 % , with a machine learning based system outperforming a system using hand-written rules . a careful error analysis suggests that when we account for annotation errors in the gold standard , the error rate drops to 0.8 % , with the hand-written rules outperforming the machine learning-based system .

handling information access dialogue through qa technologies a novel challenge for open-domain question answering
a novel challenge for evaluating open-domain question answering technologies is proposed . in this challenge , question answering systems are supposed to be used interactively to answer a series of related questions , whereas in the conventional setting , systems answer isolated questions one by one . such an interaction occurs in the case of gathering information for a report on a specific topic , or when browsing information of interest to the user . in this paper , first , we explain the design of the challenge . we then discuss its reality and show how the capabilities measured by the challenge are useful and important in practical situations , and that the difficulty of the challenge is proper for evaluating the current state of open-domain question answering technologies .

multi-perspective question answering using the opqa corpus
we investigate techniques to support the answering of opinion-based questions . we first present the opqa corpus of opinion questions and answers . using the corpus , we compare and contrast the properties of fact and opinion questions and answers . based on the disparate characteristics of opinion vs. fact answers , we argue that traditional fact-based qa approaches may have difficulty in an mpqa setting without modification . as an initial step towards the development of mpqa systems , we investigate the use of machine learning and rule-based subjectivity and opinion source filters and show that they can be used to guide mpqa systems .

boosting cross-language retrieval by learning bilingual phrase associations from relevance rankings
we present an approach to learning bilingual n-gram correspondences from relevance rankings of english documents for japanese queries . we show that directly optimizing cross-lingual rankings rivals and complements machine translation-based cross-language information retrieval ( clir ) . we propose an efficient boosting algorithm that deals with very large cross-product spaces of word correspondences . we show in an experimental evaluation on patent prior art search that our approach , and in particular a consensus-based combination of boosting and translation-based approaches , yields substantial improvements in clir performance . our training and test data are made publicly available .

semi-automatic generation of dialogue applications in the gemini project temic speech dialog systems
gemini ( generic environment for multilingual interactive natural interfaces ) is an ec funded research project , which has two main objectives : first , the development of a flexible platform able to produce user-friendly interactive multilingual and multi-modal dialogue interfaces to databases with a minimum of human effort , and , second , the demonstration of the platforms efficiency through the development of two different applications based on this platform : eg-banking , a voice-portal for highquality interactions for bank customers , and citizencare , an e-government platform framework for citizen-to-administration interaction which are available for spoken and web-based user interaction .

discriminative language modeling with conditional random fields and the perceptron algorithm
this paper describes discriminative language modeling for a large vocabulary speech recognition task . we contrast two parameter estimation methods : the perceptron algorithm , and a method based on conditional random fields ( crfs ) . the models are encoded as deterministic weighted finite state automata , and are applied by intersecting the automata with word-lattices that are the output from a baseline recognizer . the perceptron algorithm has the benefit of automatically selecting a relatively small feature set in just a couple of passes over the training data . however , using the feature set output from the perceptron algorithm ( initialized with their weights ) , crf training provides an additional 0.5 % reduction in word error rate , for a total 1.8 % absolute reduction from the baseline of 39.2 % .

synonymous collocation extraction using translation information
automatically acquiring synonymous collocation pairs such as < turn on , obj , light > and < switch on , obj , light > from corpora is a challenging task . for this task , we can , in general , have a large monolingual corpus and/or a very limited bilingual corpus . methods that use monolingual corpora alone or use bilingual corpora alone are apparently inadequate because of low precision or low coverage . in this paper , we propose a method that uses both these resources to get an optimal compromise of precision and coverage . this method first gets candidates of synonymous collocation pairs based on a monolingual corpus and a word thesaurus , and then selects the appropriate pairs from the candidates using their translations in a second language . the translations of the candidates are obtained with a statistical translation model which is trained with a small bilingual corpus and a large monolingual corpus . the translation information is proved as effective to select synonymous collocation pairs . experimental results indicate that the average precision and recall of our approach are 74 % and 64 % respectively , which outperform those methods that only use monolingual corpora and those that only use bilingual corpora .

bayesian inference for pcfgs via markov chain monte carlo
this paper presents two markov chain monte carlo ( mcmc ) algorithms for bayesian inference of probabilistic context free grammars ( pcfgs ) from terminal strings , providing an alternative to maximum-likelihood estimation using the inside-outside algorithm . we illustrate these methods by estimating a sparse grammar describing the morphology of the bantu language sesotho , demonstrating that with suitable priors bayesian techniques can infer linguistic structure in situations where maximum likelihood methods such as the inside-outside algorithm only produce a trivial grammar .

global path-based refinement of noisy graphs applied to verb semantics
to mine semantic resources . the result is often a noisy graph of relations between words . we propose a mathematically rigorous refinement framework , which uses path-based analysis , updating the likelihood of a relation between a pair of nodes using evidence provided by multiple indirect paths between the nodes . evaluation on refining temporal verb relations in a semantic resource called verbocean showed a 16.1 % error reduction after refinement .

extreme case formulations in cypriot greek
this article is concerned with extreme case formulations ( ecfs ) ( edwards , 2000 ; pomerantz , 1986 ) in spontaneous cypriot greek conversations.1 this study confirms the occurrence of ecfs in complaints as identified by edwards ( 2000 ) pomerantz ( 1986 ) , but goes one step further to analyse the sequential and interaction work accomplished with ecfs in reporting opposition-type stories ( schegloff , 1984 ) and in complaining about a non-present partys misbehaviour . opposition-type stories report the oppositional conversation of the teller with a third non-present party ( id . ) . interestingly , in the conversational extracts examined in this study , the conversation reported is culminated with the opponents reported extreme claim ( ecf ) occupying the last turn . the occurrence of an ecf at that marked place , that is , at the punchline of the telling , is associated with issues of affiliation and stance since it is placed exactly before the recipients slot upon story completion , which is a regular place for the occurrence of evaluation ( schegloff , 1984 ) .

automatically creating datasets for measures of semantic relatedness
semantic relatedness is a special form of linguistic distance between words . evaluating semantic relatedness measures is usually performed by comparison with human judgments . previous test datasets had been created analytically and were limited in size . we propose a corpus-based system for automatically creating test datasets.1 experiments with human subjects show that the resulting datasets cover all degrees of relatedness . as a result of the corpus-based approach , test datasets cover all types of lexical-semantic relations and contain domain-specific words naturally occurring in texts .

specification of temporal and event expressions in korean text
timeml , timebank , and ttk ( tarsqi project ) have been playing an important role in enhancement of ie , qa , and other nlp applications . timeml is a specification language for events and temporal expressions in text . this paper presents the problems and solutions for porting timeml to korean as a part of the korean tarsqi project . we also introduce the kttk which is an automatic markup tool of temporal and event-denoting expressions in korean text .

maximum entropy translation model in dependency-based mt framework
maximum entropy principle has been used successfully in various nlp tasks . in this paper we propose a forward translation model consisting of a set of maximum entropy classifiers : a separate classifier is trained for each ( sufficiently frequent ) source-side lemma . in this way the estimates of translation probabilities can be sensitive to a large number of features derived from the source sentence ( including non-local features , features making use of sentence syntactic structure , etc . ) . when integrated into english-toczech dependency-based translation scenario implemented in the tectomt framework , the new translation model significantly outperforms the baseline model ( mle ) in terms of bleu . the performance is further boosted in a configuration inspired by hidden tree markov models which combines the maximum entropy translation model with the target-language dependency tree model .

automatic verb extraction from historical swedish texts of language technology
even though historical texts reveal a lot of interesting information on culture and social structure in the past , information access is limited and in most cases the only way to find the information you are looking for is to manually go through large volumes of text , searching for interesting text segments . in this paper we will explore the idea of facilitating this timeconsuming manual effort , using existing natural language processing techniques . attention is focused on automatically identifying verbs in early modern swedish texts ( 15501800 ) . the results indicate that it is possible to identify linguistic categories such as verbs in texts from this period with a high level of precision and recall , using morphological tools developed for present-day swedish , if the text is normalised into a more modern spelling before the morphological tools are applied .

unsupervised induction of cross-lingual semantic relations
creating a language-independent meaning representation would benefit many crosslingual nlp tasks . we introduce the first unsupervised approach to this problem , learning clusters of semantically equivalent english and french relations between referring expressions , based on their named-entity arguments in large monolingual corpora . the clusters can be used as language-independent semantic relations , by mapping clustered expressions in different languages onto the same relation . our approach needs no parallel text for training , but outperforms a baseline that uses machine translation on a cross-lingual question answering task . we also show how to use the semantics to improve the accuracy of machine translation , by using it in a simple reranker .

lexico-syntactic text simplification and compression with typed
we describe two systems for text simplification using typed dependency structures , one that performs lexical and syntactic simplification , and another that performs sentence compression optimised to satisfy global text constraints such as lexical density , the ratio of difficult words , and text length . we report a substantial evaluation that demonstrates the superiority of our systems , individually and in combination , over the state of the art , and also report a comprehension based evaluation of contemporary automatic text simplification systems with target non-native readers .

parma : a predicate argument
we introduce parma , a system for crossdocument , semantic predicate and argument alignment . our system combines a number of linguistic resources familiar to researchers in areas such as recognizing textual entailment and question answering , integrating them into a simple discriminative model . parma achieves state of the art results on an existing and a new dataset . we suggest that previous efforts have focussed on data that is biased and too easy , and we provide a more difficult dataset based on translation data with a low baseline which we beat by 17 % f1 .

machine translation by triangulation : making effective use of multi-parallel corpora
current phrase-based smt systems perform poorly when using small training sets . this is a consequence of unreliable translation estimates and low coverage over source and target phrases . this paper presents a method which alleviates this problem by exploiting multiple translations of the same source phrase . central to our approach is triangulation , the process of translating from a source to a target language via an intermediate third language . this allows the use of a much wider range of parallel corpora for training , and can be combined with a standard phrase-table using conventional smoothing methods . experimental results demonstrate bleu improvements for triangulated models over a standard phrase-based system .

preliminary test of a real-time , interactive silent speech interface based on electromagnetic articulograph
a silent speech interface ( ssi ) maps articulatory movement data to speech output . although still in experimental stages , silent speech interfaces hold significant potential for facilitating oral communication in persons after laryngectomy or with other severe voice impairments . despite the recent efforts on silent speech recognition algorithm development using offline data analysis , online test of ssis have rarely been conducted . in this paper , we present a preliminary , online test of a real-time , interactive ssi based on electromagnetic motion tracking . the ssi played back synthesized speech sounds in response to the users tongue and lip movements . three english talkers participated in this test , where they mouthed ( silently articulated ) phrases using the device to complete a phrase-reading task . among the three participants , 96.67 % to 100 % of the mouthed phrases were correctly recognized and corresponding synthesized sounds were played after a short delay . furthermore , one participant demonstrated the feasibility of using the ssi for a short conversation . the experimental results demonstrated the feasibility and potential of silent speech interfaces based on electromagnetic articulograph for future clinical applications .

a system for the simplification of numerical expressions at different levels
the purpose of this paper is to motivate and describe a system that simplifies numerical expression in texts , along with an evaluation study in which experts in numeracy and literacy assessed the outputs of this system . we have worked with a collection of newspaper articles with a significant number of numerical expressions . the results are discussed in comparison to conclusions obtained from a prior empirical survey .

supervised all-words lexical substitution using delexicalized features
we propose a supervised lexical substitution system that does not use separate classifiers per word and is therefore applicable to any word in the vocabulary . instead of learning word-specific substitution patterns , a global model for lexical substitution is trained on delexicalized ( i.e. , non lexical ) features , which allows to exploit the power of supervised methods while being able to generalize beyond target words in the training set . this way , our approach remains technically straightforward , provides better performance and similar coverage in comparison to unsupervised approaches . using features from lexical resources , as well as a variety of features computed from large corpora ( n-gram counts , distributional similarity ) and a ranking method based on the posterior probabilities obtained from a maximum entropy classifier , we improve over the state of the art in the lexsub best-precision metric and the generalized average precision measure . robustness of our approach is demonstrated by evaluating it successfully on two different datasets .

automatic detection of plagiarized spoken responses educational testing service
this paper addresses the task of automatically detecting plagiarized responses in the context of a test of spoken english proficiency for non-native speakers . a corpus of spoken responses containing plagiarized content was collected from a high-stakes assessment of english proficiency for non-native speakers , and several text-to-text similarity metrics were implemented to compare these responses to a set of materials that were identified as likely sources for the plagiarized content . finally , a classifier was trained using these similarity metrics to predict whether a given spoken response is plagiarized or not . the classifier was evaluated on a data set containing the responses with plagiarized content and non-plagiarized control responses and achieved accuracies of 92.0 % using transcriptions and 87.1 % using asr output ( with a baseline accuracy of 50.0 % ) .

detecting health related discussions in everyday telephone conversations for studying medical events in the lives of older adults
we apply semi-supervised topic modeling techniques to detect health-related discussions in everyday telephone conversations , which has applications in large-scale epidemiological studies and for clinical interventions for older adults . the privacy requirements associated with utilizing everyday telephone conversations preclude manual annotations ; hence , we explore semi-supervised methods in this task . we adopt a semi-supervised version of latent dirichlet allocation ( lda ) to guide the learning process . within this framework , we investigate a strategy to discard irrelevant words in the topic distribution and demonstrate that this strategy improves the average f-score on the in-domain task and an out-of-domain task ( fisher corpus ) . our results show that the increase in discussion of health related conversations is statistically associated with actual medical events obtained through weekly selfreports .

determining recurrent sound correspondences by inducing translation models
i present a novel approach to the determination of recurrent sound correspondences in bilingual wordlists . the idea is to relate correspondences between sounds in wordlists to translational equivalences between words in bitexts ( bilingual corpora ) . my method induces models of sound correspondence that are similar to models developed for statistical machine translation . the experiments show that the method is able to determine recurrent sound correspondences in bilingual wordlists in which less than 30 % of the pairs are cognates . by employing the discovered correspondences , the method can identify cognates with higher accuracy than the previously reported algorithms .

on the role of context and prosody in the interpretation of okay
we examine the effect of contextual and acoustic cues in the disambiguation of three discourse-pragmatic functions of the word okay . results of a perception study show that contextual cues are stronger predictors of discourse function than acoustic cues . however , acoustic features capturing the pitch excursion at the right edge of okay feature prominently in disambiguation , whether other contextual cues are present or not .

chinese named entity recognition with a multi-phase model deptartment of computer science , nanjing
chinese named entity recognition is one of the difficult and challenging tasks of nlp . in this paper , we present a chinese named entity recognition system using a multi-phase model . first , we segment the text with a character-level crf model . then we apply three word-level crf models to the labeling person names , location names and organization names in the segmentation results , respectively . our systems participated in the ner tests on open and closed tracks of microsoft research ( msra ) . the actual evaluation results show that our system performs well on both the open tracks and closed tracks .

an account for compound prepositions
there are some sorts of preposition + noun combinations in farsi that apparently a prepositional phrase almost behaves as compound prepositions . as they are not completely behaving as compounds , it is doubtful that the process of word formation is a morphological one . the analysis put forward by this paper proposes incorporation by which an no is incorporated to a po constructing a compound preposition . in this way tagging prepositions and parsing texts in natural language processing is defined in a proper manner .

resolving ellipsis in clarification
we offer a computational analysis of the resolution of ellipsis in certain cases of dialogue clarification . we show that this goes beyond standard techniques used in anaphora and ellipsis resolution and requires operations on highly structured , linguistically heterogeneous representations . we characterize these operations and the representations on which they operate . we offer an analysis couched in a version of head-driven phrase structure grammar combined with a theory of information states ( is ) in dialogue . we sketch an algorithm for the process of utterance integration in iss which leads to grounding or clarification .

multiple similarity measures and source-pair information in story link detection
state-of-the-art story link detection systems , that is , systems that determine whether two stories are about the same event or linked , are usually based on the cosine-similarity measured between two stories . this paper presents a method for improving the performance of a link detection system by using a variety of similarity measures and using source-pair specific statistical information . the utility of a number of different similarity measures , including cosine , hellinger , tanimoto , and clarity , both alone and in combination , was investigated . we also compared several machine learning techniques for combining the different types of information . the techniques investigated were svms , voting , and decision trees , each of which makes use of similarity and statistical information differently . our experimental results indicate that the combination of similarity measures and source-pair specific statistical information using an svm provides the largest improvement in estimating whether two stories are linked ; the resulting system was the bestperforming link detection system at tdt-2002 .

a ranking approach to stress prediction for letter-to-phoneme conversion
correct stress placement is important in text-to-speech systems , in terms of both the overall accuracy and the naturalness of pronunciation . in this paper , we formulate stress assignment as a sequence prediction problem . we represent words as sequences of substrings , and use the substrings as features in a support vector machine ( svm ) ranker , which is trained to rank possible stress patterns . the ranking approach facilitates inclusion of arbitrary features over both the input sequence and output stress pattern . our system advances the current state-of-the-art , predicting primary stress in english , german , and dutch with up to 98 % word accuracy on phonemes , and 96 % on letters . the system is also highly accurate in predicting secondary stress . finally , when applied in tandem with an l2p system , it substantially reduces the word error rate when predicting both phonemes and stress .

a semantic relatedness metric based on free link
while shortest paths in wordnet are known to correlate well with semantic similarity , an is-a hierarchy is less suited for estimating semantic relatedness . we demonstrate this by comparing two free scale networks ( conceptnet and wikipedia ) to wordnet . using the finkelstein353 dataset we show that a shortest path metric run on wikipedia attains a better correlation than wordnet-based metrics . conceptnet attains a good correlation as well , but suffers from a low concept coverage .

tagging with hidden markov models using ambiguous tags
part of speech taggers based on hidden markov models rely on a series of hypotheses which make certain errors inevitable . the idea developed in this paper consists in allowing a limited , controlled ambiguity in the output of the tagger in order to avoid a number of errors . the ambiguity takes the form of ambiguous tags which denote subsets of the tagset . these tags are used when the tagger hesitates between the different components of the ambiguous tags . they are introduced in an existing lexicon and 3-gram database . their lexical and syntactic counts are computed on the basis of the lexical and syntactic counts of their constituents , using impurity functions . the tagging process itself , based on the viterbi algorithm , is unchanged . experiments conducted on the brown corpus show a recall of 0.982 , for an ambiguity rate of 1.233 which is to be compared with a baseline recall of 0.978 for an ambiguity rate of 1.414 using the same ambiguous tags and with a recall of 0.955 corresponding to the one best solution of standard tagging ( without ambiguous tags ) .

automatically restructuring practice guidelines using the gem dtd amanda bouffier thierry poibeau
this paper describes a system capable of semi-automatically filling an xml template from free texts in the clinical domain ( practice guidelines ) . the xml template includes semantic information not explicitly encoded in the text ( pairs of conditions and actions/recommendations ) . therefore , there is a need to compute the exact scope of conditions over text sequences expressing the required actions . we present a system developed for this task . we show that it yields good performance when applied to the analysis of french practice guidelines .

map translation using geo-tagged social media
this paper discusses the problem of map translation , of servicing spatial entities in multiple languages . existing work on entity translation harvests translation evidence from text resources , not considering spatial locality in translation . in contrast , we mine geo-tagged sources for multilingual tags to improve recall , and consider spatial properties of tags for translation to improve precision . our approach empirically improves accuracy from 0.562 to 0.746 using taiwanese spatial entities .

enriching dictionaries with images from the internet - targeting wikipedia and a japanese semantic lexicon : lexeed sanae fujita
we propose a simple but effective method for enriching dictionary definitions with images based on image searches . various query expansion methods using synonyms/hypernyms ( or related words ) are evaluated . we demonstrate that our method is effective in obtaining highprecision images that complement dictionary entries , even for words with abstract or multiple meanings .

on the use of virtual evidence in conditional random fields
virtual evidence ( ve ) , first introduced by ( pearl , 1988 ) , provides a convenient way of incorporating prior knowledge into bayesian networks . this work generalizes the use of ve to undirected graphical models and , in particular , to conditional random fields ( crfs ) . we show that ve can be naturally encoded into a crf model as potential functions . more importantly , we propose a novel semisupervised machine learning objective for estimating a crf model integrated with ve . the objective can be optimized using the expectation-maximization algorithm while maintaining the discriminative nature of crfs . when evaluated on the classifieds data , our approach significantly outperforms the best known solutions reported on this task .

resources for urdu language processing
urdu is spoken by more than 100 million speakers . this paper summarizes the corpus and lexical resources being developed for urdu by the crulp , in pakistan .

on native language identification
this paper describes limsis participation to the first shared task on native language identification . our submission uses a maximum entropy classifier , using as features character and chunk n-grams , spelling and grammatical mistakes , and lexical preferences . performance was slightly improved by using a twostep classifier to better distinguish otherwise easily confused native languages .

challenges in automating maze detection
salt is a widely used annotation approach for analyzing natural language transcripts of children . nine annotated corpora are distributed along with scoring software to provide norming data . we explore automatic identification of mazes salts version of disfluency annotations and find that cross-corpus generalization is very poor . this surprising lack of crosscorpus generalization suggests substantial differences between the corpora . this is the first paper to investigate the salt corpora from the lens of natural language processing , and to compare the utility of different corpora collected in a clinical setting to train an automatic annotation system .

automatic diacritization of arabic for acoustic modeling in
automatic recognition of arabic dialectal speech is a challenging task because arabic dialects are essentially spoken varieties . only few dialectal resources are available to date ; moreover , most available acoustic data collections are transcribed without diacritics . such a transcription omits essential pronunciation information about a word , such as short vowels . in this paper we investigate various procedures that enable us to use such training data by automatically inserting the missing diacritics into the transcription . these procedures use acoustic information in combination with different levels of morphological and contextual constraints . we evaluate their performance against manually diacritized transcriptions . in addition , we demonstrate the effect of their accuracy on the recognition performance of acoustic models trained on automatically diacritized training data .

supporting rule-based representations with corpus-derived lexical
the pervasive ambiguity of language allows sentences that differ in just one lexical item to have rather different inference patterns . this would be no problem if the different lexical items fell into clearly definable and easy to represent classes . but this is not the case . to draw the correct inferences we need to look how the referents of the lexical items in the sentence ( or broader context ) interact in the described situation . given that the knowledge our systems have of the represented situation will typically be incomplete , the classifications we come up with can only be probabilistic . we illustrate this problem with an investigation of various inference patterns associated with predications of the form verb from x to y , especially go from x to y. we characterize the various readings and make an initial proposal about how to create the lexical classes that will allow us to draw the correct inferences in the different cases .

semantic roles in grammar engineering
the aim of this paper is to discuss difficulties involved in adopting an existing system of semantic roles in a grammar engineering task . two typical repertoires of semantic roles are considered , namely , verbnet and sowas system . we report on experiments showing the low inter-annotator agreement when using such systems and suggest that , at least in case of languages with rich morphosyntax , an approximation of semantic roles derived from syntactic ( grammatical functions ) and morphosyntactic ( grammatical cases ) features of arguments may actually be beneficial for applications such as textual entailment .

youve got answers : towards personalized models for predicting success in community question answering
question answering communities such as yahoo ! answers have emerged as a popular alternative to general-purpose web search . by directly interacting with other participants , information seekers can obtain specific answers to their questions . however , user success in obtaining satisfactory answers varies greatly . we hypothesize that satisfaction with the contributed answers is largely determined by the askers prior experience , expectations , and personal preferences . hence , we begin to develop personalized models of asker satisfaction to predict whether a particular question author will be satisfied with the answers contributed by the community participants . we formalize this problem , and explore a variety of content , structure , and interaction features for this task using standard machine learning techniques . our experimental evaluation over thousands of real questions indicates that indeed it is beneficial to personalize satisfaction predictions when sufficient prior user history exists , significantly improving accuracy over a one-size-fits-all prediction model .

semantic and logical inference model for textual entailment
we compare two approaches to the problem of textual entailment : slim , a compositional approach modeling the task based on identifying relations in the entailment pair , and boli , a lexical matching algorithm . slims framework incorporates a range of resources that solve local entailment problems . a search-based inference procedure unifies these resources , permitting them to interact flexibly . boli uses wordnet and other lexical similarity resources to detect correspondence between related words in the hypothesis and the text . in this paper we describe both systems in some detail and evaluate their performance on the 3rd pascal rte challenge . while the lexical method outperforms the relation-based approach , we argue that the relation-based model offers better long-term prospects for entailment recognition .

a novel method for content consistency and efficient full-text
a problem associated with current p2p ( peer-to-peer ) systems is that the consistency between copied contents is not guaranteed . additionally , the limitation of fulltext search capability in most of the popular p2p systems hinders the scalability of p2p-based content sharing systems . we proposed a new p2p content sharing system in which the consistency of contents in the network is maintained after updates or modifications have been made to the contents . links to the downloaded contents are maintained on a server . as a result , the updates and modifications to the contents can be instantly detected and hence get reflected in future p2p downloads . natural language processing including morphological analysis is performed distributedly by the p2p clients and the update of the inverted index on the server is conducted concurrently to provide an efficient full-text search . the scheme and a preliminary experimental result have been mentioned

vartra : a comparable corpus for analysis of translation variation
this paper presents a comparable translation corpus created to investigate translation variation phenomena in terms of contrasts between languages , text types and translation methods ( machine vs. computer-aided vs. human ) . these phenomena are reflected in linguistic features of translated texts belonging to different registers and produced with different translation methods . for their analysis , we combine methods derived from translation studies , language variation and machine translation , concentrating especially on textual and lexico-grammatical variation . to our knowledge , none of the existing corpora can provide comparable resources for a comprehensive analysis of variation across text types and translation methods . therefore , the corpus resources created , as well as our analysis results will find application in different research areas , such as translation studies , machine translation , and others .

sconeedit : a text-guided domain knowledge editor
we will demonstrate sconeedit , a new tool for exploring and editing knowledge bases ( kbs ) that leverages interaction with domain texts . the tool provides an annotated view of user-selected text , allowing a user to see which concepts from the text are in the kb and to edit the kb directly from this text view . alongside the text view , sconeedit provides a navigable kb view of the knowledge base , centered on concepts that appear in the text . this unified tool gives the user a text-driven way to explore a kb and add new knowledge .

non-native users in the lets go ! ! spoken dialogue system : dealing with linguistic mismatch
this paper describes the cmu lets go ! ! bus information system , an experimental system designed to study the use of spoken dialogue interfaces by non-native speakers . the differences in performance of the speech recognition and language understanding modules of the system when confronted with native and non-native spontaneous speech are analyzed . focus is placed on the linguistic mismatch between the user input and the systems expectations , and on its implications in terms of language modeling and parsing performance . the effect of including non-native data when building the speech recognition and language understanding modules is discussed . in order to close the gap between non-native and native input , a method is proposed to automatically generate confirmation prompts that are both close to the users input and covered by the systems language model and grammar , in order to help the user acquire idiomatic expressions appropriate to the task .

automated detection of language issues affecting accuracy , ambiguity and verifiability in software requirements written in natural language allan berrocal rojas , gabriela barrantes sliesarieva
most embedded systems for the avionics industry are considered safety critical systems ; as a result , strict software development standards exist to ensure critical software is built with the highest quality possible . one of such standards , do-178b , establishes a number of properties that software requirements must satisfy including : accuracy , non-ambiguity and verifiability . from a language perspective , it is possible to automate the analysis of software requirements to determine whether or not they satisfy some quality properties . this work suggests a bounded definition for three properties ( accuracy , non-ambiguity and verifiability ) considering the main characteristics that software requirements must exhibit to satisfy those objectives . a software prototype that combines natural language processing ( nlp ) techniques and specialized dictionaries was built to examine software requirements written in english with the goal of identifying whether or not they satisfy the desired properties . preliminary results are presented showing how the tool effectively identifies critical issues that are normally ignored by human reviewers .

ichi : a bilingual dictionary generating tool
in this paper we introduce a bilingual dictionary generating tool that does not use any large bilingual corpora . with this tool we implement our novel pivot based bilingual dictionary generation method that uses mainly the wordnet of the pivot language to build a new bilingual dictionary . we propose the usage of wordnet for good accuracy , introducing also a double directional selection method with local thresholds to maximize recall .

synchronous dependency insertion grammars a grammar formalism for syntax based statistical mt yuan ding and martha palmer
this paper introduces a grammar formalism specifically designed for syntax-based statistical machine translation . the synchronous grammar formalism we propose in this paper takes into consideration the pervasive structure divergence between languages , which many other synchronous grammars are unable to model . a dependency insertion grammars ( dig ) is a generative grammar formalism that captures word order phenomena within the dependency representation . synchronous dependency insertion grammars ( sdig ) is the synchronous version of dig which aims at capturing structural divergences across the languages . while both dig and sdig have comparatively simpler mathematical forms , we prove that dig nevertheless has a generation capacity weakly equivalent to that of cfg . by making a comparison to tag and synchronous tag , we show how such formalisms are linguistically motivated . we then introduce a probabilistic extension of sdig . we finally evaluated our current implementation of a simplified version of sdig for syntax based statistical machine translation .

local textual inference : can it be defined or circumscribed
this paper argues that local textual inferences come in three well-defined varieties ( entailments , conventional implicatures/presuppositions , and conversational implicatures ) and one less clearly defined one , generally available world knowledge . based on this taxonomy , it discusses some of the examples in the pascal text suite and shows that these examples do not fall into any of them . it proposes to enlarge the test suite with examples that are more directly related to the inference patterns discussed .

generation of quantified referring expressions : evidence from experimental data
we present the results from an elicitation experiment in which human speakers were asked to produced quantified referring expressions ( qres ) , as in the crate with 10 apples , the crate with many apples , etc . these results suggest that some subtle contextual factors govern the choice between different types of qres , and that numerals are highly preferred for subitizable quantities despite the availability of coarser-grained expressions .

scoring algorithms for wordspotting systems
when evaluating wordspotting systems , one normally compares receiver operating characteristic curves and different measures of accuracy . however , there are many other factors that are relevant to the systems usability for searching speech . in this paper , we discuss both measures of quality for confidence scores and propose algorithms for producing scores that are optimal with respect to these criteria .

linguistic models of deceptive opinion spam abstract of the talk
consumers increasingly inform their purchase decisions with opinions and other information found on the web . unfortunately , the ease of posting content online , potentially anonymously , combined with the public 's trust and growing reliance on this content , creates opportunities and incentives for abuse . this is especially worrisome in the case of online reviews of products and services , where businesses may feel pressure to post deceptive opinion spam -- -fictitious reviews disguised to look like authentic customer reviews . in recent years , several approaches have been proposed to identify deceptive opinion spam based on linguistic cues in a review 's text . in this talk i will summarize a few of these approaches . i will additionally discuss some of the challenges researchers face when studying this problem , including the difficulty of obtaining labeled data , uncertainties surrounding the prevalence of deception , and how linguistic cues to deceptive opinion spam vary with the text 's sentiment ( e.g. , 5-star vs 1- and 2star reviews ) , domain ( e.g. , hotel vs. restaurant reviews ) and the domain expertise of the author ( e.g. , crowdsourced vs. employee-written deceptive opinion spam ) . 31

using wordnet and semantic similarity for bilingual terminology mining from comparable corpora
this paper presents an extension of the standard approach used for bilingual lexicon extraction from comparable corpora . we study of the ambiguity problem revealed by the seed bilingual dictionary used to translate context vectors . for this purpose , we augment the standard approach by a word sense disambiguation process relying on a wordnet-based semantic similarity measure . the aim of this process is to identify the translations that are more likely to give the best representation of words in the target language . on two specialized french-english comparable corpora , empirical experimental results show that the proposed method consistently outperforms the standard approach .

word sense disambiguation using automatically translated
we present an unsupervised approach to word sense disambiguation ( wsd ) . we automatically acquire english sense examples using an english-chinese bilingual dictionary , chinese monolingual corpora and chinese-english machine translation software . we then train machine learning classifiers on these sense examples and test them on two gold standard english wsd datasets , one for binary and the other for fine-grained sense identification . on binary disambiguation , performance of our unsupervised system has approached that of the state-of-the-art supervised ones . on multi-way disambiguation , it has achieved a very good result that is competitive to other state-of-the-art unsupervised systems . given the fact that our approach does not rely on manually annotated resources , such as sense-tagged data or parallel corpora , the results are very promising .

authorship attribution and verification with many authors and limited
most studies in statistical or machine learning based authorship attribution focus on two or a few authors . this leads to an overestimation of the importance of the features extracted from the training data and found to be discriminating for these small sets of authors . most studies also use sizes of training data that are unrealistic for situations in which stylometry is applied ( e.g. , forensics ) , and thereby overestimate the accuracy of their approach in these situations . a more realistic interpretation of the task is as an authorship verification problem that we approximate by pooling data from many different authors as negative examples . in this paper , we show , on the basis of a new corpus with 145 authors , what the effect is of many authors on feature selection and learning , and show robustness of a memory-based learning approach in doing authorship attribution and verification with many authors and limited training data when compared to eager learning methods such as svms and maximum entropy learning .

modelling compression with discourse constraints
sentence compression holds promise for many applications ranging from summarisation to subtitle generation and subtitle generation . the task is typically performed on isolated sentences without taking the surrounding context into account , even though most applications would operate over entire documents . in this paper we present a discourse informed model which is capable of producing document compressions that are coherent and informative . our model is inspired by theories of local coherence and formulated within the framework of integer linear programming . experimental results show significant improvements over a stateof-the-art discourse agnostic approach .

on the geolinguistic change in northern france
with the supply of 8 closely interpreted dialectometrical maps , this paper analyses the linguistic change of the geolinguistic deep structures in northern france ( domaine dol ) between 1300 and 1900. as a matter of fact , the result will show with one exception the great stability of these deep structures .

towards a formal distributional semantics : simulating logical calculi with tensors
the development of compositional distributional models of semantics reconciling the empirical aspects of distributional semantics with the compositional aspects of formal semantics is a popular topic in the contemporary literature . this paper seeks to bring this reconciliation one step further by showing how the mathematical constructs commonly used in compositional distributional models , such as tensors and matrices , can be used to simulate different aspects of predicate logic . this paper discusses how the canonical isomorphism between tensors and multilinear maps can be exploited to simulate a full-blown quantifier-free predicate calculus using tensors . it provides tensor interpretations of the set of logical connectives required to model propositional calculi . it suggests a variant of these tensor calculi capable of modelling quantifiers , using few non-linear operations . it finally discusses the relation between these variants , and how this relation should constitute the subject of future work .

applying the semantics of negation to smt through n-best list re-ranking
although the performance of smt systems has improved over a range of different linguistic phenomena , negation has not yet received adequate treatment . previous works have considered the problem of translating negative data as one of data sparsity ( wetzel and bond ( 2012 ) ) or of structural differences between source and target language with respect to the placement of negation ( collins et al . ( 2005 ) ) . this work starts instead from the questions ofwhat is meant by negation and what makes a good translation of negation . these questions have led us to explore the use of semantics of negation in smt specifically , identifying core semantic elements of negation ( cue , event and scope ) in a source-side dependency parse and reranking hypotheses on the n-best list produced after decoding according to the extent to which an hypothesis realises these elements . the method shows considerable improvement over the baseline as measured by bleu scores and stanfords entailmentbased mt evaluation metric ( pad et al . ( 2009 ) ) .

part-of-speech tagging using virtual evidence and negative training
we present a part-of-speech tagger which introduces two new concepts : virtual evidence in the form of an observed child node , and negative training data to learn the conditional probabilities for the observed child . associated with each word is a flexible feature-set which can include binary flags , neighboring words , etc . the conditional probability of tag given word + features is implemented using a factored language-model with back-off to avoid data sparsity problems . this model remains within the framework of dynamic bayesian networks ( dbns ) and is conditionally-structured , but resolves the label bias problem inherent in the conditional markov model ( cmm ) .

translation spotting for translation memories
the term translation spotting ( ts ) refers to the task of identifying the target-language ( tl ) words that correspond to a given set of sourcelanguage ( sl ) words in a pair of text segments known to be mutual translations . this article examines this task within the context of a sub-sentential translation-memory system , i.e . a translation support tool capable of proposing translations for portions of a sl sentence , extracted from an archive of existing translations . different methods are proposed , based on a statistical translation model . these methods take advantage of certain characteristics of the application , to produce tl segments submitted to constraints of contiguity and compositionality . experiments show that imposing these constraints allows important gains in accuracy , with regard to the most probable alignments predicted by the model .

a question answer system based on confirmed knowledge developed by using mails posted to a mailing list
in this paper , we report a qa system which can answer how type questions based on the confirmed knowledge base which was developed by using mails posted to a mailing list . we first discuss a problem of developing a knowledge base by using natural language documents : wrong information in natural language documents . then , we describe a method of detecting wrong information in mails posted to a mailing list and developing a knowledge base by using these mails . finally , we show that question and answer mails posted to a mailing list can be used as a knowledge base for a qa system .

mapping source to target strings without alignment by analogical learning : a case study with transliteration
analogical learning over strings is a holistic model that has been investigated by a few authors as a means to map forms of a source language to forms of a target language . in this study , we revisit this learning paradigm and apply it to the transliteration task . we show that alone , it performs worse than a statistical phrase-based machine translation engine , but the combination of both approaches outperforms each one taken separately , demonstrating the usefulness of the information captured by a so-called formal analogy .

automatic evaluation of spoken summaries : the case of language educational testing service ( ets )
this paper investigates whether rouge , a popular metric for the evaluation of automated written summaries , can be applied to the assessment of spoken summaries produced by non-native speakers of english . we demonstrate that rouge , with its emphasis on the recall of information , is particularly suited to the assessment of the summarization quality of non-native speakers responses . a standard baseline implementation of rouge1 computed over the output of the automated speech recognizer has a spearman correlation of = 0.55 with experts scores of speakers proficiency ( = 0.51 for a content-vector baseline ) . further increases in agreement with experts scores can be achieved by using types instead of tokens for the computation of word frequencies for both candidate and reference summaries , as well as by using multiple reference summaries instead of a single one . these modifications increase the correlation with experts scores to a spearman correlation of = 0.65. furthermore , we found that the choice of reference summaries does not have any impact on performance , and that the adjusted metric is also robust to errors introduced by automated speech recognition ( = 0.67 for human transcriptions vs. = 0.65 for speech recognition output ) .

implicit feature detection via a constrained topic model and svm
implicit feature detection , also known as implicit feature identification , is an essential aspect of feature-specific opinion mining but previous works have often ignored it . we think , based on the explicit sentences , several support vector machine ( svm ) classifiers can be established to do this task . nevertheless , we believe it is possible to do better by using a constrained topic model instead of traditional attribute selection methods . experiments show that this method outperforms the traditional attribute selection methods by a large margin and the detection task can be completed better .

cross-lingual information retrieval system for indian
this paper describes our first participation in the indian language sub-task of the main adhoc monolingual and bilingual track in clef competition . in this track , the task is to retrieve relevant documents from an english corpus in response to a query expressed in different indian languages including hindi , tamil , telugu , bengali and marathi . groups participating in this track are required to submit a english to english monolingual run and a hindi to english bilingual run with optional runs in rest of the languages . we had submitted a monolingual english run and a hindi to english crosslingual run . we used a word alignment table that was learnt by a statistical machine translation ( smt ) system trained on aligned parallel sentences , to map a query in source language into an equivalent query in the language of the target document collection . the relevant documents are then retrieved using a language modeling based retrieval algorithm . on clef 2007 data set , our official cross-lingual performance was 54.4 % of the monolingual performance and in the post submission experiments we found that it can be significantly improved up to 73.4 % .

scaling conditional random fields using error-correcting codes and software engineering division of informatics division of informatics
conditional random fields ( crfs ) have been applied with considerable success to a number of natural language processing tasks . however , these tasks have mostly involved very small label sets . when deployed on tasks with larger label sets , the requirements for computational resources mean that training becomes intractable . this paper describes a method for training crfs on such tasks , using error correcting output codes ( ecoc ) . a number of crfs are independently trained on the separate binary labelling tasks of distinguishing between a subset of the labels and its complement . during decoding , these models are combined to produce a predicted label sequence which is resilient to errors by individual models . error-correcting crf training is much less resource intensive and has a much faster training time than a standardly formulated crf , while decoding performance remains quite comparable . this allows us to scale crfs to previously impossible tasks , as demonstrated by our experiments with large label sets .

automatic short answer marking
our aim is to investigate computational linguistics ( cl ) techniques in marking short free text responses automatically . successful automatic marking of free text answers would seem to presuppose an advanced level of performance in automated natural language understanding . however , recent advances in cl techniques have opened up the possibility of being able to automate the marking of free text responses typed into a computer without having to create systems that fully understand the answers . this paper describes some of the techniques we have tried so far vis -- vis this problem with results , discussion and description of the main issues encountered.1

chinesejapanese parallel sentence extraction from quasicomparable corpora
parallel sentences are crucial for statistical machine translation ( smt ) . however , they are quite scarce for most language pairs , such as chinesejapanese . many studies have been conducted on extracting parallel sentences from noisy parallel or comparable corpora . we extract chinesejapanese parallel sentences from quasicomparable corpora , which are available in far larger quantities . the task is significantly more difficult than the extraction from noisy parallel or comparable corpora . we extend a previous study that treats parallel sentence identification as a binary classification problem . previous method of classifier training by the cartesian product is not practical , because it differs from the real process of parallel sentence extraction . we propose a novel classifier training method that simulates the real sentence extraction process . furthermore , we use linguistic knowledge of chinese character features . experimental results on quasi comparable corpora indicate that our proposed approach performs significantly better than the previous study .

interpretation of partial utterances in virtual human dialogue systems
dialogue systems typically follow a rigid pace of interaction where the system waits until the user has finished speaking before producing a response . interpreting user utterances before they are completed allows a system to display more sophisticated conversational behavior , such as rapid turn-taking and appropriate use of backchannels and interruptions . we demonstrate a natural language understanding approach for partial utterances , and its use in a virtual human dialogue system that can often complete a users utterances in real time .

everyday language is highly
there has recently been a great deal of work aimed at trying to extract information from substantial texts for tasks such as question answering . much of this work has dealt with texts which are reasonably large , but which are known to contain reliable relevant information , e.g . faq lists , on-line encyclopaedias , rather than looking at huge unorganised resources such as the web . we believe , however , that even this work underestimates the complexity and subtlety of language , and hence will inevitably be restricted in what it can cope with . in particular , everyday use of language involves considerable amounts of reasoning over intensional objects ( properties and propositions ) . in order to respond appropriately to simple-seeming questions such as is going for a walk good for me , for instance , you have to be able to talk about event-types , which are intrinsically intensional . we discuss the issues involved in handling such items , and shows the kind of background knowledge that is required for drawing the appropriate conclusions about them .

down-stream effects of tree-to-dependency conversions
dependency analysis relies on morphosyntactic evidence , as well as semantic evidence . in some cases , however , morphosyntactic evidence seems to be in conflict with semantic evidence . for this reason dependency grammar theories , annotation guidelines and tree-to-dependency conversion schemes often differ in how they analyze various syntactic constructions . most experiments for which constituent-based treebanks such as the penn treebank are converted into dependency treebanks rely blindly on one of four-five widely used tree-to-dependency conversion schemes . this paper evaluates the down-stream effect of choice of conversion scheme , showing that it has dramatic impact on end results .

an ontology-based approach to disambiguation of semantic relations tine lassen and thomas vestskov terney
this paper describes experiments in using machine learning for relation disambiguation . there have been succesfuld experiments in combining machine learning and ontologies , or light-weight ontologies such as wordnet , for word sense disambiguation . however , what we are trying to do , is to disambiguate complex concepts consisting of two simpler concepts and the relation that holds between them . the motivation behind the approach is to expand existing methods for content based information retrieval . the experiments have been performed using an annotated extract of a corpus , consisting of prepositions surrounded by noun phrases , where the prepositions denote the relation we are trying disambiguate . the results show an unexploited opportunity of including prepositions and the relations they denote , e.g . in content based information retrieval .

cityu-hif : wsd with human-informed feature preference
this paper describes our word sense disambiguation ( wsd ) system participating in the semeval-2007 tasks . the core system is a fully supervised system based on a nave bayes classifier using multiple knowledge sources . toward a larger goal of incorporating the intrinsic nature of individual target words in disambiguation , thus introducing a cognitive element in automatic wsd , we tried to fine-tune the results obtained from the core system with humaninformed feature preference , and compared it with automatic feature selection as commonly practised in statistical wsd . despite the insignificant improvement observed in this preliminary attempt , more systematic analysis remains to be done for a cognitively plausible account of the factors underlying the lexical sensitivity of wsd , which would inform and enhance the development of wsd systems in return .

exploring linguistically-rich patterns for question generation ana cristina mendes
linguistic patterns reflect the regularities of natural language and their applicability is acknowledged in several natural language processing tasks . particularly , in the task of question generation , many systems depend on patterns to generate questions from text . the approach we follow relies on patterns that convey lexical , syntactic and semantic information , automatically learned from largescale corpora . in this paper we discuss the impact of varying several parameters during pattern learning and matching in the question generation task . in particular , we introduce semantics ( by means of named entities ) in our lexico-syntactic patterns . we evaluate and compare the number and quality of the learned patterns and the matched text segments . also , we detail the influence of the patterns in the generation of natural language questions .

another evaluation of anaphora resolution algorithms and a comparison with getaruns knowledge rich approach aldo piccolino boniforti , sara tonelli
in this paper we will present an evaluation of current state-of-the-art algorithms for anaphora resolution based on a segment of susanne corpus ( itself a portion of brown corpus ) , a much more comparable text type to what is usually required at an international level for s u c h a p p l i c a t i o n d o m a i n s a s question/answering , information extraction , text understanding , language learning . the portion of text chosen has an adequate size which lends itself to significant statistical measurements : it is portion a , counting 35,000 tokens and some 1000 third person pronominal expressions . the algorithms will then be compared to our system , getaruns , which incorporates an ar algorithm at the end of a pipeline of interconnected modules that instantiate standard architectures for nlp . fmeasure values reached by our system are significantly higher ( 75 % ) than the other ones .

generating fine-grained reviews of songs from album reviews swati tata and barbara di eugenio
music recommendation systems often recommend individual songs , as opposed to entire albums . the challenge is to generate reviews for each song , since only full album reviews are available on-line . we developed a summarizer that combines information extraction and generation techniques to produce summaries of reviews of individual songs . we present an intrinsic evaluation of the extraction components , and of the informativeness of the summaries ; and a user study of the impact of the song review summaries on users decision making processes . users were able to make quicker and more informed decisions when presented with the summary as compared to the full album review .

fast full parsing by linear-chain conditional random fields yoshimasa tsuruoka junichi tsujii sophia ananiadou
this paper presents a chunking-based discriminative approach to full parsing . we convert the task of full parsing into a series of chunking tasks and apply a conditional random field ( crf ) model to each level of chunking . the probability of an entire parse tree is computed as the product of the probabilities of individual chunking results . the parsing is performed in a bottom-up manner and the best derivation is efficiently obtained by using a depthfirst search algorithm . experimental results demonstrate that this simple parsing framework produces a fast and reasonably accurate parser .

unsupervised discovery of discourse relations for eliminating intra-sentence polarity ambiguities ministry of education , china
polarity classification of opinionated sentences with both positive and negative sentiments1 is a key challenge in sentiment analysis . this paper presents a novel unsupervised method for discovering intra-sentence level discourse relations for eliminating polarity ambiguities . firstly , a discourse scheme with discourse constraints on polarity was defined empirically based on rhetorical structure theory ( rst ) . then , a small set of cuephrase-based patterns were utilized to collect a large number of discourse instances which were later converted to semantic sequential representations ( ssrs ) . finally , an unsupervised method was adopted to generate , weigh and filter new ssrs without cue phrases for recognizing discourse relations . experimental results showed that the proposed methods not only effectively recognized the defined discourse relations but also achieved significant improvement by integrating discourse information in sentence-level polarity classification .

question answering using constraint satisfaction :
qa-by-dossier-with-constraints is a new approach to question answering whereby candidate answers confidences are adjusted by asking auxiliary questions whose answers constrain the original answers . these constraints emerge naturally from the domain of interest , and enable application of real-world knowledge to qa . we show that our approach significantly improves system performance ( 75 % relative improvement in f-measure on select question types ) and can create a dossier of information about the subject matter in the original question .

domain adaptation with latent semantic association for named entity recognition
domain adaptation is an important problem in named entity recognition ( ner ) . ner classifiers usually lose accuracy in the domain transfer due to the different data distribution between the source and the target domains . the major reason for performance degrading is that each entity type often has lots of domainspecific term representations in the different domains . the existing approaches usually need an amount of labeled target domain data for tuning the original model . however , it is a labor-intensive and time-consuming task to build annotated training data set for every target domain . we present a domain adaptation method with latent semantic association ( lasa ) . this method effectively overcomes the data distribution difference without leveraging any labeled target domain data . lasa model is constructed to capture latent semantic association among words from the unlabeled corpus . it groups words into a set of concepts according to the related context snippets . in the domain transfer , the original term spaces of both domains are projected to a concept space using lasa model at first , then the original ner model is tuned based on the semantic association features .

using web queries for learner error detection
we investigate the use of web search queries for detecting errors in non-native writing . distinguishing a correct sequence of words from a sequence with a learner error is a baseline task that any error detection and correction system needs to address . using a large corpus of error-annotated learner data , we investigate whether web search result counts can be used to distinguish correct from incorrect usage . in this investigation , we compare a variety of query formulation strategies and a number of web resources , including two major search engine apis and a large web-based n-gram corpus .

toward better chinese word segmentation for smt via bilingual
this study investigates on building a better chinese word segmentation model for statistical machine translation . it aims at leveraging word boundary information , automatically learned by bilingual character-based alignments , to induce a preferable segmentation model . we propose dealing with the induced word boundaries as soft constraints to bias the continuous learning of a supervised crfs model , trained by the treebank data ( labeled ) , on the bilingual data ( unlabeled ) . the induced word boundary information is encoded as a graph propagation constraint . the constrained model induction is accomplished by using posterior regularization algorithm . the experiments on a chinese-to-english machine translation task reveal that the proposed model can bring positive segmentation effects to translation quality .

training a perceptron with global and local features for chinese word segmentation
this paper proposes the use of global features for chinese word segmentation . these global features are combined with local features using the averaged perceptron algorithm over n-best candidate word segmentations . the n-best candidates are produced using a conditional random field ( crf ) character-based tagger for word segmentation . our experiments show that by adding global features , performance is significantly improved compared to the character-based crf tagger . performance is also improved compared to using only local features . our system obtains an f-score of 0.9355 on the cityu corpus , 0.9263 on the ckip corpus , 0.9512 on the sxu corpus , 0.9296 on the ncc corpus and 0.9501 on the ctb corpus . all results are for the closed track in the fourth sighan chinese word segmentation bakeoff .

orthographic disambiguation incorporating transliterated probability eiji aramaki takeshi imai kengo miyo kazuhiko ohe
orthographic variance is a fundamental problem for many natural language processing applications . the japanese language , in particular , contains many orthographic variants for two main reasons : ( 1 ) transliterated words allow many possible spelling variations , and ( 2 ) many characters in japanese nouns can be omitted or substituted . previous studies have mainly focused on the former problem ; in contrast , this study has addressed both problems using the same framework . first , we automatically collected both positive examples ( sets of equivalent term pairs ) and negative examples ( sets of inequivalent term pairs ) . then , by using both sets of examples , a support vector machine based classifier determined whether two terms ( t1 and t2 ) were equivalent . to boost accuracy , we added a transliterated probability p ( t1|s ) p ( t2|s ) , which is the probability that both terms ( t1 and t2 ) were transliterated from the same source term ( s ) , to the machine learning features . experimental results yielded high levels of accuracy , demonstrating the feasibility of the proposed approach .

semantic pattern learning through maximum entropy-based wsd
this paper describes a natural language learning method that extracts knowledge in the form of semantic patterns with ontology elements associated to syntactic components in the text . the method combines the use of eurowordnets ontological concepts and the correct sense of each word assigned by a word sense disambiguation ( wsd ) module to extract three sets of patterns : subject-verb , verb-direct object and verb-indirect object . these sets define the semantic behaviour of the main textual elements based on their syntactic role . on the one hand , it is shown that maximum entropy models applied to wsd tasks provide good results . the evaluation of the wsd module has revealed a accuracy rate of 64 % in a preliminary test . on the other hand , we explain how an adequate set of semantic or ontological patterns can improve the success rate of nlp tasks such us pronoun resolution . we have implemented both modules in c++ and although the evaluation has been performed for english , their general features allow the treatment of other languages like spanish . this paper has been partially supported by the spanish government ( cicyt ) project number tic2000-0664-c0202 .

a structured vector space model for hidden attribute meaning in adjective-noun phrases
we present an approach to model hidden attributes in the compositional semantics of adjective-noun phrases in a distributional model . for the representation of adjective meanings , we reformulate the pattern-based approach for attribute learning of almuhareb ( 2006 ) in a structured vector space model ( vsm ) . this model is complemented by a structured vector space representing attribute dimensions of noun meanings . the combination of these representations along the lines of compositional semantic principles exposes the underlying semantic relations in adjective-noun phrases . we show that our compositional vsm outperforms simple pattern-based approaches by circumventing their inherent sparsity problems .

a provably correct learning algorithm for latent-variable pcfgs
we introduce a provably correct learning algorithm for latent-variable pcfgs . the algorithm relies on two steps : first , the use of a matrix-decomposition algorithm applied to a co-occurrence matrix estimated from the parse trees in a training sample ; second , the use of em applied to a convex objective derived from the training samples in combination with the output from the matrix decomposition . experiments on parsing and a language modeling problem show that the algorithm is efficient and effective in practice .

assembling the kazakh language corpus
this paper presents the kazakh language corpus ( klc ) , which is one of the first attempts made within a local research community to assemble a kazakh corpus . klc is designed to be a large scale corpus containing over 135 million words and conveying five stylistic genres : literary , publicistic , official , scientific and informal . along with its primary part klc comprises such parts as : ( i ) annotated sub-corpus , containing segmented documents encoded in the extensible markup language ( xml ) that marks complete morphological , syntactic , and structural characteristics of texts ; ( ii ) as well as a sub-corpus with the annotated speech data . klc has a web-based corpus management system that helps to navigate the data and retrieve necessary information . klc is also open for contributors , who are willing to make suggestions , donate texts and help with annotation of existing materials .

statistical models for frame-semantic parsing
we present a brief history and overview of statistical methods in frame-semantic parsing the automatic analysis of text using the theory of frame semantics . we discuss how the framenet lexicon and frameannotated datasets have been used by statistical nlp researchers to build usable , state-of-the-art systems . we also focus on future directions in frame-semantic parsing research , and discuss nlp applications that could benefit from this line of work .

swiss-chocolate : sentiment detection using sparse svms and part-of-speech n-grams
we describe a classifier to predict the message-level sentiment of english microblog messages from twitter . this paper describes the classifier submitted to the semeval-2014 competition ( task 9b ) . our approach was to build up on the system of the last years winning approach by nrc canada 2013 , with some modifications and additions of features , and additional sentiment lexicons . furthermore , we used a sparse ( ` 1 -regularized ) svm , instead of the more commonly used ` 2 -regularization , resulting in a very sparse linear classifier .

entire relaxation path for maximum entropy problems
we discuss and analyze the problem of finding a distribution that minimizes the relative entropy to a prior distribution while satisfying max-norm constraints with respect to an observed distribution . this setting generalizes the classical maximum entropy problems as it relaxes the standard constraints on the observed values . we tackle the problem by introducing a re-parametrization in which the unknown distribution is distilled to a single scalar . we then describe a homotopy between the relaxation parameter and the distribution characterizing parameter . the homotopy also reveals an aesthetic symmetry between the prior distribution and the observed distribution . we then use the reformulated problem to describe a space and time efficient algorithm for tracking the entire relaxation path . our derivations are based on a compact geometric view of the relaxation path as a piecewise linear function in a two dimensional space of the relaxation-characterization parameters . we demonstrate the usability of our approach by applying the problem to zipfian distributions over a large alphabet .

non-compositional language model and pattern dictionary development for japanese compound and complex sentences
to realize high quality machine translation , we proposed a non-compositional language model , and developed a sentence pattern dictionary of 226,800 pattern pairs for japanese compound and complex sentences consisting of 2 or 3 clauses . in pattern generation from a parallel corpus , compositional constituents that could be generalized were 74 % of independent words , 24 % of phrases and only 15 % of clauses . this means that in japanese-to-english mt , most of the translation results as shown in the parallel corpus could not be obtained by methods based on compositional semantics . this dictionary achieved a syntactic coverage of 98 % and a semantic coverage of 78 % . it will substantially improve translation quality .

document re-ranking based on automatically acquired key terms in chinese information retrieval
for information retrieval , users are more concerned about the precision of top ranking documents in most practical situations . in this paper , we propose a method to improve the precision of top n ranking documents by reordering the retrieved documents from the initial retrieval . to reorder documents , we first automatically extract global key terms from document set , then use extracted global key terms to identify local key terms in a single document or query topic , finally we make use of local key terms in query and documents to reorder the initial ranking documents . the experiment with ntcir3 clir dataset shows that an average 10 % -11 % improvement and 2 % -5 % improvement in precision can be achieved at top 10 and 100 ranking documents level respectively .

the hindi discourse relation bank
we describe the hindi discourse relation bank project , aimed at developing a large corpus annotated with discourse relations . we adopt the lexically grounded approach of the penn discourse treebank , and describe our classification of hindi discourse connectives , our modifications to the sense classification of discourse relations , and some crosslinguistic comparisons based on some initial annotations carried out so far .

lookahead in deterministic left-corner parsing
to support incremental interpretation , any model of human sentence processing must not only process the sentence incrementally , it must to some degree restrict the number of analyses which it produces for any sentence prefix . deterministic parsing takes the extreme position that there can only be one analysis for any sentence prefix . experiments with an incremental statistical parser show that performance is severely degraded when the search for the most probable parse is pruned to only the most probable analysis after each prefix . one method which has been extensively used to address the difficulty of deterministic parsing is lookahead , where information about a bounded number of subsequent words is used to decide which analyses to pursue . we simulate the effects of lookahead by summing probabilities over possible parses for the lookahead words and using this sum to choose which parse to pursue . we find that a large improvement is achieved with one word lookahead , but that more lookahead results in relatively small additional improvements . this suggests that one word lookahead is sufficient , but that other modifications to our left-corner parsing model could make deterministic parsing more effective .

grawltcq : terminology and corpora building by ranking simultaneously terms , queries and documents using graph random walks
in this paper , we present grawltcq , a new bootstrapping algorithm for building specialized terminology , corpora and queries , based on a graph model . we model links between documents , terms and queries , and use a random walk with restart algorithm to compute relevance propagation . we have evaluated grawltcq on an afp english corpus of 57,441 news over 10 categories . for corpora building , grawltcq outperforms the bootcat tool , which is vastly used in the domain . for 1,000 documents retrieved , we improve mean precision by 25 % . grawltcq has also shown to be faster and more robust than bootcat over iterations .

chinese named entity and relation identification system
in this interactive presentation , a chinese named entity and relation identification system is demonstrated . the domainspecific system has a three-stage pipeline architecture which includes word segmentation and part-of-speech ( pos ) tagging , named entity recognition , and named entity relation identitfication . the experimental results have shown that the average f-measure for word segmentation and pos tagging after correcting errors achieves 92.86 and 90.01 separately . moreover , the overall average f-measure for 6 kinds of name entities and 14 kinds of named entity relations is 83.08 % and 70.46 % respectively .

a prototype text to british sign language ( bsl ) translation system
we demonstrate a text to sign language translation system for investigating sign language ( sl ) structure and assisting in production of sign narratives and informative presentations1 . the system is demonstrable on a conventional pc laptop computer .

the peoples web meets linguistic knowledge : automatic sense alignment of wikipedia and wordnet
we propose a method to automatically alignwordnet synsets andwikipedia articles to obtain a sense inventory of higher coverage and quality . for eachwordnet synset , we first extract a set of wikipedia articles as alignment candidates ; in a second step , we determine which article ( if any ) is a valid alignment , i.e . is about the same sense or concept . in this paper , we go significantly beyond stateof-the-art word overlap approaches , and apply a threshold-based personalized pagerank method for the disambiguation step . we show that wordnet synsets can be aligned to wikipedia articles with a performance of up to 0.78 f1-measure based on a comprehensive , well-balanced reference dataset consisting of 1,815 manually annotated sense alignment candidates . the fully-aligned resource as well as the reference dataset is publicly available.1

morpho-syntactic information for automatic error analysis of statistical machine translation output itc-irst , centro per la ricerca scientifica e tecnologica , trento , italy
evaluation of machine translation output is an important but difficult task . over the last years , a variety of automatic evaluation measures have been studied , some of them like word error rate ( wer ) , position independent word error rate ( per ) and bleu and nist scores have become widely used tools for comparing different systems as well as for evaluating improvements within one system . however , these measures do not give any details about the nature of translation errors . therefore some analysis of the generated output is needed in order to identify the main problems and to focus the research efforts . on the other hand , human evaluation is a time consuming and expensive task . in this paper , we investigate methods for using of morpho-syntactic information for automatic evaluation : standard error measures wer and per are calculated on distinct word classes and forms in order to get a better idea about the nature of translation errors and possibilities for improvements .

are : instance splitting strategies for dependency relation-based mstislav maslennikov hai-kiat goh tat-seng chua
information extraction ( ie ) is a fundamental technology for nlp . previous methods for ie were relying on co-occurrence relations , soft patterns and properties of the target ( for example , syntactic role ) , which result in problems of handling paraphrasing and alignment of instances . our system are ( anchor and relation ) is based on the dependency relation model and tackles these problems by unifying entities according to their dependency relations , which we found to provide more invariant relations between entities in many cases . in order to exploit the complexity and characteristics of relation paths , we further classify the relation paths into the categories of easy , average and hard , and utilize different extraction strategies based on the characteristics of those categories . our extraction method leads to improvement in performance by 3 % and 6 % for muc4 and muc6 respectively as compared to the state-of-art ie systems .

investigating a generic paraphrase-based approach for relation extraction
unsupervised paraphrase acquisition has been an active research field in recent years , but its effective coverage and performance have rarely been evaluated . we propose a generic paraphrase-based approach for relation extraction ( re ) , aiming at a dual goal : obtaining an applicative evaluation scheme for paraphrase acquisition and obtaining a generic and largely unsupervised configuration for re.we analyze the potential of our approach and evaluate an implemented prototype of it using an re dataset . our findings reveal a high potential for unsupervised paraphrase acquisition . we also identify the need for novel robust models for matching paraphrases in texts , which should address syntactic complexity and variability .

integrating high precision rules with statistical sequence classifiers for
integrating rules and statistical systems is a challenge often faced by natural language processing system builders . a common subclass is integrating high precision rules with a markov statistical sequence classifier . in this paper we suggest that using such rules to constrain the sequence classifier decoder results in superior accuracy and efficiency . in a case study of a named entity tagging system , we provide evidence that this method of combination does prove efficient than other methods . the accuracy was the same .

generalized stack decoding algorithms for statistical machine translation
in this paper we propose a generalization of the stack-based decoding paradigm for statistical machine translation . the well known single and multi-stack decoding algorithms defined in the literature have been integrated within a new formalism which also defines a new family of stackbased decoders . these decoders allows a tradeoff to be made between the advantages of using only one or multiple stacks . the key point of the new formalism consists in parameterizeing the number of stacks to be used during the decoding process , and providing an efficient method to decide in which stack each partial hypothesis generated is to be insertedduring the search process . experimental results are also reported for a search algorithm for phrase-based statistical translation models .

the stages of event extraction
event detection and recognition is a complex task consisting of multiple sub-tasks of varying difficulty . in this paper , we present a simple , modular approach to event extraction that allows us to experiment with a variety of machine learning methods for these sub-tasks , as well as to evaluate the impact on performance these sub-tasks have on the overall task .

predicting risk from financial reports with regression
we address a text regression problem : given a piece of text , predict a real-world continuous quantity associated with the texts meaning . in this work , the text is an sec-mandated financial report published annually by a publiclytraded company , and the quantity to be predicted is volatility of stock returns , an empirical measure of financial risk . we apply wellknown regression techniques to a large corpus of freely available financial reports , constructing regression models of volatility for the period following a report . our models rival past volatility ( a strong baseline ) in predicting the target variable , and a single model that uses both can significantly outperform past volatility . interestingly , our approach is more accurate for reports after the passage of the sarbanes-oxley act of 2002 , giving some evidence for the success of that legislation in making financial reports more informative .

unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition
this paper describes a novel character tagging approach to chinese word segmentation and named entity recognition ( ner ) for our participation in bakeoff-4.1 it integrates unsupervised segmentation and conditional random fields ( crfs ) learning successfully , using similar character tags and feature templates for both word segmentation and ner . it ranks at the top in all closed tests of word segmentation and gives promising results for all closed and open ner tasks in the bakeoff . tag set selection and unsupervised segmentation play a critical role in this success .

trajectory based word sense disambiguation
classifier combination is a promising way to improve performance of word sense disambiguation . we propose a new combinational method in this paper . we first construct a series of nave bayesian classifiers along a sequence of orderly varying sized windows of context , and perform sense selection for both training samples and test samples using these classifiers . we thus get a sense selection trajectory along the sequence of context windows for each sample . then we make use of these trajectories to make final k-nearest-neighbors-based sense selection for test samples . this method aims to lower the uncertainty brought by classifiers using different context windows and make more robust utilization of context while perform well . experiments show that our approach outperforms some other algorithms on both robustness and performance .

random-walk term weighting for improved text classification
this paper describes a new approach for estimating term weights in a text classification task . the approach uses term cooccurrence as a measure of dependency between word features . a random walk model is applied on a graph encoding words and co-occurrence dependencies , resulting in scores that represent a quantification of how a particular word feature contributes to a given context . we argue that by modeling feature weights using these scores , as opposed to the traditional frequency-based scores , we can achieve better results in a text classification task . experiments performed on four standard classification datasets show that the new random-walk based approach outperforms the traditional term frequency approach to feature weighting .

annotating semantic consistency of speech recognition hypotheses
recent work on natural language processing systems is aimed at more conversational , context-adaptive systems in multiple domains . an important requirement for such a system is the automatic detection of the domain and a domain consistency check of the given speech recognition hypotheses . we report a pilot study addressing these tasks , the underlying data collection and investigate the feasibility of annotating the data reliably by human annotators .

using smart bilingual projection to feature-tag a monolingual dictionary
we describe an approach to tagging a monolingual dictionary with linguistic features . in particular , we annotate the dictionary entries with parts of speech , number , and tense information . the algorithm uses a bilingual corpus as well as a statistical lexicon to find candidate training examples for specific feature values ( e.g . plural ) . then a similarity measure in the space defined by the training data serves to define a classifier for unseen data . we report evaluation results for a french dictionary , while the approach is general enough to be applied to any language pair . in a further step , we show that the proposed framework can be used to assign linguistic roles to extracted morphemes , e.g . noun plural markers . while the morphemes can be extracted using any algorithm , we present a simple algorithm for doing so . the emphasis hereby is not on the algorithm itself , but on the power of the framework to assign roles , which are ultimately indispensable for tasks such as machine translation .

adaptation using out-of-domain corpus within ebmt takao doi , eiichiro sumita , hirofumi yamamoto
in order to boost the translation quality of ebmt based on a small-sized bilingual corpus , we use an out-of-domain bilingual corpus and , in addition , the language model of an indomain monolingual corpus . we conducted experiments with an ebmt system . the two evaluation measures of the bleu score and the nist score demonstrated the effect of using an out-of-domain bilingual corpus and the possibility of using the language model .

evaluation of string distance algorithms for dialectology
we examine various string distance measures for suitability in modeling dialect distance , especially its perception . we find measures superior which do not normalize for word length , but which are are sensitive to order . we likewise find evidence for the superiority of measures which incorporate a sensitivity to phonological context , realized in the form of n-grams although we can not identify which form of context ( bigram , trigram , etc . ) is best . however , we find no clear benefit in using gradual as opposed to binary segmental difference when calculating sequence distances .

adjective based inference
in this paper , we propose a fine grained classification of english adjectives geared at modeling the distinct inference patterns licensed by each adjective class . we show how it can be implemented in description logic and illustrate the predictions made by a series of examples . the proposal has been implemented using description logic as a semantic representation language and the prediction verified using the dl theorem prover racer . topics : textual entailment , adjectival semantics

ecnu : a combination method and multiple features for aspect extraction and sentiment polarity classification
this paper reports our submissions to the four subtasks of aspect based sentiment analysis ( absa ) task ( i.e. , task 4 ) in semeval 2014 including aspect term extraction and aspect sentiment polarity classification ( aspect-level tasks ) , aspect category detection and aspect category sentiment polarity classification ( categorylevel tasks ) . for aspect term extraction , we present three methods , i.e. , noun phrase ( np ) extraction , named entity recognition ( ner ) and a combination of np and ner method . for aspect sentiment classification , we extracted several features , i.e. , topic features , sentiment lexicon features , and adopted a maximum entropy classifier . our submissions rank above average .

three bionlp tools powered by a biological lexicon
in this paper , we demonstrate three nlp applications of the biolexicon , which is a lexical resource tailored to the biology domain . the applications consist of a dictionary-based pos tagger , a syntactic parser , and query processing for biomedical information retrieval . biological terminology is a major barrier to the accurate processing of literature within biology domain . in order to address this problem , we have constructed the biolexicon using both manual and semiautomatic methods . we demonstrate the utility of the biology-oriented lexicon within three separate nlp applications .

exploiting graph structure for accelerating the calculation of shortest paths in wordnets
this paper presents an approach for substantially reducing the time needed to calculate the shortest paths between all concepts in a wordnet . the algorithm exploits the unique star-like topology of wordnets to cut down on time-expensive calculations performed by algorithms to solve the all-pairs shortest path problem in general graphs . the algorithm was applied to two wordnets of two different languages : princeton wordnet ( fellbaum , 1998 ) for english , and germanet ( kunze and lemnitzer , 2002 ) , the german language wordnet . for both wordnets , the time needed for finding all shortest paths was brought down from several days to a matter of minutes .

probabilistic context-free grammars
we investigate the problem of training probabilistic context-free grammars on the basis of a distribution defined over an infinite set of trees , by minimizing the cross-entropy . this problem can be seen as a generalization of the well-known maximum likelihood estimator on ( finite ) tree banks . we prove an unexpected theoretical property of grammars that are trained in this way , namely , we show that the derivational entropy of the grammar takes the same value as the crossentropy between the input distribution and the grammar itself . we show that the result also holds for the widely applied maximum likelihood estimator on tree banks .

crystal : analyzing predictive opinions on the web
in this paper , we present an election prediction system ( crystal ) based on web users opinions posted on an election prediction website . given a prediction message , crystal first identifies which party the message predicts to win and then aggregates prediction analysis results of a large amount of opinions to project the election results . we collect past election prediction messages from the web and automatically build a gold standard . we focus on capturing lexical patterns that people frequently use when they express their predictive opinions about a coming election . to predict election results , we apply svm-based supervised learning . to improve performance , we propose a novel technique which generalizes n-gram feature patterns . experimental results show that crystal significantly outperforms several baselines as well as a non-generalized n-gram approach . crystal predicts future elections with 81.68 % accuracy .

the lie detector : explorations in the automatic recognition of deceptive language
in this paper , we present initial experiments in the recognition of deceptive language . we introduce three data sets of true and lying texts collected for this purpose , and we show that automatic classification is a viable technique to distinguish between truth and falsehood as expressed in language . we also introduce a method for class-based feature analysis , which sheds some light on the features that are characteristic for deceptive text . you should not trust the devil , even if he tells the truth . thomas of aquin ( medieval philosopher )

digraph analysis of dictionary preposition definitions
we develop a model of preposition definitions in a machine-readable dictionary using the theory of labeled directed graphs and analyze the resulting digraphs to determine a primitive set of preposition senses . we characterize these primitives and show how they can be used to develop an inheritance hierarchy for prepositions , representing the definitions by a type and slots for its arguments . by analyzing the definitions , we develop criteria for disambiguating among the highly polysemous primitives . we show how these criteria can be used in developing the inheritance hierarchy and how they may be used in assigning theta roles to the objects of transitive verbs . finally , we describe the use of the disambiguation criteria to parse and represent the meaning of the prepositions as used in encyclopedia articles .

improving probabilistic latent semantic analysis with principal component analysis
probabilistic latent semantic analysis ( plsa ) models have been shown to provide a better model for capturing polysemy and synonymy than latent semantic analysis ( lsa ) . however , the parameters of a plsa model are trained using the expectation maximization ( em ) algorithm , and as a result , the trained model is dependent on the initialization values so that performance can be highly variable . in this paper we present a method for using lsa analysis to initialize a plsa model . we also investigated the performance of our method for the tasks of text segmentation and retrieval on personal-size corpora , and present results demonstrating the efficacy of our proposed approach .

learning hierarchical translation structure with linguistic annotations
while it is generally accepted that many translation phenomena are correlated with linguistic structures , employing linguistic syntax for translation has proven a highly non-trivial task . the key assumption behind many approaches is that translation is guided by the source and/or target language parse , employing rules extracted from the parse tree or performing tree transformations . these approaches enforce strict constraints and might overlook important translation phenomena that cross linguistic constituents . we propose a novel flexible modelling approach to introduce linguistic information of varying granularity from the source side . our method induces joint probability synchronous grammars and estimates their parameters , by selecting and weighing together linguistically motivated rules according to an objective function directly targeting generalisation over future data . we obtain statistically significant improvements across 4 different language pairs with english as source , mounting up to +1.92 bleu for chinese as target .

align , disambiguate and walk : a unified approach for measuring semantic similarity
semantic similarity is an essential component of many natural language processing applications . however , prior methods for computing semantic similarity often operate at different levels , e.g. , single words or entire documents , which requires adapting the method for each data type . we present a unified approach to semantic similarity that operates at multiple levels , all the way from comparing word senses to comparing text documents . our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data . this unified representation shows state-ofthe-art performance on three tasks : semantic textual similarity , word similarity , and word sense coarsening .

semantic similarity for detecting recognition errors in automatic diana inkpen alain dsilets
browsing through large volumes of spoken audio is known to be a challenging task for end users . one way to alleviate this problem is to allow users to gist a spoken audio document by glancing over a transcript generated through automatic speech recognition . unfortunately , such transcripts typically contain many recognition errors which are highly distracting and make gisting more difficult . in this paper we present an approach that detects recognition errors by identifying words which are semantic outliers with respect to other words in the transcript . we describe several variants of this approach . we investigate a wide range of evaluation measures and we show that we can significantly reduce the number of errors in content words , with the trade-off of losing some good content words .

to generate natural language descriptions of videos in the wild
this paper integrates techniques in natural language processing and computer vision to improve recognition and description of entities and activities in real-world videos . we propose a strategy for generating textual descriptions of videos by using a factor graph to combine visual detections with language statistics . we use state-of-the-art visual recognition systems to obtain confidences on entities , activities , and scenes present in the video . our factor graph model combines these detection confidences with probabilistic knowledge mined from text corpora to estimate the most likely subject , verb , object , and place . results on youtube videos show that our approach improves both the joint detection of these latent , diverse sentence components and the detection of some individual components when compared to using the vision system alone , as well as over a previous n-gram language-modeling approach . the joint detection allows us to automatically generate more accurate , richer sentential descriptions of videos with a wide array of possible content .

exploring the use of word embeddings and random walks on wikipedia for the cogalex shared task
in our participation on the task we wanted to test three different kinds of relatedness algorithms : one based on embeddings induced from corpora , another based on random walks on wordnet and a last one based on random walks based on wikipedia . all three of them perform similarly in noun relatedness datasets like wordsim353 , close to the highest reported values . although the task definition gave examples of nouns , the train and test data were based on the edinburgh association thesaurus , and around 50 % of the target words were not nouns . the corpus-based algorithm performed much better than the other methods in the training dataset , and was thus submitted for the test .

whats in a preposition dimensions of sense disambiguation for an interesting word class
choosing the right parameters for a word sense disambiguation task is critical to the success of the experiments . we explore this idea for prepositions , an often overlooked word class . we examine the parameters that must be considered in preposition disambiguation , namely context , features , and granularity . doing so delivers an increased performance that significantly improves over two state-ofthe-art systems , and shows potential for improving other word sense disambiguation tasks . we report accuracies of 91.8 % and 84.8 % for coarse and fine-grained preposition sense disambiguation , respectively .

teaching a weaker classifier : named entity recognition on upper case text hai leong chieu hwee tou ng
this paper describes how a machinelearning named entity recognizer ( ner ) on upper case text can be improved by using a mixed case ner and some unlabeled text . the mixed case ner can be used to tag some unlabeled mixed case text , which are then used as additional training material for the upper case ner . we show that this approach reduces the performance gap between the mixed case ner and the upper case ner substantially , by 39 % for muc-6 and 22 % for muc-7 named entity test data . our method is thus useful in improving the accuracy of ners on upper case text , such as transcribed text from automatic speech recognizers where case information is missing .

by all these lovely tokens merging conflicting tokenizations
given the contemporary trend to modular nlp architectures and multiple annotation frameworks , the existence of concurrent tokenizations of the same text represents a pervasive problem in everydays nlp practice and poses a non-trivial theoretical problem to the integration of linguistic annotations and their interpretability in general . this paper describes a solution for integrating different tokenizations using a standoff xml format , and discusses the consequences for the handling of queries on annotated corpora .

modeling word segmentation
this paper describes a computational model of word segmentation and presents simulation results on realistic acquisition in particular , we explore the capacity and limitations of statistical learning mechanisms that have recently gained prominence in cognitive psychology and linguistics .

slavonic information extraction and partial parsing insitute of computer science
information extraction ( ie ) often involves some amount of partial syntactic processing . this is clear in cases of interesting highlevel ie tasks , such as finding information about who did what to whom ( when , where , how and why ) , but it is also true in case of simpler ie tasks , such as finding company names in texts . the aim of this paper is to give an overview of slavonic phenomena which pose particular problems for ie and partial parsing , and some phenomena which seem easier to treat in slavonic than in germanic or romance ; i also mention various tools which have been used for the partial processing of slavonic .

predicting the micro-timing of user input for an incremental spoken dialogue system that completes a users ongoing turn
we present the novel task of predicting temporal features of continuations of user input , while that input is still ongoing . we show that the remaining duration of an ongoing word , as well as the duration of the next can be predicted reasonably well , and we put this information to use in a system that synchronously completes a users speech . while we focus on collaborative completions , the techniques presented here may also be useful for the alignment of back-channels and immediate turn-taking in an incremental sds , or to synchronously monitor the users speech fluency for other reasons .

testing cladistics on dialect networks and phyla nouvelle parisiii epidmiologie gntique et structure des populations
this present work deliberately abandons the purpose of capturing the global resemblance between languages and the ambition of giving a rational foundation to probability of changes in linguistics , to focus instead on cladistic approach , which was applied to different dialects and data ( gallo-romance , southern italo-romance ) through an original coding of philological derivations . results show good congruence with linguistic classification and provide new insight on how tackle various dialectological problems as borrowings .

a generic approach to parallel chart parsing with an application to lingo
multi-processor systems are becoming more commonplace and affordable . based on analyses of actual parsings , we argue that to exploit the capabilities of such machines , unification-based grammar parsers should distribute work at the level of individual unification operations . we present a generic approach to parallel chart parsing that meets this requirement , and show that an implementation of this technique for lingo achieves considerable speedups .

learning pronunciation dictionaries language complexity and word selection strategies
the speed with which pronunciation dictionaries can be bootstrapped depends on the efficiency of learning algorithms and on the ordering of words presented to the user . this paper presents an active-learning word selection strategy that is mindful of human limitations . learning rates approach that of an oracle system that knows the final lts rule set .

learning information status of discourse entities
in this paper we address the issue of automatically assigning information status to discourse entities . using an annotated corpus of conversational english and exploiting morpho-syntactic and lexical features , we train a decision tree to classify entities introduced by noun phrases as old , mediated , or new . we compare its performance with hand-crafted rules that are mainly based on morpho-syntactic features and closely relate to the guidelines that had been used for the manual annotation . the decision tree model achieves an overall accuracy of 79.5 % , significantly outperforming the hand-crafted algorithm ( 64.4 % ) . we also experiment with binary classifications by collapsing in turn two of the three target classes into one and retraining the model . the highest accuracy achieved on binary classification is 93.1 % .

multi-domain spoken dialogue system with extensibility and robustness against speech recognition errors
we developed a multi-domain spoken dialogue system that can handle user requests across multiple domains . such systems need to satisfy two requirements : extensibility and robustness against speech recognition errors . extensibility is required to allow for the modification and addition of domains independent of other domains . robustness against speech recognition errors is required because such errors are inevitable in speech recognition . however , the systems should still behave appropriately , even when their inputs are erroneous . our system was constructed on an extensible architecture and is equipped with a robust and extensible domain selection method . domain selection was based on three choices : ( i ) the previous domain , ( ii ) the domain in which the speech recognition result can be accepted with the highest recognition score , and ( iii ) other domains . with the third choice we newly introduced , our system can prevent dialogues from continuously being stuck in an erroneous domain . our experimental results , obtained with 10 subjects , showed that our method reduced the domain selection errors by 18.3 % , compared to a conventional method .

urdu and hindi : translation and sharing of linguistic resources
hindi and urdu share a common phonology , morphology and grammar but are written in different scripts . in addition , the vocabularies have also diverged significantly especially in the written form . in this paper we show that we can get reasonable quality translations ( we estimated the translation error rate at 18 % ) between the two languages even in absence of a parallel corpus . linguistic resources such as treebanks , part of speech tagged data and parallel corpora with english are limited for both these languages . we use the translation system to share linguistic resources between the two languages . we demonstrate improvements on three tasks and show : statistical machine translation from urdu to english is improved ( 0.8 in bleu score ) by using a hindi-english parallel corpus , hindi part of speech tagging is improved ( upto 6 % absolute ) by using an urdu part of speech corpus and a hindi-english word aligner is improved by using a manually word aligned urduenglish corpus ( upto 9 % absolute in fmeasure ) .

the uppsala - fbk systems at wmt 2011
this paper presents our submissions to the shared translation task at wmt 2011. we created two largely independent systems for english-to-french and haitian creole-toenglish translation to evaluate different features and components from our ongoing research on these language pairs . key features of our systems include anaphora resolution , hierarchical lexical reordering , data selection for language modelling , linear transduction grammars for word alignment and syntaxbased decoding with monolingual dependency information .

a language-independent approach to
we present likey , a language-independent keyphrase extraction method based on statistical analysis and the use of a reference corpus . likey has a very light-weight preprocessing phase and no parameters to be tuned . thus , it is not restricted to any single language or language family . we test likey having exactly the same configuration with 11 european languages . furthermore , we present an automatic evaluation method based on wikipedia intra-linking .

exploiting language models for visual recognition
the problem of learning language models from large text corpora has been widely studied within the computational linguistic community . however , little is known about the performance of these language models when applied to the computer vision domain . in this work , we compare representative models : a window-based model , a topic model , a distributional memory and a commonsense knowledge database , conceptnet , in two visual recognition scenarios : human action recognition and object prediction . we examine whether the knowledge extracted from texts through these models are compatible to the knowledge represented in images . we determine the usefulness of different language models in aiding the two visual recognition tasks . the study shows that the language models built from general text corpora can be used instead of expensive annotated images and even outperform the image model when testing on a big general dataset .

context inducing nouns
it is important to identify complementtaking nouns in order to properly analyze the grammatical and implicative structure of the sentence . this paper examines the ways in which these nouns were identified and classified for addition to the bridge natural language understanding system .

measuring and predicting orthographic associations : modelling the similarity of japanese kanji
as human beings , our mental processes for recognising linguistic symbols generate perceptual neighbourhoods around such symbols where confusion errors occur . such neighbourhoods also provide us with conscious mental associations between symbols . this paper formalises orthographic models for similarity of japanese kanji , and provides a proofof-concept dictionary extension leveraging the mental associations provided by orthographic proximity .

comparing several aspects of human-computer and the mitre corporation
while researchers have many intuitions about the differences between humancomputer and human-human interactions , most of these have not previously been subject to empirical scrutiny . this work presents some initial experiments in this direction , with the ultimate goal being to use what we learn to improve computer dialogue systems . working with data from the air travel domain , we identified a number of striking differences between the human-human and human-computer interactions .

the gate crowdsourcing plugin : crowdsourcing annotated corpora
crowdsourcing is an increasingly popular , collaborative approach for acquiring annotated corpora . despite this , reuse of corpus conversion tools and user interfaces between projects is still problematic , since these are not generally made available . this demonstration will introduce the new , open-source gate crowdsourcing plugin , which offers infrastructural support for mapping documents to crowdsourcing units and back , as well as automatically generating reusable crowdsourcing interfaces for nlp classification and selection tasks . the entire workflow will be demonstrated on : annotating named entities ; disambiguating words and named entities with respect to dbpedia uris ; annotation of opinion holders and targets ; and sentiment .

extracting and classifying urdu multiword expressions
this paper describes a method for automatically extracting and classifying multiword expressions ( mwes ) for urdu on the basis of a relatively small unannotated corpus ( around 8.12 million tokens ) . the mwes are extracted by an unsupervised method and classified into two distinct classes , namely locations and person names . the classification is based on simple heuristics that take the co-occurrence of mwes with distinct postpositions into account . the resulting classes are evaluated against a hand-annotated gold standard and achieve an f-score of 0.5 and 0.746 for locations and persons , respectively . a target application is the urdu pargram grammar , where mwes are needed to generate a more precise syntactic and semantic analysis .

accurate and robust lfg-based generation for chinese
we describe three pcfg-based models for chinese sentence realisation from lexicalfunctional grammar ( lfg ) f-structures . both the lexicalised model and the history-based model improve on the accuracy of a simple wide-coverage pcfg model by adding lexical and contextual information to weaken inappropriate independence assumptions implicit in the pcfg models . in addition , we provide techniques for lexical smoothing and rule smoothing to increase the generation coverage . trained on 15,663 automatically lfg fstructure annotated sentences of the penn chinese treebank and tested on 500 sentences randomly selected from the treebank test set , the lexicalised model achieves a bleu score of 0.7265 at 100 % coverage , while the historybased model achieves a bleu score of 0.7245 also at 100 % coverage .

utdmet : combining wordnet and corpus data for argument coercion detection
this paper describes our system for the classification of argument coercion for semeval-2010 task 7. we present two approaches to classifying an arguments semantic class , which is then compared to the predicates expected semantic class to detect coercions . the first approach is based on learning the members of an arbitrary semantic class using wordnets hypernymy structure . the second approach leverages automatically extracted semantic parse information from a large corpus to identify similar arguments by the predicates that select them . we show the results these approaches obtain on the task as well as how they can improve a traditional feature-based approach .

building timelines from narrative clinical records : initial results based-on deep natural language understanding
we present an end-to-end system that processes narrative clinical records , constructs timelines for the medical histories of patients , and visualizes the results . this work is motivated by real clinical records and our general approach is based on deep semantic natural language understanding .

it depends on the translation : unsupervised dependency parsing via word alignment
we reveal a previously unnoticed connection between dependency parsing and statistical machine translation ( smt ) , by formulating the dependency parsing task as a problem of word alignment . furthermore , we show that two well known models for these respective tasks ( dmv and the ibm models ) share common modeling assumptions . this motivates us to develop an alignment-based framework for unsupervised dependency parsing . the framework ( which will be made publicly available ) is flexible , modular and easy to extend . using this framework , we implement several algorithms based on the ibm alignment models , which prove surprisingly effective on the dependency parsing task , and demonstrate the potential of the alignment-based approach .

discriminative substring decoding for transliteration
we present a discriminative substring decoder for transliteration . this decoder extends recent approaches for discriminative character transduction by allowing for a list of known target-language words , an important resource for transliteration . our approach improves upon sherif and kondraks ( 2007b ) state-of-theart decoder , creating a 28.5 % relative improvement in transliteration accuracy on a japanese katakana-to-english task . we also conduct a controlled comparison of two feature paradigms for discriminative training : indicators and hybrid generative features . surprisingly , the generative hybrid outperforms its purely discriminative counterpart , despite losing access to rich source-context features . finally , we show that machine transliterations have a positive impact on machine translation quality , improving human judgments by 0.5 on a 4-point scale .

active learning for post-editing based incrementally retrained mt
machine translation , in particular statistical machine translation ( smt ) , is making big inroads into the localisation and translation industry . in typical workflows ( s ) mt output is checked and ( where required ) manually post-edited by human translators . recently , a significant amount of research has concentrated on capturing human post-editing outputs as early as possible to incrementally update/modify smt models to avoid repeat mistakes . typically in these approaches , mt and post-edits happen sequentially and chronologically , following the way unseen data ( the translation job ) is presented . in this paper , we add to the existing literature addressing the question whether and if so , to what extent , this process can be improved upon by active learning , where input is not presented chronologically but dynamically selected according to criteria that maximise performance with respect to ( whatever is ) the remaining data . we explore novel ( source side-only ) selection criteria and show performance increases of 0.67-2.65 points ter absolute on average on typical industry data sets compared to sequential pebased incrementally retrained smt .

using the web as a phonological corpus : a case study from tagalog
some languages orthographic properties allow written data to be used for phonological research . this paper reports on an on-going project that uses a web-derived text corpus to study the phonology of tagalog , a language for which large corpora are not otherwise available . novel findings concerning the phenomenon of intervocalic tapping are discussed in detail , and an overview of other phonological phenomena in the language that can be investigated through written data is given .

summarizing search results using plsi
in this paper , we investigate generating a set of query-focused summaries from search results . since there may be many topics related to a given query in the search results , in order to summarize these results , they should first be classified into topics , and then each topic should be summarized individually . in this summarization process , two types of redundancies need to be reduced . first , each topic summary should not contain any redundancy ( we refer to this problem as redundancy within a summary ) . second , a topic summary should not be similar to any other topic summary ( we refer to this problem as redundancy between summaries ) . in this paper , we focus on the document clustering process and the reduction of redundancy between summaries in the summarization process . we also propose a method using plsi to summarize search results . evaluation results confirm that our method performs well in classifying search results and reducing the redundancy between summaries .

design and implementation of a lexicon of dutch multiword expressions
this paper describes the design and implementation of a lexicon of dutch multiword expressions ( mwes ) . no exhaustive research on a standard lexical representation of mwes has been done for dutch before . the approach taken is innovative , since it is based on the equivalence class method . furthermore , the selection of the lexical entries and their properties is corpus-based . the design of the lexicon and the standard representation will be tested in dutch nlp systems . the purpose of the current paper is to give an overview of the decisions made in order to come to a standard lexical representation and to discuss the description fields this representation comprises .

a dependency treelet string correspondence model for statistical machine translation
this paper describes a novel model using dependency structures on the source side for syntax-based statistical machine translation : dependency treelet string correspondence model ( dtsc ) . the dtsc model maps source dependency structures to target strings . in this model translation pairs of source treelets and target strings with their word alignments are learned automatically from the parsed and aligned corpus . the dtsc model allows source treelets and target strings with variables so that the model can generalize to handle dependency structures with the same head word but with different modifiers and arguments . additionally , target strings can be also discontinuous by using gaps which are corresponding to the uncovered nodes which are not included in the source treelets . a chart-style decoding algorithm with two basic operations substituting and attachingis designed for the dtsc model . we argue that the dtsc model proposed here is capable of lexicalization , generalization , and handling discontinuous phrases which are very desirable for machine translation . we finally evaluate our current implementation of a simplified version of dtsc for statistical machine translation .

single cycle languages : empirical evidence for tag-adjoining
russian and polish lack unbounded syntactic dependencies that fall into the primary empirical domain of tag-adjoining , namely , long-distance movement/filler-gap dependencies across a tensed clause boundary . a theory that incorporates adjoining as a recursive structure building device provides a novel and straightforward account of this gap , whereas existing theories of syntactic locality , e.g . of the standard minimalist kind , face difficulties explaining the phenomenon . these languages thus supply direct linguistic evidence for adjoining .

hmm revises low marginal probability by crf for chinese word segmentation
this paper presents a chinese word segmentation system for cips-sighan 2010 chinese language processing task . firstly , based on conditional random field ( crf ) model , with local features and global features , the character-based tagging model is designed . secondly , hidden markov models ( hmm ) is used to revise the substrings with low marginal probability by crf . finally , confidence measure is used to regenerate the result and simple rules to deal with the strings within letters and numbers . as is well known that character-based approach has outstanding capability of discovering out-of-vocabulary ( oov ) word , but external information of word lost . hmm makes use of word information to increase in-vocabulary ( iv ) recall . we participate in the simplified chinese word segmentation both closed and open test on all four corpora , which belong to different domains . our system achieves better performance .

the importance of narrative and other lessons from an evaluation of an nlg system that summarises clinical data
the babytalk bt-45 system generates textual summaries of clinical data about babies in a neonatal intensive care unit . a recent taskbased evaluation of the system suggested that these summaries are useful , but not as effective as they could be . in this paper we present a qualitative analysis of problems that the evaluation highlighted in bt-45 texts . many of these problems are due to the fact that bt45 does not generate good narrative texts ; this is a topic which has not previously received much attention from the nlg research community , but seems to be quite important for creating good data-to-text systems .

aggregating opinions : explorations into graphs and media content
understanding , as opposed to reading is vital for the extraction of opinions out of a text . this is especially true , as an authors opinion is not always clearly marked . finding the overall opinion in a text can be challenging to both human readers and computers alike . media content analysis is a popular method of extracting information out of a text , by means of human coders . we describe the difficulties humans have and the process they use to extract opinions and offer a formalization that could help to automate opinion extraction within the media content analysis framework .

coverage-based evaluation of parser generalizability
we have carried out a series of coverage evaluations of diverse types of parsers using texts from several genres such as newspaper , religious , legal and biomedical texts . we compared the overall coverage of the evaluated parsers and analyzed the differences by text genre . the results indicate that the coverage typically drops several percentage points when parsers are faced with texts on genres other than newspapers .

using lexical expansion to learn inference rules from sparse data
automatic acquisition of inference rules for predicates is widely addressed by computing distributional similarity scores between vectors of argument words . in this scheme , prior work typically refrained from learning rules for low frequency predicates associated with very sparse argument vectors due to expected low reliability . to improve the learning of such rules in an unsupervised way , we propose to lexically expand sparse argument word vectors with semantically similar words . our evaluation shows that lexical expansion significantly improves performance in comparison to state-of-the-art baselines .

scaling up analogical learning
recent years have witnessed a growing interest in analogical learning for nlp applications . if the principle of analogical learning is quite simple , it does involve complex steps that seriously limit its applicability , the most computationally demanding one being the identification of analogies in the input space . in this study , we investigate different strategies for efficiently solving this problem and study their scalability .

the hidden information state dialogue manager : a real-world pomdp-based system
the hidden information state ( his ) dialogue system is the first trainable and scalable implementation of a spoken dialog system based on the partiallyobservable markov-decision-process ( pomdp ) model of dialogue . the system responds to n-best output from the speech recogniser , maintains multiple concurrent dialogue state hypotheses , and provides a visual display showing how competing hypotheses are ranked . the demo is a prototype application for the tourist information domain and achieved a task completion rate of over 90 % in a recent user study .

supersense tagging of unknown nouns in wordnet
we present a new framework for classifying common nouns that extends namedentity classification . we used a fixed set of 26 semantic labels , which we called supersenses . these are the labels used by lexicographers developing wordnet . this framework has a number of practical advantages . we show how information contained in the dictionary can be used as additional training data that improves accuracy in learning new nouns . we also define a more realistic evaluation procedure than cross-validation .

standardizing complex functional expressions in japanese predicates : applying theoretically-based paraphrasing rules
in order to accomplish the deep semantic understanding of a language , it is essential to analyze the meaning of predicate phrases , a content word plus functional expressions . in agglutinating languages such as japanese , however , sentential predicates are multi-morpheme expressions and all the functional expressions including those unnecessary to the meaning of the predicate are merged into one phrase . this triggers an increase in surface forms , which is problematic for nlp systems . we solve this by introducing simplified surface forms of predicates that retain only the crucial meaning of the functional expressions . we construct paraphrasing rules based on syntactic and semantic theories in linguistics . the results of experiments show that our system achieves the high accuracy of 77 % while reducing the differences in surface forms by 44 % , which is quite close to the performance of manually simplified predicates .

a subcategorization acquisition system for french verbs
this paper presents a system capable of automatically acquiring subcategorization frames ( scfs ) for french verbs from the analysis of large corpora . we applied the system to a large newspaper corpus ( consisting of 10 years of the french newspaper le monde ) and acquired subcategorization information for 3267 verbs . the system learned 286 scf types for these verbs . from the analysis of 25 representative verbs , we obtained 0.82 precision , 0.59 recall and 0.69 f-measure . these results are comparable with those reported in recent related work .

analytical approaches to combining mt technologies
the talk will report on recent and ongoing work dedicated to analytical methods for a systematic combination of observed strengths of translation technologies . the focus will be on different ways of exploiting existing data on mt output and performance measures for system combination and for gaining insights on strengths and weaknesses of existing technologies .

using hidden markov random fields to combine distributional and pattern-based word clustering
word clustering is a conventional and important nlp task , and the literature has suggested two kinds of approaches to this problem . one is based on the distributional similarity and the other relies on the co-occurrence of two words in lexicosyntactic patterns . although the two methods have been discussed separately , it is promising to combine them since they are complementary with each other . this paper proposes to integrate them using hidden markov random fields and demonstrates its effectiveness through experiments .

annotating expressions of appraisal in english
the appraisal framework is a theory of the language of evaluation , developed within the tradition of systemic functional linguistics . the framework describes a taxonomy of the types of language used to convey evaluation and position oneself with respect to the evaluations of other people . accurate automatic recognition of these types of language can inform an analysis of document sentiment . this paper describes the preparation of test data for algorithms for automatic appraisal analysis . the difficulty of the task is assessed by way of an inter-annotator agreement study , based on measures analogous to those used in the muc-7 evaluation .

aggregating machine learning and rule based heuristics for named karthik gali , harshit surana , ashwini vaidya , praneeth shishtla and dipti misra sharma
this paper , submitted as an entry for the nersseal-2008 shared task , describes a system build for named entity recognition for south and south east asian languages . our paper combines machine learning techniques with language specific heuristics to model the problem of ner for indian languages . the system has been tested on five languages : telugu , hindi , bengali , urdu and oriya . it uses crf ( conditional random fields ) based machine learning , followed by post processing which involves using some heuristics or rules . the system is specifically tuned for hindi and telugu , we also report the results for the other four languages .

whats in a name
the correct identification of metonymies is not normally a problem for most people . for computers , things are different , however . in natural language processing , metonymy recognition is therefore usually addressed with complex algorithms that rely on hundreds of labelled training examples . this paper investigates two approaches to metonymy recognition that dispense with this complexity , albeit in different ways . the first , an unsupervised approach to word sense discrimination , does not require any labelled training instances . the second , memory-based learning , replaces the complexity of current algorithms by a lazy learning phase . while the first approach is often able to identify a metonymical and a literal cluster in the data , it is the second in particular that produces state-of-the-art results .

a rule based approach to discourse parsing
in this paper we present an overview of recent developments in discourse theory and parsing under the linguistic discourse model ( ldm ) framework , a semantic theory of discourse structure . we give a novel approach to the problem of discourse segmentation based on discourse semantics and sketch a limited but robust approach to symbolic discourse parsing based on syntactic , semantic and lexical rules . to demonstrate the utility of the system in a real application , we briefly describe the architecture of the palsumm system , a symbolic summarization system being developed at laboratory that uses discourse structures constructed using the theory outlined to summarize written english prose texts .

corpus-based question answering for why-questions
this paper proposes a corpus-based approach for answering why-questions . conventional systems use hand-crafted patterns to extract and evaluate answer candidates . however , such hand-crafted patterns are likely to have low coverage of causal expressions , and it is also difficult to assign suitable weights to the patterns by hand . in our approach , causal expressions are automatically collected from corpora tagged with semantic relations . from the collected expressions , features are created to train an answer candidate ranker that maximizes the qa performance with regards to the corpus of why-questions and answers . nazeqa , a japanese why-qa system based on our approach , clearly outperforms a baseline that uses hand-crafted patterns with a mean reciprocal rank ( top-5 ) of 0.305 , making it presumably the best-performing fully implemented why-qa system .

upm system for the translation task vernica lpez-ludea rubn san-segundo
this paper describes the upm system for translation task at the emnlp 2011 workshop on statistical machine translation , and it has been used for both directions : spanish-english and english-spanish . this system is based on moses with two new modules for pre and post processing the sentences . the main contribution is the method proposed ( based on the similarity with the source language test set ) for selecting the sentences for training the models and adjusting the weights . with system , we have obtained a 23.2 bleu for spanish-english and 21.7 bleu for englishspanish .

discovery of term variation in japanese web search queries
in this paper we address the problem of identifying a broad range of term variations in japanese web search queries , where these variations pose a particularly thorny problem due to the multiple character types employed in its writing system . our method extends the techniques proposed for english spelling correction of web queries to handle a wider range of term variants including spelling mistakes , valid alternative spellings using multiple character types , transliterations and abbreviations . the core of our method is a statistical model built on the mart algorithm ( friedman , 2001 ) . we show that both string and semantic similarity features contribute to identifying term variation in web search queries ; specifically , the semantic similarity features used in our system are learned by mining user session and click-through logs , and are useful not only as model features but also in generating term variation candidates efficiently . the proposed method achieves 70 % precision on the term variation identification task with the recall slightly higher than 60 % , reducing the error rate of a nave baseline by 38 % .

judging grammaticality with tree substitution grammar derivations
in this paper , we show that local features computed from the derivations of tree substitution grammars such as the identify of particular fragments , and a count of large and small fragments are useful in binary grammatical classification tasks . such features outperform n-gram features and various model scores by a wide margin . although they fall short of the performance of the hand-crafted feature set of charniak and johnson ( 2005 ) developed for parse tree reranking , they do so with an order of magnitude fewer features . furthermore , since the tsgs employed are learned in a bayesian setting , the use of their derivations can be viewed as the automatic discovery of tree patterns useful for classification . on the bllip dataset , we achieve an accuracy of 89.9 % in discriminating between grammatical text and samples from an n-gram language model .

a non-negative matrix factorization based approach for active dual
in active dual supervision , not only informative examples but also features are selected for labeling to build a high quality classifier with low cost . however , how to measure the informativeness for both examples and feature on the same scale has not been well solved . in this paper , we propose a non-negative matrix factorization based approach to address this issue . we first extend the matrix factorization framework to explicitly model the corresponding relationships between feature classes and examples classes . then by making use of the reconstruction error , we propose a unified scheme to determine which feature or example a classifier is most likely to benefit from having labeled . empirical results demonstrate the effectiveness of our proposed methods .

minimally supervised event causality identification
this paper develops a minimally supervised approach , based on focused distributional similarity methods and discourse connectives , for identifying of causality relations between events in context . while it has been shown that distributional similarity can help identifying causality , we observe that discourse connectives and the particular discourse relation they evoke in context provide additional information towards determining causality between events . we show that combining discourse relation predictions and distributional similarity methods in a global inference procedure provides additional improvements towards determining event causality .

non-expert correction of automatically generated relation annotations
we explore a new way to collect human annotated relations in text using amazon mechanical turk . given a knowledge base of relations and a corpus , we identify sentences which mention both an entity and an attribute that have some relation in the knowledge base . each noisy sentence/relation pair is presented to multiple turkers , who are asked whether the sentence expresses the relation . we describe a design which encourages user efficiency and aids discovery of cheating . we also present results on inter-annotator agreement .

identification and treatment of multiword expressions
the extensive use of multiword expressions ( mwe ) in natural language texts prompts more detailed studies that aim for a more adequate treatment of these expressions . a mwe typically expresses concepts and ideas that usually can not be expressed by a single word . intuitively , with the appropriate treatment of mwes , the results of an information retrieval ( ir ) system could be improved . the aim of this paper is to apply techniques for the automatic extraction of mwes from corpora to index them as a single unit . experimental results show improvements on the retrieval of relevant documents when identifying mwes and treating them as a single indexing unit .

grounded language modeling for automatic speech recognition of sports video
grounded language models represent the relationship between words and the non-linguistic context in which they are said . this paper describes how they are learned from large corpora of unlabeled video , and are applied to the task of automatic speech recognition of sports video . results show that grounded language models improve perplexity and word error rate over text based language models , and further , support video information retrieval better than human generated speech transcriptions .

combining multiple models for speech information retrieval
in this article we present a method for combining different information retrieval models in order to increase the retrieval performance in a speech information retrieval task . the formulas for combining the models are tuned on training data . then the system is evaluated on test data . the task is particularly difficult because the text collection is automatically transcribed spontaneous speech , with many recognition errors . also , the topics are real information needs , difficult to satisfy . information retrieval systems are not able to obtain good results on this data set , except for the case when manual summaries are included .

learning the relative usefulness of questions in community qa
we present a machine learning approach for the task of ranking previously answered questions in a question repository with respect to their relevance to a new , unanswered reference question . the ranking model is trained on a collection of question groups manually annotated with a partial order relation reflecting the relative utility of questions inside each group . based on a set of meaning and structure aware features , the new ranking model is able to substantially outperformmore straightforward , unsupervised similarity measures .

polly : a conversational system that uses a shared representation to
we present a demo of our conversational system polly ( politeness in language learning ) which uses a common planning representation to generate actions to be performed by embodied agents in a virtual environment and to generate spoken utterances for dialogues about the steps involved in completing the task . in order to generate socially appropriate dialogue , brown and levinsons theory of politeness is used to constrain the dialogue generation process .

multi-word unit dependency forest-based translation rule
translation requires non-isomorphic transformation from the source to the target . however , non-isomorphism can be reduced by learning multi-word units ( mwus ) . we present a novel way of representating sentence structure based on mwus , which are not necessarily continuous word sequences . our proposed method builds a simpler structure of mwus than words using words as vertices of a dependency structure . unlike previous studies , we collect many alternative structures in a packed forest . as an application of our proposed method , we extract translation rules in form of a source mwu-forest to the target string , and verify the rule coverage empirically . as a consequence , we improve the rule coverage compare to a previous work , while retaining the linear asymptotic complexity .

a high-performance semi-supervised learning method for text chunking rie kubota andoy tong zhangz
in machine learning , whether one can build a more accurate classifier by using unlabeled data ( semi-supervised learning ) is an important issue . although a number of semi-supervised methods have been proposed , their effectiveness on nlp tasks is not always clear . this paper presents a novel semi-supervised method that employs a learning paradigm which we call structural learning . the idea is to find what good classifiers are like by learning from thousands of automatically generated auxiliary classification problems on unlabeled data . by doing so , the common predictive structure shared by the multiple classification problems can be discovered , which can then be used to improve performance on the target problem . the method produces performance higher than the previous best results on conll00 syntactic chunking and conll03 named entity chunking ( english and german ) .

an improved hierarchical bayesian model of language for document classification
this paper addresses the fundamental problem of document classification , and we focus attention on classification problems where the classes are mutually exclusive . in the course of the paper we advocate an approximate sampling distribution for word counts in documents , and demonstrate the models capacity to outperform both the simple multinomial and more recently proposed extensions on the classification task . we also compare the classifiers to a linear svm , and show that provided certain conditions are met , the new model allows performance which exceeds that of the svm and attains amongst the very best published results on the newsgroups classification task .

re-evaluating machine translation results with paraphrase support
in this paper , we present paraeval , an automatic evaluation framework that uses paraphrases to improve the quality of machine translation evaluations . previous work has focused on fixed n-gram evaluation metrics coupled with lexical identity matching . paraeval addresses three important issues : support for paraphrase/synonym matching , recall measurement , and correlation with human judgments . we show that paraeval correlates significantly better than bleu with human assessment in measurements for both fluency and adequacy .

construct state modification in the arabic treebank linguistic data consortium
earlier work in parsing arabic has speculated that attachment to construct state constructions decreases parsing performance . we make this speculation precise and define the problem of attachment to construct state constructions in the arabic treebank . we present the first statistics that quantify the problem . we provide a baseline and the results from a first attempt at a discriminative learning procedure for this task , achieving 80 % accuracy .

logistic online learning methods and their application to incremental dependency parsing
we investigate a family of update methods for online machine learning algorithms for cost-sensitive multiclass and structured classification problems . the update rules are based on multinomial logistic models . the most interesting question for such an approach is how to integrate the cost function into the learning paradigm . we propose a number of solutions to this problem . to demonstrate the applicability of the algorithms , we evaluated them on a number of classification tasks related to incremental dependency parsing . these tasks were conventional multiclass classification , hiearchical classification , and a structured classification task : complete labeled dependency tree prediction . the performance figures of the logistic algorithms range from slightly lower to slightly higher than margin-based online algorithms .

character-to-character sentiment analysis in shakespeares plays
we present an automatic method for analyzing sentiment dynamics between characters in plays . this literary formats structured dialogue allows us to make assumptions about who is participating in a conversation . once we have an idea of who a character is speaking to , the sentiment in his or her speech can be attributed accordingly , allowing us to generate lists of a characters enemies and allies as well as pinpoint scenes critical to a characters emotional development . results of experiments on shakespeares plays are presented along with discussion of how this work can be extended to unstructured texts ( i.e . novels ) .

practical linguistic steganography using contextual synonym substitution and vertex colour coding
linguistic steganography is concerned with hiding information in natural language text . one of the major transformations used in linguistic steganography is synonym substitution . however , few existing studies have studied the practical application of this approach . in this paper we propose two improvements to the use of synonym substitution for encoding hidden bits of information . first , we use the web 1t google n-gram corpus for checking the applicability of a synonym in context , and we evaluate this method using data from the semeval lexical substitution task . second , we address the problem that arises from words with more than one sense , which creates a potential ambiguity in terms of which bits are encoded by a particular word . we develop a novel method in which words are the vertices in a graph , synonyms are linked by edges , and the bits assigned to a word are determined by a vertex colouring algorithm . this method ensures that each word encodes a unique sequence of bits , without cutting out large number of synonyms , and thus maintaining a reasonable embedding capacity .

combining sample selection and error-driven pruning for machine learning of coreference rules
most machine learning solutions to noun phrase coreference resolution recast the problem as a classification task . we examine three potential problems with this reformulation , namely , skewed class distributions , the inclusion of hard training instances , and the loss of transitivity inherent in the original coreference relation . we show how these problems can be handled via intelligent sample selection and error-driven pruning of classification rulesets . the resulting system achieves an fmeasure of 69.5 and 63.4 on the muc6 and muc-7 coreference resolution data sets , respectively , surpassing the performance of the best muc-6 and muc-7 coreference systems . in particular , the system outperforms the best-performing learning-based coreference system to date .

statistical machine translation part ii : tree-based smt
one of the most active and promising areas of statistical machine translation ( smt ) research are tree-based smt approaches . tree-based smt has the potential to overcome the weaknesses of early smt architectures which ( a ) do not handle long-distance dependencies well , and ( b ) are underconstrained in that they allow too much flexibility in word reordering .

fast and accurate arc filtering for dependency parsing
we propose a series of learned arc filters to speed up graph-based dependency parsing . a cascade of filters identify implausible head-modifier pairs , with time complexity that is first linear , and then quadratic in the length of the sentence . the linear filters reliably predict , in context , words that are roots or leaves of dependency trees , and words that are likely to have heads on their left or right . we use this information to quickly prune arcs from the dependency graph . more than 78 % of total arcs are pruned while retaining 99.5 % of the true dependencies . these filters improve the speed of two state-ofthe-art dependency parsers , with low overhead and negligible loss in accuracy .

improving interaction quality recognition using error correction
determining the quality of an ongoing interaction in the field of spoken dialogue systems is a hard task . while existing methods employing automatic estimation already achieve reasonable results , still there is a lot of room for improvement . hence , we aim at tackling the task by estimating the error of the applied statistical classification algorithms in a two-stage approach . correcting the hypotheses using the estimated model error increases performance by up to 4.1 % relative improvement in unweighted average recall .

efficient extraction of grammatical relations and ted briscoe
we present a novel approach for applying the inside-outside algorithm to a packed parse forest produced by a unificationbased parser . the approach allows a node in the forest to be assigned multiple inside and outside probabilities , enabling a set of weighted grs to be computed directly from the forest . the approach improves on previous work which either loses efficiency by unpacking the parse forest before extracting weighted grs , or places extra constraints on which nodes can be packed , leading to less compact forests . our experiments demonstrate substantial increases in parser accuracy and throughput for weighted gr output .

boeings nlp system and the challenges of semantic
we describe boeings nlp system , blue , comprising a pipeline of a parser , a logical form ( lf ) generator , an initial logic generator , and further processingmodules . the initial logic generator produces logic whose structure closely mirrors the structure of the original text . the subsequent processing modules then perform , with somewhat limited scope , additional transformations to convert this into a more usable representation with respect to a specific target ontology , better able to support inference . generating a semantic representation is challenging , due to the wide variety of semantic phenomena which can occur in text . we identify seventeen such phenomena which occurred in the step 2008 '' shared task '' texts , comment on blues ability to handle them or otherwise , and discuss the more general question of what exactly constitutes a '' semantic representation '' , arguing that a spectrum of interpretations exist .

an annotation tool for multimodal dialogue corpora using global document annotation
this paper reports a tool which assists the user in annotating a video corpus and enables the user to search for a semantic or pragmatic structure in a gda tagged corpus . an xql format is allowed for search patterns as well as a plain phrase . this tool is capable of generating a gda timestamped corpus from a video file manually . it will be publicly available for academic purposes .

crowdsourcing interaction logs to understand text reuse from the web
we report on the construction of the webis text reuse corpus 2012 for advanced research on text reuse . the corpus compiles manually written documents obtained from a completely controlled , yet representative environment that emulates the web . each of the 297 documents in the corpus is about one of the 150 topics used at the trec web tracks 20092011 , thus forming a strong connection with existing evaluation efforts . writers , hired at the crowdsourcing platform odesk , had to retrieve sources for a given topic and to reuse text from what they found . part of the corpus are detailed interaction logs that consistently cover the search for sources as well as the creation of documents . this will allow for in-depth analyses of how text is composed if a writer is at liberty to reuse texts from a third partya setting which has not been studied so far . in addition , the corpus provides an original resource for the evaluation of text reuse and plagiarism detectors , where currently only less realistic resources are employed .

encoding syntactic dependencies by vector permutation
distributional approaches are based on a simple hypothesis : the meaning of a word can be inferred from its usage . the application of that idea to the vector space model makes possible the construction of a wordspace in which words are represented by mathematical points in a geometric space . similar words are represented close in this space and the definition of word usage depends on the definition of the context used to build the space , which can be the whole document , the sentence in which the word occurs , a fixed window of words , or a specific syntactic context . however , in its original formulation wordspace can take into account only one definition of context at a time . we propose an approach based on vector permutation and random indexing to encode several syntactic contexts in a single wordspace . moreover , we propose some operations in this space and report the results of an evaluation performed using the gems 2011 shared evaluation data .

improving context vector models by feature clustering for automatic thesaurus construction
thesauruses are useful resources for nlp ; however , manual construction of thesaurus is time consuming and suffers low coverage . automatic thesaurus construction is developed to solve the problem . conventional way to automatically construct thesaurus is by finding similar words based on context vector models and then organizing similar words into thesaurus structure . but the context vector methods suffer from the problems of vast feature dimensions and data sparseness . latent semantic index ( lsi ) was commonly used to overcome the problems . in this paper , we propose a feature clustering method to overcome the same problems . the experimental results show that it performs better than the lsi models and do enhance contextual information for infrequent words .

learning to extract folktale keywords
manually assigned keywords provide a valuable means for accessing large document collections . they can serve as a shallow document summary and enable more efficient retrieval and aggregation of information . in this paper we investigate keywords in the context of the dutch folktale database , a large collection of stories including fairy tales , jokes and urban legends . we carry out a quantitative and qualitative analysis of the keywords in the collection . up to 80 % of the assigned keywords ( or a minor variation ) appear in the text itself . human annotators show moderate to substantial agreement in their judgment of keywords . finally , we evaluate a learning to rank approach to extract and rank keyword candidates . we conclude that this is a promising approach to automate this time intensive task .

a development environment for large-scale multi-lingual parsing systems
we describe the development environment available to linguistic developers in our lab in writing large-scale grammars for multiple languages . the environment consists of the tools that assist writing linguistic rules and running regression testing against large corpora , both of which are indispensable for realistic development of large-scale parsing systems . we also emphasize the importance of parser efficiency as an integral part of efficient parser development . the tools and methods described in this paper are actively used in the daily development of broad-coverage natural language understanding systems in seven languages ( chinese , english , french , german , japanese , korean and spanish ) .

automating analysis of social media communication
a growing body of research analyzes the linguistic and discourse properties of communication in online social media . most of the analysis , especially at the discourse level , is done manually by human researchers . this talk explores how the findings and techniques of computer-mediated discourse analysis ( cmda ) , a paradigm i have been developing and teaching for 18 years , can inform computational approaches to communication in social media . i start by reviewing established automation approaches , which mainly focus on structural linguistic phenomena , and emergent approaches , such as machine learning models that identify semantically- and pragmatically-richer phenomena , through the lens of cmda , pointing out the strengths and limitations of each . the basic problem is that patterns in the discourse of social media users can be identified by humans that do not appear to lend themselves to reliable automated identification using existing approaches . to begin to address this problem , i draw on examples of recent work on twitter , wikipedia , and web-based discussion forums to suggest an approach that synthesizes linguistically-informed manual analysis and existing automated techniques . i consider how such an approach could scale up , while still making use of human analysts , and i identify a number of real-world problems that automated cmda could help address .

chinese syntactic reordering for statistical machine translation
syntactic reordering approaches are an effective method for handling word-order differences between source and target languages in statistical machine translation ( smt ) systems . this paper introduces a reordering approach for translation from chinese to english . we describe a set of syntactic reordering rules that exploit systematic differences between chinese and english word order . the resulting system is used as a preprocessor for both training and test sentences , transforming chinese sentences to be much closer to english in terms of their word order . we evaluated the reordering approach within the moses phrase-based smt system ( koehn et al , 2007 ) . the reordering approach improved the bleu score for the moses system from 28.52 to 30.86 on the nist 2006 evaluation data . we also conducted a series of experiments to analyze the accuracy and impact of different types of reordering rules .

online relative margin maximization for statistical machine translation
recent advances in large-margin learning have shown that better generalization can be achieved by incorporating higher order information into the optimization , such as the spread of the data . however , these solutions are impractical in complex structured prediction problems such as statistical machine translation . we present an online gradient-based algorithm for relative margin maximization , which bounds the spread of the projected data while maximizing the margin . we evaluate our optimizer on chinese-english and arabicenglish translation tasks , each with small and large feature sets , and show that our learner is able to achieve significant improvements of 1.2-2 bleu and 1.7-4.3 ter on average over state-of-the-art optimizers with the large feature set .

of ambiguous names
this paper describes adaptations of unsupervised word sense discrimination techniques to the problem of name discrimination . these methods cluster the contexts containing an ambiguous name , such that each cluster refers to a unique underlying person or place . we also present new techniques to assign meaningful labels to the discovered clusters .

an empirical study of active learning with support vector machines for japanese word segmentation
we explore how active learning with support vector machines works well for a non-trivial task in natural language processing . we use japanese word segmentation as a test case . in particular , we discuss how the size of a pool affects the learning curve . it is found that in the early stage of training with a larger pool , more labeled examples are required to achieve a given level of accuracy than those with a smaller pool . in addition , we propose a novel technique to use a large number of unlabeled examples effectively by adding them gradually to a pool . the experimental results show that our technique requires less labeled examples than those with the technique in previous research . to achieve 97.0 % accuracy , the proposed technique needs 59.3 % of labeled examples that are required when using the previous technique and only 17.4 % of labeled examples with random sampling .

assisting translators in indirect lexical transfer
we present the design and evaluation of a translators amenuensis that uses comparable corpora to propose and rank nonliteral solutions to the translation of expressions from the general lexicon . using distributional similarity and bilingual dictionaries , the method outperforms established techniques for extracting translation equivalents from parallel corpora .

metaphor identification as interpretation
automatic metaphor identification and interpretation in text have been traditionally considered as two separate tasks in natural language processing ( nlp ) and addressed individually within computational frameworks . however , cognitive evidence suggests that humans are likely to perform these two tasks simultaneously , as part of a holistic metaphor comprehension process . we present a novel method that performs metaphor identification through its interpretation , being the first one in nlp to combine the two tasks in one step . it outperforms the previous approaches to metaphor identification both in terms of accuracy and coverage , as well as providing an interpretation for each identified expression .

leveraging synthetic discourse data via multi-task learning for implicit discourse relation recognition
to overcome the shortage of labeled data for implicit discourse relation recognition , previous works attempted to automatically generate training data by removing explicit discourse connectives from sentences and then built models on these synthetic implicit examples . however , a previous study ( sporleder and lascarides , 2008 ) showed that models trained on these synthetic data do not generalize very well to natural ( i.e . genuine ) implicit discourse data . in this work we revisit this issue and present a multi-task learning based system which can effectively use synthetic data for implicit discourse relation recognition . results on pdtb data show that under the multi-task learning framework our models with the use of the prediction of explicit discourse connectives as auxiliary learning tasks , can achieve an averaged f1 improvement of 5.86 % over baseline models .

natural language searching in onomasiological dictionaries
when consulting a dictionary , people can find the meaning of a word via the definition , which usually contains the relevant information to fulfil their requirement . lexicographers produce dictionaries and their work consists in presenting information essential for grasping the meaning of words . however , when people need to find a word it is likely that they do not obtain the information they are looking for . there is a gap between dictionary definitions and the information being available in peoples mind . this paper attempts to present the conceptualisation people engage in , in order to arrive at a word from its meaning . the insights of an experiment conducted show us the differences between the knowledge available in peoples minds and in dictionary definitions .

a graph-theoretic algorithm for automatic extension of translation beate dorow florian laws lukas michelbacher christian scheible jason utt
this paper presents a graph-theoretic approach to the identification of yetunknown word translations . the proposed algorithm is based on the recursive simrank algorithm and relies on the intuition that two words are similar if they establish similar grammatical relationships with similar other words . we also present a formulation of simrank in matrix form and extensions for edge weights , edge labels and multiple graphs .

a multi-teraflop constituency parser using gpus
constituency parsing with rich grammars remains a computational challenge . graphics processing units ( gpus ) have previously been used to accelerate cky chart evaluation , but gains over cpu parsers were modest . in this paper , we describe a collection of new techniques that enable chart evaluation at close to the gpus practical maximum speed ( a teraflop ) , or around a half-trillion rule evaluations per second . net parser performance on a 4-gpu system is over 1 thousand length30 sentences/second ( 1 trillion rules/sec ) , and 400 general sentences/second for the berkeley parser grammar . the techniques we introduce include grammar compilation , recursive symbol blocking , and cache-sharing .

fast joint compression and summarization via graph cuts
extractive summarization typically uses sentences as summarization units . in contrast , joint compression and summarization can use smaller units such as words and phrases , resulting in summaries containing more information . the goal of compressive summarization is to find a subset of words that maximize the total score of concepts and cutting dependency arcs under the grammar constraints and summary length constraint . we propose an efficient decoding algorithm for fast compressive summarization using graph cuts . our approach first relaxes the length constraint using lagrangian relaxation . then we propose to bound the relaxed objective function by the supermodular binary quadratic programming problem , which can be solved efficiently using graph max-flow/min-cut . since finding the tightest lower bound suffers from local optimality , we use convex relaxation for initialization . experimental results on tac2008 dataset demonstrate our method achieves competitive rouge score and has good readability , while is much faster than the integer linear programming ( ilp ) method .

context-based message expansion for disentanglement of interleaved text conversations
computational processing of text exchanged in interactive venues in which participants engage in simultaneous conversations can benefit from techniques for automatically grouping overlapping sequences of messages into separate conversations , a problem known as disentanglement . while previous methods exploit both lexical and non-lexical information that exists in conversations for this task , the inter-dependency between the meaning of a message and its temporal and social contexts is largely ignored . our approach exploits contextual properties ( both explicit and hidden ) to probabilistically expand each message to provide a more accurate message representation . extensive experimental evaluations show our approach outperforms the best previously known technique .

a bio-inspired approach for multi-word expression extraction
this paper proposes a new approach for multi-word expression ( mwe ) extraction on the motivation of gene sequence alignment because textual sequence is similar to gene sequence in pattern analysis . theory of longest common subsequence ( lcs ) originates from computer science and has been established as affine gap model in bioinformatics . we perform this developed lcs technique combined with linguistic criteria in mwe extraction . in comparison with traditional n-gram method , which is the major technique for mwe extraction , lcs approach is applied with great efficiency and performance guarantee . experimental results show that lcs-based approach achieves better results than n-gram .

towards strict sentence intersection : decoding and evaluation strategies
we examine the task of strict sentence intersection : a variant of sentence fusion in which the output must only contain the information present in all input sentences and nothing more . our proposed approach involves alignment and generalization over the input sentences to produce a generation lattice ; we then compare a standard search-based approach for decoding an intersection from this lattice to an integer linear program that preserves aligned content while minimizing the disfluency in interleaving text segments . in addition , we introduce novel evaluation strategies for intersection problems that employ entailmentstyle judgments for determining the validity of system-generated intersections . our experiments show that the proposed models produce valid intersections a majority of the time and that the segmented decoder yields advantages over the search-based approach .

multiword noun compound bracketing using wikipedia
this research suggests two contributions in relation to the multiword noun compound bracketing problem : first , demonstrate the usefulness of wikipedia for the task , and second , present a novel bracketing method relying on a word association model . the intent of the association model is to represent combined evidence about the possibly lexical , relational or coordinate nature of links between all pairs of words within a compound . as for wikipedia , it is promoted for its encyclopedic nature , meaning it describes terms and named entities , as well as for its size , large enough for corpus-based statistical analysis . both types of information will be used in measuring evidence about lexical units , noun relations and noun coordinates in order to feed the association model in the bracketing algorithm . using a gold standard of around 4800 multiword noun compounds , we show performances of 73 % in a strict match evaluation , comparing favourably to results reported in the literature using unsupervised approaches .

optimal constituent alignment with edge covers for semantic projection
given a parallel corpus , semantic projection attempts to transfer semantic role annotations from one language to another , typically by exploiting word alignments . in this paper , we present an improved method for obtaining constituent alignments between parallel sentences to guide the role projection task . our extensions are twofold : ( a ) we model constituent alignment as minimum weight edge covers in a bipartite graph , which allows us to find a globally optimal solution efficiently ; ( b ) we propose tree pruning as a promising strategy for reducing alignment noise . experimental results on an english-german parallel corpus demonstrate improvements over state-of-the-art models .

learning with lookahead : can history-based models rival globally optimized models yoshimasa tsuruoka yusuke miyao junichi kazama
this paper shows that the performance of history-based models can be significantly improved by performing lookahead in the state space when making each classification decision . instead of simply using the best action output by the classifier , we determine the best action by looking into possible sequences of future actions and evaluating the final states realized by those action sequences . we present a perceptron-based parameter optimization method for this learning framework and show its convergence properties . the proposed framework is evaluated on partof-speech tagging , chunking , named entity recognition and dependency parsing , using standard data sets and features . experimental results demonstrate that history-based models with lookahead are as competitive as globally optimized models including conditional random fields ( crfs ) and structured perceptrons .

using mechanical turk to build machine translation evaluation sets human language technology
building machine translation ( mt ) test sets is a relatively expensive task . as mt becomes increasingly desired for more and more language pairs and more and more domains , it becomes necessary to build test sets for each case . in this paper , we investigate using amazons mechanical turk ( mturk ) to make mt test sets cheaply . we find that mturk can be used to make test sets much cheaper than professionally-produced test sets . more importantly , in experiments with multiple mt systems , we find that the mturk-produced test sets yield essentially the same conclusions regarding system performance as the professionally-produced test sets yield .

experiments with interactive question-answering language computer corporation
this paper describes a novel framework for interactive question-answering ( q/a ) based on predictive questioning . generated off-line from topic representations of complex scenarios , predictive questions represent requests for information that capture the most salient ( and diverse ) aspects of a topic . we present experimental results from large user studies ( featuring a fully-implemented interactive q/a system named ferret ) that demonstrates that surprising performance is achieved by integrating predictive questions into the context of a q/a dialogue .

what pushes their buttons predicting comment polarity from the content of political blog posts
political blogs as a form of social media allow for an uniquely interactive form of political discourse . this is especially evident in focused blogs with a strong ideological identity . we investigate techniques to identify topics within the context of the community , which when discussed in a blog post evoke a discernible positive or negative collective opinion from readers who respond to posts in comments . this is done by using computational methods to assign sentiment polarity to blog comments and learning community specific models that summarize issues tackled by blogs and predict the polarity based on the topics discussed in a blog post .

combining knowledge - based methods and supervised learning for effective italian word sense disambiguation
this paper presents a wsd strategy which combines a knowledge-based method that exploits sense definitions in a dictionary and relations among senses in a semantic network , with supervised learning methods on annotated corpora . the idea behind the approach is that the knowledge-based method can cope with the possible lack of training data , while supervised learning can improve the precision of a knowledge-based method when training data are available . this makes the proposed method suitable for disambiguation of languages for which the available resources are lacking in training data or sense definitions . in order to evaluate the effectiveness of the proposed approach , experimental sessions were carried out on the dataset used for the wsd task in the evalita 2007 initiative , devoted to the evaluation of natural language processing tools for italian . the most effective hybrid wsd strategy is the one that integrates the knowledgebased approach into the supervised learning method , which outperforms both methods taken singularly .

the annotation conundrum
without lengthy , iterative refinement of guidelines , and equally lengthy and iterative training of annotators , the level of inter-subjective agreement on simple tasks of phonetic , phonological , syntactic , semantic , and pragmatic annotation is shockingly low . this is a significant practical problem in speech and language technology , but it poses questions of interest to psychologists , philosophers of language , and theoretical linguists as well .

co-regularizing character-based and word-based models for semi-supervised chinese word segmentation
this paper presents a semi-supervised chinese word segmentation ( cws ) approach that co-regularizes character-based and word-based models . similarly to multi-view learning , the segmentation agreements between the two different types of view are used to overcome the scarcity of the label information on unlabeled data . the proposed approach trains a character-based and word-based model on labeled data , respectively , as the initial models . then , the two models are constantly updated using unlabeled examples , where the learning objective is maximizing their segmentation agreements . the agreements are regarded as a set of valuable constraints for regularizing the learning of both models on unlabeled data . the segmentation for an input sentence is decoded by using a joint scoring function combining the two induced models . the evaluation on the chinese tree bank reveals that our model results in better gains over the state-of-the-art semi-supervised models reported in the literature .

semantic frames to predict stock price movement
semantic frames are a rich linguistic resource . there has been much work on semantic frame parsers , but less that applies them to general nlp problems . we address a task to predict change in stock price from financial news . semantic frames help to generalize from specific sentences to scenarios , and to detect the ( positive or negative ) roles of specific companies . we introduce a novel tree representation , and use it to train predictive models with tree kernels using support vector machines . our experiments test multiple text representations on two binary classification tasks , change of price and polarity . experiments show that features derived from semantic frame parsing have significantly better performance across years on the polarity task .

syntactic transfer patterns of german particle verbs and their impact
german particle verbs , like anblicken ( to gaze at ) combine a base verb ( blicken ) with a particle ( an ) to form a special kind of multi word expression . particle verbs may share the semantics of the base verb and the particle to a variable degree . however , while syntactic subcategorization frames tend to be good predictor for the semantics of verbs in general ( verbs that are similar in meaning also tend to have similar subcategorization frames and selectional preferences ) , there are regular changes in subcategorization frames by particle verbs with regard to the corresponding base verbs . this paper demonstrates that the syntactic behavior of particle verbs and base verbs together ( modeling regular changes in subcategorization frames by particle verbs and corresponding base verbs ) and applying clustering techniques allows us to distinguish particle verb meaning and shows the tight connection between transfer patterns and the semantic classes of particle verbs .

rule based morphological analyzer of kazakh language
having a morphological analyzer is a very critical issue especially for nlp related tasks on agglutinative languages . this paper presents a detailed computational analysis of kazakh language which is an agglutinative language . with a detailed analysis of kazakh language morphology , the formalization of rules over all morphotactics of kazakh language is worked out and a rule-based morphological analyzer is developed for kazakh language . the morphological analyzer is constructed using two-level morphology approach with xerox finite state tools and some implementation details of rule-based morphological analyzer have been presented in this paper .

inferring tutorial dialogue structure with hidden markov modeling
the field of intelligent tutoring systems has seen many successes in recent years . a significant remaining challenge is the automatic creation of corpus-based tutorial dialogue management models . this paper reports on early work toward this goal . we identify tutorial dialogue modes in an unsupervised fashion using hidden markov models ( hmms ) trained on input sequences of manually-labeled dialogue acts and adjacency pairs . the two best-fit hmms are presented and compared with respect to the dialogue structure they suggest ; we also discuss potential uses of the methodology for future work .

interactively exploring a machine translation model
this paper describes a method of interactively visualizing and directing the process of translating a sentence . the method allows a user to explore a model of syntax-based statistical machine translation ( mt ) , to understand the models strengths and weaknesses , and to compare it to other mt systems . using this visualization method , we can find and address conceptual and practical problems in an mt system . in our demonstration at acl , new users of our tool will drive a syntaxbased decoder for themselves .

a named-entity disambiguation framework for arabic text
there has been recently a great progress in the field of automatically generated knowledge bases and corresponding disambiguation systems that are capable of mapping text mentions onto canonical entities . efforts like the before mentioned have enabled researchers and analysts from various disciplines to semantically understand contents . however , most of the approaches have been specifically designed for the english language and - in particular - support for arabic is still in its infancy . since the amount of arabic web contents ( e.g . in social media ) has been increasing dramatically over the last years , we see a great potential for endeavors that support an entity-level analytics of these data . to this end , we have developed a framework called aidarabic that extends the existing aida system by additional components that allow the disambiguation of arabic texts based on an automatically generated knowledge base distilled from wikipedia . even further , we overcome the still existing sparsity of the arabic wikipedia by exploiting the interwiki links between arabic and english contents in wikipedia , thus , enriching the entity catalog as well as disambiguation context .

discriminative word alignment with conditional random fields
in this paper we present a novel approach for inducing word alignments from sentence aligned data . we use a conditional random field ( crf ) , a discriminative model , which is estimated on a small supervised training set . the crf is conditioned on both the source and target texts , and thus allows for the use of arbitrary and overlapping features over these data . moreover , the crf has efficient training and decoding processes which both find globally optimal solutions . we apply this alignment model to both french-english and romanian-english language pairs . we show how a large number of highly predictive features can be easily incorporated into the crf , and demonstrate that even with only a few hundred word-aligned training sentences , our model improves over the current state-ofthe-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively .

word-based dialect identification with georeferenced rules
we present a novel approach for ( written ) dialect identification based on the discriminative potential of entire words . we generate swiss german dialect words from a standard german lexicon with the help of hand-crafted phonetic/graphemic rules that are associated with occurrence maps extracted from a linguistic atlas created through extensive empirical fieldwork . in comparison with a charactern-gram approach to dialect identification , our model is more robust to individual spelling differences , which are frequently encountered in non-standardized dialect writing . moreover , it covers the whole swiss german dialect continuum , which trained models struggle to achieve due to sparsity of training data .

using higher-level linguistic knowledge for speech recognition error
speech interface is often required in many application environments such as telephonebased information retrieval , car navigation systems , and user-friendly interfaces , but the low speech recognition rate makes it difficult to extend its application to new fields . several approaches to increase the accuracy of the recognition rate have been researched by error correction of the recognition results , but previous approaches were mainly lexical-oriented ones in post error correction . we suggest an improved syllable-based model and a new semantic-oriented approach to correct both semantic and lexical errors , which is also more accurate for especially domain-specific speech error correction . through extensive experiments using a speech-driven in-vehicle telematics information retrieval , we demonstrate the superior performance of our approach and some advantages over previous lexical-oriented approaches .

international standard for a linguistic annotation framework equipe langue et dialogue
this paper describes the outline of a linguistic annotation framework under development by iso tc37 sc wg1-1 . this international standard will provide an architecture for the creation , annotation , and manipulation of linguistic resources and processing software . the outline described here results from a meeting of approximately 20 experts in the field , who determined the principles and fundamental structure of the framework . the goal is to provide maximum flexibility for encoders and annotators , while at the same time enabling interchange and re-use of annotated linguistic resources .

hybrid selection of language model training data using linguistic
we explore the selection of training data for language models using perplexity . we introduce three novel models that make use of linguistic information and evaluate them on three different corpora and two languages . in four out of the six scenarios a linguistically motivated method outperforms the purely statistical state-of-theart approach . finally , a method which combines surface forms and the linguistically motivated methods outperforms the baseline in all the scenarios , selecting data whose perplexity is between 3.49 % and 8.17 % ( depending on the corpus and language ) lower than that of the baseline .

converting italian treebanks : towards an italian stanford dependency treebank
the paper addresses the challenge of converting midt , an existing dependency based italian treebank resulting from the harmonization and merging of smaller resources , into the stanford dependencies annotation formalism , with the final aim of constructing a standardcompliant resource for the italian language . achieved results include a methodology for converting treebank annotations belonging to the same dependencybased family , the italian stanford dependency treebank ( isdt ) , and an italian localization of the stanford dependency scheme .

projective dependency parsing with perceptron
we describe an online learning dependency parser for the conll-x shared task , based on the bottom-up projective algorithm of eisner ( 2000 ) . we experiment with a large feature set that models : the tokens involved in dependencies and their immediate context , the surfacetext distance between tokens , and the syntactic context dominated by each dependency . in experiments , the treatment of multilingual information was totally blind .

how to train your multi bottom-up tree transducer
the local multi bottom-up tree transducer is introduced and related to the ( non-contiguous ) synchronous tree sequence substitution grammar . it is then shown how to obtain a weighted local multi bottom-up tree transducer from a bilingual and biparsed corpus . finally , the problem of non-preservation of regularity is addressed . three properties that ensure preservation are introduced , and it is discussed how to adjust the rule extraction process such that they are automatically fulfilled .

a machine learning approach for recognizing textual entailment in spanish julio javier castillo
this paper presents a system that uses machine learning algorithms for the task of recognizing textual entailment in spanish language . the datasets used include sparte corpus and a translated version to spanish of rte3 , rte4 and rte5 datasets . the features chosen quantify lexical , syntactic and semantic level matching between text and hypothesis sentences . we analyze how the different sizes of datasets and classifiers could impact on the final overall performance of the rte classification of two-way task in spanish . the rte system yields 60.83 % of accuracy and a competitive result of 66.50 % of accuracy is reported by train and test set taken from sparte corpus with 70 % split .

three knowledge-free methods for automatic lexical chain extraction fg language technology
we present three approaches to lexical chaining based on the lda topic model and evaluate them intrinsically on a manually annotated set of german documents . after motivating the choice of statistical methods for lexical chaining with their adaptability to different languages and subject domains , we describe our new two-level chain annotation scheme , which rooted in the concept of cohesive harmony . also , we propose a new measure for direct evaluation of lexical chains . our three lda-based approaches outperform two knowledge-based state-of-the art methods to lexical chaining by a large margin , which can be attributed to lacking coverage of the knowledge resource . subsequent analysis shows that the three methods yield a different chaining behavior , which could be utilized in tasks that use lexical chaining as a component within nlp applications .

feature-based segmentation of narrative documents
in this paper we examine topic segmentation of narrative documents , which are characterized by long passages of text with few headings . we first present results suggesting that previous topic segmentation approaches are not appropriate for narrative text . we then present a featurebased method that combines features from diverse sources as well as learned features . applied to narrative books and encyclopedia articles , our method shows results that are significantly better than previous segmentation approaches . an analysis of individual features is also provided and the benefit of generalization using outside resources is shown .

word alignment with cohesion constraint
we present a syntax-based constraint for word alignment , known as the cohesion constraint . it requires disjoint english phrases to be mapped to non-overlapping intervals in the french sentence . we evaluate the utility of this constraint in two different algorithms . the results show that it can provide a significant improvement in alignment quality .

comparison of extended lexicon models in search and rescoring for smt
we show how the integration of an extended lexicon model into the decoder can improve translation performance . the model is based on lexical triggers that capture long-distance dependencies on the sentence level . the results are compared to variants of the model that are applied in reranking of n-best lists . we present how a combined application of these models in search and rescoring gives promising results . experiments are reported on the gale chinese-english task with improvements of up to +0.9 % bleu and -1.5 % ter absolute on a competitive baseline .

acquiring collocations for lexical choice between near-synonyms
we extend a lexical knowledge-base of near-synonym differences with knowledge about their collocational behaviour . this type of knowledge is useful in the process of lexical choice between nearsynonyms . we acquire collocations for the near-synonyms of interest from a corpus ( only collocations with the appropriate sense and part-of-speech ) . for each word that collocates with a near-synonym we use a differential test to learn whether the word forms a less-preferred collocation or an anti-collocation with other near-synonyms in the same cluster . for this task we use a much larger corpus ( the web ) . we also look at associations ( longer-distance co-occurrences ) as a possible source of learning more about nuances that the near-synonyms may carry .

modeling scientific impact with topical influence regression james foulds padhraic smyth
when reviewing scientific literature , it would be useful to have automatic tools that identify the most influential scientific articles as well as how ideas propagate between articles . in this context , this paper introduces topical influence , a quantitative measure of the extent to which an article tends to spread its topics to the articles that cite it . given the text of the articles and their citation graph , we show how to learn a probabilistic model to recover both the degree of topical influence of each article and the influence relationships between articles . experimental results on corpora from two well-known computer science conferences are used to illustrate and validate the proposed approach .

toward construction of spoken dialogue system that evokes users spontaneous backchannels
this paper addresses a first step toward a spoken dialogue system that evokes users spontaneous backchannels . we construct an hmm-based dialogue-style text-to-speech ( tts ) system that generates human-like cues that evoke users backchannels . a spoken dialogue system for information navigation was implemented and the tts was evaluated in terms of evoked user backchannels . we conducted user experiments and demonstrated that the user backchannels evoked by our tts are more informative for the system in detecting users feelings than those by conventional reading-style tts .

alfara del patriarca ( valencia ) , spain
this paper describes the system presented for the english-spanish translation task by the collaboration between ceu-uch and upv for 2011 wmt . a comparison of independent phrase-based translation models interpolation for each available training corpora were tested , giving an improvement of 0.4 bleu points over the baseline . output n -best lists were rescored via a target neural network language model . an improvement of one bleu point over the baseline was obtained adding the two features , giving 31.5 bleu and 57.9 ter for the primary system , computed over lowercased and detokenized outputs . the system was positioned second in the final ranking .

a simple domain-independent probabilistic approach to generation
we present a simple , robust generation system which performs content selection and surface realization in a unified , domain-independent framework . in our approach , we break up the end-to-end generation process into a sequence of local decisions , arranged hierarchically and each trained discriminatively . we deployed our system in three different domainsrobocup sportscasting , technical weather forecasts , and common weather forecasts , obtaining results comparable to state-ofthe-art domain-specific systems both in terms of bleu scores and human evaluation .

semantic parsing with relaxed hybrid trees information systems technology and design
we propose a novel model for parsing natural language sentences into their formal semantic representations . the model is able to perform integrated lexicon acquisition and semantic parsing , mapping each atomic element in a complete semantic representation to a contiguous word sequence in the input sentence in a recursive manner , where certain overlappings amongst such word sequences are allowed . it defines distributions over the novel relaxed hybrid tree structures which jointly represent both sentences and semantics . such structures allow tractable dynamic programming algorithms to be developed for efficient learning and decoding . trained under a discriminative setting , our model is able to incorporate a rich set of features where certain unbounded long-distance dependencies can be captured in a principled manner . we demonstrate through experiments that by exploiting a large collection of simple features , our model is shown to be competitive to previous works and achieves state-of-theart performance on standard benchmark data across four different languages . the system and code can be downloaded from http : //statnlp.org/research/sp/ .

interactive feature space construction using semantic information
specifying an appropriate feature space is an important aspect of achieving good performance when designing systems based upon learned classifiers . effectively incorporating information regarding semantically related words into the feature space is known to produce robust , accurate classifiers and is one apparent motivation for efforts to automatically generate such resources . however , naive incorporation of this semantic information may result in poor performance due to increased ambiguity . to overcome this limitation , we introduce the interactive feature space construction protocol , where the learner identifies inadequate regions of the feature space and in coordination with a domain expert adds descriptiveness through existing semantic resources . we demonstrate effectiveness on an entity and relation extraction system including both performance improvements and robustness to reductions in annotated data .

effectiveness and efficiency of open relation extraction filipe mesquita jordan schmidek denilson barbosa
a large number of open relation extraction approaches have been proposed recently , covering a wide range of nlp machinery , from shallow ( e.g. , part-of-speech tagging ) to deep ( e.g. , semantic role labelingsrl ) . a natural question then is what is the tradeoff between nlp depth ( and associated computational cost ) versus effectiveness . this paper presents a fair and objective experimental comparison of 8 state-of-the-art approaches over 5 different datasets , and sheds some light on the issue . the paper also describes a novel method , exemplar , which adapts ideas from srl to less costly nlp machinery , resulting in substantial gains both in efficiency and effectiveness , over binary and n-ary relation extraction tasks .

a structured vector space model for word meaning in context
we address the task of computing vector space representations for the meaning of word occurrences , which can vary widely according to context . this task is a crucial step towards a robust , vector-based compositional account of sentence meaning . we argue that existing models for this task do not take syntactic structure sufficiently into account . we present a novel structured vector space model that addresses these issues by incorporating the selectional preferences for words argument positions . this makes it possible to integrate syntax into the computation of word meaning in context . in addition , the model performs at and above the state of the art for modeling the contextual adequacy of paraphrases .

challenges of cheap resource creation for morphological tagging
we describe the challenges of resource creation for a resource-light system for morphological tagging of fusional languages ( feldman and hana , 2010 ) . the constraints on resources ( time , expertise , and money ) introduce challenges that are not present in development of morphological tools and corpora in the usual , resource intensive way .

pattern abstraction and term similarity for word sense disambiguation
this paper summarizes irsts participation in senseval-3 . we participated both in the english allwords task and in some lexical sample tasks ( english , basque , catalan , italian , spanish ) . we followed two perspectives . on one hand , for the allwords task , we tried to refine the domain driven disambiguation that we presented at senseval-2 . the refinements consist of both exploiting a new technique ( domain relevance estimation ) for domain detection in texts , and experimenting with the use of latent semantic analysis to avoid reliance on manually annotated domain resources ( e.g . wordnet domains ) . on the other hand , for the lexical sample tasks , we explored the direction of pattern abstraction and we demonstrated the feasibility of leveraging external knowledge using kernel methods .

unsupervised ontology induction from text
extracting knowledge from unstructured text is a long-standing goal of nlp . although learning approaches to many of its subtasks have been developed ( e.g. , parsing , taxonomy induction , information extraction ) , all end-to-end solutions to date require heavy supervision and/or manual engineering , limiting their scope and scalability . we present ontousp , a system that induces and populates a probabilistic ontology using only dependency-parsed text as input . ontousp builds on the usp unsupervised semantic parser by jointly forming isa and is-part hierarchies of lambda-form clusters . the isa hierarchy allows more general knowledge to be learned , and the use of smoothing for parameter estimation . we evaluate ontousp by using it to extract a knowledge base from biomedical abstracts and answer questions . ontousp improves on the recall of usp by 47 % and greatly outperforms previous state-of-the-art approaches .

sense and similarity : a study of sense-level similarity measures and torsten zesch
in this paper , we investigate the difference between word and sense similarity measures and present means to convert a state-of-the-art word similarity measure into a sense similarity measure . in order to evaluate the new measure , we create a special sense similarity dataset and re-rate an existing word similarity dataset using two different sense inventories from wordnet and wikipedia . we discover that word-level measures were not able to differentiate between different senses of one word , while sense-level measures actually increase correlation when shifting to sense similarities . sense-level similarity measures improve when evaluated with a re-rated sense-aware gold standard , while correlation with word-level similarity measures decreases .

guessing parts-of-speech of unknown words using global information
in this paper , we present a method for guessing pos tags of unknown words using local and global information . although many existing methods use only local information ( i.e . limited window size or intra-sentential features ) , global information ( extra-sentential features ) provides valuable clues for predicting pos tags of unknown words . we propose a probabilistic model for pos guessing of unknown words using global information as well as local information , and estimate its parameters using gibbs sampling . we also attempt to apply the model to semisupervised learning , and conduct experiments on multiple corpora .

xists xml in speech technology systems
this paper describes the use of xml in three generic interacting speech technologysystems . the first , a phonological syllable recognition system , generates feature-based finite-state automaton representations of phonotactic constraints in xml . it employs axioms of event logic to interpret multilinear representations of speech utterances and outputs candidate syllables to the second system , an xml syllable lexicon . this system enables users to generate their own lexicons and its default lexicon is used to accept or reject the candidate syllables output by the speech recognition system . furthermore its xml representation facilitates its use by the third system which generates additional lexicons , based on different feature sets , by means of a transduction process . the applicability of these alternative feature sets in the generation of synthetic speech can then be tested using these new lexicons .

lexically-based terminology structuring : some inherent limits
terminology structuring has been the subject of much work in the context of terms extracted from corpora : given a set of terms , obtained from an existing resource or extracted from a corpus , identifying hierarchical ( or other types of ) relations between these terms . the present paper focusses on terminology structuring by lexical methods , which match terms on the basis on their content words , taking morphological variants into account . experiments are done on a flat list of terms obtained from an originally hierarchically-structured terminology : the french version of the us national library of medicine mesh thesaurus . we compare the lexically-induced relations with the original mesh relations : after a quantitative evaluation of their congruence through recall and precision metrics , we perform a qualitative , human analysis of the new relations not present in the mesh . this analysis shows , on the one hand , the limits of the lexical structuring method . on the other hand , it also reveals some specific structuring choices and naming conventions made by the mesh designers , and emphasizes ontological commitments that can not be left to automatic structuring .

generating quantifiers and negation to explain homework testing
we describe prograder , a software package for automatic checking of requirements for programming homework assignments . prograder lets instructors specify requirements in natural language as well as explains grading results to students in natural language . it does so using a grammar that generates as well as parses to translate between a small fragment of english and a first-order logical specification language that can be executed directly in python . this execution embodies multiple semanticsboth to check the requirement and to search for evidence that proves or disproves the requirement . such a checker needs to interpret and generate sentences containing quantifiers and negation . to handle quantifier and negation scope , we systematically simulate continuation grammars using record structures in the grammatical framework .

towards building a competitive opinion summarization system :
this paper presents an overview of our participation in the tac 2008 opinion pilot summarization task , as well as the proposed and evaluated post-competition improvements . we first describe our opinion summarization system and the results obtained . further on , we identify the systems weak points and suggest several improvements , focused both on information content , as well as linguistic and readability aspects . we obtain encouraging results , especially as far as fmeasure is concerned , outperforming the competition results by approximately 80 % .

collective content selection for concept-to-text generation
a content selection component determines which information should be conveyed in the output of a natural language generation system . we present an efficient method for automatically learning content selection rules from a corpus and its related database . our modeling framework treats content selection as a collective classification problem , thus allowing us to capture contextual dependencies between input items . experiments in a sports domain demonstrate that this approach achieves a substantial improvement over context-agnostic methods .

integrating shallow linguistic processing into a unication based spanish grammar
this paper describes to what extent deep processing may benet from shallow processing techniques and it presents a nlp system which integrates a linguistic pos tagger and chunker as a preprocessing module of a broadcoverage unicationbased grammar of spanish . experiments show that the efficiency of the overall analysis improves signicantly and that our sys tem also provides robustness to the linguistic processing , while maintaining both the accuracy and the precision of the grammar

relation acquisition using word classes and partial patterns chikara hashimoto ichiro yamada jong hoon oh istvan varga yulan yan
this paper proposes a semi-supervised relation acquisition method that does not rely on extraction patterns ( e.g . x causes y for causal relations ) but instead learns a combination of indirect evidence for the target relation semantic word classes and partial patterns . this method can extract long tail instances of semantic relations like causality from rare and complex expressions in a large japaneseweb corpus in extreme cases , patterns that occur only once in the entire corpus . such patterns are beyond the reach of current pattern based methods . we show that our method performs on par with state-of-the-art pattern based methods , and maintains a reasonable level of accuracy even for instances acquired from infrequent patterns . this ability to acquire long tail instances is crucial for risk management and innovation , where an exhaustive database of high-level semantic relations like causation is of vital importance .

redundancy-based correction of automatically extracted facts
the accuracy of event extraction is limited by a number of complicating factors , with errors compounded at all sages inside the information extraction pipeline . in this paper , we present methods for recovering automatically from errors committed in the pipeline processing . recovery is achieved via post-processing facts aggregated over a large collection of documents , and suggesting corrections based on evidence external to the document . a further improvement is derived from propagating multiple , locally non-best slot fills through the pipeline . evaluation shows that the global analysis is over 10 times more likely to suggest valid corrections to the local-only analysis than it is to suggest erroneous ones . this yields a substantial overall gain , with no supervised training .

statistical acquisition of content selection rules for natural language generation
a natural language generation system produces text using as input semantic data . one of its very first tasks is to decide which pieces of information to convey in the output . this task , called content selection , is quite domain dependent , requiring considerable re-engineering to transport the system from one scenario to another . in this paper , we present a method to acquire content selection rules automatically from a corpus of text and associated semantics . our proposed technique was evaluated by comparing its output with information selected by human authors in unseen texts , where we were able to filter half the input data set without loss of recall .

syntactic and semantic dependencies in multiple languages
for the 11th straight year , the conference on computational natural language learning has been accompanied by a shared task whose purpose is to promote natural language processing applications and evaluate them in a standard setting . in 2009 , the shared task was dedicated to the joint parsing of syntactic and semantic dependencies in multiple languages . this shared task combines the shared tasks of the previous five years under a unique dependency-based formalism similar to the 2008 task . in this paper , we define the shared task , describe how the data sets were created and show their quantitative properties , report the results and summarize the approaches of the participating systems .

wordform- and class-based prediction of the components of german nominal compounds
in word prediction systems for augmentative and alternative communication ( aac ) , productive wordformation processes such as compounding pose a serious problem . we present a model that predicts german nominal compounds by splitting them into their modifier and head components , instead of trying to predict them as a whole . the model is improved further by the use of class-based modifierhead bigrams constructed using semantic classes automatically extracted from a corpus . the evaluation shows that the split compound model with class bigrams leads to an improvement in keystroke savings of more than 15 % over a no split compound baseline model . we also present preliminary results obtained with a word prediction model integrating compound and simple word prediction .

discriminative training of a neural network statistical parser
discriminative methods have shown significant improvements over traditional generative methods in many machine learning applications , but there has been difficulty in extending them to natural language parsing . one problem is that much of the work on discriminative methods conflates changes to the learning method with changes to the parameterization of the problem . we show how a parser can be trained with a discriminative learning method while still parameterizing the problem according to a generative probability model . we present three methods for training a neural network to estimate the probabilities for a statistical parser , one generative , one discriminative , and one where the probability model is generative but the training criteria is discriminative . the latter model outperforms the previous two , achieving state-ofthe-art levels of performance ( 90.1 % f-measure on constituents ) .

the fourth international chinese language processing bakeoff : chinese word segmentation , named entity recognition and chinese pos tagging
the fourth international chinese language processing bakeoff was held in 2007 to assess the state of the art in three important tasks : chinese word segmentation , named entity recognition and chinese pos tagging . twenty-eight groups submitted result sets in the three tasks across two tracks and a total of seven corpora . strong results have been found in all the tasks as well as continuing challenges .

deverbal compound noun analysis based on lexical conceptual structure koichi takeuchi kyo kageura
this paper proposes a principled approach for analysis of semantic relations between constituents in compound nouns based on lexical semantic structure . one of the difficulties of compound noun analysis is that the mechanisms governing the decision system of semantic relations and the representation method of semantic relations associated with lexical and contextual meaning are not obvious . the aim of our research is to clarify how lexical semantics contribute to the relations in compound nouns since such nouns are very productive and are supposed to be governed by systematic mechanisms . the results of applying our approach to the analysis of noun-deverbal compounds in japanese and english show that lexical conceptual structure contributes to the restrictional rules in compounds .

a two-stage approach for computing associative responses to a set of
this paper describes the system submitted by the iiit-h team for the cogalex-2014 shared task on multiword association . the task involves generating a ranked list of responses to a set of stimulus words . the two-stage approach combines the strength of neural network based word embeddings and frequency based association measures . the system achieves an accuracy of 34.9 % over the test set .

inducing german semantic verb classes from purely syntactic subcategorisation information
the paper describes the application of kmeans , a standard clustering technique , to the task of inducing semantic classes for german verbs . using probability distributions over verb subcategorisation frames , we obtained an intuitively plausible clustering of 57 verbs into 14 classes . the automatic clustering was evaluated against independently motivated , handconstructed semantic verb classes . a series of post-hoc cluster analyses explored the influence of specific frames and frame groups on the coherence of the verb classes , and supported the tight connection between the syntactic behaviour of the verbs and their lexical meaning components .

extrinsic parse selection
this paper reports on one aspect of locutus , a natural language interface to databases ( nlidb ) which uses the output of a highprecision broad-coverage grammar to build semantic representations and ultimately sql queries . rather than selecting just a subset of the parses provided by the grammar to use in further processing , locutus uses all of them . if the meaning of a parse does not conform to the semantic domain of the database , no query is built for it . thus , intended parses are chosen extrinsically . the parser gives an average of 3.01 parses to the sentences in the geoquery250 corpus . locutus generates an average of 1.02 queries per sentence for this corpus , all of them correct .

argviz : interactive visualization of topic dynamics in multi-party
we introduce an efficient , interactive frameworkargvizfor experts to analyze the dynamic topical structure of multi-party conversations . users inject their needs , expertise , and insights into models via iterative topic refinement . the refined topics feed into a segmentation model , whose outputs are shown to users via multiple coordinated views .

broadcast audio and video bimodal corpus exploitation and
the main purpose of this paper is the exploitation and application of an audio and video bimodal corpus of the chinese language in broadcasting . it deals with the designation of the size and structure of speech samples according to radio and television program features . secondly , it discusses annotation method of broadcast speech with achievements made and suggested future improvements . finally , it presents an attempt to describe the distribution of annotated items in our corpus .

japanese dependency analysis using the ancestor-descendant relation akihiro tamura hiroya takamura manabu okumura
we propose a novel method for japanese dependency analysis , which is usually reduced to the construction of a dependency tree . in deterministic approaches to this task , dependency trees are constructed by series of actions of attaching a bunsetsu chunk to one of the nodes in the tree being constructed . conventional techniques select the node based on whether the new bunsetsu chunk and each node in the trees are in a parent-child relation or not . however , tree structures include relations between two nodes other than the parent-child relation . therefore , we use ancestor-descendant relations in addition to parent-child relations , so that the added redundancy helps errors be corrected . experimental results show that the proposed method achieves higher accuracy .

local constraints on sentence markers and focus in somali
we present a computationally tractable account of the interactions between sentence markers and focus marking in somali . somali , as a cushitic language , has a basic pattern wherein a small core clause is preceded , and in some cases followed by , a set of topics , which provide sceneseting information against which the core is interpreted . some topics appear to carry a focus marker , indicating that they are particularly salient . we will outline a computationally tractable grammar for somali in which focus marking emerges naturally from a consideration of the use of a range of sentence markers .

semantic and pragmatic presupposition in discourse representation
this paper investigates semantic and pragmatic presupposition in discourse representation theory ( drt ) and enhances the pragmatic perspective of presupposition in drt . in doing so , it draws attention to the need to account for agent presupposition ( i.e . both speaker and hearer presupposition ) when dealing with pragmatic presupposition . furthermore , this paper links this pragmatic conception of presupposition with the semantic one ( sentence presupposition ) through using information checks which agents are hypothesized to employ when making and receiving utterances.1

automatic question answering : beyond the factoid
in this paper we describe and evaluate a question answering system that goes beyond answering factoid questions . we focus on faqlike questions and answers , and build our system around a noisy-channel architecture which exploits both a language model for answers and a transformation model for answer/question terms , trained on a corpus of 1 million question/answer pairs collected from the web .

case markers and morphology : addressing the crux of the fluency problem in english-hindi smt
we report in this paper our work on accurately generating case markers and suffixes in english-to-hindi smt . hindi is a relatively free word-order language , and makes use of a comparatively richer set of case markers and morphological suffixes for correct meaning representation . from our experience of large-scale english-hindi mt , we are convinced that fluency and fidelity in the hindi output get an order of magnitude facelift if accurate case markers and suffixes are produced . now , the moot question is : what entity on the english side encodes the information contained in case markers and suffixes on the hindi side our studies of correspondences in the two languages show that case markers and suffixes in hindi are predominantly determined by the combination of suffixes and semantic relations on the english side . we , therefore , augment the aligned corpus of the two languages , with the correspondence of english suffixes and semantic relations with hindi suffixes and case markers . our results on 400 test sentences , translated using an smt system trained on around 13000 parallel sentences , show that suffix + semantic relation case marker/suffix is a very useful translation factor , in the sense of making a significant difference to output quality as indicated by subjective evaluation as well as bleu scores .

what : an xslt-based infrastructure for the integration of natural language processing components
the idea of the whiteboard project is to integrate deep and shallow natural language processing components in order to benefit from their synergy . the project came up with the first fully integrated hybrid system consisting of a fast hpsg parser that utilizes tokenization , pos , morphology , lexical , named entity , phrase chunk and ( for german ) topological sentence field analyses from shallow components . this integration increases robustness , directs the search space and hence reduces processing time of the deep parser . in this paper , we focus on one of the central integration facilities , the xslt-based whiteboard annotation transformer ( what ) , report on the benefits of xslt-based nlp component integration , and present examples of xsl transformation of shallow and deep annotations used in the integrated architecture . the infrastructure is open , portable and well suited for , but not restricted to the development of hybrid nlp architectures as well as nlp applications .

automatic partial parsing rule acquisition using decision tree induction
the grammar involved automatically . in this paper , we present a method for automatically extracting partial parsing rules from a tree-annotated corpus using decision tree induction . we define the partial parsing rules as those that can decide the structure of a substring in an input sentence deterministically . this decision can be considered as a classification ; as such , for a substring in an input sentence , a proper structure is chosen among the structures occurred in the corpus . for the classification , we use decision tree induction , and induce partial parsing rules from the decision tree . the acquired grammar is similar to a phrase structure grammar , with contextual and lexical information , but it allows building structures of depth one or more . our experiments showed that the proposed partial parser using the automatically extracted rules is not only accurate and efficient , but also achieves reasonable coverage for korean .

continuous space language models for statistical machine translation
statistical machine translation systems are based on one or more translation models and a language model of the target language . while many different translation models and phrase extraction algorithms have been proposed , a standard word n-gram back-off language model is used in most systems . in this work , we propose to use a new statistical language model that is based on a continuous representation of the words in the vocabulary . a neural network is used to perform the projection and the probability estimation . we consider the translation of european parliament speeches . this task is part of an international evaluation organized by the tc-star project in 2006. the proposed method achieves consistent improvements in the bleu score on the development and test data . we also present algorithms to improve the estimation of the language model probabilities when splitting long sentences into shorter chunks .

scaling web-based acquisition of entailment relations
paraphrase recognition is a critical step for natural language interpretation . accordingly , many nlp applications would benefit from high coverage knowledge bases of paraphrases . however , the scalability of state-of-the-art paraphrase acquisition approaches is still limited . we present a fully unsupervised learning algorithm for web-based extraction of entailment relations , an extended model of paraphrases . we focus on increased scalability and generality with respect to prior work , eventually aiming at a full scale knowledge base . our current implementation of the algorithm takes as its input a verb lexicon and for each verb searches the web for related syntactic entailment templates . experiments show promising results with respect to the ultimate goal , achieving much better scalability than prior web-based methods .

on grammar documentation
this paper addresses the documentation of large-scale grammars.1 we argue that grammar implementation differs from ordinary software programs : the concept of modules , as known from software engineering , can not be transferred directly to grammar implementations , due to grammar-specific properties . these properties also put special constraints on the form of grammar documentation . to fulfill these constraints , we propose an xml-based , grammar-specific documentation technique .

finding your inner-annotator : an experiment in annotator independence for rating discourse coherence quality in essays jill burstein swapna somasundaran martin chodorow
an experimental annotation method is described , showing promise for a subjective labeling task discourse coherence quality of essays . annotators developed personal protocols , reducing front-end resources : protocol development and annotator training . substantial inter-annotator agreement was achieved for a 4-point scale . correlational analyses revealed how unique linguistic phenomena were considered in annotation . systems trained with the annotator data demonstrated utility of the data .

syntax-based statistical machine translation
in this paper , we propose a new syntaxbased machine translation ( mt ) approach based on reducing the mt task to a treelabeling task , which is further decomposed into a sequence of simple decisions for which discriminative classifiers can be trained . the approach is very flexible and we believe that it is particularly well-suited for exploiting the linguistic knowledge encoded in deep grammars whenever possible , while at the same time taking advantage of data-based techniques that have proven a powerful basis for mt , as recent advances in statistical mt show . a full system using the lexical-functional grammar ( lfg ) parsing system xle and the grammars from the parallel grammar development project ( pargram ; ( butt et al. , 2002 ) ) has been implemented , and we present preliminary results on english-togerman translation with a tree-labeling system trained on a small subsection of the europarl corpus .

using sequence kernels to identify opinion entities in urdu suny at buffalo , ny suny at buffalo , ny
automatic extraction of opinion holders and targets ( together referred to as opinion entities ) is an important subtask of sentiment analysis . in this work , we attempt to accurately extract opinion entities from urdu newswire . due to the lack of resources required for training role labelers and dependency parsers ( as in english ) for urdu , a more robust approach based on ( i ) generating candidate word sequences corresponding to opinion entities , and ( ii ) subsequently disambiguating these sequences as opinion holders or targets is presented . detecting the boundaries of such candidate sequences in urdu is very different than in english since in urdu , grammatical categories such as tense , gender and case are captured in word inflections . in this work , we exploit the morphological inflections associated with nouns and verbs to correctly identify sequence boundaries . different levels of information that capture context are encoded to train standard linear and sequence kernels . to this end the best performance obtained for opinion entity detection for urdu sentiment analysis is 58.06 % f-score using sequence kernels and 61.55 % f-score using a combination of sequence and linear kernels .

whose thumb is it anyway classifying author personality from weblog text
we report initial results on the relatively novel task of automatic classification of author personality . using a corpus of personal weblogs , or blogs , we investigate the accuracy that can be achieved when classifying authors on four important personality traits . we explore both binary and multiple classification , using differing sets of n-gram features . results are promising for all four traits examined .

a cognitive-based annotation system for emotion computing
emotion computing is very important for expressive information extraction . in this paper , we provide a robust and versatile emotion annotation scheme based on cognitive emotion theories , which not only can annotate both explicit and implicit emotion expressions , but also can encode different levels of emotion information for the given emotion content . in addition , motivated by a cognitive framework , an automatic emotion annotation system is developed , and large and comparatively high-quality emotion corpora are created for emotion computing , one in chinese and the other in english . such an annotation system can be easily adapted for different kinds of emotion applications and be extended to other languages .

influence of parser choice on dependency-based mt
accuracy of dependency parsers is one of the key factors limiting the quality of dependencybased machine translation . this paper deals with the influence of various dependency parsing approaches ( and also different training data size ) on the overall performance of an english-to-czech dependency-based statistical translation system implemented in the treex framework . we also study the relationship between parsing accuracy in terms of unlabeled attachment score and machine translation quality in terms of bleu .

a corpus of human-written summaries of line graphs
we describe a corpus of human-written english language summaries of line graphs . this corpus is intended to help develop a system to automatically generate summaries capturing the most salient information conveyed by line graphs in popular media , as well as to evaluate the output of such a system .

chunking with support vector machines
we apply support vector machines ( svms ) to identify english base phrases ( chunks ) . svms are known to achieve high generalization performance even with input data of high dimensional feature spaces . furthermore , by the kernel principle , svms can carry out training with smaller computational overhead independent of their dimensionality . we apply weighted voting of 8 svmsbased systems trained with distinct chunk representations . experimental results show that our approach achieves higher accuracy than previous approaches .

chunking using conditional random fields in korean texts
random fields ( crfs ) , a recently introduced probabilistic model for labeling and segmenting sequence of data . in agglutinative languages such as korean and japanese , a rule-based chunking method is predominantly used for its simplicity and efficiency . a hybrid of a rule-based and machine learning method was also proposed to handle exceptional cases of the rules . in this paper , we present how crfs can be applied to the task of chunking in korean texts . experiments using the step 2000 dataset show that the proposed method significantly improves the performance as well as outperforms previous systems .

deep syntactic annotation : tectogrammatical representation and beyond and applied linguistics
the requirements of the depth and precision of annotation vary for different intended uses of the corpus but it has been commonly accepted nowadays that the standard annotations of surface structure are only the first steps in a more ambitious research program , aiming at a creation of advanced resources for most different systems of natural language processing and for testing and further enrichment of linguistic and computational theories . among the several possible directions in which we believe the standard annotation systems should go ( and in some cases already attempt to go ) beyond the pos tagging or shallow syntactic annotations , the following four are characterized in the present contribution : ( i ) predicateargument representation of the underlying syntactic relations as basically corresponding to a rooted tree that can be univocally linearized , ( ii ) the inclusion of the information structure using very simple means ( the left-to-right order of the nodes and three attribute values ) , ( iii ) relating this underlying structure ( rendering the linguistic meaning , i.e . the semantically relevant counterparts of the grammatical means of expression ) to certain central aspects of referential semantics ( reference assignment and coreferential relations ) , and ( iv ) handling of word sense disambiguation . the first three issues are documented in the present paper on the basis of our experience with the development of the structure and scenario of the prague dependency treebank which provides for syntactico-semantic annotation of large text segments from the czech national corpus and which is based on a solid theoretical framework .

bilingual terminology acquisition from comparable corpora and phrasal translation to cross-language information retrieval
the present paper will seek to present an approach to bilingual lexicon extraction from non-aligned comparable corpora , phrasal translation as well as evaluations on cross-language information retrieval . a two-stages translation model is proposed for the acquisition of bilingual terminology from comparable corpora , disambiguation and selection of best translation alternatives according to their linguistics-based knowledge . different rescoring techniques are proposed and evaluated in order to select best phrasal translation alternatives . results demonstrate that the proposed translation model yields better translations and retrieval effectiveness could be achieved across japaneseenglish language pair .

unsupervised verb inference from nouns crossing root boundary soon gill hong daejeon , republic of korea daejeon , republic of korea mun yong yi daejeon , republic of korea
inference about whether a word in one text has similar meaning to another word in the other text is an essential task in order to understand whether two texts have similar meaning . however , this inference becomes difficult especially when two words do not share a lexical root , do not have the same argument structure , or do not have the same part-of-speech . this paper presents an unsupervised approach for inferring verbs from nouns along with a new online resource predic ( predicate dictionary ) that contains verbs inferred from nouns sharing similar concepts but not the root . the verbs in predic are categorized into three groups , enabling applications to target precision-oriented , recall-oriented , or harmony-oriented results as needed . the experiment results show that the proposed unsupervised approach performs similar to or better than wordnet and nomlex . furthermore , a new domain-verb association measure is presented to show the association relationships between inferred verbs and domains to which the verbs are possibly applied .

vietnamese lexical development for word segmentation
as the web content becomes more accessible to the vietnamese community across the globe , there is a need to process vietnamese query texts properly to find relevant information . the recent deployment of a vietnamese translation tool on a well-known search engine justifies its importance in gaining popularity with the world wide web . there are still problems in the translation and retrieval of vietnamese language as its word recognition is not fully addressed . in this paper we introduce a semi-supervised approach in building a general scalable web corpus for vietnamese using search engine to facilitate the word segmentation process . moreover , we also propose a segmentation algorithm which recognizes effectively out-ofvocabulary ( oov ) words . the result indicates that our solution is scalable and can be applied for real time translation program and other linguistic applications . this work is here is a continuation of the work of nguyen d. ( 2008 ) .

feature-based selection of dependency paths in ad hoc information retrieval
techniques that compare short text segments using dependency paths ( or simply , paths ) appear in a wide range of automated language processing applications including question answering ( qa ) . however , few models in ad hoc information retrieval ( ir ) use paths for document ranking due to the prohibitive cost of parsing a retrieval collection . in this paper , we introduce a flexible notion of paths that describe chains of words on a dependency path . these chains , or catenae , are readily applied in standard ir models . informative catenae are selected using supervised machine learning with linguistically informed features and compared to both non-linguistic terms and catenae selected heuristically with filters derived from work on paths . automatically selected catenae of 1-2 words deliver significant performance gains on three trec collections .

on the predictability of human assessment : when matrix completion meets nlp evaluation
this paper tackles the problem of collecting reliable human assessments . we show that knowing multiple scores for each example instead of a single score results in a more reliable estimation of a system quality . to reduce the cost of collecting these multiple ratings , we propose to use matrix completion techniques to predict some scores knowing only scores of other judges and some common ratings . even if prediction performance is pretty low , decisions made using the predicted score proved to be more reliable than decision based on a single rating of each example .

towards relational pomdps for adaptive dialogue management
open-ended spoken interactions are typically characterised by both structural complexity and high levels of uncertainty , making dialogue management in such settings a particularly challenging problem . traditional approaches have focused on providing theoretical accounts for either the uncertainty or the complexity of spoken dialogue , but rarely considered the two issues simultaneously . this paper describes ongoing work on a new approach to dialogue management which attempts to fill this gap . we represent the interaction as a partially observable markov decision process ( pomdp ) over a rich state space incorporating both dialogue , user , and environment models . the tractability of the resulting pomdp can be preserved using a mechanism for dynamically constraining the action space based on prior knowledge over locally relevant dialogue structures . these constraints are encoded in a small set of general rules expressed as a markov logic network . the first-order expressivity of markov logic enables us to leverage the rich relational structure of the problem and efficiently abstract over large regions of the state and action spaces .

a semi-supervised approach for natural language call routing
natural language call routing remains a complex and challenging research area in machine intelligence and language understanding . this paper is in the area of classifying user utterances into different categories . the focus is on design of algorithm that combines supervised and unsupervised learning models in order to improve classification quality . we have shown that the proposed approach is able to outperform existing methods on a large dataset and do not require morphological and stop-word filtering . in this paper we present a new formula for term relevance estimation , which is a modification of fuzzy rules relevance estimation for fuzzy classifier . using this formula and only 300 frequent words for each class , we achieve an accuracy rate of 85.55 % on the database excluding the garbage class ( it includes utterances that can not be assigned to any useful class or that can be assigned to more than one class ) . dividing the garbage class into the set of subclasses by agglomerative hierarchical clustering we achieve about 9 % improvement of accuracy rate on the whole database .

multilingual resources for entity extraction linguistic data consortium linguistic data consortium linguistic data consortium
progress in human language technology requires increasing amounts of data and annotation in a growing variety of languages . research in named entity extraction is no exception . linguistic data consortium is creating annotated corpora to support information extraction in english , chinese , arabic , and other languages for a variety of us governmentsponsored programs . this paper covers the scope of annotation and research tasks within these programs , describes some of the challenges of multilingual corpus development for entity extraction , and concludes with a description of the corpora developed to support this research .

using semantic roles to improve question answering spoken language systems
shallow semantic parsing , the automatic identification and labeling of sentential constituents , has recently received much attention . our work examines whether semantic role information is beneficial to question answering . we introduce a general framework for answer extraction which exploits semantic role annotations in the framenet paradigm . we view semantic role assignment as an optimization problem in a bipartite graph and answer extraction as an instance of graph matching . experimental results on the trec datasets demonstrate improvements over state-of-the-art models .

online statistics for a unification-based dialogue parser
we describe a method for augmenting unification-based deep parsing with statistical methods . we extend and adapt the bikel parser , which uses head-driven lexical statistics , to dialogue . we show that our augmented parser produces significantly fewer constituents than the baseline system and achieves comparable bracketing accuracy , even yielding slight improvements for longer sentences .

real-time decision detection in multi-party dialogue matthew frampton , jia huang , trung huu bui and stanley peters
we describe a process for automatically detecting decision-making sub-dialogues in multi-party , human-human meetings in real-time . our basic approach to decision detection involves distinguishing between different utterance types based on the roles that they play in the formulation of a decision . in this paper , we describe how this approach can be implemented in real-time , and show that the resulting systems performance compares well with other detectors , including an off-line version .

a smorgasbord of features for automatic mt evaluation
this document describes the approach by the nlp group at the technical university of catalonia ( upc-lsi ) , for the shared task on automatic evaluation of machine translation at the acl 2008 third smt workshop .

post-hoc manipulations of vector space models
in this paper , we introduce several vector space manipulation methods that are applied to trained vector space models in a post-hoc fashion , and present an application of these techniques in semantic role labeling for finnish and english . specifically , we show that the vectors can be circularly shifted to encode syntactic information and subsequently averaged to produce representations of predicate senses and arguments . further , we show that it is possible to effectively learn a linear transformation between the vector representations of predicates and their arguments , within the same vector space .

automatic named entity pre-annotation for out-of-domain human annotation sophie rosset , cyril grouin , thomas lavergne , , mohamed ben jannet , , ,
automatic pre-annotation is often used to improve human annotation speed and accuracy . we address here out-of-domain named entity annotation , and examine whether automatic pre-annotation is still beneficial in this setting . our study design includes two different corpora , three pre-annotation schemes linked to two annotation levels , both expert and novice annotators , a questionnaire-based subjective assessment and a corpus-based quantitative assessment . we observe that preannotation helps in all cases , both for speed and for accuracy , and that the subjective assessment of the annotators does not always match the actual benefits measured in the annotation outcome .

link type based pre-cluster pair model for coreference resolution
this paper presents our participation in the conll-2011 shared task , modeling unrestricted coreference in ontonotes . coreference resolution , as a difficult and challenging problem in nlp , has attracted a lot of attention in the research community for a long time . its objective is to determine whether two mentions in a piece of text refer to the same entity . in our system , we implement mention detection and coreference resolution seperately . for mention detection , a simple classification based method combined with several effective features is developed . for coreference resolution , we propose a link type based pre-cluster pair model . in this model , pre-clustering of all the mentions in a single document is first performed . then for different link types , different classification models are trained to determine wheter two pre-clusters refer to the same entity . the final clustering results are generated by closest-first clustering method . official test results for closed track reveal that our method gives a muc f-score of 59.95 % , a b-cubed f-score of 63.23 % , and a ceaf f-score of 35.96 % on development dataset .

pos tagger combinations on hungarian text
in this paper we will briefly survey the key results achieved so far in hungarian pos tagging and show how classifier combination techniques can aid the pos taggers . methods are evaluated on a manually annotated corpus containing 1.2 million words . pos tagger tests were performed on single-domain , multiple domain and cross-domain test settings , and , to improve the accuracy of the taggers , various combination rules were implemented . the results indicate that combination schemas ( like the boosting algorithm ) are promising tools which can significantly degrade the classification errors , and produce a more effective tagger application .

non-expert evaluation of summarization systems is risky
we provide evidence that intrinsic evaluation of summaries using amazons mechanical turk is quite difficult . experiments mirroring evaluation at the text analysis conferences summarization track show that nonexpert judges are not able to recover system rankings derived from experts .

computing logical form on regulatory texts
the computation of logical form has been proposed as an intermediate step in the translation of sentences to logic . logical form encodes the resolution of scope ambiguities . in this paper , we describe experiments on a modestsized corpus of regulation annotated with a novel variant of logical form , called abstract syntax trees ( asts ) . the main step in computing asts is to order scope-taking operators . a learning model for ranking is adapted for this ordering . we design features by studying the problem of comparing the scope of one operator to another . the scope comparisons are used to compute asts , with an f-score of 90.6 % on the set of ordering decisons .

reducing weight undertraining in structured discriminative learning
discriminative probabilistic models are very popular in nlp because of the latitude they afford in designing features . but training involves complex trade-offs among weights , which can be dangerous : a few highlyindicative features can swamp the contribution of many individually weaker features , causing their weights to be undertrained . such a model is less robust , for the highly-indicative features may be noisy or missing in the test data . to ameliorate this weight undertraining , we introduce several new feature bagging methods , in which separate models are trained on subsets of the original features , and combined using a mixture model or a product of experts . these methods include the logarithmic opinion pools used by smith et al ( 2005 ) . we evaluate feature bagging on linear-chain conditional random fields for two natural-language tasks . on both tasks , the feature-bagged crf performs better than simply training a single crf on all the features .

sub-sentential paraphrasing by contextual pivot translation
the ability to generate or to recognize paraphrases is key to the vast majority of nlp applications . as correctly exploiting context during translation has been shown to be successful , using context information for paraphrasing could also lead to improved performance . in this article , we adopt the pivot approach based on parallel multilingual corpora proposed by ( bannard and callison-burch , 2005 ) , which finds short paraphrases by finding appropriate pivot phrases in one or several auxiliary languages and back-translating these pivot phrases into the original language . we show how context can be exploited both when attempting to find pivot phrases , and when looking for the most appropriate paraphrase in the original subsentential envelope . this framework allows the use of paraphrasing units ranging from words to large sub-sentential fragments for which context information from the sentence can be successfully exploited . we report experiments on a text revision task , and show that in these experiments our contextual sub-sentential paraphrasing system outperforms a strong baseline system .

soft cross-lingual syntax projection for dependency parsing
this paper proposes a simple yet effective framework of soft cross-lingual syntax projection to transfer syntactic structures from source language to target language using monolingual treebanks and large-scale bilingual parallel text . here , soft means that we only project reliable dependencies to compose high-quality target structures . the projected instances are then used as additional training data to improve the performance of supervised parsers . the major issues for this idea are 1 ) errors from the source-language parser and unsupervised word aligner ; 2 ) intrinsic syntactic non-isomorphism between languages ; 3 ) incomplete parse trees after projection . to handle the first two issues , we propose to use a probabilistic dependency parser trained on the target-language treebank , and prune out unlikely projected dependencies that have low marginal probabilities . to make use of the incomplete projected syntactic structures , we adopt a new learning technique based on ambiguous labelings . for a word that has no head words after projection , we enrich the projected structure with all other words as its candidate heads as long as the newly-added dependency does not cross any projected dependencies . in this way , the syntactic structure of a sentence becomes a parse forest ( ambiguous labels ) instead of a single parse tree . during training , the objective is to maximize the mixed likelihood of manually labeled instances and projected instances with ambiguous labelings . experimental results on benchmark data show that our method significantly outperforms a strong baseline supervised parser and previous syntax projection methods .

combine person name and person identity recognition and document clustering for chinese person name disambiguation
this paper presents the hitsz_cityu system in the cips-sighan bakeoff 2010 task 3 , chinese person name disambiguation . this system incorporates person name string recognition , person identity string recognition and an agglomerative hierarchical clustering for grouping the documents to each identical person . firstly , for the given name index string , three segmentors are applied to segment the sentences having the index string into chinese words , respectively . their outputs are compared and analyzed . an unsupervised clustering is applied here to help the personal name recognition . the document set is then divided into subsets according to each recognized person name string . next , the system identifies/extracts the person identity string from the sentences based on lexicon and heuristic rules . by incorporating the recognized person identity string , person name , organization name and contextual content words as features , an agglomerative hierarchical clustering is applied to group the similar documents in the document subsets to obtain the final person name disambiguation results . evaluations show that the proposed system , which incorporates extraction and clustering technique , achieves encouraging recall and good overall performance .

competitive generative models with structure learning for nlp
in this paper we show that generative models are competitive with and sometimes superior to discriminative models , when both kinds of models are allowed to learn structures that are optimal for discrimination . in particular , we compare bayesian networks and conditional loglinear models on two nlp tasks . we observe that when the structure of the generative model encodes very strong independence assumptions ( a la naive bayes ) , a discriminative model is superior , but when the generative model is allowed to weaken these independence assumptions via learning a more complex structure , it can achieve very similar or better performance than a corresponding discriminative model . in addition , as structure learning for generative models is far more efficient , they may be preferable for some tasks .

looking for trouble and communications technology
this paper presents a method for mining potential troubles or obstacles related to the use of a given object . some example instances of this relation are medicine , side effect and amusement park , height restriction . our acquisition method consists of three steps . first , we use an unsupervised method to collect training samples from web documents . second , a set of expressions generally referring to troubles is acquired by a supervised learning method . finally , the acquired troubles are associated with objects so that each of the resulting pairs consists of an object and a trouble or obstacle in using that object . to show the effectiveness of our method we conducted experiments using a large collection of japanese web documents for acquisition . experimental results show an 85.5 % precision for the top 10,000 acquired troubles , and a 74 % precision for the top 10 % of over 60,000 acquired object-trouble pairs .

automated pyramid scoring of summaries using distributional semantics
the pyramid method for content evaluation of automated summarizers produces scores that are shown to correlate well with manual scores used in educational assessment of students summaries . this motivates the development of a more accurate automated method to compute pyramid scores . of three methods tested here , the one that performs best relies on latent semantics .

incorporating gesture and gaze into multimodal models of
structural information in language is important for obtaining a better understanding of a human communication ( e.g. , sentence segmentation , speaker turns , and topic segmentation ) . human communication involves a variety of multimodal behaviors that signal both propositional content and structure , e.g. , gesture , gaze , and body posture . these non-verbal signals have tight temporal and semantic links to spoken content . in my thesis , i am working on incorporating non-verbal cues into a multimodal model to better predict the structural events to further improve the understanding of human communication . some research results are summarized in this document and my future research plan is described .

a discourse resource for turkish : annotating discourse connectives in the metu corpus
this paper describes first steps towards extending the metu turkish corpus from a sentence-level language resource to a discourse-level resource by annotating its discourse connectives and their arguments . the project is based on the same principles as the penn discourse treebank and is supported by tubitak , the scientific and technological research council of turkey . we first present the goals of the project and the metu turkish corpus . we then describe how we decided what to take as explicit discourse connectives and the range of syntactic classes they come from . with representative examples of each class , we examine explicit connectives , their linear ordering , and types of syntactic units that can serve as their arguments . we then touch upon connectives with respect to free word order in turkish and punctuation , as well as the important issue of how much material is needed to specify an argument . we close with a brief discussion of current plans .

minimally supervised model of early language acquisition
theories of human language acquisition assume that learning to understand sentences is a partially-supervised task ( at best ) . instead of using gold-standard feedback , we train a simplified baby semantic role labeling system by combining world knowledge and simple grammatical constraints to form a potentially noisy training signal . this combination of knowledge sources is vital for learning ; a training signal derived from a single component leads the learner astray . when this largely unsupervised training approach is applied to a corpus of child directed speech , the babysrl learns shallow structural cues that allow it to mimic striking behaviors found in experiments with children and begin to correctly identify agents in a sentence .

cisuc-kis : tackling message polarity classification with a large and diverse set of features
this paper presents the approach of the cisuc-kis team to the semeval 2014 task on sentiment analysis in twitter , more precisely subtask b - message polarity classification . we followed a machine learning approach where a svm classifier was trained from a large and diverse set of features that included lexical , syntactic , sentiment and semantic-based aspects . this led to very interesting results which , in different datasets , put us always in the top-7 scores , including second position in the livejournal2014 dataset .

a unified model for soft linguistic reordering constraints in statistical machine translation
this paper explores a simple and effective unified framework for incorporating soft linguistic reordering constraints into a hierarchical phrase-based translation system : 1 ) a syntactic reordering model that explores reorderings for context free grammar rules ; and 2 ) a semantic reordering model that focuses on the reordering of predicate-argument structures . we develop novel features based on both models and use them as soft constraints to guide the translation process . experiments on chinese-english translation show that the reordering approach can significantly improve a state-of-the-art hierarchical phrase-based translation system . however , the gain achieved by the semantic reordering model is limited in the presence of the syntactic reordering model , and we therefore provide a detailed analysis of the behavior differences between the two .

boosting precision and recall of dictionary-based protein name crest , jst ( japan science and technology corporation )
dictionary-based protein name recognition is the first step for practical information extraction from biomedical documents because it provides id information of recognized terms unlike machine learning based approaches . however , dictionary based approaches have two serious problems : ( 1 ) a large number of false recognitions mainly caused by short names . ( 2 ) low recall due to spelling variation . in this paper , we tackle the former problem by using a machine learning method to filter out false positives . we also present an approximate string searching method to alleviate the latter problem . experimental results using the genia corpus show that the filtering using a naive bayes classifier greatly improves precision with slight loss of recall , resulting in a much better f-score .

context comparison of bursty events in web search and online media
in this paper , we conducted a systematic comparative analysis of language in different contexts of bursty topics , including web search , news media , blogging , and social bookmarking . we analyze ( 1 ) the content similarity and predictability between contexts , ( 2 ) the coverage of search content by each context , and ( 3 ) the intrinsic coherence of information in each context . our experiments show that social bookmarking is a better predictor to the bursty search queries , but news media and social blogging media have a much more compelling coverage . this comparison provides insights on how the search behaviors and social information sharing behaviors of users are correlated to the professional news media in the context of bursty events .

investigation of question classifier in question answering
in this paper , we investigate how an accurate question classifier contributes to a question answering system . we first present a maximum entropy ( me ) based question classifier which makes use of head word features and their wordnet hypernyms . we show that our question classifier can achieve the state of the art performance in the standard uiuc question dataset . we then investigate quantitatively the contribution of this question classifier to a feature driven question answering system . with our accurate question classifier and some standard question answer features , our question answering system performs close to the state of the art using trec corpus .

deriving verb-meaning clusters from syntactic structure
this paper presents a methodology for using the argument structure of sentences , as encoded by the propbank project , to develop clusters of verbs with similar meaning and usage . these clusters can be favorably compared to the classes developed by the verbnet project . the most interesting cases are those where the clustering methodology suggests new members for verbnet classes which will then be associated with the semantic predicates for that class .

exponential reservoir sampling for streaming language models
we show how rapidly changing textual streams such as twitter can be modelled in fixed space . our approach is based upon a randomised algorithm called exponential reservoir sampling , unexplored by this community until now . using language models over twitter and newswire as a testbed , our experimental results based on perplexity support the intuition that recently observed data generally outweighs that seen in the past , but that at times , the past can have valuable signals enabling better modelling of the present .

identifying sources of opinions with conditional random fields and
recent systems have been developed for sentiment classification , opinion recognition , and opinion analysis ( e.g. , detecting polarity and strength ) . we pursue another aspect of opinion analysis : identifying the sources of opinions , emotions , and sentiments . we view this problem as an information extraction task and adopt a hybrid approach that combines conditional random fields ( lafferty et al , 2001 ) and a variation of autoslog ( riloff , 1996a ) . while crfs model source identification as a sequence tagging task , autoslog learns extraction patterns . our results show that the combination of these two methods performs better than either one alone . the resulting system identifies opinion sources with 79.3 % precision and 59.5 % recall using a head noun matching measure , and 81.2 % precision and 60.6 % recall using an overlap measure .

good seed makes a good crop : accelerating active learning using language modeling
active learning ( al ) is typically initialized with a small seed of examples selected randomly . however , when the distribution of classes in the data is skewed , some classes may be missed , resulting in a slow learning progress . our contribution is twofold : ( 1 ) we show that an unsupervised language modeling based technique is effective in selecting rare class examples , and ( 2 ) we use this technique for seeding al and demonstrate that it leads to a higher learning rate . the evaluation is conducted in the context of word sense disambiguation .

distributional phrase structure induction
unsupervised grammar induction systems commonly judge potential constituents on the basis of their effects on the likelihood of the data . linguistic justifications of constituency , on the other hand , rely on notions such as substitutability and varying external contexts . we describe two systems for distributional grammar induction which operate on such principles , using part-of-speech tags as the contextual features . the advantages and disadvantages of these systems are examined , including precision/recall trade-offs , error analysis , and extensibility .

uth : semantic relation classification using physical sizes eiji aramaki takeshi imai kengo miyo kazuhiko ohe
although researchers have shown increasing interest in extracting/classifying semantic relations , most previous studies have basically relied on lexical patterns between terms . this paper proposes a novel way to accomplish the task : a system that captures a physical size of an entity . experimental results revealed that our proposed method is feasible and prevents the problems inherent in other methods .

unsupervised relation extraction from web documents
the idex system is a prototype of an interactive dynamic information extraction ( ie ) system . a user of the system expresses an information request in the form of a topic description , which is used for an initial search in order to retrieve a relevant set of documents . on basis of this set of documents , unsupervised relation extraction and clustering is done by the system . the results of these operations can then be interactively inspected by the user . in this paper we describe the relation extraction and clustering components of the idex system . preliminary evaluation results of these components are presented and an overview is given of possible enhancements to improve the relation extraction and clustering components .

dialog & task performance when vision is bandwidth-limited
the prospect of human commanders teaming with mobile robots smart enough to undertake joint exploratory tasksespecially tasks that neither commander nor robot could perform alonerequires novel methods of preparing and testing human-robot teams for these ventures prior to real-time operations . in this paper , we report work-in-progress that maintains face validity of selected configurations of resources and people , as would be available in emergency circumstances . more specifically , from an off-site post , we ask human commanders ( c ) to perform an exploratory task in collaboration with a remotely located human robot-navigator ( rn ) who controls the navigation of , but can not see the physical robot ( r ) . we impose network bandwidth restrictions in two mission scenarios comparable to real circumstances by varying the availability of sensor , image , and video signals to rn , in effect limiting the human rn to function as an automation stand-in . to better understand the capabilities and language required in such configurations , we constructed multi-modal corpora of time-synced dialog , video , and lidar files recorded during task sessions . we can now examine commander/robot dialogs while replaying what c and rn saw , to assess their task performance under these varied conditions .

benefits of the massively parallel rosetta stone :
in this paper , we describe our experiences in extending a standard cross-language information retrieval ( clir ) approach which uses parallel aligned corpora and latent semantic indexing . most , if not all , previous work which follows this approach has focused on bilingual retrieval ; two examples involve the use of frenchenglish or english-greek parallel corpora . our extension to the approach is massively parallel in two senses , one linguistic and the other computational . first , we make use of a parallel aligned corpus consisting of almost 50 parallel translations in over 30 distinct languages , each in over 30,000 documents . given the size of this dataset , a massively parallel approach was also necessitated in the more usual computational sense . our results indicate that , far from adding more noise , more linguistic parallelism is better when it comes to cross-language retrieval precision , in addition to the self-evident benefit that clir can be performed on more languages .

impact on relation extraction spoken language systems
we present a weakly-supervised induction method to assign semantic information to food items . we consider two tasks of categorizations being food-type classification and the distinction of whether a food item is composite or not . the categorizations are induced by a graph-based algorithm applied on a large unlabeled domain-specific corpus . we show that the usage of a domain-specific corpus is vital . we do not only outperform a manually designed open-domain ontology but also prove the usefulness of these categorizations in relation extraction , outperforming state-of-the-art features that include syntactic information and brown clustering .

creation of a new domain and evaluation of comparison generation in a natural language generation system
we describe the creation of a new domain for the methodius natural language generation system , and an evaluation of methodius parameterized comparison generation algorithm . the new domain was based around music and performers , and texts about the domain were generated using methodius . our evaluation showed that test subjects learned more from texts that contained comparisons than from those that did not . we also established that the comparison generation algorithm could generalize to the music domain .

a bootstrapping algorithm for automatically harvesting semantic relations
in this paper , we present espresso , a weakly-supervised iterative algorithm combined with a web-based knowledge expansion technique , for extracting binary semantic relations . given a small set of seed instances for a particular relation , the system learns lexical patterns , applies them to extract new instances , and then uses the web to filter and expand the instances . preliminary experiments show that espresso extracts highly precise lists of a wide variety of semantic relations when compared with two state of the art systems .

a dictionary data processing environment and its application in algorithmic processing of pali dictionary data for future nlp tasks
this paper presents a highly flexible infrastructure for processing digitized dictionaries and that can be used to build nlp tools in the future . this infrastructure is especially suitable for low resource languages where some digitized information is available but not ( yet ) suitable for algorithmic use . it allows researchers to do at least some processing in an algorithmic way using the full power of the c # programming language , reducing the effort of manual editing of the data . to test this in practice , the paper describes the processing steps taken by making use of this infrastructure in order to identify word classes and cross references in the dictionary of pali in the context of the senereko project . we also conduct an experiment to make use of this data and show the importance of the dictionary . this paper presents the experiences and results of the selected approach .

museli : a multi-source evidence integration approach to topic segmentation of spontaneous dialogue jaime arguello carolyn ros
we introduce a novel topic segmentation approach that combines evidence of topic shifts from lexical cohesion with linguistic evidence such as syntactically distinct features of segment initial contributions . our evaluation demonstrates that this hybrid approach outperforms state-of-the-art algorithms even when applied to loosely structured , spontaneous dialogue .

improving diversity in ranking using absorbing random walks
we introduce a novel ranking algorithm called grasshopper , which ranks items with an emphasis on diversity . that is , the top items should be different from each other in order to have a broad coverage of the whole item set . many natural language processing tasks can benefit from such diversity ranking . our algorithm is based on random walks in an absorbing markov chain . we turn ranked items into absorbing states , which effectively prevents redundant items from receiving a high rank . we demonstrate grasshoppers effectiveness on extractive text summarization : our algorithm ranks between the 1st and 2nd systems on duc 2004 task 2 ; and on a social network analysis task that identifies movie stars of the world .

quantification and implication in semantic calendar expressions represented with finite-state transducers
this paper elaborates a model for representing semantic calendar expressions ( sces ) , which correspond to the intensional meanings of natural-language calendar phrases . the model uses finite-state transducers ( fsts ) to mark denoted periods of time on a set of timelines represented as a finite-state automaton ( fsa ) . we present a treatment of sces corresponding to quantified phrases ( any monday ; every may ) and an implication operation for requiring the denotation of one sce to contain completely that of another .

automatically learning source-side reordering rules for large scale
we describe an approach to automatically learn reordering rules to be applied as a preprocessing step in phrase-based machine translation . we learn rules for 8 different language pairs , showing bleu improvements for all of them , and demonstrate that many important order transformations ( svo to sov or vso , headmodifier , verb movement ) can be captured by this approach .

interpretation of chinese discourse connectives for explicit discourse relation recognition
this paper addresses the specific features of chinese discourse connectives , including types ( word-pair and single-word ) , linking directions ( forward and backward linking ) , positions and ambiguous degrees , and discusses how they affect the discourse relation recognition . a semisupervised learning method is proposed to learn the probability distributions of discourse functions of connectives from a small labeled dataset and a big unlabeled dataset . the statistics learned from the dataset demonstrates some interesting linguistic phenomena such as connective synonyms sharing similar distributions , multiple discourse functions of connectives , and couple-linking elements providing strong clues for discourse relation resolution .

adaptive model weighting and transductive regression for predicting best system combinations
we analyze adaptive model weighting techniques for reranking using instance scores obtained by l1 regularized transductive regression . competitive statistical machine translation is an on-line learning technique for sequential translation tasks where we try to select the best among competing statistical machine translators . the competitive predictor assigns a probability per model weighted by the sequential performance . we define additive , multiplicative , and lossbased weight updates with exponential loss functions for competitive statistical machine translation . without any pre-knowledge of the performance of the translation models , we succeed in achieving the performance of the best model in all systems and surpass their performance in most of the language pairs we considered .

a web-based instructional platform for constraint-based grammar seminar fur sprachwissenschaft
we propose the creation of a web-based training framework comprising a set of topics that revolve around the use of feature structures as the core data structure in linguistic theory , its formal foundations , and its use in syntactic processing .

chinese sketch engine and the extraction of grammatical collocations
technology for collocation extraction in chinese . sketch engine ( kilgarriff et al , 2004 ) has proven to be a very effective tool for automatic description of lexical information , including collocation extraction , based on large-scale corpus . the original work of sketch engine was based on bnc . we extend sketch engine to chinese based on gigaword corpus from ldc . we discuss the available functions of the prototype chinese sketch engine ( cse ) as well as the robustness of language-independent adaptation of sketch engine . we conclude by discussing how chinese-specific linguistic information can be incorporated to improve the cse prototype .

automated alignment and extraction of bilingual domain ontology for medical domain web search
this paper proposes an approach to automated ontology alignment and domain ontology extraction from two knowledge bases . first , wordnet and hownet knowledge bases are aligned to construct a bilingual universal ontology based on the co-occurrence of the words in a parallel corpus . the bilingual universal ontology has the merit that it contains more structural and semantic information coverage from two complementary knowledge bases , wordnet and hownet . for domain-specific applications , a medical domain ontology is further extracted from the universal ontology using the islanddriven algorithm and a medical domain corpus . finally , the domain-dependent terms and some axioms between medical terms based on a medical encyclopaedia are added into the ontology . for ontology evaluation , experiments on web search were conducted using the constructed ontology . the experimental results show that the proposed approach can automatically align and extract the domain-specific ontology . in addition , the extracted ontology also shows its promising ability for medical web search .

unsupervised methods of topical text segmentation for polish
this paper describes a study on performance of existing unsupervised algorithms of text documents topical segmentation when applied to polish plain text documents . for performance measurement five existing topical segmentation algorithms were selected , three different polish test collections were created and seven approaches to text preprocessing were implemented . based on quantitative results ( pk and windowdiff metrics ) use of specific algorithm was recommended and impact of pre-processing strategies was assessed . thanks to use of standardized metrics and application of previously described methodology for test collection development , comparative results for polish and english were also obtained .

swat : cross-lingual lexical substitution using local context matching ,
we present two systems that select the most appropriate spanish substitutes for a marked word in an english test sentence . these systems were official entries to the semeval-2010 cross-lingual lexical substitution task . the first system , swat-e , finds spanish substitutions by first finding english substitutions in the english sentence and then translating these substitutions into spanish using an english-spanish dictionary . the second system , swat-s , translates each english sentence into spanish and then finds the spanish substitutions in the spanish sentence . both systems exceeded the baseline and all other participating systems by a wide margin using one of the two official scoring metrics .

graph-based clustering for semantic classication of onomatopoetic
this paper presents a method for semantic classication of onomatopoetic words like ( hum ) and ( clip clop ) which exist in every language , especially japanese being rich in onomatopoetic words . we used a graph-based clustering algorithm called newman clustering . the algorithm calculates a simple quality function to test whether a particular division is meaningful . the quality function is calculated based on the weights of edges between nodes . we combined two different similarity measures , distributional similarity , and orthographic similarity to calculate weights . the results obtained by using the web data showed a 9.0 % improvement over the baseline single distributional similarity measure .

automatic processing of diabetic patients hospital documentation
the paper presents a rule-based information extraction ( ie ) system for polish medical texts . we select the most important information from diabetic patients records . most data being processed are free-form texts , only a part is in table form . the work has three goals : to test classical ie methods on texts in polish , to create relational database containing the extracted data , and to prepare annotated data for further ie experiments .

inducing word sense with automatically learned hidden concepts
word sense induction ( wsi ) aims to automatically induce meanings of a polysemous word from unlabeled corpora . in this paper , we first propose a novel bayesian parametric model to wsi . unlike previous work , our research introduces a layer of hidden concepts and view senses as mixtures of concepts . we believe that concepts generalize the contexts , allowing the model to measure the sense similarity at a more general level . the zipfs law of meaning is used as a way of pre-setting the sense number for the parametric model . we further extend the parametric model to non-parametric model which not only simplifies the problem of model selection but also brings improved performance . we test our model on the benchmark datasets released by semeval-2010 and semeval-2007 . the test results show that our model outperforms state-of-theart systems .

improving data-driven dependency parsing using large-scale lfg grammars
this paper presents experiments which combine a grammar-driven and a datadriven parser . we show how the conversion of lfg output to dependency representation allows for a technique of parser stacking , whereby the output of the grammar-driven parser supplies features for a data-driven dependency parser . we evaluate on english and german and show significant improvements stemming from the proposed dependency structure as well as various other , deep linguistic features derived from the respective grammars .

semi-automatic construction of korean-chinese verb patterns based on translation equivalency
this paper addresses a new method of constructing korean-chinese verb patterns from existing patterns . a verb pattern is a subcategorization frame of a predicate extended by translation information . korean-chinese verb patterns are invaluable linguistic resources that only used for korean-chinese transfer but also for korean parsing . usually a verb pattern has been either hand-coded by expert lexicographers or extracted automatically from bilingual corpus . in the first case , the dependence on the linguistic intuition of lexicographers may lead to the incompleteness and the inconsistency of a dictionary . in the second case , extracted patterns can be domain-dependent . in this paper , we present a method to construct koreanchinese verb patterns semiautomatically from existing koreanchinese verb patterns that are manually written by lexicographers .

automatic identification of sentiment vocabulary : exploiting low association with known sentiment terms
we describe an extension to the technique for the automatic identification and labeling of sentiment terms described in turney ( 2002 ) and turney and littman ( 2002 ) . their basic assumption is that sentiment terms of similar orientation tend to co-occur at the document level . we add a second assumption , namely that sentiment terms of opposite orientation tend not to co-occur at the sentence level . this additional assumption allows us to identify sentiment-bearing terms very reliably . we then use these newly identified terms in various scenarios for the sentiment classification of sentences . we show that our approach outperforms turneys original approach . combining our approach with a naive bayes bootstrapping method yields a further small improvement of classifier performance . we finally compare our results to precision and recall figures that can be obtained on the same data set with labeled data .

cut the noise : mutually reinforcing reordering and alignments for improved machine translation
preordering of a source language sentence to match target word order has proved to be useful for improving machine translation systems . previous work has shown that a reordering model can be learned from high quality manual word alignments to improve machine translation performance . in this paper , we focus on further improving the performance of the reordering model ( and thereby machine translation ) by using a larger corpus of sentence aligned data for which manual word alignments are not available but automatic machine generated alignments are available . the main challenge we tackle is to generate quality data for training the reordering model in spite of the machine alignments being noisy . to mitigate the effect of noisy machine alignments , we propose a novel approach that improves reorderings produced given noisy alignments and also improves word alignments using information from the reordering model . this approach generates alignments that are 2.6 f-measure points better than a baseline supervised aligner . the data generated allows us to train a reordering model that gives an improvement of 1.8 bleu points on the nist mt-08 urdu-english evaluation set over a reordering model that only uses manual word alignments , and a gain of 5.2 bleu points over a standard phrase-based baseline .

the sawa corpus : a parallel corpus english - swahili peter waiganjo wagacha
research in data-driven methods for machine translation has greatly benefited from the increasing availability of parallel corpora . processing the same text in two different languages yields useful information on how words and phrases are translated from a source language into a target language . to investigate this , a parallel corpus is typically aligned by linking linguistic tokens in the source language to the corresponding units in the target language . an aligned parallel corpus therefore facilitates the automatic development of a machine translation system and can also bootstrap annotation through projection . in this paper , we describe data collection and annotation efforts and preliminary experimental results with a parallel corpus english - swahili .

a simple feature-copying approach for long-distance dependencies
this paper is concerned with statistical meth-ods for treating long-distance dependencies . we focus in particular on a case of substantial recent interest : that of long-distance depend-ency effects in entity extraction . we intro-duce a new approach to capturing these effects through a simple feature copying preprocess , and demonstrate substantial performance gains on several entity extraction tasks .

staying on topic : an indicator of power in political debates
we study the topic dynamics of interactions in political debates using the 2012 republican presidential primary debates as data . we show that the tendency of candidates to shift topics changes over the course of the election campaign , and that it is correlated with their relative power . we also show that our topic shift features help predict candidates relative rankings .

unsupervised , corpus-based method for extending a biomedical terminology for biomedical communications national library of medicine avenue du pr lon bernard
objectives : to automatically extend downwards an existing biomedical terminology using a corpus and both lexical and terminological knowledge . methods : adjectival modifiers are removed from terms extracted from the corpus ( three million noun phrases extracted from medline ) , and demodified terms are searched for in the terminology ( umls metathesaurus , restricted to disorders and procedures ) . a phrase from medline becomes a candidate term in the metathesaurus if the following two requirements are met : 1 ) a demodified term created from this phrase is found in the terminology and 2 ) the modifiers removed to create the demodified term also modify existing terms from the terminology , for a given semantic category . a manual review of a sample of candidate terms was performed . results : out of the 3 million simple phrases randomly extracted from medline , 125,000 new terms were identified for inclusion in the umls . 83 % of the 1000 terms reviewed manually were associated with a relevant umls concept . discussion : the limitations of this approach are discussed , as well as adaptation and generalization issues .

towards a description of symbolic maps
symbolic resources for text synthesis and text analysis are typically created and stored separately . in our case , we have a kpmlresource ( nigel ) and a ccg for english . in this paper , we argue that reversing efficient resources such as ours can not in general be achieved . for this reason , we propose a symbolic map that can be converted automatically into both synthesis- and analysis-oriented resources . we show that completeness of description can only be achieved by such a map while efficiency concerns can only be tackled by the directed rules of task-oriented resources not because of the current state of the art , but because reversing task-oriented symbolic resources is impossible in principle .

unsupervised word segmentation in context
this paper extends existing word segmentation models to take non-linguistic context into account . it improves the token f-score of a top performing segmentation models by 2.5 % on a 27k utterances dataset . we posit that word segmentation is easier in-context because the learner is not trying to access irrelevant lexical items . we use topics from a latent dirichlet allocation model as a proxy for activities contexts , to label the providence corpus . we present adaptor grammar models that use these context labels , and we study their performance with and without context annotations at test time .

connective-based measuring of the inter-annotator agreement in the annotation of discourse in pdt
we present several ways of measuring the inter-annotator agreement in the ongoing annotation of semantic inter-sentential discourse relations in the prague dependency treebank ( pdt ) . two ways have been employed to overcome limitations of measuring the agreement on the exact location of the start/end points of the relations . both methods skipping one tree level in the start/end nodes , and the connective-based measure are focused on a recognition of the existence and of the type of the relations , rather than on fixing the exact positions of the start/end points of the connecting arrows .

study of some distance measures for language and encoding anil kumar singh
to determine how close two language models ( e.g. , n-grams models ) are , we can use several distance measures . if we can represent the models as distributions , then the similarity is basically the similarity of distributions . and a number of measures are based on information theoretic approach . in this paper we present some experiments on using such similarity measures for an old natural language processing ( nlp ) problem . one of the measures considered is perhaps a novel one , which we have called mutual cross entropy . other measures are either well known or based on well known measures , but the results obtained with them vis-avis one-another might help in gaining an insight into how similarity measures work in practice . the first step in processing a text is to identify the language and encoding of its contents . this is a practical problem since for many languages , there are no universally followed text encoding standards . the method we have used in this paper for language and encoding identification uses pruned character n-grams , alone as well augmented with word n-grams . this method seems to give results comparable to other methods .

arabic morphological tagging , diacritization , and lemmatization using lexeme models and feature ranking
we investigate the tasks of general morphological tagging , diacritization , and lemmatization for arabic . we show that for all tasks we consider , both modeling the lexeme explicitly , and retuning the weights of individual classifiers for the specific task , improve the performance .

detection of entity mentions occurring in english and chinese text
in this paper , we describe an integrated approach to entity mention detection that yields a monolithic , almost language independent system . it is optimal in the sense that all categorical constraints are simultaneously considered . the system is compact and easy to develop and maintain , since only a single set of features and classifiers are needed to be designed and optimized . it is implemented using oneversus-all support vector machine ( svm ) classifiers and a number of feature extractors at several linguistic levels . svms are well known for their ability to handle a large set of overlapping features with theoretically sound generalization properties . data sparsity might be an important issue as a result of a large number of classes and relatively moderate training data size . however , we report results that the integrated system performs as good as a pipelined system that decomposes the problem into a few smaller subtasks . we conduct all our experiments using ace 2004 data , evaluate the systems using ace metrics and report competitive performance .

corpus-based sinhala lexicon
lexicon is in important resource in any kind of language processing application . corpus-based lexica have several advantages over other traditional approaches . the lexicon developed for sinhala was based on the text obtained from a corpus of 10 million words drawn from diverse genres . the words extracted from the corpus have been labeled with parts of speech categories defined according to a novel classification proposed for sinhala . the lexicon reports 80 % coverage over unrestricted text obtained from online sources . the lexicon has been implemented in lexical mark up framework .

bilingual termbank creation via log-likelihood comparison and phrase-based statistical machine translation
bilingual termbanks are important for many natural language processing ( nlp ) applications , especially in translation workflows in industrial settings . in this paper , we apply a log-likelihood comparison method to extract monolingual terminology from the source and target sides of a parallel corpus . then , using a phrase-based statistical machine translation model , we create a bilingual terminology with the extracted monolingual term lists . we manually evaluate our novel terminology extraction model on english-to-spanish and english-to-hindi data sets , and observe excellent performance for all domains . furthermore , we report the performance of our monolingual terminology extraction model comparing with a number of the state-of-the-art terminology extraction models on the english-to-hindi datasets .

semantic similarity : what for
linguistic similarity has been a prominent notion and tool in computational linguistics and related areas , as elaborated nicely in the announcement of this workshop . yet , what exactly counts as similarity , or when two linguistic concepts should be regarded as similar , often remains rather vague and ill posed , which is in fact quite typical for unsupervised notions . this talk will focus on similarity at the semantic level , and will explore the perspective that different notions of similarity may be defined relative to concrete modeling goals . in particular , i will refer to the two major goals in semantic modeling : predicting likelihood of occurrence , which is the typical goal in disambiguation and language modeling , and recognizing target meanings , which is the typical semantic goal in text understanding applications such as question answering , information extraction , summarization and information retrieval . we will discuss each goal and present corresponding semantic similarity approaches . 7

regression for sentence-level mt evaluation with pseudo references
many automatic evaluation metrics for machine translation ( mt ) rely on making comparisons to human translations , a resource that may not always be available . we present a method for developing sentence-level mt evaluation metrics that do not directly rely on human reference translations . our metrics are developed using regression learning and are based on a set of weaker indicators of fluency and adequacy ( pseudo references ) . experimental results suggest that they rival standard reference-based metrics in terms of correlations with human judgments on new test instances .

disambiguation of period characters in clinical narratives
the period characters meaning is highly ambiguous due to the frequency of abbreviations that require to be followed by a period . we have developed a hybrid method for period character disambiguation and the identification of abbreviations , combining rules that explore regularities in the right context of the period with lexicon-based , statistical methods which scrutinize the preceding token . the texts under scrutiny are clinical discharge summaries . both abbreviation detection and sentence delimitation showed an accuracy of about 93 % . an error analysis demonstrated potential for further improvements .

active learning for hpsg parse selection
we describe new features and algorithms for hpsg parse selection models and address the task of creating annotated material to train them . we evaluate the ability of several sample selection methods to reduce the number of annotated sentences necessary to achieve a given level of performance . our best method achieves a 60 % reduction in the amount of training material without any loss in accuracy .

improved-edit-distance kernel for chinese relation extraction
in this paper , a novel kernel-based method is presented for the problem of relation extraction between named entities from chinese texts . the kernel is defined over the original chinese string representations around particular entities . as a kernel function , the improved-edit-distance ( ied ) is used to calculate the similarity between two chinese strings . by employing the voted perceptron and support vector machine ( svm ) kernel machines with the ied kernel as the classifiers , we tested the method by extracting person-affiliation relation from chinese texts . by comparing with traditional feature-based learning methods , we conclude that our method needs less manual efforts in feature transformation and achieves a better performance .

studies on automatic recognition of common chinese adverbs
the study on automatic recognizing usages of modern chinese adverbs is one of the important parts of the nlp-oriented research of chinese functional words knowledge base . to solve the problems of the existing rule-based method of adverbs usages recognition based on the previous work , this paper has studied automatically recognizing common chinese adverbs usages using statistical methods . three statistical models , viz . crf , me , and svm , are used to label several common chinese adverbs usages on the segmentation and part-of-speech tagged corpus of peoples daily ( jan 1998 ) . the experiment results show that statisticalbased method is effective in automatically recognizing of several common adverbs usages and has good application prospects .

iitp : a supervised approach for disorder mention detection and utpal kumar sikdar , asif ekbal and sriparna saha
in this paper we briefly describe our supervised machine learning approach for disorder mention detection system that we submitted as part of our participation in the semeval-2014 shared task . the main goal of this task is to build a system that automatically identifies mentions of clinical conditions from the clinical texts . the main challenge lies due in the fact that the same mention of concept may be represented in many surface forms . we develop the system based on the supervised machine learning algorithms , namely conditional random field and support vector machine . one appealing characteristics of our system is that most of the features for learning are extracted automatically from the given training or test datasets without using deep domain specific resources and/or tools . we submitted three runs , and best performing system is based on conditional random field . for task a , it shows the precision , recall and f-measure values of 50.00 % , 47.90 % and 48.90 % , respectively under the strict matching criterion . when the matching criterion is relaxed , it shows the precision , recall and f-measure of 81.50 % , 79.70 % and 80.60 % , respectively . for task b , we obtain the accuracies of 33.30 % and 69.60 % for the relaxed and strict matches , respectively .

a simple pattern-matching algorithm for recovering empty nodes and their antecedents
this paper describes a simple patternmatching algorithm for recovering empty nodes and identifying their co-indexed antecedents in phrase structure trees that do not contain this information . the patterns are minimal connected tree fragments containing an empty node and all other nodes co-indexed with it . this paper also proposes an evaluation procedure for empty node recovery procedures which is independent of most of the details of phrase structure , which makes it possible to compare the performance of empty node recovery on parser output with the empty node annotations in a goldstandard corpus . evaluating the algorithm on the output of charniaks parser ( charniak , 2000 ) and the penn treebank ( marcus et al , 1993 ) shows that the patternmatching algorithm does surprisingly well on the most frequently occuring types of empty nodes given its simplicity .

to link or not to link a study on end-to-end tweet entity linking ming-wei chang emre kcman
information extraction from microblog posts is an important task , as today microblogs capture an unprecedented amount of information and provide a view into the pulse of the world . as the core component of information extraction , we consider the task of twitter entity linking in this paper . in the current entity linking literature , mention detection and entity disambiguation are frequently cast as equally important but distinct problems . however , in our task , we find that mention detection is often the performance bottleneck . the reason is that messages on micro-blogs are short , noisy and informal texts with little context , and often contain phrases with ambiguous meanings . to rigorously address the twitter entity linking problem , we propose a structural svm algorithm for entity linking that jointly optimizes mention detection and entity disambiguation as a single end-to-end task . by combining structural learning and a variety of firstorder , second-order , and context-sensitive features , our system is able to outperform existing state-of-the art entity linking systems by 15 % f1 .

prototype-driven learning for sequence models
we investigate prototype-driven learning for primarily unsupervised sequence modeling . prior knowledge is specified declaratively , by providing a few canonical examples of each target annotation label . this sparse prototype information is then propagated across a corpus using distributional similarity features in a log-linear generative model . on part-of-speech induction in english and chinese , as well as an information extraction task , prototype features provide substantial error rate reductions over competitive baselines and outperform previous work . for example , we can achieve an english part-of-speech tagging accuracy of 80.5 % using only three examples of each tag and no dictionary constraints . we also compare to semi-supervised learning and discuss the systems error trends .

ananthramakrishnana sankarkuppan sobhalalithadevi
this paper presents our ongoing work to automatically generate lyrics for a given melody , for phonetic languages such as tamil.weapproachthetaskofidentifying therequiredsyllablepatternforthelyricas asequencelabelingproblemandhenceuse thepopularcrf++toolkit for learning . a corpuscomprisingof10melodieswasused totrainthesystemtounderstandthesyllable patterns.thetrainedmodelisthenusedto guessthesyllabicpatternforanewmelody toproduceanoptimalsequenceofsyllables . thissequenceispresentedtothesentence generationmodulewhichusesthedijkstra 's shortest pathalgorithmtocomeupwitha meaningful phrase matching the syllabic pattern .

computing and evaluating syntactic complexity features for automated scoring of spontaneous non-native speech
this paper focuses on identifying , extracting and evaluating features related to syntactic complexity of spontaneous spoken responses as part of an effort to expand the current feature set of an automated speech scoring system in order to cover additional aspects considered important in the construct of communicative competence . our goal is to find effective features , selected from a large set of features proposed previously and some new features designed in analogous ways from a syntactic complexity perspective that correlate well with human ratings of the same spoken responses , and to build automatic scoring models based on the most promising features by using machine learning methods . on human transcriptions with manually annotated clause and sentence boundaries , our best scoring model achieves an overall pearson correlation with human rater scores of r=0.49 on an unseen test set , whereas correlations of models using sentence or clause boundaries from automated classifiers are around r=0.2 .

generating synthetic children 's acoustic models from adult models
this work focuses on generating childrens hmm-based acoustic models for speech recognition from adult acoustic models . collecting childrens speech data is more costly compared to adults speech . the patentpending method developed in this work requires only adult data to estimate synthetic childrens acoustic models in any language and works as follows : for a new language where only adult data is available , an adult male and an adult female model is trained . a linear transformation from each male hmm mean vector to its closest female mean vector is estimated . this transform is then scaled to a certain power and applied to the female model to obtain a synthetic childrens model . in a pronunciation verification task the method yields 19 % and 3.7 % relative improvement on native english and spanish childrens data , respectively , compared to the best adult model . for spanish data , the new model outperforms the available real childrens data based model by 13 % relative .

collapsed consonant and vowel models : new approaches for sarvnaz karimi falk scholer andrew turpin
we propose a novel algorithm for english to persian transliteration . previous methods proposed for this language pair apply a word alignment tool for training . by contrast , we introduce an alignment algorithm particularly designed for transliteration . our new model improves the english to persian transliteration accuracy by 14 % over an n-gram baseline . we also propose a novel back-transliteration method for this language pair , a previously unstudied problem . experimental results demonstrate that our algorithm leads to an absolute improvement of 25 % over standard transliteration approaches .

some tests of an unsupervised model of language acquisition
we outline an unsupervised language acquisition algorithm and offer some psycholinguistic support for a model based on it . our approach resembles the construction grammar in its general philosophy , and the tree adjoining grammar in its computational characteristics . the model is trained on a corpus of transcribed child-directed speech ( childes ) . the models ability to process novel inputs makes it capable of taking various standard tests of english that rely on forced-choice judgment and on magnitude estimation of linguistic acceptability . we report encouraging results from several such tests , and discuss the limitations revealed by other tests in our present method of dealing with novel stimuli .

the web as a baseline : evaluating the performance of unsupervised web-based models for a range of nlp tasks
previous work demonstrated that web counts can be used to approximate bigram frequencies , and thus should be useful for a wide variety of nlp tasks . so far , only two generation tasks ( candidate selection for machine translation and confusion-set disambiguation ) have been tested using web-scale data sets . the present paper investigates if these results generalize to tasks covering both syntax and semantics , both generation and analysis , and a larger range of n-grams . for the majority of tasks , we find that simple , unsupervised models perform better when n-gram frequencies are obtained from the web rather than from a large corpus . however , in most cases , web-based models fail to outperform more sophisticated state-of-theart models trained on small corpora . we argue that web-based models should therefore be used as a baseline for , rather than an alternative to , standard models .

using rbmt systems to produce bilingual corpus for smt
this paper proposes a method using the existing rule-based machine translation ( rbmt ) system as a black box to produce synthetic bilingual corpus , which will be used as training data for the statistical machine translation ( smt ) system . we use the existing rbmt system to translate the monolingual corpus into synthetic bilingual corpus . with the synthetic bilingual corpus , we can build an smt system even if there is no real bilingual corpus . in our experiments using bleu as a metric , the system achieves a relative improvement of 11.7 % over the best rbmt system that is used to produce the synthetic bilingual corpora . we also interpolate the model trained on a real bilingual corpus and the models trained on the synthetic bilingual corpora . the interpolated model achieves an absolute improvement of 0.0245 bleu score ( 13.1 % relative ) as compared with the individual model trained on the real bilingual corpus .

recognizing textual entailment challenge hoa trang dang
we present the results of the joint student response analysis and 8th recognizing textual entailment challenge , aiming to bring together researchers in educational nlp technology and textual entailment . the task of giving feedback on student answers requires semantic inference and therefore is related to recognizing textual entailment . thus , we offered to the community a 5-way student response labeling task , as well as 3-way and 2way rte-style tasks on educational data . in addition , a partial entailment task was piloted . we present and compare results from 9 participating teams , and discuss future directions .

modelling the lexicon in unsupervised part of speech induction
automatically inducing the syntactic partof-speech categories for words in text is a fundamental task in computational linguistics . while the performance of unsupervised tagging models has been slowly improving , current state-of-the-art systems make the obviously incorrect assumption that all tokens of a given word type must share a single part-of-speech tag . this one-tag-per-type heuristic counters the tendency of hidden markov model based taggers to over generate tags for a given word type . however , it is clearly incompatible with basic syntactic theory . in this paper we extend a state-ofthe-art pitman-yor hidden markov model tagger with an explicit model of the lexicon . in doing so we are able to incorporate a soft bias towards inducing few tags per type . we develop a particle filter for drawing samples from the posterior of our model and present empirical results that show that our model is competitive with and faster than the state-of-the-art without making any unrealistic restrictions .

back to basics for monolingual alignment : exploiting word similarity and contextural evidence
we present a simple , easy-to-replicate monolingual aligner that demonstrates state-of-the-art performance while relying on almost no supervision and a very small number of external resources . based on the hypothesis that words with similar meanings represent potential pairs for alignment if located in similar contexts , we propose a system that operates by finding such pairs . in two intrinsic evaluations on alignment test data , our system achieves f1 scores of 88 92 % , demonstrating 13 % absolute improvement over the previous best system . moreover , in two extrinsic evaluations our aligner outperforms existing aligners , and even a naive application of the aligner approaches state-ofthe-art performance in each extrinsic task .

hand gestures in disambiguating types of you expressions in multiparty
the second person pronoun you serves different functions in english . each of these different types often corresponds to a different term when translated into another language . correctly identifying different types of you can be beneficial to machine translation systems . to address this issue , we investigate disambiguation of different types of you occurrences in multiparty meetings with a new focus on the role of hand gesture . our empirical results have shown that incorporation of gesture improves performance on differentiating between the generic use of you ( e.g. , refer to people in general ) and the referential use of you ( e.g. , refer to a specific person or a group of people ) . incorporation of gesture can also compensate for limitations in automated language processing ( e.g. , reliable recognition of dialogue acts ) and achieve comparable results .

resolving and generating definite anaphora
we demonstrate an original and successful approach for both resolving and generating definite anaphora . we propose and evaluate unsupervised models for extracting hypernym relations by mining cooccurrence data of definite nps and potential antecedents in an unlabeled corpus . the algorithm outperforms a standard wordnet-based approach to resolving and generating definite anaphora . it also substantially outperforms recent related work using pattern-based extraction of such hypernym relations for coreference resolution .

weakly supervised construction of a repository of iconic images
we present a first attempt at semi-automatically harvesting a dataset of iconic images , namely images that depict objects or scenes , which arouse associations to abstract topics . our method starts with representative topic-evoking images from wikipedia , which are labeled with relevant concepts and entities found in their associated captions . these are used to query an online image repository ( i.e. , flickr ) , in order to further acquire additional examples of topic-specific iconic relations . to this end , we leverage a combination of visual similarity measures , image clustering and matching algorithms to acquire clusters of iconic images that are topically connected to the original seed images , while also allowing for various degrees of diversity . our first results are promising in that they indicate the feasibility of the task and that we are able to build a first version of our resource with minimal supervision .

chinese temporal tagging with heideltime
temporal information is important for many nlp tasks , and there has been extensive research on temporal tagging with a particular focus on english texts . recently , other languages have also been addressed , e.g. , heideltime was extended to process eight languages . chinese temporal tagging has achieved less attention , and no chinese temporal tagger is publicly available . in this paper , we address the full task of chinese temporal tagging ( extraction and normalization ) by developing chinese heideltime resources . our evaluation on a publicly available corpus which we also partially re-annotated due to its rather low quality demonstrates the effectiveness of our approach , and we outperform a recent approach to normalize temporal expressions . the chinese heideltime resource as well as the corrected corpus are made publicly available .

experimenting with transitive verbs in a discocat
formal and distributional semantic models offer complementary benefits in modeling meaning . the categorical compositional distributional model of meaning of coecke et al ( 2010 ) ( abbreviated to discocat in the title ) combines aspects of both to provide a general framework in which meanings of words , obtained distributionally , are composed using methods from the logical setting to form sentence meaning . concrete consequences of this general abstract setting and applications to empirical data are under active study ( grefenstette et al , 2011 ; grefenstette and sadrzadeh , 2011 ) . in this paper , we extend this study by examining transitive verbs , represented as matrices in a discocat . we discuss three ways of constructing such matrices , and evaluate each method in a disambiguation task developed by grefenstette and sadrzadeh ( 2011 ) .

chinese-english organization name translation based on correlative expansion
this paper presents an approach to translating chinese organization names into english based on correlative expansion . firstly , some candidate translations are generated by using statistical translation method . and several correlative named entities for the input are retrieved from a correlative named entity list . secondly , three kinds of expansion methods are used to generate some expanded queries . finally , these queries are submitted to a search engine , and the refined translation results are mined and re-ranked by using the returned web pages . experimental results show that this approach outperforms the compared system in overall translation accuracy .

effective structural inference for large xml documents
this paper investigates methods to automatically infer structural information from large xml documents . using xml as a reference format , we approach the schema generation problem by application of inductive inference theory . in doing so , we review and extend results relating to the search spaces of grammatical inferences for large data set . we evaluate the result of an inference process using the concept of minimum message length . comprehensive experimentation reveals our new hybrid method to be the most effective for large documents . finally tractability issues , including scalability analysis , are discussed .

comparing lexical relationships observed within japanese collocation data and japanese word association norms
while large-scale corpora and various corpus query tools have long been recognized as essential language resources , the value of word association norms as language resources has been largely overlooked . this paper conducts some initial comparisons of the lexical relationships observed within japanese collocation data extracted from a large corpus using the japanese language version of the sketch engine ( ske ) tool ( srdanovi et al , 2008 ) and the relationships found within japanese word association sets taken from the large-scale japanese word association database ( jwad ) under ongoing construction by joyce ( 2005 , 2007 ) . the comparison results indicate that while some relationships are common to both linguistic resources , many lexical relationships are only observed in one resource . these findings suggest that both resources are necessary in order to more adequately cover the diverse range of lexical relationships . finally , the paper reflects briefly on the implementation of association-based word-search strategies into electronic dictionaries proposed by zock and bilac ( 2004 ) and zock ( 2006 ) .

combining multiple models for speech information retrieval
in this article we present a method for combining different information retrieval models in order to increase the retrieval performance in a speech information retrieval task . the formulas for combining the models are tuned on training data . then the system is evaluated on test data . the task is particularly difficult because the text collection is automatically transcribed spontaneous speech , with many recognition errors . also , the topics are real information needs , difficult to satisfy . information retrieval systems are not able to obtain good results on this data set , except for the case when manual summaries are included .

unsupervised learning of morphology for english and inuktitut
we describe a simple unsupervised technique for learning morphology by identifying hubs in an automaton . for our purposes , a hub is a node in a graph with in-degree greater than one and out-degree greater than one . we create a word-trie , transform it into a minimal dfa , then identify hubs . those hubs mark the boundary between root and suffix , achieving similar performance to more complex mixtures of techniques .

oms-j : an opinion mining system for japanese weblog reviews using a combination of supervised and unsupervised approaches
we introduce a simple opinion mining system for analyzing japanese weblog reviews called oms-j . oms-j is designed to provide an intuitive visual gui of opinion mining graphs for a comparison of different products of the same type to help a user make a quick purchase decision . we first use an opinion mining method using a combination of supervised ( a naive bayes classifier ) and unsupervised ( an improved so-pmi : semantic orientation using pointwise mutual information ) learning .

discriminative modeling of extraction sets for machine translation
we present a discriminative model that directly predicts which set of phrasal translation rules should be extracted from a sentence pair . our model scores extraction sets : nested collections of all the overlapping phrase pairs consistent with an underlying word alignment . extraction set models provide two principle advantages over word-factored alignment models . first , we can incorporate features on phrase pairs , in addition to word links . second , we can optimize for an extraction-based loss function that relates directly to the end task of generating translations . our model gives improvements in alignment quality relative to state-of-the-art unsupervised and supervised baselines , as well as providing up to a 1.4 improvement in bleu score in chinese-to-english translation experiments .

knowledge sources for word sense disambiguation of biomedical text
like text in other domains , biomedical documents contain a range of terms with more than one possible meaning . these ambiguities form a significant obstacle to the automatic processing of biomedical texts . previous approaches to resolving this problem have made use of a variety of knowledge sources including linguistic information ( from the context in which the ambiguous term is used ) and domain-specific resources ( such as umls ) . in this paper we compare a range of knowledge sources which have been previously used and introduce a novel one : mesh terms . the best performance is obtained using linguistic features in combination with mesh terms . results from our system outperform published results for previously reported systems on a standard test set ( the nlm-wsd corpus ) .

how to semantically relate dialectal dictionaries in the linked data framework
we describe on-going work towards publishing language resources included in dialectal dictionaries in the linked open data ( lod ) cloud , and so to support wider access to the diverse cultural data associated with such dictionary entries , like the various historical and geographical variations of the use of such words . beyond this , our approach allows the cross-linking of entries of dialectal dictionaries on the basis of the semantic representation of their senses , and also to link the entries of the dialectal dictionaries to lexical senses available in the lod framework . this paper focuses on the description of the steps leading to a skos-xl and lemon encoding of the entries of two austrian dialectal dictionaries , and how this work supports their cross-linking and linking to other language data in the lod .

information retrieval oriented word segmentation based on character associative strength ranking
this paper presents a novel , ranking-style word segmentation approach , called rsvmseg , which is well tailored to chinese information retrieval ( cir ) . this strategy makes segmentation decision based on the ranking of the internal associative strength between each pair of adjacent characters of the sentence . on the training corpus composed of query items , a ranking model is learned by a widely-used tool ranking svm , with some useful statistical features , such as mutual information , difference of t-test , frequency and dictionary information . experimental results show that , this method is able to eliminate overlapping ambiguity much more effectively , compared to the current word segmentation methods . furthermore , as this strategy naturally generates segmentation results with different granularity , the performance of cir systems is improved and achieves the state of the art .

cross-media cross-genre information ranking multi-media information
current web technology has brought us a scenario that information about a certain topic is widely dispersed in data from different domains and data modalities , such as texts and images from news and social media . automatic extraction of the most informative and important multimedia summary ( e.g . a ranked list of inter-connected texts and images ) from massive amounts of cross-media and cross-genre data can significantly save users time and effort that is consumed in browsing . in this paper , we propose a novel method to address this new task based on automatically constructed multi-media information networks ( minets ) by incorporating cross-genre knowledge and inferring implicit similarity across texts and images . the facts from minets are exploited in a novel random walk-based algorithm to iteratively propagate ranking scores across multiple data modalities . experimental results demonstrated the effectiveness of our minets-based approach and the power of cross-media cross-genre inference .

sentiment analysis of twitter data apoorv agarwal boyi xie ilia vovsha owen rambow rebecca passonneau
we examine sentiment analysis on twitter data . the contributions of this paper are : ( 1 ) we introduce pos-specific prior polarity features . ( 2 ) we explore the use of a tree kernel to obviate the need for tedious feature engineering . the new features ( in conjunction with previously proposed features ) and the tree kernel perform approximately at the same level , both outperforming the state-of-the-art baseline .

rare word translation extraction from aligned comparable documents
we present a first known result of high precision rare word bilingual extraction from comparable corpora , using aligned comparable documents and supervised classification . we incorporate two features , a context-vector similarity and a co-occurrence model between words in aligned documents in a machine learning approach . we test our hypothesis on different pairs of languages and corpora . we obtain very high f-measure between 80 % and 98 % for recognizing and extracting correct translations for rare terms ( from 1 to 5 occurrences ) . moreover , we show that our system can be trained on a pair of languages and test on a different pair of languages , obtaining a f-measure of 77 % for the classification of chinese-english translations using a training corpus of spanish-french . our method is therefore even potentially applicable to low resources languages without training data .

an interactive tool for supporting error analysis for text mining
this demo abstract presents an interactive tool for supporting error analysis for text mining , which is situated within the summarization integrated development environment ( side ) . this freely downloadable tool was designed based on repeated experience teaching text mining over a number of years , and has been successfully tested in that context as a tool for students to use in conjunction with machine learning projects .

hybrid statistical and structural semantic modeling for thai multistage spoken language understanding chai wutiwiwatchai and sadaoki furui
this article proposes a hybrid statistical and structural semantic model for multi-stage spoken language understanding ( slu ) . the first stage of this slu utilizes a weighted finite-state transducer ( wfst ) -based parser , which encodes the regular grammar of concepts to be extracted . the proposed method improves the regular grammar model by incorporating a well-known n-gram semantic tagger . this hybrid model thus enhances the syntax of n-gram outputs while providing robustness against speech-recognition errors . with applications to a thai hotel reservation domain , it is shown to outperform both individual models at every stage of the slu system . under the probabilistic wfst framework , the use of n-best hypotheses from the speech recognizer instead of the 1best can further improve performance requiring only a small additional processing time .

the cadim arabic dependency parser
we describe the submission from the columbia arabic & dialect modeling group ( cadim ) for the shared task at the fourth workshop on statistical parsing of morphologically rich languages ( spmrl2013 ) . we participate in the arabic dependency parsing task for predicted pos tags and features . our system is based on marton et al ( 2013 ) .

local ambiguity packing and discontinuity in german
we report on recent advances in hpsg parsing of german with local ambiguity packing ( oepen and carroll , 2000 ) , achieving a speed-up factor of 2 on a balanced test-suite . in contrast to earlier studies carried out for english using the same packing algorithm , we show that restricting semantic features only is insufficient for achieving acceptable runtime performance with a german hpsg grammar . in a series of experiments relating to the three different types of discontinuities in german ( head movement , extraction , extraposition ) , we examine the effects of restrictor choice , ultimately showing that extraction and head movement require partial restriction of the respective features encoding the dependency , whereas full restriction gives best results for extraposition .

robust textual inference via graph matching
we present a system for deciding whether a given sentence can be inferred from text . each sentence is represented as a directed graph ( extracted from a dependency parser ) in which the nodes represent words or phrases , and the links represent syntactic and semantic relationships . we develop a learned graph matching approach to approximate entailment using the amount of the sentences semantic content which is contained in the text . we present results on the recognizing textual entailment dataset ( dagan et al , 2005 ) , and show that our approach outperforms bag-of-words and tf-idf models . in addition , we explore common sources of errors in our approach and how to remedy them .

building a web-based parallel corpus and filtering out machinetranslated text
we describe a set of techniques that have been developed while collecting parallel texts for russian-english language pair and building a corpus of parallel sentences for training a statistical machine translation system . we discuss issues of verifying potential parallel texts and filtering out automatically translated documents . finally we evaluate the quality of the 1-millionsentence corpus which we believe may be a useful resource for machine translation research .

does negation really matter
we explore the role negation and speculation identification plays in the multi-label document-level classification of medical reports for diseases . we identify the polarity of assertions made on noun phrases which reference diseases in the medical reports . we experiment with two machine learning classifiers : one based upon lucene and the other based upon boostexter . we find the performance of these systems on document-level classification of medical reports for diseases fails to show improvement when their input is enhanced by the polarity of assertions made on noun phrases . we conclude that due to the nature of our machine learning classifiers , information on the polarity of phrase-level assertions does not improve performance on our data in a multilabel document-level classification task .

turn-yielding cues in task-oriented dialogue
we examine a number of objective , automatically computable turn-yielding cues distinct prosodic , acoustic and syntactic events in a speakers speech that tend to precede a smooth turn exchange in the columbia games corpus , a large corpus of task-oriented dialogues . we show that the likelihood of occurrence of a turn-taking attempt from the interlocutor increases linearly with the number of cues conjointly displayed by the speaker . our results are important for improving the coordination of speaking turns in interactive voice-response systems , so that systems can correctly estimate when the user is willing to yield the conversational floor , and so that they can produce their own turn-yielding cues appropriately .

speech to speech translation for nurse patient interaction
s-minds is a speech translation system , which allows an english speaker to communicate with a limited english proficiency speaker easily within a question-and-answer , interview-style format . it can handle dialogs in specific settings such as nurse-patient interaction , or medical triage . we have built and tested an english-spanish system for enabling nurse-patient interaction in a number of domains in kaiser permanente achieving a total translation accuracy of 92.8 % ( for both english and spanish ) . we will give an overview of the system as well as the quantitative and qualitatively system performance .

fast consensus decoding over translation forests
the minimum bayes risk ( mbr ) decoding objective improves bleu scores for machine translation output relative to the standard viterbi objective of maximizing model score . however , mbr targeting bleu is prohibitively slow to optimize over k-best lists for large k. in this paper , we introduce and analyze an alternative to mbr that is equally effective at improving performance , yet is asymptotically faster running 80 times faster than mbr in experiments with 1000-best lists . furthermore , our fast decoding procedure can select output sentences based on distributions over entire forests of translations , in addition to k-best lists . we evaluate our procedure on translation forests from two large-scale , state-of-the-art hierarchical machine translation systems . our forest-based decoding objective consistently outperforms k-best list mbr , giving improvements of up to 1.0 bleu .

untangling the cross-lingual link structure of wikipedia
wikipedia articles in different languages are connected by interwiki links that are increasingly being recognized as a valuable source of cross-lingual information . unfortunately , large numbers of links are imprecise or simply wrong . in this paper , techniques to detect such problems are identified . we formalize their removal as an optimization task based on graph repair operations . we then present an algorithm with provable properties that uses linear programming and a region growing technique to tackle this challenge . this allows us to transform wikipedia into a much more consistent multilingual register of the worlds entities and concepts .

meta-structure transformation model for statistical machine translation
we propose a novel syntax-based model for statistical machine translation in which meta-structure ( ms ) and meta-structure sequence ( sms ) of a parse tree are defined . in this framework , a parse tree is decomposed into sms to deal with the structure divergence and the alignment can be reconstructed at different levels of recombination of ms ( rm ) . rm pairs extracted can perform the mapping between the substructures across languages . as a result , we have got not only the translation for the target language , but an sms of its parse tree at the same time . experiments with bleu metric show that the model significantly outperforms pharaoh , a state-art-theart phrase-based system .

frustratingly easy semi-supervised domain adaptation
in this work , we propose a semisupervised extension to a well-known supervised domain adaptation approach ( ea ) ( daume iii , 2007 ) . our proposed approach ( ea++ ) builds on the notion of augmented space ( introduced in ea ) and harnesses unlabeled data in target domain to ameliorate the transfer of information from source to target . this semisupervised approach to domain adaptation is extremely simple to implement , and can be applied as a pre-processing step to any supervised learner . experimental results on sequential labeling tasks demonstrate the efficacy of the proposed method .

exploiting ccg structures with tree kernels for speculation detection
our conll-2010 speculative sentence detector disambiguates putative keywords based on the following considerations : a speculative keyword may be composed of one or more word tokens ; a speculative sentence may have one or more speculative keywords ; and if a sentence contains at least one real speculative keyword , it is deemed speculative . a tree kernel classifier is used to assess whether a potential speculative keyword conveys speculation . we exploit information implicit in tree structures . for prediction efficiency , only a segment of the whole tree around a speculation keyword is considered , along with morphological features inside the segment and information about the containing document . a maximum entropy classifier is used for sentences not covered by the tree kernel classifier . experiments on the wikipedia data set show that our system achieves 0.55 f-measure ( in-domain ) .

extraction phenomena in synchronous tag syntax and semantics
we present a proposal for the structure of noun phrases in synchronous treeadjoining grammar ( stag ) syntax and semantics that permits an elegant and uniform analysis of a variety of phenomena , including quantifier scope and extraction phenomena such as wh-questions with both moved and in-place wh-words , pied-piping , stranding of prepositions , and topicalization . the tight coupling between syntax and semantics enforced by the stag helps to illuminate the critical relationships and filter out analyses that may be appealing for either syntax or semantics alone but do not allow for a meaningful relationship between them .

growing related words from seed via user behaviors : a re-ranking
motivated by google sets , we study the problem of growing related words from a single seed word by leveraging user behaviors hiding in user records of chinese input method . our proposed method is motivated by the observation that the more frequently two words cooccur in user records , the more related they are . first , we utilize user behaviors to generate candidate words . then , we utilize search engine to enrich candidate words with adequate semantic features . finally , we reorder candidate words according to their semantic relatedness to the seed word . experimental results on a chinese input method dataset show that our method gains better performance .

constituent parsing by classification
ordinary classification techniques can drive a conceptually simple constituent parser that achieves near state-of-the-art accuracy on standard test sets . here we present such a parser , which avoids some of the limitations of other discriminative parsers . in particular , it does not place any restrictions upon which types of features are allowed . we also present several innovations for faster training of discriminative parsers : we show how training can be parallelized , how examples can be generated prior to training without a working parser , and how independently trained sub-classifiers that have never done any parsing can be effectively combined into a working parser . finally , we propose a new figure-of-merit for bestfirst parsing with confidence-rated inferences . our implementation is freely available at : http : //cs.nyu.edu/turian/ software/parser/

finding predominant word senses in untagged text
in word sense disambiguation ( wsd ) , the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed . the problem with using the predominant , or first sense heuristic , aside from the fact that it does not take surrounding context into account , is that it assumes some quantity of handtagged data . whilst there are a few hand-tagged corpora available for some languages , one would expect the frequency distribution of the senses of words , particularly topical words , to depend on the genre and domain of the text under consideration . we present work on the use of a thesaurus acquired from raw textual corpora and the wordnet similarity package to find predominant noun senses automatically . the acquired predominant senses give a precision of 64 % on the nouns of the senseval2 english all-words task . this is a very promising result given that our method does not require any hand-tagged text , such as semcor . furthermore , we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora .

automatic detection of multilingual dictionaries on the web
this paper presents an approach to query construction to detect multilingual dictionaries for predetermined language combinations on the web , based on the identification of terms which are likely to occur in bilingual dictionaries but not in general web documents . we use eight target languages for our case study , and train our method on pre-identified multilingual dictionaries and the wikipedia dump for each of our languages .

exploring normalization techniques for human judgments of machine translation adequacy collected using amazon mechanical turk
this paper discusses a machine translation evaluation task conducted using amazon mechanical turk . we present a translation adequacy assessment task for untrained arabicspeaking annotators and discuss several techniques for normalizing the resulting data . we present a novel 2-stage normalization technique shown to have the best performance on this task and further discuss the results of all techniques and the usability of the resulting adequacy scores .

an information theoretic approach to bilingual word clustering
we present an information theoretic objective for bilingual word clustering that incorporates both monolingual distributional evidence as well as cross-lingual evidence from parallel corpora to learn high quality word clusters jointly in any number of languages . the monolingual component of our objective is the average mutual information of clusters of adjacent words in each language , while the bilingual component is the average mutual information of the aligned clusters . to evaluate our method , we use the word clusters in an ner system and demonstrate a statistically significant improvement in f1 score when using bilingual word clusters instead of monolingual clusters .

construction of disambiguated folksonomy ontologies using wikipedia
one of the difficulties in using folksonomies in computational systems is tag ambiguity : tags with multiple meanings . this paper presents a novel method for building folksonomy tag ontologies in which the nodes are disambiguated . our method utilizes a clustering algorithm called dscbc , which was originally developed in natural language processing ( nlp ) , to derive committees of tags , each of which corresponds to one meaning or domain . in this work , we use wikipedia as the external knowledge source for the domains of the tags . using the committees , an ambiguous tag is identified as one which belongs to more than one committee . then we apply a hierarchical agglomerative clustering algorithm to build an ontology of tags . the nodes in the derived ontology are disambiguated in that an ambiguous tag appears in several nodes in the ontology , each of which corresponds to one meaning of the tag . we evaluate the derived ontology for its ontological density ( how close similar tags are placed ) , and its usefulness in applications , in particular for a personalized tag retrieval task . the results showed marked improvements over other approaches .

improving word alignment using word similarity
we show that semantic relationships can be used to improve word alignment , in addition to the lexical and syntactic features that are typically used . in this paper , we present a method based on a neural network to automatically derive word similarity from monolingual data . we present an extension to word alignment models that exploits word similarity . our experiments , in both large-scale and resourcelimited settings , show improvements in word alignment tasks as well as translation tasks .

lexicon-based orthographic disambiguation in cjk intelligent information retrieval
the orthographical complexity of chinese , japanese and korean ( cjk ) poses a special challenge to the developers of computational linguistic tools , especially in the area of intelligent information retrieval . these difficulties are exacerbated by the lack of a standardized orthography in these languages , especially the highly irregular japanese orthography . this paper focuses on the typology of cjk orthographic variation , provides a brief analysis of the linguistic issues , and discusses why lexical databases should play a central role in the disambiguation process .

public dialogue : analysis of tolerance in online discussions
social media platforms have enabled people to freely express their views and discuss issues of interest with others . while it is important to discover the topics in discussions , it is equally useful to mine the nature of such discussions or debates and the behavior of the participants . there are many questions that can be asked . one key question is whether the participants give reasoned arguments with justifiable claims via constructive debates or exhibit dogmatism and egotistic clashes of ideologies . the central idea of this question is tolerance , which is a key concept in the field of communications . in this work , we perform a computational study of tolerance in the context of online discussions . we aim to identify tolerant vs. intolerant participants and investigate how disagreement affects tolerance in discussions in a quantitative framework . to the best of our knowledge , this is the first such study . our experiments using real-life discussions demonstrate the effectiveness of the proposed technique and also provide some key insights into the psycholinguistic phenomenon of tolerance in online discussions .

statistical dependency parsing of turkish
this paper presents results from the first statistical dependency parser for turkish . turkish is a free-constituent order language with complex agglutinative inflectional and derivational morphology and presents interesting challenges for statistical parsing , as in general , dependency relations are between portions of words called inflectional groups . we have explored statistical models that use different representational units for parsing . we have used the turkish dependency treebank to train and test our parser but have limited this initial exploration to that subset of the treebank sentences with only left-to-right non-crossing dependency links . our results indicate that the best accuracy in terms of the dependency relations between inflectional groups is obtained when we use inflectional groups as units in parsing , and when contexts around the dependent are employed .

a translation model for sentence retrieval
in this work we propose a translation model for monolingual sentence retrieval . we propose four methods for constructing a parallel corpus . of the four methods proposed , a lexicon learned from a bilingual arabicenglish corpus aligned at the sentence level performs best , significantly improving results over the query likelihood baseline . further , we demonstrate that smoothing from the local context of the sentence improves retrieval over the query likelihood baseline .

sentence level discourse parsing using syntactic and lexical information
we introduce two probabilistic models that can be used to identify elementary discourse units and build sentence-level discourse parse trees . the models use syntactic and lexical features . a discourse parsing algorithm that implements these models derives discourse parse trees with an error reduction of 18.8 % over a state-ofthe-art decision-based discourse parser . a set of empirical evaluations shows that our discourse parsing model is sophisticated enough to yield discourse trees at an accuracy level that matches near-human levels of performance .

sentence compression beyond word deletion
in this paper we generalise the sentence compression task . rather than simply shorten a sentence by deleting words or constituents , as in previous work , we rewrite it using additional operations such as substitution , reordering , and insertion . we present a new corpus that is suited to our task and a discriminative tree-totree transduction model that can naturally account for structural and lexical mismatches . the model incorporates a novel grammar extraction method , uses a language model for coherent output , and can be easily tuned to a wide range of compression specific loss functions .

a data mining approach to learn reorder rules for smt
in this paper , we describe a syntax based source side reordering method for phrasebased statistical machine translation ( smt ) systems . the source side training corpus is first parsed , then reordering rules are automatically learnt from source-side phrases and word alignments . later the source side training and test corpus are reordered and given to the smt system . reordering is a common problem observed in language pairs of distant language origins . this paper describes an automated approach for learning reorder rules from a word-aligned parallel corpus using association rule mining . reordered and generalized rules are the most significant in our approach . our experiments were conducted on an english-hindi eilmt corpus .

creating a manually error-tagged and shallow-parsed learner corpus edward whittaker vera sheinman
the availability of learner corpora , especially those which have been manually error-tagged or shallow-parsed , is still limited . this means that researchers do not have a common development and test set for natural language processing of learner english such as for grammatical error detection . given this background , we created a novel learner corpus that was manually error-tagged and shallowparsed . this corpus is available for research and educational purposes on the web . in this paper , we describe it in detail together with its data-collection method and annotation schemes . another contribution of this paper is that we take the first step toward evaluating the performance of existing postagging/chunking techniques on learner corpora using the created corpus . these contributions will facilitate further research in related areas such as grammatical error detection and automated essay scoring .

incremental hypothesis alignment with flexible matching for building
this paper describes the incremental hypothesis alignment algorithm used in the bbn submissions to the wmt09 system combination task . the alignment algorithm used a sentence specific alignment order , flexible matching , and new shift heuristics . these refinements yield more compact confusion networks compared to using the pair-wise or incremental ter alignment algorithms . this should reduce the number of spurious insertions in the system combination output and the system combination weight tuning converges faster . system combination experiments on the wmt09 test sets from five source languages to english are presented . the best bleu scores were achieved by combing the english outputs of three systems from all five source languages .

how much do word embeddings encode about syntax
do continuous word embeddings encode any useful information for constituency parsing we isolate three ways in which word embeddings might augment a stateof-the-art statistical parser : by connecting out-of-vocabulary words to known ones , by encouraging common behavior among related in-vocabulary words , and by directly providing features for the lexicon . we test each of these hypotheses with a targeted change to a state-of-the-art baseline . despite small gains on extremely small supervised training sets , we find that extra information from embeddings appears to make little or no difference to a parser with adequate training data . our results support an overall hypothesis that word embeddings import syntactic information that is ultimately redundant with distinctions learned from treebanks in other ways .

raising the compatibility of heterogeneous annotations : a case study on protein mention recognition
while there are several corpora which claim to have annotations for protein references , the heterogeneity between the annotations is recognized as an obstacle to develop expensive resources in a synergistic way . here we present a series of experimental results which show the differences of protein mention annotations made to two corpora , genia and aimed .

shallow semantic analysis of interactive learner sentences
focusing on applications for analyzing learner language which evaluate semantic appropriateness and accuracy , we collect data from a task which models some aspects of interaction , namely a picture description task ( pdt ) . we parse responses to the pdt into dependency graphs with an an off-the-shelf parser , then use a decision tree to classify sentences into syntactic types and extract the logical subject , verb , and object , finding 92 % accuracy in such extraction . the specific goal in this paper is to examine the challenges involved in extracting these simple semantic representations from interactive learner sentences .

but-typed : using domain knowledge for computing typed similarity
this paper deals with knowledge-based text processing which aims at an intuitive notion of textual similarity . entities and relations relevant for a particular domain are identified and disambiguated by means of semi-supervised machine learning techniques and resulting annotations are applied for computing typedsimilarity of individual texts . the work described in this paper particularly shows effects of the mentioned processes in the context of the *sem 2013 pilot task on typed-similarity , a part of the semantic textual similarity shared task . the goal is to evaluate the degree of semantic similarity between semi-structured records . as the evaluation dataset has been taken from europeana a collection of records on european cultural heritage objects we focus on computing a semantic distance on field author which has the highest potential to benefit from the domain knowledge . specific features that are employed in our system but-typed are briefly introduced together with a discussion on their efficient acquisition . support vector regression is then used to combine the features and to provide a final similarity score . the system ranked third on the attribute author among 15 submitted runs in the typed-similarity task .

efficient and robust lfg parsing : sxlfg
in this paper , we introduce a new parser , called sxlfg , based on the lexicalfunctional grammars formalism ( lfg ) . we describe the underlying context-free parser and how functional structures are efficiently computed on top of the cfg shared forest thanks to computation sharing , lazy evaluation , and compact data representation . we then present various error recovery techniques we implemented in order to build a robust parser . finally , we offer concrete results when sxlfg is used with an existing grammar for french . we show that our parser is both efficient and robust , although the grammar is very ambiguous .

easy first dependency parsing of modern hebrew
we investigate the performance of an easyfirst , non-directional dependency parser on the hebrew dependency treebank . we show that with a basic feature set the greedy parsers accuracy is on a par with that of a first-order globally optimized mst parser . the addition of morphological-agreement feature improves the parsing accuracy , making it on-par with a second-order globally optimized mst parser . the improvement due to the morphological agreement information is persistent both when gold-standard and automatically-induced morphological information is used .

unifying synchronous tree-adjoining grammars and tree transducers via bimorphisms
we place synchronous tree-adjoining grammars and tree transducers in the single overarching framework of bimorphisms , continuing the unification of synchronous grammars and tree transducers initiated by shieber ( 2004 ) . along the way , we present a new definition of the tree-adjoining grammar derivation relation based on a novel direct inter-reduction of tag and monadic macro tree transducers .

cross-lingual induction for deep broad-coverage syntax : a case study on german participles
this paper is a case study on cross-lingual induction of lexical resources for deep , broad-coverage syntactic analysis of german . we use a parallel corpus to induce a classifier for german participles which can predict their syntactic category . by means of this classifier , we induce a resource of adverbial participles from a huge monolingual corpus of german . we integrate the resource into a german lfg grammar and show that it improves parsing coverage while maintaining accuracy .

extracting opinion targets in a single- and cross-domain setting with conditional random fields
in this paper , we focus on the opinion target extraction as part of the opinion mining task . we model the problem as an information extraction task , which we address based on conditional random fields ( crf ) . as a baseline we employ the supervised algorithm by zhuang et al ( 2006 ) , which represents the state-of-the-art on the employed data . we evaluate the algorithms comprehensively on datasets from four different domains annotated with individual opinion target instances on a sentence level . furthermore , we investigate the performance of our crf-based approach and the baseline in a single- and cross-domain opinion target extraction setting . our crf-based approach improves the performance by 0.077 , 0.126 , 0.071 and 0.178 regarding f-measure in the single-domain extraction in the four domains . in the crossdomain setting our approach improves the performance by 0.409 , 0.242 , 0.294 and 0.343 regarding f-measure over the baseline .

a framework for representing lexical resources
our goal is to propose a description model for the lexicon . we describe a software framework for representing the lexicon and its variations called proteus . various examples show the different possibilities offered by this tool . we conclude with a demonstration of the use of lexical resources in complex , real examples .

generation of output style variation in the sammie dialogue system
a dialogue system can present itself and/or address the user as an active agent by means of linguistic constructions in personal style , or suppress agentivity by using impersonal style . we describe how we generate and control personal and impersonal style variation in the output of sammie , a multimodal in-car dialogue system for an mp3 player . we carried out an experiment to compare subjective evaluation judgments and input style alignment behavior of users interacting with versions of the system generating output in personal vs. impersonal style . although our results are consistent with earlier findings obtained with simulated systems , the effects are weaker .

optimal parsing strategies for linear context-free rewriting systems
factorization is the operation of transforming a production in a linear context-free rewriting system ( lcfrs ) into two simpler productions by factoring out a subset of the nonterminals on the productions righthand side . factorization lowers the rank of a production but may increase its fan-out . we show how to apply factorization in order to minimize the parsing complexity of the resulting grammar , and study the relationship between rank , fanout , and parsing complexity . we show that it is always possible to obtain optimum parsing complexity with rank two . however , among transformed grammars of rank two , minimum parsing complexity is not always possible with minimum fan-out . applying our factorization algorithm to lcfrs rules extracted from dependency treebanks allows us to find the most efficient parsing strategy for the syntactic phenomena found in non-projective trees .

chinese word segmentation based on mixing model
this paper presents our recent work for participation in the second international chinese word segmentation bakeoff . according to difficulties , we divide word segmentation into several sub-tasks , which are solved by mixed language models , so as to take advantage of each approach in addressing special problems . the experiment indicated that this system achieved 96.7 % and 97.2 % in f-measure in pku and msr open test respectively .

fast , deep-linguistic statistical dependency parsing
we present and evaluate an implemented statistical minimal parsing strategy exploiting dg charateristics to permit fast , robust , deeplinguistic analysis of unrestricted text , and compare its probability model to ( collins , 1999 ) and an adaptation , ( dubey and keller , 2003 ) . we show that dg allows for the expression of the majority of english ldds in a context-free way and o ers simple yet powerful statistical models .

using lexical dependency and ontological knowledge to improve a detailed syntactic and semantic tagger of english
this paper presents a detailed study of the integration of knowledge from both dependency parses and hierarchical word ontologies into a maximum-entropy-based tagging model that simultaneously labels words with both syntax and semantics . our findings show that information from both these sources can lead to strong improvements in overall system accuracy : dependency knowledge improved performance over all classes of word , and knowledge of the position of a word in an ontological hierarchy increased accuracy for words not seen in the training data . the resulting tagger offers the highest reported tagging accuracy on this tagset to date .

bridging text and knowledge with frames
framenet is the best currently operational version of chuck fillmores frame semantics . as framenet has evolved over the years , we have been building a series of increasingly ambitious prototype systems that exploit framenet as a semantic resource . results from this work point to frames as a natural representation for applications that require linking textual meaning to world knowledge .

author verification using common n-gram profiles of text documents
authorship verification is the problem of answering the question whether or not a sample text document was written by a specific person , given a few other documents known to be authored by them . we propose a proximity based method for one-class classification that applies the common n-gram ( cng ) dissimilarity measure . the cng dissimilarity ( keselj et al. , 2003 ) is based on the differences in the frequencies of n-grams of tokens ( characters , words ) that are most common in the considered documents . our method utilizes the pairs of most dissimilar documents among documents of known authorship . we evaluate various variants of the method in the setting of a single classifier or an ensemble of classifiers , on a multilingual authorship verification corpus of the pan 2013 author identification evaluation framework . our method yields competitive results when compared to the results achieved by the participants of the pan 2013 competition on the entire set , as well as separately on two subsets english and spanish ones out of the three language subsets of the corpus .

cmu : arc-factored , discriminative semantic dependency parsing
we present an arc-factored statistical model for semantic dependency parsing , as defined by the semeval 2014 shared task 8 on broad-coverage semantic dependency parsing . our entry in the open track placed second in the competition .

for investigating information structure
we present discourse-level annotation of newspaper texts in german and english , as part of an ongoing project aimed at investigating information structure from a cross-linguistic perspective . rather than annotating some specific notion of information structure , we propose a theory-neutral annotation of basic features at the levels of syntax , prosody and discourse , using treebank data as a starting point . our discourse-level annotation scheme covers properties of discourse referents ( e.g. , semantic sort , delimitation , quantification , familiarity status ) and anaphoric links ( coreference and bridging ) . we illustrate what investigations this data serves and discuss some integration issues involved in combining different levels of stand-off annotations , created by using different tools .

efficient unsupervised discovery of word categories using symmetric patterns and high frequency words
we present a novel approach for discovering word categories , sets of words sharing a significant aspect of their meaning . we utilize meta-patterns of highfrequency words and content words in order to discover pattern candidates . symmetric patterns are then identified using graph-based measures , and word categories are created based on graph clique sets . our method is the first pattern-based method that requires no corpus annotation or manually provided seed patterns or words . we evaluate our algorithm on very large corpora in two languages , using both human judgments and wordnetbased evaluation . our fully unsupervised results are superior to previous work that used a pos tagged corpus , and computation time for huge corpora are orders of magnitude faster than previously reported .

learning the hyperparameters to learn morphology
we perform hyperparameter inference within a model of morphology learning ( goldwater et al. , 2011 ) and find that it affects model behaviour drastically . changing the model structure successfully avoids the unsegmented solution , but results in oversegmentation instead .

visualization , search , and error analysis for coreference annotations artner anders bj orkelund gregor thiele wolfgang seeker jonas kuhn
we present the icarus coreference explorer , an interactive tool to browse and search coreference-annotated data . it can display coreference annotations as a tree , as an entity grid , or in a standard textbased display mode , and lets the user switch freely between the different modes . the tool can compare two different annotations on the same document , allowing system developers to evaluate errors in automatic system predictions . it features a flexible search engine , which enables the user to graphically construct search queries over sets of documents annotated with coreference .

association-based bilingual word alignment
bilingual word alignment forms the foundation of current work on statistical machine translation . standard wordalignment methods involve the use of probabilistic generative models that are complex to implement and slow to train . in this paper we show that it is possible to approach the alignment accuracy of the standard models using algorithms that are much faster , and in some ways simpler , based on basic word-association statistics .

engkoo : mining the web for language learning
this paper presents engkoo , a system for exploring and learning language . it is built primarily by mining translation knowledge from billions of web pages - using the internet to catch language in motion . currently engkoo is built for chinese users who are learning english ; however the technology itself is language independent and can be extended in the future . at a system level , engkoo is an application platform that supports a multitude of nlp technologies such as cross language retrieval , alignment , sentence classification , and statistical machine translation . the data set that supports this system is primarily built from mining a massive set of bilingual terms and sentences from across the web . specifically , web pages that contain both chinese and english are discovered and analyzed for parallelism , extracted and formulated into clear term definitions and sample sentences . this approach allows us to build perhaps the worlds largest lexicon linking both chinese and english together - at the same time covering the most up-to-date terms as captured by the net .

a preliminary approach to recognize generic drug names by combining isabel segura-bedmar paloma martnez doaa samy
this paper presents a system1 for drug name identification and classification in biomedical texts .

hypothesis selection in machine transliteration : a web mining approach
we propose a new method of selecting hypotheses for machine transliteration . we generate a set of chinese , japanese , and korean transliteration hypotheses for a given english word . we then use the set of transliteration hypotheses as a guide to finding relevant web pages and mining contextual information for the transliteration hypotheses from the web page . finally , we use the mined information for machine-learning algorithms including support vector machines and maximum entropy model designed to select the correct transliteration hypothesis . in our experiments , our proposed method based on web mining consistently outperformed systems based on simple web counts used in previous work , regardless of the language .

identifying metaphorical word use with tree kernels
a metaphor is a figure of speech that refers to one concept in terms of another , as in he is such a sweet person . metaphors are ubiquitous and they present nlp with a range of challenges for wsd , ie , etc . identifying metaphors is thus an important step in language understanding . however , since almost any word can serve as a metaphor , they are impossible to list . to identify metaphorical use , we assume that it results in unusual semantic patterns between the metaphor and its dependencies . to identify these cases , we use svms with tree-kernels on a balanced corpus of 3872 instances , created by bootstrapping from available metaphor lists.1 we outperform two baselines , a sequential and a vectorbased approach , and achieve an f1-score of 0.75 .

corpus building for mongolian language
this paper presents an ongoing research aimed to build the first corpus , 5 million words , for mongolian language by focusing on annotating and tagging corpus texts according to tei xml ( mcqueen , 2004 ) format . also , a tool , mcbuilder , which provides support for flexibly and manually annotating and manipulating the corpus texts with xml structure , is presented .

an online cascaded approach to biomedical named entity recognition
we present an online cascaded approach to biomedical named entity recognition . this approach uses an online training method to substantially reduce the training time required and a cascaded framework to relax the memory requirement . we conduct detailed experiments on the bionlp dataset from the jnlpba shared task and compare the results with other systems and published works . our experimental results show that our approach achieves comparable performance with great reductions in time and space requirements .

using context vectors in improving a machine translation system with bridge language samira tofighi zahabi somayeh bakhshaei shahram khadivi
mapping phrases between languages as translation of each other by using an intermediate language ( pivot language ) may generate translation pairs that are wrong . since a word or a phrase has different meanings in different contexts , we should map source and target phrases in an intelligent way . we propose a pruning method based on the context vectors to remove those phrase pairs that connect to each other by a polysemous pivot phrase or by weak translations . we use context vectors to implicitly disambiguate the phrase senses and to recognize irrelevant phrase translation pairs . using the proposed method a relative improvement of 2.8 percent in terms of bleu score is achieved .

classifying relations for biomedical named entity disambiguation
named entity disambiguation concerns linking a potentially ambiguous mention of named entity in text to an unambiguous identifier in a standard database . one approach to this task is supervised classification . however , the availability of training data is often limited , and the available data sets tend to be imbalanced and , in some cases , heterogeneous . we propose a new method that distinguishes a named entity by finding the informative keywords in its surrounding context , and then trains a model to predict whether each keyword indicates the semantic class of the entity . while maintaining a comparable performance to supervised classification , this method avoids using expensive manually annotated data for each new domain , and thus achieves better portability .

data homogeneity and semantic role tagging in chinese
this paper reports on a study of semantic role tagging in chinese in the absence of a parser . we tackle the task by identifying the relevant headwords in a sentence as a first step to partially locate the corresponding constituents to be labelled . we also explore the effect of data homogeneity by experimenting with a textbook corpus and a news corpus , representing simple data and complex data respectively . results suggest that while the headword location method remains to be improved , the homogeneity between the training and testing data is important especially in view of the characteristic syntaxsemantics interface in chinese . we also plan to explore some class-based techniques for the task with reference to existing semantic lexicons , and to modify the method and augment the feature set with more linguistic input .

improving reordering with linguistically informed bilingual n-grams josep maria crego
we present a new reordering model estimated as a standard n-gram language model with units built from morphosyntactic information of the source and target languages . it can be seen as a model that translates the morpho-syntactic structure of the input sentence , in contrast to standard translation models which take care of the surface word forms . we take advantage from the fact that such units are less sparse than standard translation units to increase the size of bilingual context that is considered during the translation process , thus effectively accounting for mid-range reorderings . empirical results on french-english and germanenglish translation tasks show that our model achieves higher translation accuracy levels than those obtained with the widely used lexicalized reordering model .

separating disambiguation from composition in distributional semantics
most compositional-distributional models of meaning are based on ambiguous vector representations , where all the senses of a word are fused into the same vector . this paper provides evidence that the addition of a vector disambiguation step prior to the actual composition would be beneficial to the whole process , producing better composite representations . furthermore , we relate this issue with the current evaluation practice , showing that disambiguation-based tasks can not reliably assess the quality of composition . using a word sense disambiguation scheme based on the generic procedure of schtze ( 1998 ) , we first provide a proof of concept for the necessity of separating disambiguation from composition . then we demonstrate the benefits of an unambiguous system on a composition-only task .

using integer linear programming in concept-to-text generation to produce more compact texts
we present an ilp model of concept-totext generation . unlike pipeline architectures , our model jointly considers the choices in content selection , lexicalization , and aggregation to avoid greedy decisions and produce more compact texts .

a handsome set of metrics to measure utterance classification performance in spoken dialog systems
we present a set of metrics describing classification performance for individual contexts of a spoken dialog system as well as for the entire system . we show how these metrics can be used to train and tune system components and how they are related to caller experience , a subjective measure describing how well a caller was treated by the dialog system .

gender inference of twitter users in non-english contexts
while much work has considered the problem of latent attribute inference for users of social media such as twitter , little has been done on non-english-based content and users . here , we conduct the first assessment of latent attribute inference in languages beyond english , focusing on gender inference . we find that the gender inference problem in quite diverse languages can be addressed using existing machinery . further , accuracy gains can be made by taking language-specific features into account . we identify languages with complex orthography , such as japanese , as difficult for existing methods , suggesting a valuable direction for future research .

composing simple image descriptions using web-scale n-grams
studying natural language , and especially how people describe the world around them can help us better understand the visual world . in turn , it can also help us in the quest to generate natural language that describes this world in a human manner . we present a simple yet effective approach to automatically compose image descriptions given computer vision based inputs and using web-scale n-grams . unlike most previous work that summarizes or retrieves pre-existing text relevant to an image , our method composes sentences entirely from scratch . experimental results indicate that it is viable to generate simple textual descriptions that are pertinent to the specific content of an image , while permitting creativity in the description making for more human-like annotations than previous approaches .

improved smoothing for n-gram language models based on ordinary counts
kneser-ney ( 1995 ) smoothing and its variants are generally recognized as having the best perplexity of any known method for estimating n-gram language models . kneser-ney smoothing , however , requires nonstandard n-gram counts for the lowerorder models used to smooth the highestorder model . for some applications , this makes kneser-ney smoothing inappropriate or inconvenient . in this paper , we introduce a new smoothing method based on ordinary counts that outperforms all of the previous ordinary-count methods we have tested , with the new method eliminating most of the gap between kneser-ney and those methods .

cancer stage prediction based on patient online discourse
forums and mailing lists dedicated to particular diseases are increasingly popular online . automatically inferring the health status of a patient can be useful for both forum users and health researchers who study patients online behaviors . in this paper , we focus on breast cancer forums and present a method to predict the stage of patients cancers from their online discourse . we show that what the patients talk about ( content-based features ) and whom they interact with ( social networkbased features ) provide complementary cues to predicting cancer stage and can be leveraged for better prediction . our methods are extendable and can be applied to other tasks of acquiring contextual information about online health forum participants .

a hybrid relational approach for wsd first results
we present a novel hybrid approach for word sense disambiguation ( wsd ) which makes use of a relational formalism to represent instances and background knowledge . it is built using inductive logic programming techniques to combine evidence coming from both sources during the learning process , producing a rule-based wsd model . we experimented with this approach to disambiguate 7 highly ambiguous verbs in englishportuguese translation . results showed that the approach is promising , achieving an average accuracy of 75 % , which outperforms the other machine learning techniques investigated ( 66 % ) .

semi-supervised training for the averaged perceptron pos tagger drahomra johanka spoustova jan hajic jan raab miroslav spousta
this paper describes pos tagging experiments with semi-supervised training as an extension to the ( supervised ) averaged perceptron algorithm , first introduced for this task by ( collins , 2002 ) . experiments with an iterative training on standard-sized supervised ( manually annotated ) dataset ( 106 tokens ) combined with a relatively modest ( in the order of 108 tokens ) unsupervised ( plain ) data in a bagging-like fashion showed significant improvement of the pos classification task on typologically different languages , yielding better than state-of-the-art results for english and czech ( 4.12 % and 4.86 % relative error reduction , respectively ; absolute accuracies being 97.44 % and 95.89 % ) .

montague meets markov : deep semantics with probabilistic logical form
we combine logical and distributional representations of natural language meaning by transforming distributional similarity judgments into weighted inference rules using markov logic networks ( mlns ) . we show that this framework supports both judging sentence similarity and recognizing textual entailment by appropriately adapting the mln implementation of logical connectives . we also show that distributional phrase similarity , used as textual inference rules created on the fly , improves its performance .

a hybrid model for grammatical error correction
this paper presents a hybrid model for the conll-2013 shared task which focuses on the problem of grammatical error correction . this years task includes determiner , preposition , noun number , verb form , and subject-verb agreement errors which is more comprehensive than previous error correction tasks . we correct these five types of errors in different modules where either machine learning based or rule-based methods are applied . preprocessing and post-processing procedures are employed to keep idiomatic phrases from being corrected . we achieved precision of 35.65 % , recall of 16.56 % , f1 of 22.61 % in the official evaluation and precision of 41.75 % , recall of 20.29 % , f1 of 27.3 % in the revised version . some further comparisons employing different strategies are made in our experiments .

a self-learning agent for exchanging pop trivia
this paper describes a self-learning software agent who collects and learns knowledge from the web and also exchanges her knowledge via dialogues with the users . the agent is built on top of information extraction , web mining , question answering and dialogue system technologies , and users can freely formulate their questions within the gossip domain and obtain the answers in multiple ways : textual response , graph-based visualization of the related concepts and speech output .

quality estimation for machine translation using the joint method of evaluation criteria and statistical modeling
this paper is to introduce our participation in the wmt13 shared tasks on quality estimation for machine translation without using reference translations . we submitted the results for task 1.1 ( sentence-level quality estimation ) , task 1.2 ( system selection ) and task 2 ( word-level quality estimation ) . in task 1.1 , we used an enhanced version of bleu metric without using reference translations to evaluate the translation quality . in task 1.2 , we utilized a probability model nave bayes ( nb ) as a classification algorithm with the features borrowed from the traditional evaluation metrics . in task 2 , to take the contextual information into account , we employed a discriminative undirected probabilistic graphical model conditional random field ( crf ) , in addition to the nb algorithm . the training experiments on the past wmt corpora showed that the designed methods of this paper yielded promising results especially the statistical models of crf and nb . the official results show that our crf model achieved the highest f-score 0.8297 in binary classification of task 2 .

automatic committed belief tagging
we go beyond simple propositional meaning extraction and present experiments in determining which propositions in text the author believes . we show that deep syntactic parsing helps for this task . our best feature combination achieves an fmeasure of 64 % , a relative reduction in fmeasure error of 21 % over not using syntactic features .

a weakly-supervised approach to argumentative zoning of scientific cnrs & ens , france
argumentative zoning ( az ) analysis of the argumentative structure of a scientific paper has proved useful for a number of information access tasks . current approaches to az rely on supervised machine learning ( ml ) . requiring large amounts of annotated data , these approaches are expensive to develop and port to different domains and tasks . a potential solution to this problem is to use weaklysupervised ml instead . we investigate the performance of four weakly-supervised classifiers on scientific abstract data annotated for multiple az classes . our best classifier based on the combination of active learning and selftraining outperforms our best supervised classifier , yielding a high accuracy of 81 % when using just 10 % of the labeled data . this result suggests that weakly-supervised learning could be employed to improve the practical applicability and portability of az across different information access tasks .

bucking the trend : large-scale cost-focused active learning for statistical machine translation human language technology
we explore how to improve machine translation systems by adding more translation data in situations where we already have substantial resources . the main challenge is how to buck the trend of diminishing returns that is commonly encountered . we present an active learning-style data solicitation algorithm to meet this challenge . we test it , gathering annotations via amazon mechanical turk , and find that we get an order of magnitude increase in performance rates of improvement .

infoxtract : a customizable intermediate level information
information extraction ( ie ) systems assist analysts to assimilate information from electronic documents . this paper focuses on ie tasks designed to support information discovery applications . since information discovery implies examining large volumes of documents drawn from various sources for situations that can not be anticipated a priori , they require ie systems to have breadth as well as depth . this implies the need for a domain-independent ie system that can easily be customized for specific domains : end users must be given tools to customize the system on their own . it also implies the need for defining new intermediate level ie tasks that are richer than the subject-verb-object ( svo ) triples produced by shallow systems , yet not as complex as the domain-specific scenarios defined by the message understanding conference ( muc ) . this paper describes a robust , scalable ie engine designed for such purposes . it describes new ie tasks such as entity profiles , and concept-based general events which represent realistic goals in terms of what can be accomplished in the near-term as well as providing useful , actionable information . these new tasks also facilitate the correlation of output from an ie engine with existing structured data . benchmarking results for the core engine and applications utilizing the engine are presented .

towards a data model for the universal corpus
we describe the design of a comparable corpus that spans all of the worlds languages and facilitates large-scale cross-linguistic processing . this universal corpus consists of text collections aligned at the document and sentence level , multilingual wordlists , and a small set of morphological , lexical , and syntactic annotations . the design encompasses submission , storage , and access . submission preserves the integrity of the work , allows asynchronous updates , and facilitates scholarly citation . storage employs a cloud-hosted filestore containing normalized source data together with a database of texts and annotations . access is permitted to the filestore , the database , and an application programming interface . all aspects of the universal corpus are open , and we invite community participation in its design and implementation , and in supplying and using its data .

combining multiple models for speech information retrieval
in this article we present a method for combining different information retrieval models in order to increase the retrieval performance in a speech information retrieval task . the formulas for combining the models are tuned on training data . then the system is evaluated on test data . the task is particularly difficult because the text collection is automatically transcribed spontaneous speech , with many recognition errors . also , the topics are real information needs , difficult to satisfy . information retrieval systems are not able to obtain good results on this data set , except for the case when manual summaries are included .

irasubcat , a highly customizable , language independent tool for the acquisition of verbal subcategorization information from corpus
irasubcat is a language-independent tool to acquire information about the subcategorization of verbs from corpus . the tool can extract information from corpora annotated at various levels , including almost raw text , where only verbs are identified . it can also aggregate information from a pre-existing lexicon with verbal subcategorization information . the system is highly customizable , and works with xml as input and output format . irasubcat identifies patterns of constituents in the corpus , and associates patterns with verbs if their association strength is over a frequency threshold and passes the likelihood ratio hypothesis test . it also implements a procedure to identify verbal constituents that could be playing the role of an adjunct in a pattern . thresholds controlling frequency and identification of adjuncts can be customized by the user , or else they are given a default value .

when frequency data meet dispersion data in the extraction of multi-word units from a corpus : a study of trigrams in chinese
one of the main approaches to extract multi-word units is the frequency threshold approach , but the way this approach considers dispersion data still leaves a lot to be desired . this study adopts griess ( 2008 ) dispersion measure to extract trigrams from a chinese corpus , and the results are compared with those of the frequency threshold approach . it is found that the overlap between the two approaches is not very large . this demonstrates the necessity of taking dispersion data more seriously and the dynamic nature of lexical representations . moreover , the trigrams extracted in the present study can be used in a wide range of language resources in chinese .

training mrf-based phrase translation models using gradient ascent
this paper presents a general , statistical framework for modeling phrase translation via markov random fields . the model allows for arbituary features extracted from a phrase pair to be incorporated as evidence . the parameters of the model are estimated using a large-scale discriminative training approach that is based on stochastic gradient ascent and an n-best list based expected bleu as the objective function . the model is easy to be incoporated into a standard phrase-based statistical machine translation system , requiring no code change in the runtime engine . evaluation is performed on two europarl translation tasks , germanenglish and french-english . results show that incoporating the markov random field model significantly improves the performance of a state-of-the-art phrase-based machine translation system , leading to a gain of 0.8-1.3 bleu points .

an experiment on automatic detection of named entities in bangla bidyut baran chaudhuri head- cvpr unit
several preprocessing steps are necessary in various problems of automatic natural language processing . one major step is named-entity detection , which is relatively simple in english , because such entities start with an uppercase character . for indian scripts like bangla , no such indicator exists and the problem of identification is more complex , especially for human names , which may be common nouns and adjectives as well . in this paper we have proposed a three-stage approach of namedentity detection . the stages are based on the use of named-entity ( ne ) dictionary , rules for named-entity and left-right cooccurrence statistics . experimental results obtained on anandabazar patrika ( most popular bangla newspaper ) corpus are quite encouraging .

acquistion of the morphological structure of the lexicon based on lexical similarity and formal analogy
the paper presents a computational model aiming at making the morphological structure of the lexicon emerge from the formal and semantic regularities of the words it contains . the model is purely lexemebased . the proposed morphological structure consists of ( 1 ) binary relations that connect each headword with words that are morphologically related , and especially with the members of its morphological family and its derivational series , and of ( 2 ) the analogies that hold between the words . the model has been tested on the lexicon of french using the tlfi machine readable dictionary .

the effect of rhythm on structural disambiguation in chinese
the length of a constituent ( number of syllables in a word or number of words in a phrase ) , or rhythm , plays an important role in chinese syntax . this paper systematically surveys the distribution of rhythm in constructions in chinese from the statistical data acquired from a shallow tree bank . based on our survey , we then used the rhythm feature in a practical shallow parsing task by using rhythm as a statistical feature to augment a pcfg model . our results show that using the probabilistic rhythm feature significantly improves the performance of our shallow parser .

the role of lexico-semantic feedback in open-domain textual
this paper presents an open-domain textual question-answering system that uses several feedback loops to enhance its performance . these feedback loops combine in a new way statistical results with syntactic , semantic or pragmatic information derived from texts and lexical databases . the paper presents the contribution of each feedback loop to the overall performance of 76 % human-assessed precise answers .

integrating sentence- and word-level error identification for disfluency correction
while speaking spontaneously , speakers often make errors such as self-correction or false starts which interfere with the successful application of natural language processing techniques like summarization and machine translation to this data . there is active work on reconstructing this errorful data into a clean and fluent transcript by identifying and removing these simple errors . previous research has approximated the potential benefit of conducting word-level reconstruction of simple errors only on those sentences known to have errors . in this work , we explore new approaches for automatically identifying speaker construction errors on the utterance level , and quantify the impact that this initial step has on word- and sentence-level reconstruction accuracy .

comparison of different algebras for inducing the temporal structure of texts
this paper investigates the impact of using different temporal algebras for learning temporal relations between events . specifically , we compare three intervalbased algebras : allen ( 1983 ) algebra , bruce ( 1972 ) algebra , and the algebra derived from the tempeval-07 campaign . these algebras encode different granularities of relations and have different inferential properties . they in turn behave differently when used to enforce global consistency constraints on the building of a temporal representation . through various experiments on the timebank/aquaint corpus , we show that although the tempeval relation set leads to the best classification accuracy performance , it is too vague to be used for enforcing consistency . by contrast , the other two relation sets are similarly harder to learn , but more useful when global consistency is important . overall , the bruce algebra is shown to give the best compromise between learnability and expressive power .

two knives cut better than one : chinese word segmentation with dual decomposition
there are two dominant approaches to chinese word segmentation : word-based and character-based models , each with respective strengths . prior work has shown that gains in segmentation performance can be achieved from combining these two types of models ; however , past efforts have not provided a practical technique to allow mainstream adoption . we propose a method that effectively combines the strength of both segmentation schemes using an efficient dual-decomposition algorithm for joint inference . our method is simple and easy to implement . experiments on sighan 2003 and 2005 evaluation datasets show that our method achieves the best reported results to date on 6 out of 7 datasets .

incremental syntactic language models for phrase-based translation
this paper describes a novel technique for incorporating syntactic knowledge into phrasebased machine translation through incremental syntactic parsing . bottom-up and topdown parsers typically require a completed string as input . this requirement makes it difficult to incorporate them into phrase-based translation , which generates partial hypothesized translations from left-to-right . incremental syntactic language models score sentences in a similar left-to-right fashion , and are therefore a good mechanism for incorporating syntax into phrase-based translation . we give a formal definition of one such lineartime syntactic language model , detail its relation to phrase-based decoding , and integrate the model with the moses phrase-based translation system . we present empirical results on a constrained urdu-english translation task that demonstrate a significant bleu score improvement and a large decrease in perplexity .

gene name extraction using flybase resources the mitre corporation
machine-learning based entity extraction requires a large corpus of annotated training to achieve acceptable results . however , the cost of expert annotation of relevant data , coupled with issues of inter-annotator variability , makes it expensive and time-consuming to create the necessary corpora . we report here on a simple method for the automatic creation of large quantities of imperfect training data for a biological entity ( gene or protein ) extraction system . we used resources available in the flybase model organism database ; these resources include a curated lists of genes and the articles from which the entries were drawn , together a synonym lexicon . we applied simple pattern matching to identify gene names in the associated abstracts and filtered these entities using the list of curated entries for the article . this process created a data set that could be used to train a simple hidden markov model ( hmm ) entity tagger . the results from the hmm tagger were comparable to those reported by other groups ( f-measure of 0.75 ) . this method has the advantage of being rapidly transferable to new domains that have similar existing resources .

who evoked that frame some thoughts on context effects and event types
lexical substitution is an annotation task in which annotators provide one-word paraphrases ( lexical substitutes ) for individual target words in a sentence context . lexical substitution yields a fine-grained characterization of word meaning that can be done by non-expert annotators . we discuss results of a recent lexical substitution annotation effort , where we found strong contextual modulation effects : many substitutes were not synonyms , hyponyms or hypernyms of the targets , but were highly specific to the situation at hand . this data provides some food for thought for framesemantic analysis .

a fully-lexicalized probabilistic model for japanese syntactic and case structure analysis
we present an integrated probabilistic model for japanese syntactic and case structure analysis . syntactic and case structure are simultaneously analyzed based on wide-coverage case frames that are constructed from a huge raw corpus in an unsupervised manner . this model selects the syntactic and case structure that has the highest generative probability . we evaluate both syntactic structure and case structure . in particular , the experimental results for syntactic analysis on web sentences show that the proposed model significantly outperforms known syntactic analyzers .

thai grapheme-based speech recognition
in this paper we present the results for building a grapheme-based speech recognition system for thai . we experiment with different settings for the initial context independent system , different number of acoustic models and different contexts for the speech unit . in addition , we investigate the potential of an enhanced tree clustering method as a way of sharing parameters across models . we compare our system with two phoneme-based systems ; one that uses a hand-crafted dictionary and another that uses an automatically generated dictionary . experiment results show that the grapheme-based system with enhanced tree clustering outperforms the phoneme-based system using an automatically generated dictionary , and has comparable results to the phoneme-based system with the handcrafted dictionary .

route communication in dialogue : a matter of principles
the present study uses the dialogue paradigm to explore route communication . it revolves around the analysis of a corpus of route instructions produced in real-time interaction with the follower . it explores the variation in forming route instructions and the factors that contribute in it . the results show that visual co-presence influences the performance , conversation patterns and configuration of instructions . most importantly , the results suggest an analogy between the choices of instructiongivers and the communicative actions of their partners .

an example-based decoder for spoken language machine translation
in this paper , we propose an example-based decoder for a statistical machine translation ( smt ) system , which is used for spoken language machine translation . in this way , it will help to solve the re-ordering problem and other problems for spoken language mt , such as lots of omissions , idioms etc . through experiments , we show that this approach obtains improvements over the baseline on a chinese-english spoken language translation task .

discovering semantic classes for urdu n-v complex predicates
this paper reports on an exploratory investigation as to whether classes of urdu n-v complex predicates can be identified on the basis syntactic patterns and lexical choices associated with the n-v complex predicates . working with data from a pos annotated corpus , we show that choices with respect to the number of arguments , case marking on subjects and which light verbs are felicitous with which nouns depend heavily on the semantics of the noun in the n-v complex predicate . this initial work represents an important step towards identifying semantic criteria relevant for complex predicate formation . identifying the semantic criteria and being able to systematically code them in turn represents a first step towards building up a lexical resource for nouns as part of developing natural language processing tools for the underresourced south asian language urdu .

a flexible stand-off data model with query language for multi-level annotation
we present an implemented xml data model and a new , simplified query language for multi-level annotated corpora . the new query language involves automatic conversion of queries into the underlying , more complicated mmaxql query language . it supports queries for sequential and hierarchical , but also associative ( e.g . coreferential ) relations . the simplified query language has been designed with non-expert users in mind .

latent semantic analysis for dialogue act classification barbara di eugenio
this paper presents our experiments in applying latent semantic analysis ( lsa ) to dialogue act classification . we employ both lsa proper and lsa augmented in two ways . we report results on diag , our own corpus of tutoring dialogues , and on the callhome spanish corpus . our work has the theoretical goal of assessing whether lsa , an approach based only on raw text , can be improved by using additional features of the text .

a proposal for a configurable silver standard
among the many proposals to promote alternatives to costly to create gold standards , just recently the idea of a fully automatically , and thus cheaply , to set up silver standard has been launched . however , the current construction policy for such a silver standard requires crucial parameters ( such as similarity thresholds and agreement cut-offs ) to be set a priori , based on extensive testing though , at corpus compile time . accordingly , such a corpus is static , once it is released . we here propose an alternative policy where silver standards can be dynamically optimized and customized on demand ( given a specific goal function ) using a gold standard as an oracle .

dependency-based pre-ordering for chinese-english machine translation
in statistical machine translation ( smt ) , syntax-based pre-ordering of the source language is an effective method for dealing with language pairs where there are great differences in their respective word orders . this paper introduces a novel pre-ordering approach based on dependency parsing for chinese-english smt . we present a set of dependency-based preordering rules which improved the bleu score by 1.61 on the nist 2006 evaluation data . we also investigate the accuracy of the rule set by conducting human evaluations .

extracting pronunciation-translated names from chinese texts using bootstrapping approach
pronunciation-translated names ( p-names ) bring more ambiguities to chinese word segmentation and generic named entity recognition . as there are few annotated resources that can be used to develop a good p-name extraction system , this paper presents a bootstrapping algorithm , called pn-finder , to tackle this problem . starting from a small set of p-name characters and context cue-words , the algorithm iteratively locates more p-names from the internet . the algorithm uses a combination of p-name and context word probabilities to identify new p-names . experiments show that our pn-finder is able to locate a large number of p-names ( over 100,000 ) from the internet with a high recognition accuracy of over 85 % . further tests on the met-2 test set show that our pn-finder can achieve a performance of over 90 % in f1 value in locating p-names . the results demonstrate that our pn-finder is effective .

an unsupervised model for instance level subcategorization acquisition
most existing systems for subcategorization frame ( scf ) acquisition rely on supervised parsing and infer scf distributions at type , rather than instance level . these systems suffer from poor portability across domains and their benefit for nlp tasks that involve sentence-level processing is limited . we propose a new unsupervised , markov random field-based model for scf acquisition which is designed to address these problems . the system relies on supervised pos tagging rather than parsing , and is capable of learning scfs at instance level . we perform evaluation against gold standard data which shows that our system outperforms several supervised and type-level scf baselines . we also conduct task-based evaluation in the context of verb similarity prediction , demonstrating that a vector space model based on our scfs substantially outperforms a lexical model and a model based on a supervised parser .

softcardinality : hierarchical text overlap for student response analysis
in this paper we describe our system used to participate in the student-response-analysis task-7 at semeval 2013. this system is based on text overlap through the soft cardinality and a new mechanism for weight propagation . although there are several official performance measures , taking into account the overall accuracy throughout the two availabe data sets ( beetle and scientsbank ) , our system ranked first in the 2 way classification task and second in the others . furthermore , our system performs particularly well with unseendomains instances , which was the more challenging test set . this paper also describes another system that integrates this method with the lexical-overlap baseline provided by the task organizers obtaining better results than the best official results . we concluded that the soft cardinality method is a very competitive baseline for the automatic evaluation of student responses .

collocations and the structure of proper names
this paper establishes a connection between two apparently very different kinds of probabilistic models . latent dirichlet allocation ( lda ) models are used as topic models to produce a lowdimensional representation of documents , while probabilistic context-free grammars ( pcfgs ) define distributions over trees . the paper begins by showing that lda topic models can be viewed as a special kind of pcfg , so bayesian inference for pcfgs can be used to infer topic models as well . adaptor grammars ( ags ) are a hierarchical , non-parameteric bayesian extension of pcfgs . exploiting the close relationship between lda and pcfgs just described , we propose two novel probabilistic models that combine insights from lda and ag models . the first replaces the unigram component of lda topic models with multi-word sequences or collocations generated by an ag . the second extension builds on the first one to learn aspects of the internal structure of proper names .

putop : turning predominant senses into a topic model for word sense
we extend on mccarthy et als predominant sense method to create an unsupervised method of word sense disambiguation that uses automatically derived topics using latent dirichlet alocation . using topicspecific synset similarity measures , we create predictions for each word in each document using only word frequency information . it is hoped that this procedure can improve upon the method for larger numbers of topics by providing more relevant training corpora for the individual topics . this method is evaluated on semeval-2007 task 1 and task 17 .

the swedish model of public outreach of linguistics to young scientists stockholm young scientists stockholm
what is it that we want to achieve with our linguistic olympiads and how do the contests vary in different countries the swedish olympiad has been running for 7 years now and is primarily focused on public outreach - spreading linguistics to secondary school students . the contest involves not only a test but also lectures , school visits and teaching material . the effort is put on promoting the interest of linguistics to students through fun material and good contact with teachers of languages . this presentation contains an overview of the swedish version of olympiads in linguistics as well as some concrete examples of workshop material on linguistic problems for secondary school students .

building effective question answering characters
in this paper , we describe methods for building and evaluation of limited domain question-answering characters . several classification techniques are tested , including text classification using support vector machines , language-model based retrieval , and cross-language information retrieval techniques , with the latter having the highest success rate . we also evaluated the effect of speech recognition errors on performance with users , finding that retrieval is robust until recognition reaches over 50 % wer .

recovering non-local dependencies for chinese josef van genabith
to date , work on non-local dependencies ( nlds ) has focused almost exclusively on english and it is an open research question how well these approaches migrate to other languages . this paper surveys non-local dependency constructions in chinese as represented in the penn chinese treebank ( ctb ) and provides an approach for generating proper predicate-argument-modifier structures including nlds from surface contextfree phrase structure trees . our approach recovers non-local dependencies at the level of lexical-functional grammar f-structures , using automatically acquired subcategorisation frames and f-structure paths linking antecedents and traces in nlds . currently our algorithm achieves 92.2 % f-score for trace insertion and 84.3 % for antecedent recovery evaluating on gold-standard ctb trees , and 64.7 % and 54.7 % , respectively , on ctbtrained state-of-the-art parser output trees .

fiffflffi ! ! `` ff # % $ %
how can proteins fold so quickly into their unique native structures we show here that there is a natural analogy between parsing and the protein folding problem , and demonstrate that cky can find the native structures of a simplified lattice model of proteins with high accuracy .

putting the user in the loop : interactive maximal marginal relevance for
this work represents an initial attempt to move beyond single-shot summarization to interactive summarization . we present an extension to the classic maximal marginal relevance ( mmr ) algorithm that places a user in the loop to assist in candidate selection . experiments in the complex interactive question answering ( ciqa ) task at trec 2007 show that interactively-constructed responses are significantly higher in quality than automatically-generated ones . this novel algorithm provides a starting point for future work on interactive summarization .

opinion mining and topic categorization with novel term weighting
in this paper we investigate the efficiency of the novel term weighting algorithm for opinion mining and topic categorization of articles from newspapers and internet . we compare the novel term weighting technique with existing approaches such as tf-idf and confweight . the performance on the data from the text-mining campaigns deft07 and deft08 shows that the proposed method can compete with existing information retrieval models in classification quality and that it is computationally faster . the proposed text preprocessing method can be applied in large-scale information retrieval and data mining problems and it can be easily transported to different domains and different languages since it does not require any domain-related or linguistic information .

a multimodal home entertainment interface via a mobile device
we describe a multimodal dialogue system for interacting with a home entertainment center via a mobile device . in our working prototype , users may utilize both a graphical and speech user interface to search tv listings , record and play television programs , and listen to music . the developed framework is quite generic , potentially supporting a wide variety of applications , as we demonstrate by integrating a weather forecast application . in the prototype , the mobile device serves as the locus of interaction , providing both a small touchscreen display , and speech input and output ; while the tv screen features a larger , richer gui . the system architecture is agnostic to the location of the natural language processing components : a consistent user experience is maintained regardless of whether they run on a remote server or on the device itself .

transliteration of proper names in cross-lingual information retrieval
we address the problem of transliterating english names using chinese orthography in support of cross-lingual speech and text processing applications . we demonstrate the application of statistical machine translation techniques to translate the phonemic representation of an english name , obtained by using an automatic text-to-speech system , to a sequence of initials and finals , commonly used subword units of pronunciation for chinese . we then use another statistical translation model to map the initial/final sequence to chinese characters . we also present an evaluation of this module in retrieval of mandarin spoken documents from the tdt corpus using english text queries .

resolving it , this , and that in unrestricted multi-party dialog
we present an implemented system for the resolution of it , this , and that in transcribed multi-party dialog . the system handles np-anaphoric as well as discoursedeictic anaphors , i.e . pronouns with vp antecedents . selectional preferences for np or vp antecedents are determined on the basis of corpus counts . our results show that the system performs significantly better than a recency-based baseline .

the utility of parse-derived features for automatic discourse segmentation
we investigate different feature sets for performing automatic sentence-level discourse segmentation within a general machine learning approach , including features derived from either finite-state or contextfree annotations . we achieve the best reported performance on this task , and demonstrate that our spade-inspired context-free features are critical to achieving this level of accuracy . this counters recent results suggesting that purely finite-state approaches can perform competitively .

performance evaluation and error analysis for multimodal reference resolution in a conversation system
multimodal reference resolution is a process that automatically identifies what users refer to during multimodal human-machine conversation . given the substantial work on multimodal reference resolution ; it is important to evaluate the current state of the art , understand the limitations , and identify directions for future improvement . we conducted a series of user studies to evaluate the capability of reference resolution in a multimodal conversation system . this paper analyzes the main error sources during real-time human-machine interaction and presents key strategies for designing robust multimodal reference resolution algorithms .

the construction of a chinese shallow treebank
this paper presents the construction of a manually annotated chinese shallow treebank , named polyu treebank . different from traditional chinese treebank based on full parsing , the polyu treebank is based on shallow parsing in which only partial syntactical structures are annotated . this treebank can be used to support shallow parser training , testing and other natural language applications . phrase-based grammar , proposed by peking university , is used to guide the design and implementation of the polyu treebank . the design principles include good resource sharing , low structural complexity , sufficient syntactic information and large data scale . the design issues , including corpus material preparation , standard for word segmentation and pos tagging , and the guideline for phrase bracketing and annotation , are presented in this paper . well-designed workflow and effective semiautomatic and automatic annotation checking are used to ensure annotation accuracy and consistency . currently , the polyu treebank has completed the annotation of a 1-million-word corpus . the evaluation shows that the accuracy of annotation is higher than 98 % .

a hybrid approach to biomedical named entity recognition and richard tzong-han tsai
in this paper , we describe our hybrid approach to two key nlp technologies : biomedical named entity recognition ( bio-ner ) and ( bio-srl ) . in bio-ner , our system successfully integrates linguistic features into the crf framework . in addition , we employ web lexicons and template-based post-processing to further boost its performance . through these broad linguistic features and the nature of crf , our system outperforms state-ofthe-art machine-learning-based systems , especially in the recognition of protein names ( f=78.5 % ) . in bio-srl , first , we construct a proposition bank on top of the popular biomedical genia treebank following the propbank annotation scheme . we only annotate the predicate-argument structures ( pass ) of thirty frequently used biomedical verbs ( predicates ) and their corresponding arguments . second , we use our proposition bank to train a biomedical srl system , which uses a maximum entropy ( me ) machinelearning model . thirdly , we automatically generate argument-type templates , which can be used to improve classification of biomedical argument roles . our experimental results show that a newswire english srl system that achieves an f-score of 86.29 % in the newswire english domain can maintain an f-score of 64.64 % when ported to the biomedical domain . by using our annotated biomedical corpus , we can increase that f-score by 22.9 % .

duluth : word sense induction applied to web page clustering
the duluth systems that participated in task 11 of semeval2013 carried out word sense induction ( wsi ) in order to cluster web search results . they relied on an approach that represented web snippets using secondorder co occurrences . these systems were all implemented using senseclusters , a freely available open source software package .

statistically-driven alignment-based multiword expression identification for technical domains
multiword expressions ( mwes ) are one of the stumbling blocks for more precise natural language processing ( nlp ) systems . particularly , the lack of coverage of mwes in resources can impact negatively on the performance of tasks and applications , and can lead to loss of information or communication errors . this is especially problematic in technical domains , where a significant portion of the vocabulary is composed of mwes . this paper investigates the use of a statisticallydriven alignment-based approach to the identification of mwes in technical corpora . we look at the use of several sources of data , including parallel corpora , using english and portuguese data from a corpus of pediatrics , and examining how a second language can provide relevant cues for this tasks . we report results obtained by a combination of statistical measures and linguistic information , and compare these to the reported in the literature . such an approach to the ( semi- ) automatic identification of mwes can considerably speed up lexicographic work , providing a more targeted list of mwe candidates .

linear complexity context-free parsing pipelines via chart constraints division of biomedical computer science
in this paper , we extend methods from roark and hollingshead ( 2008 ) for reducing the worst-case complexity of a context-free parsing pipeline via hard constraints derived from finite-state tagging pre-processing . methods from our previous paper achieved quadratic worst-case complexity . we prove here that alternate methods for choosing constraints can achieve either linear oro ( n log2n ) complexity . these worst-case bounds on processing are demonstrated to be achieved without reducing the parsing accuracy , in fact in some cases improving the accuracy . the new methods achieve observed performance comparable to the previously published quadratic complexity method . finally , we demonstrate improved performance by combining complexity bounding methods with additional high precision constraints .

mctest : a challenge dataset for the open-domain machine comprehension of text
we present mctest , a freely available set of stories and associated questions intended for research on the machine comprehension of text . previous work on machine comprehension ( e.g. , semantic modeling ) has made great strides , but primarily focuses either on limited-domain datasets , or on solving a more restricted goal ( e.g. , open-domain relation extraction ) . in contrast , mctest requires machines to answer multiple-choice reading comprehension questions about fictional stories , directly tackling the high-level goal of open-domain machine comprehension . reading comprehension can test advanced abilities such as causal reasoning and understanding the world , yet , by being multiple-choice , still provide a clear metric . by being fictional , the answer typically can be found only in the story itself . the stories and questions are also carefully limited to those a young child would understand , reducing the world knowledge that is required for the task . we present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions . by screening workers ( with grammar tests ) and stories ( with grading ) , we have ensured that the data is the same quality as another set that we manually edited , but at one tenth the editing cost . by being open-domain , yet carefully restricted , we hope mctest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text .

trap hunting : finding personal data management issues in next generation aac devices
advances in natural language generation and speech processing techniques , combined with changes in the commercial landscape , have brought within reach dramatic improvements in augmentative alternative communication ( aac ) . these improvements , though overwhelmingly positive , amplify a family of personal data use problems . this paper argues that the aac design and implementation process needs to identify and address personal data use problems . accordingly , this paper explores personal data management problems and proposes responses . this paper is situated in the context of aac technology but the responses could be generalised for other communities affected by low digital literacy , low literacy levels and cognitive challenges .

generalizing local and non-local word-reordering patterns for syntax-based machine translation
syntactic word reordering is essential for translations across different grammar structures between syntactically distant languagepairs . in this paper , we propose to embed local and non-local word reordering decisions in a synchronous context free grammar , and leverages the grammar in a chartbased decoder . local word-reordering is effectively encoded in hiero-like rules ; whereas non-local word-reordering , which allows for long-range movements of syntactic chunks , is represented in tree-based reordering rules , which contain variables correspond to sourceside syntactic constituents . we demonstrate how these rules are learned from parallel corpora . our proposed shallow tree-to-string rules show significant improvements in translation quality across different test sets .

question classification using head words and their hypernyms
question classification plays an important role in question answering . features are the key to obtain an accurate question classifier . in contrast to li and roth ( 2002 ) s approach which makes use of very rich feature space , we propose a compact yet effective feature set . in particular , we propose head word feature and present two approaches to augment semantic features of such head words using wordnet . in addition , lesks word sense disambiguation ( wsd ) algorithm is adapted and the depth of hypernym feature is optimized . with further augment of other standard features such as unigrams , our linear svm and maximum entropy ( me ) models reach the accuracy of 89.2 % and 89.0 % respectively over a standard benchmark dataset , which outperform the best previously reported accuracy of 86.2 % .

resolving object and attribute coreference in opinion mining
coreference resolution is a classic nlp problem and has been studied extensively by many researchers . most existing studies , however , are generic in the sense that they are not focused on any specific text . in the past few years , opinion mining became a popular topic of research because of a wide range of applications . however , limited work has been done on coreference resolution in opinionated text . in this paper , we deal with object and attribute coreference resolution . such coreference resolutions are important because without solving it a great deal of opinion information will be lost , and opinions may be assigned to wrong entities . we show that some important features related to opinions can be exploited to perform the task more accurately . experimental results using blog posts demonstrate the effectiveness of the technique .

unsupervised relation extraction from web documents
the idex system is a prototype of an interactive dynamic information extraction ( ie ) system . a user of the system expresses an information request in the form of a topic description , which is used for an initial search in order to retrieve a relevant set of documents . on basis of this set of documents , unsupervised relation extraction and clustering is done by the system . the results of these operations can then be interactively inspected by the user . in this paper we describe the relation extraction and clustering components of the idex system . preliminary evaluation results of these components are presented and an overview is given of possible enhancements to improve the relation extraction and clustering components .

generating natural language descriptions of ontology concepts
this paper gives an overview of ongoing work on a system for the generation of nl descriptions of classes defined in owl ontologies . we present a general structuring approach for such descriptions . since owl ontologies do not by default contain the information necessary for lexicalization , lexical information has to be added to the data via annotations . a rulebased mechanism for automatically deriving these annotations is presented .

selection of effective contextual information for automatic synonym acquisition
various methods have been proposed for automatic synonym acquisition , as synonyms are one of the most fundamental lexical knowledge . whereas many methods are based on contextual clues of words , little attention has been paid to what kind of categories of contextual information are useful for the purpose . this study has experimentally investigated the impact of contextual information selection , by extracting three kinds of word relationships from corpora : dependency , sentence co-occurrence , and proximity . the evaluation result shows that while dependency and proximity perform relatively well by themselves , combination of two or more kinds of contextual information gives more stable performance . weve further investigated useful selection of dependency relations and modification categories , and it is found that modification has the greatest contribution , even greater than the widely adopted subjectobject combination .

a mapping-based approach for general formal human computer interaction using natural language
we consider the problem of mapping natural language written utterances expressing operational instructions1 to formal language expressions , applied to french and the r programming language . developing a learning operational assistant requires the means to train and evaluate it , that is , a baseline system able to interact with the user . after presenting the guidelines of our work , we propose a model to represent the problem and discuss the fit of direct mapping methods to our task . finally , we show that , while not resulting in excellent scores , a simple approach seems to be sufficient to provide a baseline for an interactive learning system .

letter n-gram-based input encoding for continuous space language
we present a letter-based encoding for words in continuous space language models . we represent the words completely by letter n-grams instead of using the word index . this way , similar words will automatically have a similar representation . with this we hope to better generalize to unknown or rare words and to also capture morphological information . we show their influence in the task of machine translation using continuous space language models based on restricted boltzmann machines . we evaluate the translation quality as well as the training time on a german-to-english translation task of ted and university lectures as well as on the news translation task translating from english to german . using our new approach a gain in bleu score by up to 0.4 points can be achieved .

memory-based grammatical error correction
we describe the tilb team entry for the conll-2013 shared task . our system consists of five memory-based classifiers that generate correction suggestions for center positions in small text windows of two words to the left and to the right . trained on the google web 1t corpus , the first two classifiers determine the presence of a determiner or a preposition between all words in a text . the second pair of classifiers determine which is the most likely correction of an occurring determiner or preposition . the fifth classifier is a general word predictor which is used to suggest noun and verb form corrections . we report on the scores attained and errors corrected and missed . we point out a number of obvious improvements to boost the scores obtained by the system .

word fragment identification using acoustic-prosodic features in
word fragments pose serious problems for speech recognizers . accurate identification of word fragments will not only improve recognition accuracy , but also be very helpful for disfluency detection algorithm because the occurrence of word fragments is a good indicator of speech disfluencies . different from the previous effort of including word fragments in the acoustic model , in this paper , we investigate the problem of word fragment identification from another approach , i.e . building classifiers using acoustic-prosodic features . our experiments show that , by combining a few voice quality measures and prosodic features extracted from the forced alignments with the human transcriptions , we obtain a precision rate of 74.3 % and a recall rate of 70.1 % on the downsampled data of spontaneous speech . the overall accuracy is 72.9 % , which is significantly better than chance performance of 50 % .

mining spoken dialogue corpora for system evaluation and
we are interested in the problem of modeling and evaluating spoken language systems in the context of human-machine dialogs . spoken dialog corpora allow for a multidimensional analysis of speech recognition and language understanding models of dialog systems . therefore language models can be directly trained based either on the dialog history or its equivalence class ( or cluster ) . in this paper we propose an algorithm to mine dialog traces which exhibit similar patterns and are identified by the same class . for this purpose we apply data clustering methods to large human-machine spoken dialogue corpora . the resulting clusters can be used for system evaluation and language modeling . by clustering dialog traces we expect to learn about the behavior of the system with regards to not only the automation rate but the nature of the interaction ( e.g . easy vs difficult dialogs ) . the equivalence classes can also be used in order to automatically adapt the language model , the understanding module and the dialogue strategy to better fit the kind of interaction detected . this paper investigates different ways for encoding dialogues into multidimensional structures and different clustering methods .

how can you say such things ! : recognizing disagreement in informal political argument
the recent proliferation of political and social forums has given rise to a wealth of freely accessible naturalistic arguments . people can talk to anyone they want , at any time , in any location , about any topic . here we use a mechanical turk annotated corpus of forum discussions as a gold standard for the recognition of disagreement in online ideological forums . we analyze the utility of meta-post features , contextual features , dependency features and word-based features for signaling the disagreement relation . we show that using contextual and dialogic features we can achieve accuracies up to 68 % as compared to a unigram baseline of 63 % .

considerations on automatic mapping large-scale heterogeneous language resources : sejong semantic classes and korlex woo chul park and
this paper presents an automatic mapping method among large-scale heterogeneous language resources : sejong semantic classes ( sjsc ) and korlex . korlex is a large-scale korean wordnet , but it lacks specific syntactic & semantic information . sejong electronic dictionary ( sjd ) , of which semantic segmentation depends on sjsc , has much lower lexical coverage than korlex , but shows refined syntactic & semantic information . the goal of this study is to build a rich language resource for improving korean semantico-syntactic parsing technology . therefore , we consider integration of them and propose automatic mapping method with three approaches : 1 ) information of monosemy/polysemy of word senses ( impw ) , 2 ) instances between nouns of sjd and word senses of korlex ( inw ) , and 3 ) semantically related words between nouns of sjd and synsets of korlex ( srns ) . we obtain good performance using combined three approaches : recall 0.837 , precision 0.717 , and f1 0.773 .

which system differences matter
we investigate how to jointly explain the performance and behavioral differences of two spoken dialogue systems . the join evaluation and differences identification ( jedi ) , finds differences between systems relevant to performance by formulating the problem as a multi-task feature selection question . jedi provides evidence on the usefulness of a recent method , `1/`p-regularized regression ( obozinski et al , 2007 ) . we evaluate against manually annotated success criteria from real users interacting with five different spoken user interfaces that give bus schedule information .

individuality and alignment in generated dialogues
it would be useful to enable dialogue agents to project , through linguistic means , their individuality or personality . equally , each member of a pair of agents ought to adjust its language ( to a greater or lesser extent ) to match that of its interlocutor . we describe crag , which generates dialogues between pairs of agents , who are linguistically distinguishable , but able to align . crag-2 makes use of openccg and an over-generation and ranking approach , guided by a set of language models covering both personality and alignment . we illustrate with examples of output , and briefly note results from user studies with the earlier crag-1 , indicating how crag-2 will be further evaluated . related work is discussed , along with current limitations and future directions .

a situated context model for resolution and generation of referring expressions
the background for this paper is the aim to build robotic assistants that can naturally interact with humans . one prerequisite for this is that the robot can correctly identify objects or places a user refers to , and produce comprehensible references itself . as robots typically act in environments that are larger than what is immediately perceivable , the problem arises how to identify the appropriate context , against which to resolve or produce a referring expression ( re ) . existing algorithms for generating res generally bypass this problem by assuming a given context . in this paper , we explicitly address this problem , proposing a method for context determination in large-scale space . we show how it can be applied both for resolving and producing res .

efficient parsing for head-split dependency trees
head splitting techniques have been successfully exploited to improve the asymptotic runtime of parsing algorithms for projective dependency trees , under the arc-factored model . in this article we extend these techniques to a class of non-projective dependency trees , called well-nested dependency trees with block-degree at most 2 , which has been previously investigated in the literature . we define a structural property that allows head splitting for these trees , and present two algorithms that improve over the runtime of existing algorithms at no significant loss in coverage .

a cohesion graph based approach for unsupervised recognition of literal and non-literal use of multiword expressions
we present a graph-based model for representing the lexical cohesion of a discourse . in the graph structure , vertices correspond to the content words of a text and edges connecting pairs of words encode how closely the words are related semantically . we show that such a structure can be used to distinguish literal and non-literal usages of multi-word expressions .

hybrid methods for pos guessing of chinese unknown words
this paper describes a hybrid model that combines a rule-based model with two statistical models for the task of pos guessing of chinese unknown words . the rule-based model is sensitive to the type , length , and internal structure of unknown words , and the two statistical models utilize contextual information and the likelihood for a character to appear in a particular position of words of a particular length and pos category . by combining models that use different sources of information , the hybrid model achieves a precision of 89 % , a significant improvement over the best result reported in previous studies , which was 69 % .

question classification for email
question classifiers are used within question answering to predict the expected answer type for a given question . this paper describes the first steps towards applying a similar methodology to identifying question classes in dialogue contexts , beginning with a study of questions drawn from the enron email corpus . human-annotated data is used as a gold standard for assessing the output from an existing , open-source question classifier ( qa-sys ) . problem areas are identified and potential solutions discussed .

two-stage stochastic email synthesizer
this paper presents the design and implementation details of an email synthesizer using two-stage stochastic natural language generation , where the first stage structures the emails according to sender style and topic structure , and the second stage synthesizes text content based on the particulars of an email structure element and the goals of a given communication for surface realization . the synthesized emails reflect sender style and the intent of communication , which can be further used as synthetic evidence for developing other applications .

sentiment classification using automatically extracted subgraph features
in this work , we propose a novel representation of text based on patterns derived from linguistic annotation graphs . we use a subgraph mining algorithm to automatically derive features as frequent subgraphs from the annotation graph . this process generates a very large number of features , many of which are highly correlated . we propose a genetic programming based approach to feature construction which creates a fixed number of strong classification predictors from these subgraphs . we evaluate the benefit gained from evolved structured features , when used in addition to the bag-of-words features , for a sentiment classification task .

automatic annotation of the penn - treebank with lfg f - structure information
we describe a new interactive annotation scheme between a human annotator who carries out simplified annotations on cfg trees , and a statistical parser that converts the human annotations automatically into a richly annotated hpsg treebank . in order to check the proposed schemes effectiveness , we performed automatic pseudo-annotations that emulate the systems idealized behavior and measured the performance of the parser trained on those annotations . in addition , we implemented a prototype system and conducted manual annotation experiments on a small test set .

cross-narrative temporal ordering of medical events
cross-narrative temporal ordering of medical events is essential to the task of generating a comprehensive timeline over a patients history . we address the problem of aligning multiple medical event sequences , corresponding to different clinical narratives , comparing the following approaches : ( 1 ) a novel weighted finite state transducer representation of medical event sequences that enables composition and search for decoding , and ( 2 ) dynamic programming with iterative pairwise alignment of multiple sequences using global and local alignment algorithms . the cross-narrative coreference and temporal relation weights used in both these approaches are learned from a corpus of clinical narratives . we present results using both approaches and observe that the finite state transducer approach performs performs significantly better than the dynamic programming one by 6.8 % for the problem of multiple-sequence alignment .

an equivalent pseudoword solution to chinese word sense disambiguation
this paper presents a new approach based on equivalent pseudowords ( eps ) to tackle word sense disambiguation ( wsd ) in chinese language . eps are particular artificial ambiguous words , which can be used to realize unsupervised wsd . a bayesian classifier is implemented to test the efficacy of the ep solution on senseval-3 chinese test set . the performance is better than state-of-the-art results with an average f-measure of 0.80. the experiment verifies the value of ep for unsupervised wsd .

relational inference for wikification
wikification , commonly referred to as disambiguation to wikipedia ( d2w ) , is the task of identifying concepts and entities in text and disambiguating them into the most specific corresponding wikipedia pages . previous approaches to d2w focused on the use of local and global statistics over the given text , wikipedia articles and its link structures , to evaluate context compatibility among a list of probable candidates . however , these methods fail ( often , embarrassingly ) , when some level of text understanding is needed to support wikification . in this paper we introduce a novel approach to wikification by incorporating , along with statistical methods , richer relational analysis of the text . we provide an extensible , efficient and modular integer linear programming ( ilp ) formulation of wikification that incorporates the entity-relation inference problem , and show that the ability to identify relations in text helps both candidate generation and ranking wikipedia titles considerably . our results show significant improvements in both wikification and the tac entity linking task .

a novel disambiguation method for unification-based grammars using probabilistic context-free approximations
we present a novel disambiguation method for unification-based grammars ( ubgs ) . in contrast to other methods , our approach obviates the need for probability models on the ubg side in that it shifts the responsibility to simpler context-free models , indirectly obtained from the ubg . our approach has three advantages : ( i ) training can be effectively done in practice , ( ii ) parsing and disambiguation of context-free readings requires only cubic time , and ( iii ) involved probability distributions are mathematically clean . in an experiment for a mid-size ubg , we show that our novel approach is feasible . using unsupervised training , we achieve 88 % accuracy on an exact-match task .

landmark classification for route directions
in order for automated navigation systems to operate effectively , the route instructions they produce must be clear , concise and easily understood by users . in order to incorporate a landmark within a coherent sentence , it is necessary to first understand how that landmark is conceptualised by travellers whether it is perceived as point-like , linelike or area-like . this paper investigates the viability of automatically classifying the conceptualisation of landmarks relative to a given city context . we use web data to learn the default conceptualisation of those landmarks , crucially analysing preposition and verb collocations in the classification .

a systematic exploration of the feature space for relation extraction
relation extraction is the task of finding semantic relations between entities from text . the state-of-the-art methods for relation extraction are mostly based on statistical learning , and thus all have to deal with feature selection , which can significantly affect the classification performance . in this paper , we systematically explore a large space of features for relation extraction and evaluate the effectiveness of different feature subspaces . we present a general definition of feature spaces based on a graphic representation of relation instances , and explore three different representations of relation instances and features of different complexities within this framework . our experiments show that using only basic unit features is generally sufficient to achieve state-of-the-art performance , while overinclusion of complex features may hurt the performance . a combination of features of different levels of complexity and from different sentence representations , coupled with task-oriented feature pruning , gives the best performance .

the benefits of a model of annotation
this paper presents a case study of a difficult and important categorical annotation task ( word sense ) to demonstrate a probabilistic annotation model applied to crowdsourced data . it is argued that standard ( chance-adjusted ) agreement levels are neither necessary nor sufficient to ensure high quality gold standard labels . compared to conventional agreement measures , application of an annotation model to instances with crowdsourced labels yields higher quality labels at lower cost .

exploiting translational correspondences for pattern-independent mwe
based on a study of verb translations in the europarl corpus , we argue that a wide range of mwe patterns can be identified in translations that exhibit a correspondence between a single lexical item in the source language and a group of lexical items in the target language . we show that these correspondences can be reliably detected on dependency-parsed , word-aligned sentences . we propose an extraction method that combines word alignment with syntactic filters and is independent of the structural pattern of the translation .

a structured language model based on context-sensitive probabilistic left corner parsing
recent contributions to statistical language modeling for speech recognition have shown that probabilistically parsing a partial word sequence aids the prediction of the next word , leading to structured language models that have the potential to outperform n-grams . existing approaches to structured language modeling construct nodes in the partial parse tree after all of the underlying words have been predicted . this paper presents a different approach , based on probabilistic left-corner grammar ( plcg ) parsing , that extends a partial parse both from the bottom up and from the top down , leading to a more focused and more accurate , though somewhat less robust , search of the parse space . at the core of our new structured language model is a fast context-sensitive and lexicalized plcg parsing algorithm that uses dynamic programming . preliminary perplexity and word-accuracy results appear to be competitive with previous ones , while speed is increased .

improving arabic-chinese statistical machine translation using english as pivot language
we present a comparison of two approaches for arabic-chinese machine translation using english as a pivot language : sentence pivoting and phrase-table pivoting . our results show that using english as a pivot in either approach outperforms direct translation from arabic to chinese . our best result is the phrase-pivot system which scores higher than direct translation by 1.1 bleu points . an error analysis of our best system shows that we successfully handle many complex arabic-chinese syntactic variations .

combining multiple models for speech information retrieval
in this article we present a method for combining different information retrieval models in order to increase the retrieval performance in a speech information retrieval task . the formulas for combining the models are tuned on training data . then the system is evaluated on test data . the task is particularly difficult because the text collection is automatically transcribed spontaneous speech , with many recognition errors . also , the topics are real information needs , difficult to satisfy . information retrieval systems are not able to obtain good results on this data set , except for the case when manual summaries are included .

fast and robust compressive summarization with dual decomposition and multi-task learning
we present a dual decomposition framework for multi-document summarization , using a model that jointly extracts and compresses sentences . compared with previous work based on integer linear programming , our approach does not require external solvers , is significantly faster , and is modular in the three qualities a summary should have : conciseness , informativeness , and grammaticality . in addition , we propose a multi-task learning framework to take advantage of existing data for extractive summarization and sentence compression . experiments in the tac2008 dataset yield the highest published rouge scores to date , with runtimes that rival those of extractive summarizers .

unsupervised extraction of semantic relations using discourse cues juliette conrath stergos afantenos nicholas asher philippe muller
this paper presents a knowledge base containing triples involving pairs of verbs associated with semantic or discourse relations . the relations in these triples are marked by discourse connectors between two adjacent instances of the verbs in the triple in the large french corpus , frwac . we detail several measures that evaluate the relevance of the triples and the strength of their association . we use manual annotations to evaluate our method , and also study the coverage of our resource with respect to the discourse annotated corpus annodis . our positive results show the potential impact of our resource for discourse analysis tasks as well as other semantically oriented tasks like temporal and causal information extraction .

building an annotated textual inference corpus for motion and space
this paper presents an approach for building a corpus for the domain of motion and spatial inference using a specific class of verbs . the approach creates a distribution of inference features that maximize the discriminatory power of a system trained on the corpus . the paper addresses the issue of using an existing textual inference system for generating the examples . this enables the corpus annotation method to assert whether more data is necessary .

an empirical study of the impact of idioms on phrase based statistical machine translation of english to brazilian-portuguese
this paper describes an experiment to evaluate the impact of idioms on statistical machine translation ( smt ) process using the language pair english/brazilianportuguese . our results show that on sentences containing idioms a standard smt system achieves about half the bleu score of the same system when applied to sentences that do not contain idioms . we also provide a short error analysis and outline our planned work to overcome this limitation .

using clustering to improve retrieval evaluation without
retrieval evaluation without relevance judgments is a hard but also very meaningful work . in this paper , we use clustering technique to improve the performance of judgment free retrieval evaluation . by using one system to represent all the systems that are similar to it , we can largely reduce the negative effect of similar retrieval results in retrieval evaluation . experimental results demonstrated that our method outperformed all the previous judgment free evaluation methods significantly . its overall average performance outperformed the best previous result by 20.5 % . besides , our work is a general framework that can be applied to any other judgment free evaluation method for performance improvement .

financial keyword expansion via continuous word vector representations program in digital content and technology
this paper proposes to apply the continuous vector representations of words for discovering keywords from a financial sentiment lexicon . in order to capture more keywords , we also incorporate syntactic information into the continuous bag-ofwords ( cbow ) model . experimental results on a task of financial risk prediction using the discovered keywords demonstrate that the proposed approach is good at predicting financial risk .

iiith : a corpus-driven co-occurrence based probabilistic model for noun compound paraphrasing
this paper presents a system for automatically generating a set of plausible paraphrases for a given noun compound and rank them in decreasing order of their usage represented by the confidence value provided by the human annotators . our system implements a corpusdriven probabilistic co-occurrence based model for predicting the paraphrases , that uses a seed list of paraphrases extracted from corpus to predict other paraphrases based on their co-occurrences . the corpus study reveals that the prepositional paraphrases for the noun compounds are quite frequent and well covered but the verb paraphrases , on the other hand , are scarce , revealing the unsuitability of the model for standalone corpus-driven approach . therefore , to predict other paraphrases , we adopt a two-fold approach : ( i ) prediction based on verb-verb cooccurrences , in case the seed paraphrases are greater than threshold ; and ( ii ) prediction based on semantic relation of nc , otherwise . the system achieves a comparabale score of 0.23 for the isomorphic system while maintaining a score of 0.26 for the non-isomorphic system .

dialogue based question answering system in telugu
a dialogue based question answering ( qa ) system for railway information in telugu has been described . telugu is an important language in india belonging to the dravidian family . the main component of our qa system is the dialogue manager ( dm ) , to handle the dialogues between user and system . it is necessary in generating dialogue for clarifying partially understood questions , resolving anaphora and co-reference problems . besides , different modules have been developed for processing the query and its translation into formal database query language statement ( s ) . based on the result from the database , a natural language answer is generated . the empirical results obtained on the current system are encouraging . testing with a set of questions in railway domain , the qa system showed 96.34 % of precision and 83.96 % of dialogue success rate . such a question answering system can be effectively utilized when integrated with a speech input and speech output system .

a categorial variation database for english
we describe our approach to the construction and evaluation of a large-scale database called catvar which contains categorial variations of english lexemes . due to the prevalence of cross-language categorial variation in multilingual applications , our categorial-variation resource may serve as an integral part of a diverse range of natural language applications . thus , the research reported herein overlaps heavily with that of the machine-translation , lexicon-construction , and information-retrieval communities . we apply the information-retrieval metrics of precision and recall to evaluate the accuracy and coverage of our database with respect to a human-produced gold standard . this evaluation reveals that the categorial database achieves a high degree of precision and recall . additionally , we demonstrate that the database improves on the linkability of porter stemmer by over 30 % .

semantic relations between nouns paul nulty fintan costello
this paper investigates methods for using lexical patterns in a corpus to deduce the semantic relation that holds between two nouns in a noun-noun compound phrase such as flu virus or morning exercise . much of the previous work in this area has used automated queries to commercial web search engines . in our experiments we use the google web 1t corpus . this corpus contains every 2,3 , 4 and 5 gram occurring more than 40 times in google 's index of the web , but has the advantage of being available to researchers directly rather than through a web interface . this paper evaluates the performance of the web 1t corpus on the task compared to similar systems in the literature , and also investigates what kind of lexical patterns are most informative when trying to identify a semantic relation between two nouns .

investigating the effects of selective sampling on the annotation task
we report on an active learning experiment for named entity recognition in the astronomy domain . active learning has been shown to reduce the amount of labelled data required to train a supervised learner by selectively sampling more informative data points for human annotation . we inspect double annotation data from the same domain and quantify potential problems concerning annotators performance . for data selectively sampled according to different selection metrics , we find lower inter-annotator agreement and higher per token annotation times . however , overall results confirm the utility of active learning .

medslt : a limited-domain unidirectional grammar-based medical beth ann hockey
medslt is a unidirectional medical speech translation system intended for use in doctor-patient diagnosis dialogues , which provides coverage of several different language pairs and subdomains . vocabulary ranges from about 350 to 1000 surface words , depending on the language and subdomain . we will demo both the system itself and the development environment , which uses a combination of rule-based and data-driven methods to construct efficient recognisers , generators and transfer rule sets from small corpora .

